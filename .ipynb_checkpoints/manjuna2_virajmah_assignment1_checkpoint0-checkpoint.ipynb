{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea45b68",
   "metadata": {},
   "source": [
    "## Step 1 - Import Libraries.\n",
    "\n",
    "Import required libraries (not allowed: scikit-learn or any other libraries with inbuilt functions that help to implement ML methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba90a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all required libraries imported \n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f84ea1",
   "metadata": {},
   "source": [
    "## Step 2 - Reading the data and printing the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315438a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the File\n",
    "#pd.set_option('display.max_rows', None)\n",
    "data=pd.read_csv(\"C:/Users/viraj/Code_a/ML_Prof_Alina/Assignment_1/datasets (1)/datasets/penguins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487d224",
   "metadata": {},
   "source": [
    "Read, preprocess, and print the main statistics about the dataset (you can reuse\n",
    "your code from Assignment 0 with a proper citation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66372b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN     NaN  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showcasing the first 5  rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcc4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "year                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the null values\n",
    "data.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf187eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the rows having NAN values\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fe05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 343\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    object \n",
      " 1   island             333 non-null    object \n",
      " 2   bill_length_mm     333 non-null    float64\n",
      " 3   bill_depth_mm      333 non-null    float64\n",
      " 4   flipper_length_mm  333 non-null    float64\n",
      " 5   body_mass_g        333 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      " 7   year               333 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#showcasing the Datatype of elements of the columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d65d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.992793</td>\n",
       "      <td>17.164865</td>\n",
       "      <td>200.966967</td>\n",
       "      <td>4207.057057</td>\n",
       "      <td>2008.042042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.468668</td>\n",
       "      <td>1.969235</td>\n",
       "      <td>14.015765</td>\n",
       "      <td>805.215802</td>\n",
       "      <td>0.812944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.100000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.500000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>3550.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.500000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>4050.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.600000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>4775.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.600000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>6300.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "count      333.000000     333.000000         333.000000   333.000000   \n",
       "mean        43.992793      17.164865         200.966967  4207.057057   \n",
       "std          5.468668       1.969235          14.015765   805.215802   \n",
       "min         32.100000      13.100000         172.000000  2700.000000   \n",
       "25%         39.500000      15.600000         190.000000  3550.000000   \n",
       "50%         44.500000      17.300000         197.000000  4050.000000   \n",
       "75%         48.600000      18.700000         213.000000  4775.000000   \n",
       "max         59.600000      21.500000         231.000000  6300.000000   \n",
       "\n",
       "              year  \n",
       "count   333.000000  \n",
       "mean   2008.042042  \n",
       "std       0.812944  \n",
       "min    2007.000000  \n",
       "25%    2007.000000  \n",
       "50%    2008.000000  \n",
       "75%    2009.000000  \n",
       "max    2009.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The main statistics of the dataSet\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ea2914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count        333\n",
      "unique         3\n",
      "top       Adelie\n",
      "freq         146\n",
      "Name: species, dtype: object count        333\n",
      "unique         3\n",
      "top       Biscoe\n",
      "freq         163\n",
      "Name: island, dtype: object count      333\n",
      "unique       2\n",
      "top       male\n",
      "freq       168\n",
      "Name: sex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "d1=data.species.describe()   \n",
    "d2=data.island.describe() \n",
    "d3=data.sex.describe() \n",
    "print(d1,d2,d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749e9804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species\n",
      "island\n",
      "bill_length_mm\n",
      "bill_depth_mm\n",
      "flipper_length_mm\n",
      "body_mass_g\n",
      "sex\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "#printing the column names\n",
    "for col in data.columns:\n",
    "    print(col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527c0916",
   "metadata": {},
   "source": [
    "## STEP 3 - Convert features with string datatype to categorical (species, island, sex).\n",
    "\n",
    "Example: suppose you have a dataset that contains information about movies,\n",
    "with the following features: title (string), director (string), genre (string). You need\n",
    "to convert these features of string datatype to categorical features. This can be\n",
    "done by assigning a unique numerical value to each unique string value in each\n",
    "categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510b7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Species,islands and sex has datatype as string \n",
    "\n",
    "data[['species','island','sex']] = data[['species','island','sex']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575dba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Categorical value into numerical value\n",
    "data['sex'] = pd.factorize(data['sex'])[0]\n",
    "data['island'] = pd.factorize(data['island'])[0]\n",
    "data['species'] = pd.factorize(data['species'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53be2792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 343\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    int64  \n",
      " 1   island             333 non-null    int64  \n",
      " 2   bill_length_mm     333 non-null    float64\n",
      " 3   bill_depth_mm      333 non-null    float64\n",
      " 4   flipper_length_mm  333 non-null    float64\n",
      " 5   body_mass_g        333 non-null    float64\n",
      " 6   sex                333 non-null    int64  \n",
      " 7   year               333 non-null    int64  \n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 23.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Here we could see the datatype has been changed of the following \n",
    "data.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9976998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species                int64\n",
      "island                 int64\n",
      "bill_length_mm       float64\n",
      "bill_depth_mm        float64\n",
      "flipper_length_mm    float64\n",
      "body_mass_g          float64\n",
      "sex                    int64\n",
      "year                   int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a08e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f235e",
   "metadata": {},
   "source": [
    "## 4. Normalize non-categorical features (bill_length_mm, bill_depth_mm,flipper_length_mm, body_mass_g).\n",
    "\n",
    "a. Find the min and max values for each column.\n",
    "\n",
    "b. Rescale dataset columns to the range from 0 to 1\n",
    "\n",
    "\n",
    "Why do we do this? Normalization is to transform features to be on a similar\n",
    "scale. This improves the performance and training stability of the model.\n",
    "\n",
    "###### Note: normalize() is not allowed as it is a part of scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee6ac22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0        0       0            39.1           18.7              181.0   \n",
       "1        0       0            39.5           17.4              186.0   \n",
       "2        0       0            40.3           18.0              195.0   \n",
       "4        0       0            36.7           19.3              193.0   \n",
       "5        0       0            39.3           20.6              190.0   \n",
       "\n",
       "   body_mass_g  sex  year  \n",
       "0       3750.0    0  2007  \n",
       "1       3800.0    1  2007  \n",
       "2       3250.0    1  2007  \n",
       "4       3450.0    1  2007  \n",
       "5       3650.0    0  2007  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59bae274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. Find the min and max values for each column.\n",
    "\n",
    "bill_length_mm_min , bill_length_mm_max = min(data['bill_length_mm']),max(data['bill_length_mm'])\n",
    "\n",
    "bill_depth_mm_min ,  bill_depth_mm_max = min(data['bill_depth_mm']) ,max(data['bill_depth_mm'])\n",
    "\n",
    "flipper_length__mm_min ,flipper_length_mm_max = min(data['flipper_length_mm']) , max(data['flipper_length_mm'])\n",
    " \n",
    "body_mass_g_min , body_mass_g_max = min(data['body_mass_g']) , max(data['body_mass_g'])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b1d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c46d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Rescale dataset columns to the range from 0 to 1\n",
    "\n",
    "data1['bill_length_mm'] = (data1['bill_length_mm'] - bill_length_mm_min) / (bill_length_mm_max - bill_length_mm_min)\n",
    "data1['bill_depth_mm']  = (data1['bill_depth_mm']  - bill_depth_mm_min)  / (bill_depth_mm_max  - bill_depth_mm_min)\n",
    "data1['flipper_length_mm'] = (data1['flipper_length_mm'] - flipper_length__mm_min)/(flipper_length_mm_max - flipper_length__mm_min)\n",
    "data1['body_mass_g'] = (data1['body_mass_g'] - body_mass_g_min)/(body_mass_g_max - body_mass_g_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a39f2dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261818</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861818</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.414545</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658182</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0          0       0        0.254545       0.666667           0.152542   \n",
       "1          0       0        0.269091       0.511905           0.237288   \n",
       "2          0       0        0.298182       0.583333           0.389831   \n",
       "4          0       0        0.167273       0.738095           0.355932   \n",
       "5          0       0        0.261818       0.892857           0.305085   \n",
       "..       ...     ...             ...            ...                ...   \n",
       "339        2       2        0.861818       0.797619           0.593220   \n",
       "340        2       2        0.414545       0.595238           0.508475   \n",
       "341        2       2        0.636364       0.607143           0.355932   \n",
       "342        2       2        0.680000       0.702381           0.644068   \n",
       "343        2       2        0.658182       0.666667           0.440678   \n",
       "\n",
       "     body_mass_g  sex  year  \n",
       "0       0.291667    0  2007  \n",
       "1       0.305556    1  2007  \n",
       "2       0.152778    1  2007  \n",
       "4       0.208333    1  2007  \n",
       "5       0.263889    0  2007  \n",
       "..           ...  ...   ...  \n",
       "339     0.361111    0  2009  \n",
       "340     0.194444    1  2009  \n",
       "341     0.298611    0  2009  \n",
       "342     0.388889    0  2009  \n",
       "343     0.298611    1  2009  \n",
       "\n",
       "[333 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914980fa",
   "metadata": {},
   "source": [
    "## 5. Choose your target Y. For this dataset, there are several options:\n",
    "a. We can use a binary classifier to predict which gender a penguin belongs to (female or male). In this case, column sex can be used as Y (target)\n",
    "\n",
    "\n",
    "b. We can use a binary classifier to predict if a penguin‚Äôs location is Torgersen island or not. In this case, column island can be used as Y (target) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79430698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Choosing target\n",
    "# Here we have chosen sex as the target, rest all are the inputs some of them are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608d313",
   "metadata": {},
   "source": [
    "## STEP 6 - Create the data matrices for X (input) and Y (target) in a shape,X = ùëÅ x ùëë and Y = ùëÅ x 1, were ùëÅ is a number of data samples and ùëë has a number of features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13900d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261818</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861818</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.414545</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658182</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0          0       0        0.254545       0.666667           0.152542   \n",
       "1          0       0        0.269091       0.511905           0.237288   \n",
       "2          0       0        0.298182       0.583333           0.389831   \n",
       "4          0       0        0.167273       0.738095           0.355932   \n",
       "5          0       0        0.261818       0.892857           0.305085   \n",
       "..       ...     ...             ...            ...                ...   \n",
       "339        2       2        0.861818       0.797619           0.593220   \n",
       "340        2       2        0.414545       0.595238           0.508475   \n",
       "341        2       2        0.636364       0.607143           0.355932   \n",
       "342        2       2        0.680000       0.702381           0.644068   \n",
       "343        2       2        0.658182       0.666667           0.440678   \n",
       "\n",
       "     body_mass_g  sex  year  \n",
       "0       0.291667    0  2007  \n",
       "1       0.305556    1  2007  \n",
       "2       0.152778    1  2007  \n",
       "4       0.208333    1  2007  \n",
       "5       0.263889    0  2007  \n",
       "..           ...  ...   ...  \n",
       "339     0.361111    0  2009  \n",
       "340     0.194444    1  2009  \n",
       "341     0.298611    0  2009  \n",
       "342     0.388889    0  2009  \n",
       "343     0.298611    1  2009  \n",
       "\n",
       "[333 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15be1e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261818</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861818</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.414545</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658182</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0          0       0        0.254545       0.666667           0.152542   \n",
       "1          0       0        0.269091       0.511905           0.237288   \n",
       "2          0       0        0.298182       0.583333           0.389831   \n",
       "4          0       0        0.167273       0.738095           0.355932   \n",
       "5          0       0        0.261818       0.892857           0.305085   \n",
       "..       ...     ...             ...            ...                ...   \n",
       "339        2       2        0.861818       0.797619           0.593220   \n",
       "340        2       2        0.414545       0.595238           0.508475   \n",
       "341        2       2        0.636364       0.607143           0.355932   \n",
       "342        2       2        0.680000       0.702381           0.644068   \n",
       "343        2       2        0.658182       0.666667           0.440678   \n",
       "\n",
       "     body_mass_g  sex  year  \n",
       "0       0.291667    0  2007  \n",
       "1       0.305556    1  2007  \n",
       "2       0.152778    1  2007  \n",
       "4       0.208333    1  2007  \n",
       "5       0.263889    0  2007  \n",
       "..           ...  ...   ...  \n",
       "339     0.361111    0  2009  \n",
       "340     0.194444    1  2009  \n",
       "341     0.298611    0  2009  \n",
       "342     0.388889    0  2009  \n",
       "343     0.298611    1  2009  \n",
       "\n",
       "[333 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebff642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['sex'] = pd.factorize(data1['sex'])[0]\n",
    "data1['island'] = pd.factorize(data1['island'])[0]\n",
    "data1['species'] = pd.factorize(data1['species'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51a5381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 343\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    int64  \n",
      " 1   island             333 non-null    int64  \n",
      " 2   bill_length_mm     333 non-null    float64\n",
      " 3   bill_depth_mm      333 non-null    float64\n",
      " 4   flipper_length_mm  333 non-null    float64\n",
      " 5   body_mass_g        333 non-null    float64\n",
      " 6   sex                333 non-null    int64  \n",
      " 7   year               333 non-null    int64  \n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 23.4 KB\n"
     ]
    }
   ],
   "source": [
    "data1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b3e08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data1 = data1.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "800eb457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_except_sex_X=data1[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "data2_except_sex_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05ada7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333,)\n"
     ]
    }
   ],
   "source": [
    "data3_sex_column_Y=data1['sex']\n",
    "print(data3_sex_column_Y.shape)\n",
    "data3_sex_column_Y=data3_sex_column_Y.astype(int)\n",
    "#print(data3_sex_column_Y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf8279",
   "metadata": {},
   "source": [
    "## Step 7 - Divide the dataset into training and test, as 80% training, 20% testing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e33b6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(46)\n",
    "\n",
    "train_size = int(len(data2_except_sex_X) * 0.8)\n",
    "\n",
    "X_train = data2_except_sex_X[0:train_size]\n",
    "Y_train = data3_sex_column_Y[0:train_size]\n",
    "\n",
    "X_test = data2_except_sex_X[train_size : ]\n",
    "Y_test = data3_sex_column_Y[train_size : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e11ea",
   "metadata": {},
   "source": [
    "## Step 8 -Print the shape of your X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd82174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 4)\n",
      "(266,)\n",
      "(67, 4)\n",
      "(67,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8c9d1",
   "metadata": {},
   "source": [
    "## Step 9 - Code for the Logistic Regression using the recommended structure of the code for defining logistic regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b6aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LogitRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, iterations = 10000):\n",
    "        # Takes as an input hyperparameters: learning rate and the number of iterations\n",
    "        # Has weights and bias also.\n",
    "        # We have self.losses to append the losses.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.losses = []\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Defining the sigmoid function.\n",
    "        sigma = 1/(1 + np.exp(-x))\n",
    "        return sigma\n",
    "    \n",
    "    def cost(self, y, y_pred):\n",
    "        # Defining the loss function.\n",
    "        N = len(y)\n",
    "        # Formula for the loss.\n",
    "        cost = -(1/N) * (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "        return cost\n",
    "        \n",
    "    def gradient(self, n, X, y, y_pred):\n",
    "        # Defining gradient function.\n",
    "        delta = y_pred - y\n",
    "        # Formula for the weights.\n",
    "        dw = (1 / n) * np.dot(np.transpose(X), (delta))\n",
    "        # Formula for the bias.\n",
    "        db = (1 / n) * np.sum(delta)\n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, no_feature  = X.shape\n",
    "        # Assigning random weights and bias zero.\n",
    "        self.weights = np.random.uniform(0, 1, 4)\n",
    "        self.bias = 0\n",
    "        for i in range(self.iterations):\n",
    "            # Using sigmoid defined to get prediction.\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            # Getting weights and bias.\n",
    "            dw, db = self.gradient(n, X, y, y_pred)\n",
    "            # Calculating loss\n",
    "            loss = self.cost(y, y_pred)\n",
    "            print(f\"Iteration {i}: loss = {np.mean(loss)}\")\n",
    "            # Appending loss to the list.\n",
    "            self.losses.append(loss)\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate*dw\n",
    "            self.bias -= self.learning_rate*db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        y_sex = []\n",
    "        for i in y_pred:\n",
    "            # Appending 1 if i is greater than or equal to 0.5.\n",
    "            if i >= 0.5:\n",
    "                y_sex.append(1)\n",
    "            # Appending 0 if i is less than 0.5.\n",
    "            else:\n",
    "                y_sex.append(0)\n",
    "        weight = self.weights\n",
    "        return y_sex, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a3744",
   "metadata": {},
   "source": [
    "## Step 10 -  Train the model:\n",
    "a. Define a model by calling LogitRegression class and passing\n",
    "hyperparameters, e.g.\n",
    "model = LogitRegression(learning_rate, iterations)\n",
    "b. Train the model, by calling fit function and passing your training dataset,\n",
    "e.g\n",
    "model.fit(X_train, y_train)\n",
    "c. Suggested hyperparameters:\n",
    "Note: You can try different learning rates and number of iterations to\n",
    "improve your accuracy (accuracy of greater than 64% is expected)\n",
    "learning_rate=1e-3\n",
    "iterations=100000\n",
    "weights = np.random.uniform(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.003054533381146852\n",
      "Iteration 1: loss = 0.0030542183039948766\n",
      "Iteration 2: loss = 0.0030539034787160635\n",
      "Iteration 3: loss = 0.003053588905132497\n",
      "Iteration 4: loss = 0.0030532745830663317\n",
      "Iteration 5: loss = 0.003052960512339789\n",
      "Iteration 6: loss = 0.00305264669277516\n",
      "Iteration 7: loss = 0.0030523331241948033\n",
      "Iteration 8: loss = 0.003052019806421146\n",
      "Iteration 9: loss = 0.0030517067392766837\n",
      "Iteration 10: loss = 0.0030513939225839818\n",
      "Iteration 11: loss = 0.0030510813561656722\n",
      "Iteration 12: loss = 0.003050769039844457\n",
      "Iteration 13: loss = 0.003050456973443109\n",
      "Iteration 14: loss = 0.003050145156784466\n",
      "Iteration 15: loss = 0.003049833589691437\n",
      "Iteration 16: loss = 0.003049522271987\n",
      "Iteration 17: loss = 0.0030492112034942033\n",
      "Iteration 18: loss = 0.0030489003840361625\n",
      "Iteration 19: loss = 0.0030485898134360624\n",
      "Iteration 20: loss = 0.0030482794915171604\n",
      "Iteration 21: loss = 0.0030479694181027787\n",
      "Iteration 22: loss = 0.0030476595930163126\n",
      "Iteration 23: loss = 0.003047350016081226\n",
      "Iteration 24: loss = 0.0030470406871210528\n",
      "Iteration 25: loss = 0.0030467316059593946\n",
      "Iteration 26: loss = 0.0030464227724199253\n",
      "Iteration 27: loss = 0.0030461141863263883\n",
      "Iteration 28: loss = 0.0030458058475025967\n",
      "Iteration 29: loss = 0.0030454977557724324\n",
      "Iteration 30: loss = 0.0030451899109598503\n",
      "Iteration 31: loss = 0.0030448823128888724\n",
      "Iteration 32: loss = 0.003044574961383594\n",
      "Iteration 33: loss = 0.003044267856268177\n",
      "Iteration 34: loss = 0.0030439609973668576\n",
      "Iteration 35: loss = 0.0030436543845039406\n",
      "Iteration 36: loss = 0.0030433480175038013\n",
      "Iteration 37: loss = 0.0030430418961908848\n",
      "Iteration 38: loss = 0.00304273602038971\n",
      "Iteration 39: loss = 0.003042430389924864\n",
      "Iteration 40: loss = 0.003042125004621005\n",
      "Iteration 41: loss = 0.003041819864302863\n",
      "Iteration 42: loss = 0.003041514968795238\n",
      "Iteration 43: loss = 0.003041210317923002\n",
      "Iteration 44: loss = 0.0030409059115110975\n",
      "Iteration 45: loss = 0.0030406017493845383\n",
      "Iteration 46: loss = 0.00304029783136841\n",
      "Iteration 47: loss = 0.0030399941572878683\n",
      "Iteration 48: loss = 0.003039690726968142\n",
      "Iteration 49: loss = 0.003039387540234531\n",
      "Iteration 50: loss = 0.003039084596912406\n",
      "Iteration 51: loss = 0.003038781896827208\n",
      "Iteration 52: loss = 0.0030384794398044535\n",
      "Iteration 53: loss = 0.0030381772256697272\n",
      "Iteration 54: loss = 0.003037875254248688\n",
      "Iteration 55: loss = 0.0030375735253670645\n",
      "Iteration 56: loss = 0.00303727203885066\n",
      "Iteration 57: loss = 0.003036970794525347\n",
      "Iteration 58: loss = 0.0030366697922170715\n",
      "Iteration 59: loss = 0.003036369031751852\n",
      "Iteration 60: loss = 0.0030360685129557793\n",
      "Iteration 61: loss = 0.003035768235655015\n",
      "Iteration 62: loss = 0.003035468199675794\n",
      "Iteration 63: loss = 0.0030351684048444245\n",
      "Iteration 64: loss = 0.003034868850987285\n",
      "Iteration 65: loss = 0.0030345695379308296\n",
      "Iteration 66: loss = 0.0030342704655015817\n",
      "Iteration 67: loss = 0.0030339716335261403\n",
      "Iteration 68: loss = 0.003033673041831176\n",
      "Iteration 69: loss = 0.0030333746902434303\n",
      "Iteration 70: loss = 0.0030330765785897213\n",
      "Iteration 71: loss = 0.003032778706696937\n",
      "Iteration 72: loss = 0.00303248107439204\n",
      "Iteration 73: loss = 0.0030321836815020648\n",
      "Iteration 74: loss = 0.003031886527854121\n",
      "Iteration 75: loss = 0.003031589613275389\n",
      "Iteration 76: loss = 0.0030312929375931243\n",
      "Iteration 77: loss = 0.003030996500634654\n",
      "Iteration 78: loss = 0.0030307003022273815\n",
      "Iteration 79: loss = 0.0030304043421987806\n",
      "Iteration 80: loss = 0.0030301086203763997\n",
      "Iteration 81: loss = 0.003029813136587861\n",
      "Iteration 82: loss = 0.0030295178906608604\n",
      "Iteration 83: loss = 0.0030292228824231673\n",
      "Iteration 84: loss = 0.003028928111702625\n",
      "Iteration 85: loss = 0.0030286335783271503\n",
      "Iteration 86: loss = 0.003028339282124734\n",
      "Iteration 87: loss = 0.003028045222923442\n",
      "Iteration 88: loss = 0.0030277514005514106\n",
      "Iteration 89: loss = 0.0030274578148368563\n",
      "Iteration 90: loss = 0.0030271644656080627\n",
      "Iteration 91: loss = 0.003026871352693394\n",
      "Iteration 92: loss = 0.003026578475921284\n",
      "Iteration 93: loss = 0.0030262858351202426\n",
      "Iteration 94: loss = 0.0030259934301188544\n",
      "Iteration 95: loss = 0.0030257012607457783\n",
      "Iteration 96: loss = 0.0030254093268297466\n",
      "Iteration 97: loss = 0.003025117628199568\n",
      "Iteration 98: loss = 0.003024826164684123\n",
      "Iteration 99: loss = 0.0030245349361123714\n",
      "Iteration 100: loss = 0.0030242439423133417\n",
      "Iteration 101: loss = 0.0030239531831161423\n",
      "Iteration 102: loss = 0.0030236626583499547\n",
      "Iteration 103: loss = 0.003023372367844034\n",
      "Iteration 104: loss = 0.0030230823114277116\n",
      "Iteration 105: loss = 0.0030227924889303947\n",
      "Iteration 106: loss = 0.0030225029001815624\n",
      "Iteration 107: loss = 0.003022213545010773\n",
      "Iteration 108: loss = 0.0030219244232476583\n",
      "Iteration 109: loss = 0.0030216355347219234\n",
      "Iteration 110: loss = 0.003021346879263351\n",
      "Iteration 111: loss = 0.0030210584567017993\n",
      "Iteration 112: loss = 0.0030207702668672005\n",
      "Iteration 113: loss = 0.003020482309589563\n",
      "Iteration 114: loss = 0.003020194584698971\n",
      "Iteration 115: loss = 0.0030199070920255834\n",
      "Iteration 116: loss = 0.0030196198313996356\n",
      "Iteration 117: loss = 0.0030193328026514384\n",
      "Iteration 118: loss = 0.0030190460056113786\n",
      "Iteration 119: loss = 0.0030187594401099183\n",
      "Iteration 120: loss = 0.003018473105977595\n",
      "Iteration 121: loss = 0.0030181870030450233\n",
      "Iteration 122: loss = 0.0030179011311428938\n",
      "Iteration 123: loss = 0.0030176154901019716\n",
      "Iteration 124: loss = 0.0030173300797531\n",
      "Iteration 125: loss = 0.003017044899927196\n",
      "Iteration 126: loss = 0.0030167599504552552\n",
      "Iteration 127: loss = 0.0030164752311683473\n",
      "Iteration 128: loss = 0.0030161907418976204\n",
      "Iteration 129: loss = 0.0030159064824742954\n",
      "Iteration 130: loss = 0.003015622452729675\n",
      "Iteration 131: loss = 0.0030153386524951332\n",
      "Iteration 132: loss = 0.003015055081602124\n",
      "Iteration 133: loss = 0.0030147717398821758\n",
      "Iteration 134: loss = 0.003014488627166895\n",
      "Iteration 135: loss = 0.003014205743287963\n",
      "Iteration 136: loss = 0.00301392308807714\n",
      "Iteration 137: loss = 0.0030136406613662614\n",
      "Iteration 138: loss = 0.00301335846298724\n",
      "Iteration 139: loss = 0.003013076492772065\n",
      "Iteration 140: loss = 0.003012794750552803\n",
      "Iteration 141: loss = 0.0030125132361615986\n",
      "Iteration 142: loss = 0.0030122319494306706\n",
      "Iteration 143: loss = 0.003011950890192317\n",
      "Iteration 144: loss = 0.003011670058278913\n",
      "Iteration 145: loss = 0.00301138945352291\n",
      "Iteration 146: loss = 0.0030111090757568362\n",
      "Iteration 147: loss = 0.003010828924813299\n",
      "Iteration 148: loss = 0.0030105490005249815\n",
      "Iteration 149: loss = 0.0030102693027246447\n",
      "Iteration 150: loss = 0.0030099898312451264\n",
      "Iteration 151: loss = 0.0030097105859193435\n",
      "Iteration 152: loss = 0.003009431566580288\n",
      "Iteration 153: loss = 0.003009152773061031\n",
      "Iteration 154: loss = 0.003008874205194722\n",
      "Iteration 155: loss = 0.0030085958628145856\n",
      "Iteration 156: loss = 0.0030083177457539264\n",
      "Iteration 157: loss = 0.003008039853846126\n",
      "Iteration 158: loss = 0.003007762186924643\n",
      "Iteration 159: loss = 0.003007484744823015\n",
      "Iteration 160: loss = 0.0030072075273748573\n",
      "Iteration 161: loss = 0.0030069305344138622\n",
      "Iteration 162: loss = 0.003006653765773801\n",
      "Iteration 163: loss = 0.003006377221288523\n",
      "Iteration 164: loss = 0.0030061009007919547\n",
      "Iteration 165: loss = 0.003005824804118102\n",
      "Iteration 166: loss = 0.0030055489311010476\n",
      "Iteration 167: loss = 0.0030052732815749523\n",
      "Iteration 168: loss = 0.0030049978553740573\n",
      "Iteration 169: loss = 0.003004722652332681\n",
      "Iteration 170: loss = 0.003004447672285218\n",
      "Iteration 171: loss = 0.0030041729150661447\n",
      "Iteration 172: loss = 0.0030038983805100125\n",
      "Iteration 173: loss = 0.0030036240684514558\n",
      "Iteration 174: loss = 0.0030033499787251836\n",
      "Iteration 175: loss = 0.003003076111165984\n",
      "Iteration 176: loss = 0.0030028024656087253\n",
      "Iteration 177: loss = 0.0030025290418883533\n",
      "Iteration 178: loss = 0.003002255839839894\n",
      "Iteration 179: loss = 0.0030019828592984495\n",
      "Iteration 180: loss = 0.0030017101000992027\n",
      "Iteration 181: loss = 0.0030014375620774146\n",
      "Iteration 182: loss = 0.0030011652450684253\n",
      "Iteration 183: loss = 0.0030008931489076546\n",
      "Iteration 184: loss = 0.0030006212734305995\n",
      "Iteration 185: loss = 0.0030003496184728374\n",
      "Iteration 186: loss = 0.0030000781838700244\n",
      "Iteration 187: loss = 0.0029998069694578957\n",
      "Iteration 188: loss = 0.002999535975072265\n",
      "Iteration 189: loss = 0.002999265200549026\n",
      "Iteration 190: loss = 0.0029989946457241517\n",
      "Iteration 191: loss = 0.0029987243104336934\n",
      "Iteration 192: loss = 0.0029984541945137825\n",
      "Iteration 193: loss = 0.002998184297800629\n",
      "Iteration 194: loss = 0.002997914620130524\n",
      "Iteration 195: loss = 0.002997645161339836\n",
      "Iteration 196: loss = 0.0029973759212650144\n",
      "Iteration 197: loss = 0.002997106899742587\n",
      "Iteration 198: loss = 0.0029968380966091616\n",
      "Iteration 199: loss = 0.0029965695117014257\n",
      "Iteration 200: loss = 0.0029963011448561464\n",
      "Iteration 201: loss = 0.0029960329959101708\n",
      "Iteration 202: loss = 0.0029957650647004243\n",
      "Iteration 203: loss = 0.0029954973510639142\n",
      "Iteration 204: loss = 0.0029952298548377256\n",
      "Iteration 205: loss = 0.002994962575859025\n",
      "Iteration 206: loss = 0.002994695513965057\n",
      "Iteration 207: loss = 0.0029944286689931486\n",
      "Iteration 208: loss = 0.0029941620407807038\n",
      "Iteration 209: loss = 0.0029938956291652085\n",
      "Iteration 210: loss = 0.002993629433984228\n",
      "Iteration 211: loss = 0.002993363455075409\n",
      "Iteration 212: loss = 0.002993097692276475\n",
      "Iteration 213: loss = 0.0029928321454252334\n",
      "Iteration 214: loss = 0.0029925668143595693\n",
      "Iteration 215: loss = 0.002992301698917449\n",
      "Iteration 216: loss = 0.002992036798936919\n",
      "Iteration 217: loss = 0.0029917721142561048\n",
      "Iteration 218: loss = 0.002991507644713215\n",
      "Iteration 219: loss = 0.002991243390146535\n",
      "Iteration 220: loss = 0.0029909793503944343\n",
      "Iteration 221: loss = 0.0029907155252953594\n",
      "Iteration 222: loss = 0.00299045191468784\n",
      "Iteration 223: loss = 0.0029901885184104846\n",
      "Iteration 224: loss = 0.0029899253363019833\n",
      "Iteration 225: loss = 0.0029896623682011043\n",
      "Iteration 226: loss = 0.0029893996139467006\n",
      "Iteration 227: loss = 0.0029891370733777026\n",
      "Iteration 228: loss = 0.0029888747463331215\n",
      "Iteration 229: loss = 0.0029886126326520517\n",
      "Iteration 230: loss = 0.002988350732173666\n",
      "Iteration 231: loss = 0.0029880890447372176\n",
      "Iteration 232: loss = 0.002987827570182043\n",
      "Iteration 233: loss = 0.002987566308347557\n",
      "Iteration 234: loss = 0.002987305259073257\n",
      "Iteration 235: loss = 0.0029870444221987203\n",
      "Iteration 236: loss = 0.002986783797563607\n",
      "Iteration 237: loss = 0.0029865233850076543\n",
      "Iteration 238: loss = 0.0029862631843706844\n",
      "Iteration 239: loss = 0.0029860031954925983\n",
      "Iteration 240: loss = 0.002985743418213379\n",
      "Iteration 241: loss = 0.002985483852373091\n",
      "Iteration 242: loss = 0.002985224497811879\n",
      "Iteration 243: loss = 0.0029849653543699674\n",
      "Iteration 244: loss = 0.0029847064218876673\n",
      "Iteration 245: loss = 0.002984447700205364\n",
      "Iteration 246: loss = 0.002984189189163528\n",
      "Iteration 247: loss = 0.002983930888602712\n",
      "Iteration 248: loss = 0.002983672798363548\n",
      "Iteration 249: loss = 0.002983414918286749\n",
      "Iteration 250: loss = 0.0029831572482131117\n",
      "Iteration 251: loss = 0.0029828997879835116\n",
      "Iteration 252: loss = 0.002982642537438908\n",
      "Iteration 253: loss = 0.0029823854964203403\n",
      "Iteration 254: loss = 0.002982128664768929\n",
      "Iteration 255: loss = 0.002981872042325878\n",
      "Iteration 256: loss = 0.002981615628932471\n",
      "Iteration 257: loss = 0.0029813594244300756\n",
      "Iteration 258: loss = 0.0029811034286601366\n",
      "Iteration 259: loss = 0.0029808476414641856\n",
      "Iteration 260: loss = 0.002980592062683833\n",
      "Iteration 261: loss = 0.002980336692160772\n",
      "Iteration 262: loss = 0.0029800815297367765\n",
      "Iteration 263: loss = 0.0029798265752537033\n",
      "Iteration 264: loss = 0.002979571828553491\n",
      "Iteration 265: loss = 0.002979317289478159\n",
      "Iteration 266: loss = 0.002979062957869809\n",
      "Iteration 267: loss = 0.002978808833570625\n",
      "Iteration 268: loss = 0.002978554916422874\n",
      "Iteration 269: loss = 0.0029783012062689017\n",
      "Iteration 270: loss = 0.00297804770295114\n",
      "Iteration 271: loss = 0.002977794406312099\n",
      "Iteration 272: loss = 0.0029775413161943734\n",
      "Iteration 273: loss = 0.0029772884324406395\n",
      "Iteration 274: loss = 0.0029770357548936544\n",
      "Iteration 275: loss = 0.002976783283396258\n",
      "Iteration 276: loss = 0.0029765310177913744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 277: loss = 0.002976278957922006\n",
      "Iteration 278: loss = 0.0029760271036312416\n",
      "Iteration 279: loss = 0.002975775454762248\n",
      "Iteration 280: loss = 0.0029755240111582785\n",
      "Iteration 281: loss = 0.002975272772662666\n",
      "Iteration 282: loss = 0.002975021739118825\n",
      "Iteration 283: loss = 0.0029747709103702556\n",
      "Iteration 284: loss = 0.0029745202862605383\n",
      "Iteration 285: loss = 0.002974269866633335\n",
      "Iteration 286: loss = 0.0029740196513323913\n",
      "Iteration 287: loss = 0.002973769640201536\n",
      "Iteration 288: loss = 0.002973519833084679\n",
      "Iteration 289: loss = 0.002973270229825813\n",
      "Iteration 290: loss = 0.0029730208302690144\n",
      "Iteration 291: loss = 0.002972771634258439\n",
      "Iteration 292: loss = 0.0029725226416383297\n",
      "Iteration 293: loss = 0.0029722738522530087\n",
      "Iteration 294: loss = 0.0029720252659468814\n",
      "Iteration 295: loss = 0.0029717768825644367\n",
      "Iteration 296: loss = 0.0029715287019502455\n",
      "Iteration 297: loss = 0.0029712807239489613\n",
      "Iteration 298: loss = 0.0029710329484053223\n",
      "Iteration 299: loss = 0.002970785375164146\n",
      "Iteration 300: loss = 0.002970538004070335\n",
      "Iteration 301: loss = 0.002970290834968875\n",
      "Iteration 302: loss = 0.002970043867704832\n",
      "Iteration 303: loss = 0.0029697971021233572\n",
      "Iteration 304: loss = 0.002969550538069685\n",
      "Iteration 305: loss = 0.0029693041753891306\n",
      "Iteration 306: loss = 0.002969058013927094\n",
      "Iteration 307: loss = 0.0029688120535290565\n",
      "Iteration 308: loss = 0.0029685662940405833\n",
      "Iteration 309: loss = 0.002968320735307323\n",
      "Iteration 310: loss = 0.002968075377175006\n",
      "Iteration 311: loss = 0.0029678302194894477\n",
      "Iteration 312: loss = 0.0029675852620965437\n",
      "Iteration 313: loss = 0.0029673405048422745\n",
      "Iteration 314: loss = 0.0029670959475727042\n",
      "Iteration 315: loss = 0.002966851590133979\n",
      "Iteration 316: loss = 0.002966607432372327\n",
      "Iteration 317: loss = 0.0029663634741340624\n",
      "Iteration 318: loss = 0.0029661197152655804\n",
      "Iteration 319: loss = 0.002965876155613361\n",
      "Iteration 320: loss = 0.002965632795023965\n",
      "Iteration 321: loss = 0.002965389633344039\n",
      "Iteration 322: loss = 0.0029651466704203105\n",
      "Iteration 323: loss = 0.002964903906099593\n",
      "Iteration 324: loss = 0.002964661340228781\n",
      "Iteration 325: loss = 0.0029644189726548533\n",
      "Iteration 326: loss = 0.002964176803224872\n",
      "Iteration 327: loss = 0.002963934831785981\n",
      "Iteration 328: loss = 0.002963693058185411\n",
      "Iteration 329: loss = 0.0029634514822704733\n",
      "Iteration 330: loss = 0.0029632101038885635\n",
      "Iteration 331: loss = 0.00296296892288716\n",
      "Iteration 332: loss = 0.0029627279391138254\n",
      "Iteration 333: loss = 0.0029624871524162053\n",
      "Iteration 334: loss = 0.00296224656264203\n",
      "Iteration 335: loss = 0.0029620061696391115\n",
      "Iteration 336: loss = 0.002961765973255346\n",
      "Iteration 337: loss = 0.0029615259733387135\n",
      "Iteration 338: loss = 0.0029612861697372777\n",
      "Iteration 339: loss = 0.0029610465622991852\n",
      "Iteration 340: loss = 0.0029608071508726674\n",
      "Iteration 341: loss = 0.0029605679353060382\n",
      "Iteration 342: loss = 0.002960328915447694\n",
      "Iteration 343: loss = 0.0029600900911461183\n",
      "Iteration 344: loss = 0.0029598514622498754\n",
      "Iteration 345: loss = 0.0029596130286076144\n",
      "Iteration 346: loss = 0.0029593747900680673\n",
      "Iteration 347: loss = 0.0029591367464800507\n",
      "Iteration 348: loss = 0.002958898897692464\n",
      "Iteration 349: loss = 0.002958661243554292\n",
      "Iteration 350: loss = 0.002958423783914602\n",
      "Iteration 351: loss = 0.002958186518622545\n",
      "Iteration 352: loss = 0.002957949447527355\n",
      "Iteration 353: loss = 0.002957712570478352\n",
      "Iteration 354: loss = 0.0029574758873249386\n",
      "Iteration 355: loss = 0.0029572393979166015\n",
      "Iteration 356: loss = 0.0029570031021029105\n",
      "Iteration 357: loss = 0.0029567669997335204\n",
      "Iteration 358: loss = 0.0029565310906581686\n",
      "Iteration 359: loss = 0.002956295374726678\n",
      "Iteration 360: loss = 0.0029560598517889545\n",
      "Iteration 361: loss = 0.0029558245216949872\n",
      "Iteration 362: loss = 0.00295558938429485\n",
      "Iteration 363: loss = 0.0029553544394387023\n",
      "Iteration 364: loss = 0.0029551196869767845\n",
      "Iteration 365: loss = 0.0029548851267594224\n",
      "Iteration 366: loss = 0.0029546507586370257\n",
      "Iteration 367: loss = 0.002954416582460089\n",
      "Iteration 368: loss = 0.002954182598079189\n",
      "Iteration 369: loss = 0.0029539488053449887\n",
      "Iteration 370: loss = 0.0029537152041082333\n",
      "Iteration 371: loss = 0.002953481794219753\n",
      "Iteration 372: loss = 0.0029532485755304617\n",
      "Iteration 373: loss = 0.0029530155478913575\n",
      "Iteration 374: loss = 0.0029527827111535234\n",
      "Iteration 375: loss = 0.002952550065168125\n",
      "Iteration 376: loss = 0.002952317609786413\n",
      "Iteration 377: loss = 0.002952085344859723\n",
      "Iteration 378: loss = 0.0029518532702394727\n",
      "Iteration 379: loss = 0.0029516213857771647\n",
      "Iteration 380: loss = 0.0029513896913243884\n",
      "Iteration 381: loss = 0.002951158186732813\n",
      "Iteration 382: loss = 0.0029509268718541956\n",
      "Iteration 383: loss = 0.0029506957465403752\n",
      "Iteration 384: loss = 0.0029504648106432763\n",
      "Iteration 385: loss = 0.002950234064014906\n",
      "Iteration 386: loss = 0.0029500035065073587\n",
      "Iteration 387: loss = 0.00294977313797281\n",
      "Iteration 388: loss = 0.0029495429582635222\n",
      "Iteration 389: loss = 0.0029493129672318393\n",
      "Iteration 390: loss = 0.0029490831647301916\n",
      "Iteration 391: loss = 0.002948853550611093\n",
      "Iteration 392: loss = 0.0029486241247271426\n",
      "Iteration 393: loss = 0.0029483948869310217\n",
      "Iteration 394: loss = 0.002948165837075498\n",
      "Iteration 395: loss = 0.0029479369750134233\n",
      "Iteration 396: loss = 0.0029477083005977323\n",
      "Iteration 397: loss = 0.0029474798136814463\n",
      "Iteration 398: loss = 0.0029472515141176687\n",
      "Iteration 399: loss = 0.002947023401759589\n",
      "Iteration 400: loss = 0.0029467954764604795\n",
      "Iteration 401: loss = 0.002946567738073699\n",
      "Iteration 402: loss = 0.002946340186452689\n",
      "Iteration 403: loss = 0.002946112821450977\n",
      "Iteration 404: loss = 0.0029458856429221717\n",
      "Iteration 405: loss = 0.002945658650719971\n",
      "Iteration 406: loss = 0.0029454318446981527\n",
      "Iteration 407: loss = 0.002945205224710583\n",
      "Iteration 408: loss = 0.002944978790611208\n",
      "Iteration 409: loss = 0.0029447525422540643\n",
      "Iteration 410: loss = 0.002944526479493266\n",
      "Iteration 411: loss = 0.0029443006021830196\n",
      "Iteration 412: loss = 0.002944074910177608\n",
      "Iteration 413: loss = 0.0029438494033314036\n",
      "Iteration 414: loss = 0.002943624081498863\n",
      "Iteration 415: loss = 0.002943398944534526\n",
      "Iteration 416: loss = 0.002943173992293017\n",
      "Iteration 417: loss = 0.002942949224629046\n",
      "Iteration 418: loss = 0.0029427246413974064\n",
      "Iteration 419: loss = 0.002942500242452977\n",
      "Iteration 420: loss = 0.0029422760276507204\n",
      "Iteration 421: loss = 0.002942051996845685\n",
      "Iteration 422: loss = 0.0029418281498930026\n",
      "Iteration 423: loss = 0.002941604486647889\n",
      "Iteration 424: loss = 0.0029413810069656462\n",
      "Iteration 425: loss = 0.002941157710701661\n",
      "Iteration 426: loss = 0.0029409345977114026\n",
      "Iteration 427: loss = 0.002940711667850427\n",
      "Iteration 428: loss = 0.0029404889209743734\n",
      "Iteration 429: loss = 0.002940266356938966\n",
      "Iteration 430: loss = 0.0029400439756000147\n",
      "Iteration 431: loss = 0.0029398217768134122\n",
      "Iteration 432: loss = 0.002939599760435137\n",
      "Iteration 433: loss = 0.0029393779263212527\n",
      "Iteration 434: loss = 0.0029391562743279055\n",
      "Iteration 435: loss = 0.0029389348043113283\n",
      "Iteration 436: loss = 0.0029387135161278387\n",
      "Iteration 437: loss = 0.002938492409633837\n",
      "Iteration 438: loss = 0.0029382714846858094\n",
      "Iteration 439: loss = 0.0029380507411403277\n",
      "Iteration 440: loss = 0.002937830178854046\n",
      "Iteration 441: loss = 0.002937609797683706\n",
      "Iteration 442: loss = 0.0029373895974861316\n",
      "Iteration 443: loss = 0.0029371695781182324\n",
      "Iteration 444: loss = 0.002936949739437003\n",
      "Iteration 445: loss = 0.002936730081299522\n",
      "Iteration 446: loss = 0.0029365106035629533\n",
      "Iteration 447: loss = 0.002936291306084545\n",
      "Iteration 448: loss = 0.0029360721887216304\n",
      "Iteration 449: loss = 0.002935853251331627\n",
      "Iteration 450: loss = 0.002935634493772037\n",
      "Iteration 451: loss = 0.0029354159159004473\n",
      "Iteration 452: loss = 0.002935197517574531\n",
      "Iteration 453: loss = 0.002934979298652044\n",
      "Iteration 454: loss = 0.002934761258990828\n",
      "Iteration 455: loss = 0.002934543398448808\n",
      "Iteration 456: loss = 0.002934325716883996\n",
      "Iteration 457: loss = 0.0029341082141544866\n",
      "Iteration 458: loss = 0.00293389089011846\n",
      "Iteration 459: loss = 0.0029336737446341824\n",
      "Iteration 460: loss = 0.002933456777560002\n",
      "Iteration 461: loss = 0.0029332399887543542\n",
      "Iteration 462: loss = 0.0029330233780757576\n",
      "Iteration 463: loss = 0.002932806945382817\n",
      "Iteration 464: loss = 0.00293259069053422\n",
      "Iteration 465: loss = 0.00293237461338874\n",
      "Iteration 466: loss = 0.002932158713805237\n",
      "Iteration 467: loss = 0.0029319429916426516\n",
      "Iteration 468: loss = 0.0029317274467600125\n",
      "Iteration 469: loss = 0.002931512079016432\n",
      "Iteration 470: loss = 0.0029312968882711076\n",
      "Iteration 471: loss = 0.0029310818743833204\n",
      "Iteration 472: loss = 0.002930867037212438\n",
      "Iteration 473: loss = 0.002930652376617911\n",
      "Iteration 474: loss = 0.002930437892459276\n",
      "Iteration 475: loss = 0.0029302235845961536\n",
      "Iteration 476: loss = 0.00293000945288825\n",
      "Iteration 477: loss = 0.0029297954971953554\n",
      "Iteration 478: loss = 0.0029295817173773447\n",
      "Iteration 479: loss = 0.002929368113294178\n",
      "Iteration 480: loss = 0.0029291546848059\n",
      "Iteration 481: loss = 0.0029289414317726402\n",
      "Iteration 482: loss = 0.0029287283540546127\n",
      "Iteration 483: loss = 0.0029285154515121163\n",
      "Iteration 484: loss = 0.0029283027240055355\n",
      "Iteration 485: loss = 0.002928090171395338\n",
      "Iteration 486: loss = 0.002927877793542076\n",
      "Iteration 487: loss = 0.00292766559030639\n",
      "Iteration 488: loss = 0.002927453561549\n",
      "Iteration 489: loss = 0.0029272417071307152\n",
      "Iteration 490: loss = 0.0029270300269124273\n",
      "Iteration 491: loss = 0.002926818520755113\n",
      "Iteration 492: loss = 0.002926607188519834\n",
      "Iteration 493: loss = 0.0029263960300677373\n",
      "Iteration 494: loss = 0.002926185045260053\n",
      "Iteration 495: loss = 0.0029259742339580977\n",
      "Iteration 496: loss = 0.0029257635960232716\n",
      "Iteration 497: loss = 0.00292555313131706\n",
      "Iteration 498: loss = 0.002925342839701034\n",
      "Iteration 499: loss = 0.002925132721036847\n",
      "Iteration 500: loss = 0.0029249227751862394\n",
      "Iteration 501: loss = 0.002924713002011035\n",
      "Iteration 502: loss = 0.002924503401373143\n",
      "Iteration 503: loss = 0.0029242939731345566\n",
      "Iteration 504: loss = 0.0029240847171573547\n",
      "Iteration 505: loss = 0.0029238756333037005\n",
      "Iteration 506: loss = 0.0029236667214358413\n",
      "Iteration 507: loss = 0.0029234579814161107\n",
      "Iteration 508: loss = 0.0029232494131069243\n",
      "Iteration 509: loss = 0.002923041016370785\n",
      "Iteration 510: loss = 0.002922832791070279\n",
      "Iteration 511: loss = 0.002922624737068078\n",
      "Iteration 512: loss = 0.002922416854226938\n",
      "Iteration 513: loss = 0.002922209142409699\n",
      "Iteration 514: loss = 0.002922001601479287\n",
      "Iteration 515: loss = 0.0029217942312987116\n",
      "Iteration 516: loss = 0.002921587031731068\n",
      "Iteration 517: loss = 0.002921380002639535\n",
      "Iteration 518: loss = 0.002921173143887377\n",
      "Iteration 519: loss = 0.0029209664553379424\n",
      "Iteration 520: loss = 0.002920759936854665\n",
      "Iteration 521: loss = 0.0029205535883010617\n",
      "Iteration 522: loss = 0.002920347409540736\n",
      "Iteration 523: loss = 0.0029201414004373753\n",
      "Iteration 524: loss = 0.002919935560854751\n",
      "Iteration 525: loss = 0.0029197298906567196\n",
      "Iteration 526: loss = 0.002919524389707223\n",
      "Iteration 527: loss = 0.0029193190578702855\n",
      "Iteration 528: loss = 0.002919113895010018\n",
      "Iteration 529: loss = 0.0029189089009906168\n",
      "Iteration 530: loss = 0.0029187040756763595\n",
      "Iteration 531: loss = 0.0029184994189316122\n",
      "Iteration 532: loss = 0.002918294930620822\n",
      "Iteration 533: loss = 0.002918090610608523\n",
      "Iteration 534: loss = 0.0029178864587593328\n",
      "Iteration 535: loss = 0.002917682474937954\n",
      "Iteration 536: loss = 0.002917478659009174\n",
      "Iteration 537: loss = 0.002917275010837865\n",
      "Iteration 538: loss = 0.002917071530288981\n",
      "Iteration 539: loss = 0.0029168682172275645\n",
      "Iteration 540: loss = 0.00291666507151874\n",
      "Iteration 541: loss = 0.0029164620930277178\n",
      "Iteration 542: loss = 0.0029162592816197918\n",
      "Iteration 543: loss = 0.002916056637160341\n",
      "Iteration 544: loss = 0.0029158541595148283\n",
      "Iteration 545: loss = 0.0029156518485488024\n",
      "Iteration 546: loss = 0.002915449704127896\n",
      "Iteration 547: loss = 0.0029152477261178236\n",
      "Iteration 548: loss = 0.0029150459143843887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 549: loss = 0.002914844268793476\n",
      "Iteration 550: loss = 0.002914642789211056\n",
      "Iteration 551: loss = 0.0029144414755031835\n",
      "Iteration 552: loss = 0.0029142403275359977\n",
      "Iteration 553: loss = 0.0029140393451757224\n",
      "Iteration 554: loss = 0.002913838528288665\n",
      "Iteration 555: loss = 0.002913637876741219\n",
      "Iteration 556: loss = 0.0029134373903998596\n",
      "Iteration 557: loss = 0.00291323706913115\n",
      "Iteration 558: loss = 0.002913036912801735\n",
      "Iteration 559: loss = 0.0029128369212783442\n",
      "Iteration 560: loss = 0.002912637094427792\n",
      "Iteration 561: loss = 0.002912437432116979\n",
      "Iteration 562: loss = 0.0029122379342128865\n",
      "Iteration 563: loss = 0.0029120386005825833\n",
      "Iteration 564: loss = 0.00291183943109322\n",
      "Iteration 565: loss = 0.002911640425612034\n",
      "Iteration 566: loss = 0.0029114415840063457\n",
      "Iteration 567: loss = 0.0029112429061435596\n",
      "Iteration 568: loss = 0.0029110443918911656\n",
      "Iteration 569: loss = 0.0029108460411167367\n",
      "Iteration 570: loss = 0.00291064785368793\n",
      "Iteration 571: loss = 0.0029104498294724892\n",
      "Iteration 572: loss = 0.00291025196833824\n",
      "Iteration 573: loss = 0.002910054270153092\n",
      "Iteration 574: loss = 0.002909856734785042\n",
      "Iteration 575: loss = 0.002909659362102168\n",
      "Iteration 576: loss = 0.0029094621519726335\n",
      "Iteration 577: loss = 0.002909265104264686\n",
      "Iteration 578: loss = 0.002909068218846658\n",
      "Iteration 579: loss = 0.0029088714955869646\n",
      "Iteration 580: loss = 0.0029086749343541067\n",
      "Iteration 581: loss = 0.0029084785350166676\n",
      "Iteration 582: loss = 0.002908282297443317\n",
      "Iteration 583: loss = 0.0029080862215028077\n",
      "Iteration 584: loss = 0.0029078903070639756\n",
      "Iteration 585: loss = 0.0029076945539957427\n",
      "Iteration 586: loss = 0.0029074989621671135\n",
      "Iteration 587: loss = 0.0029073035314471766\n",
      "Iteration 588: loss = 0.0029071082617051075\n",
      "Iteration 589: loss = 0.0029069131528101618\n",
      "Iteration 590: loss = 0.0029067182046316814\n",
      "Iteration 591: loss = 0.0029065234170390923\n",
      "Iteration 592: loss = 0.0029063287899019044\n",
      "Iteration 593: loss = 0.00290613432308971\n",
      "Iteration 594: loss = 0.0029059400164721887\n",
      "Iteration 595: loss = 0.0029057458699191007\n",
      "Iteration 596: loss = 0.0029055518833002924\n",
      "Iteration 597: loss = 0.002905358056485694\n",
      "Iteration 598: loss = 0.0029051643893453186\n",
      "Iteration 599: loss = 0.0029049708817492645\n",
      "Iteration 600: loss = 0.0029047775335677133\n",
      "Iteration 601: loss = 0.00290458434467093\n",
      "Iteration 602: loss = 0.002904391314929265\n",
      "Iteration 603: loss = 0.0029041984442131517\n",
      "Iteration 604: loss = 0.002904005732393107\n",
      "Iteration 605: loss = 0.0029038131793397326\n",
      "Iteration 606: loss = 0.002903620784923714\n",
      "Iteration 607: loss = 0.0029034285490158202\n",
      "Iteration 608: loss = 0.0029032364714869034\n",
      "Iteration 609: loss = 0.002903044552207901\n",
      "Iteration 610: loss = 0.0029028527910498337\n",
      "Iteration 611: loss = 0.002902661187883806\n",
      "Iteration 612: loss = 0.002902469742581006\n",
      "Iteration 613: loss = 0.002902278455012706\n",
      "Iteration 614: loss = 0.0029020873250502613\n",
      "Iteration 615: loss = 0.0029018963525651123\n",
      "Iteration 616: loss = 0.0029017055374287817\n",
      "Iteration 617: loss = 0.0029015148795128766\n",
      "Iteration 618: loss = 0.0029013243786890886\n",
      "Iteration 619: loss = 0.0029011340348291913\n",
      "Iteration 620: loss = 0.002900943847805043\n",
      "Iteration 621: loss = 0.0029007538174885866\n",
      "Iteration 622: loss = 0.002900563943751847\n",
      "Iteration 623: loss = 0.002900374226466934\n",
      "Iteration 624: loss = 0.0029001846655060393\n",
      "Iteration 625: loss = 0.00289999526074144\n",
      "Iteration 626: loss = 0.002899806012045497\n",
      "Iteration 627: loss = 0.0028996169192906534\n",
      "Iteration 628: loss = 0.002899427982349436\n",
      "Iteration 629: loss = 0.0028992392010944563\n",
      "Iteration 630: loss = 0.0028990505753984093\n",
      "Iteration 631: loss = 0.0028988621051340715\n",
      "Iteration 632: loss = 0.002898673790174306\n",
      "Iteration 633: loss = 0.002898485630392057\n",
      "Iteration 634: loss = 0.0028982976256603523\n",
      "Iteration 635: loss = 0.002898109775852305\n",
      "Iteration 636: loss = 0.0028979220808411105\n",
      "Iteration 637: loss = 0.0028977345405000465\n",
      "Iteration 638: loss = 0.002897547154702477\n",
      "Iteration 639: loss = 0.002897359923321846\n",
      "Iteration 640: loss = 0.002897172846231684\n",
      "Iteration 641: loss = 0.002896985923305603\n",
      "Iteration 642: loss = 0.002896799154417299\n",
      "Iteration 643: loss = 0.0028966125394405506\n",
      "Iteration 644: loss = 0.002896426078249221\n",
      "Iteration 645: loss = 0.002896239770717256\n",
      "Iteration 646: loss = 0.002896053616718685\n",
      "Iteration 647: loss = 0.00289586761612762\n",
      "Iteration 648: loss = 0.0028956817688182567\n",
      "Iteration 649: loss = 0.002895496074664875\n",
      "Iteration 650: loss = 0.0028953105335418356\n",
      "Iteration 651: loss = 0.0028951251453235853\n",
      "Iteration 652: loss = 0.002894939909884653\n",
      "Iteration 653: loss = 0.0028947548270996494\n",
      "Iteration 654: loss = 0.0028945698968432694\n",
      "Iteration 655: loss = 0.0028943851189902918\n",
      "Iteration 656: loss = 0.0028942004934155788\n",
      "Iteration 657: loss = 0.002894016019994074\n",
      "Iteration 658: loss = 0.0028938316986008043\n",
      "Iteration 659: loss = 0.0028936475291108806\n",
      "Iteration 660: loss = 0.002893463511399497\n",
      "Iteration 661: loss = 0.002893279645341931\n",
      "Iteration 662: loss = 0.0028930959308135404\n",
      "Iteration 663: loss = 0.0028929123676897693\n",
      "Iteration 664: loss = 0.0028927289558461433\n",
      "Iteration 665: loss = 0.0028925456951582715\n",
      "Iteration 666: loss = 0.0028923625855018452\n",
      "Iteration 667: loss = 0.002892179626752639\n",
      "Iteration 668: loss = 0.002891996818786511\n",
      "Iteration 669: loss = 0.0028918141614794014\n",
      "Iteration 670: loss = 0.002891631654707333\n",
      "Iteration 671: loss = 0.002891449298346413\n",
      "Iteration 672: loss = 0.0028912670922728303\n",
      "Iteration 673: loss = 0.002891085036362857\n",
      "Iteration 674: loss = 0.0028909031304928472\n",
      "Iteration 675: loss = 0.002890721374539239\n",
      "Iteration 676: loss = 0.002890539768378553\n",
      "Iteration 677: loss = 0.002890358311887392\n",
      "Iteration 678: loss = 0.002890177004942442\n",
      "Iteration 679: loss = 0.0028899958474204717\n",
      "Iteration 680: loss = 0.002889814839198332\n",
      "Iteration 681: loss = 0.0028896339801529566\n",
      "Iteration 682: loss = 0.002889453270161363\n",
      "Iteration 683: loss = 0.002889272709100651\n",
      "Iteration 684: loss = 0.002889092296848001\n",
      "Iteration 685: loss = 0.002888912033280679\n",
      "Iteration 686: loss = 0.0028887319182760306\n",
      "Iteration 687: loss = 0.002888551951711487\n",
      "Iteration 688: loss = 0.002888372133464559\n",
      "Iteration 689: loss = 0.0028881924634128428\n",
      "Iteration 690: loss = 0.002888012941434015\n",
      "Iteration 691: loss = 0.0028878335674058357\n",
      "Iteration 692: loss = 0.002887654341206147\n",
      "Iteration 693: loss = 0.002887475262712874\n",
      "Iteration 694: loss = 0.0028872963318040236\n",
      "Iteration 695: loss = 0.002887117548357685\n",
      "Iteration 696: loss = 0.0028869389122520308\n",
      "Iteration 697: loss = 0.002886760423365315\n",
      "Iteration 698: loss = 0.0028865820815758745\n",
      "Iteration 699: loss = 0.0028864038867621286\n",
      "Iteration 700: loss = 0.002886225838802578\n",
      "Iteration 701: loss = 0.002886047937575806\n",
      "Iteration 702: loss = 0.0028858701829604805\n",
      "Iteration 703: loss = 0.002885692574835348\n",
      "Iteration 704: loss = 0.0028855151130792395\n",
      "Iteration 705: loss = 0.0028853377975710678\n",
      "Iteration 706: loss = 0.0028851606281898264\n",
      "Iteration 707: loss = 0.002884983604814594\n",
      "Iteration 708: loss = 0.0028848067273245293\n",
      "Iteration 709: loss = 0.002884629995598873\n",
      "Iteration 710: loss = 0.002884453409516949\n",
      "Iteration 711: loss = 0.0028842769689581627\n",
      "Iteration 712: loss = 0.0028841006738020013\n",
      "Iteration 713: loss = 0.0028839245239280346\n",
      "Iteration 714: loss = 0.0028837485192159144\n",
      "Iteration 715: loss = 0.002883572659545374\n",
      "Iteration 716: loss = 0.0028833969447962294\n",
      "Iteration 717: loss = 0.002883221374848378\n",
      "Iteration 718: loss = 0.0028830459495817994\n",
      "Iteration 719: loss = 0.0028828706688765536\n",
      "Iteration 720: loss = 0.0028826955326127866\n",
      "Iteration 721: loss = 0.0028825205406707213\n",
      "Iteration 722: loss = 0.0028823456929306657\n",
      "Iteration 723: loss = 0.0028821709892730084\n",
      "Iteration 724: loss = 0.0028819964295782197\n",
      "Iteration 725: loss = 0.0028818220137268526\n",
      "Iteration 726: loss = 0.002881647741599541\n",
      "Iteration 727: loss = 0.0028814736130770013\n",
      "Iteration 728: loss = 0.002881299628040031\n",
      "Iteration 729: loss = 0.002881125786369509\n",
      "Iteration 730: loss = 0.002880952087946397\n",
      "Iteration 731: loss = 0.002880778532651737\n",
      "Iteration 732: loss = 0.002880605120366654\n",
      "Iteration 733: loss = 0.002880431850972354\n",
      "Iteration 734: loss = 0.002880258724350124\n",
      "Iteration 735: loss = 0.002880085740381333\n",
      "Iteration 736: loss = 0.0028799128989474313\n",
      "Iteration 737: loss = 0.002879740199929953\n",
      "Iteration 738: loss = 0.0028795676432105087\n",
      "Iteration 739: loss = 0.002879395228670796\n",
      "Iteration 740: loss = 0.0028792229561925907\n",
      "Iteration 741: loss = 0.002879050825657751\n",
      "Iteration 742: loss = 0.0028788788369482152\n",
      "Iteration 743: loss = 0.0028787069899460054\n",
      "Iteration 744: loss = 0.0028785352845332227\n",
      "Iteration 745: loss = 0.0028783637205920514\n",
      "Iteration 746: loss = 0.0028781922980047553\n",
      "Iteration 747: loss = 0.002878021016653681\n",
      "Iteration 748: loss = 0.0028778498764212563\n",
      "Iteration 749: loss = 0.0028776788771899893\n",
      "Iteration 750: loss = 0.0028775080188424697\n",
      "Iteration 751: loss = 0.002877337301261368\n",
      "Iteration 752: loss = 0.0028771667243294367\n",
      "Iteration 753: loss = 0.002876996287929509\n",
      "Iteration 754: loss = 0.0028768259919445\n",
      "Iteration 755: loss = 0.0028766558362574046\n",
      "Iteration 756: loss = 0.002876485820751299\n",
      "Iteration 757: loss = 0.002876315945309341\n",
      "Iteration 758: loss = 0.0028761462098147697\n",
      "Iteration 759: loss = 0.0028759766141509048\n",
      "Iteration 760: loss = 0.002875807158201146\n",
      "Iteration 761: loss = 0.0028756378418489754\n",
      "Iteration 762: loss = 0.002875468664977956\n",
      "Iteration 763: loss = 0.00287529962747173\n",
      "Iteration 764: loss = 0.002875130729214022\n",
      "Iteration 765: loss = 0.0028749619700886384\n",
      "Iteration 766: loss = 0.0028747933499794634\n",
      "Iteration 767: loss = 0.002874624868770465\n",
      "Iteration 768: loss = 0.002874456526345691\n",
      "Iteration 769: loss = 0.0028742883225892682\n",
      "Iteration 770: loss = 0.0028741202573854072\n",
      "Iteration 771: loss = 0.0028739523306183954\n",
      "Iteration 772: loss = 0.0028737845421726066\n",
      "Iteration 773: loss = 0.002873616891932489\n",
      "Iteration 774: loss = 0.0028734493797825758\n",
      "Iteration 775: loss = 0.0028732820056074785\n",
      "Iteration 776: loss = 0.00287311476929189\n",
      "Iteration 777: loss = 0.0028729476707205852\n",
      "Iteration 778: loss = 0.0028727807097784163\n",
      "Iteration 779: loss = 0.002872613886350319\n",
      "Iteration 780: loss = 0.002872447200321308\n",
      "Iteration 781: loss = 0.0028722806515764773\n",
      "Iteration 782: loss = 0.0028721142400010054\n",
      "Iteration 783: loss = 0.0028719479654801456\n",
      "Iteration 784: loss = 0.002871781827899237\n",
      "Iteration 785: loss = 0.0028716158271436952\n",
      "Iteration 786: loss = 0.0028714499630990184\n",
      "Iteration 787: loss = 0.002871284235650783\n",
      "Iteration 788: loss = 0.0028711186446846487\n",
      "Iteration 789: loss = 0.0028709531900863515\n",
      "Iteration 790: loss = 0.002870787871741711\n",
      "Iteration 791: loss = 0.0028706226895366263\n",
      "Iteration 792: loss = 0.002870457643357075\n",
      "Iteration 793: loss = 0.0028702927330891163\n",
      "Iteration 794: loss = 0.0028701279586188897\n",
      "Iteration 795: loss = 0.0028699633198326136\n",
      "Iteration 796: loss = 0.002869798816616588\n",
      "Iteration 797: loss = 0.0028696344488571904\n",
      "Iteration 798: loss = 0.002869470216440882\n",
      "Iteration 799: loss = 0.002869306119254201\n",
      "Iteration 800: loss = 0.002869142157183767\n",
      "Iteration 801: loss = 0.0028689783301162784\n",
      "Iteration 802: loss = 0.0028688146379385153\n",
      "Iteration 803: loss = 0.0028686510805373358\n",
      "Iteration 804: loss = 0.0028684876577996783\n",
      "Iteration 805: loss = 0.002868324369612562\n",
      "Iteration 806: loss = 0.0028681612158630854\n",
      "Iteration 807: loss = 0.002867998196438426\n",
      "Iteration 808: loss = 0.0028678353112258424\n",
      "Iteration 809: loss = 0.002867672560112672\n",
      "Iteration 810: loss = 0.0028675099429863317\n",
      "Iteration 811: loss = 0.0028673474597343197\n",
      "Iteration 812: loss = 0.002867185110244211\n",
      "Iteration 813: loss = 0.0028670228944036626\n",
      "Iteration 814: loss = 0.00286686081210041\n",
      "Iteration 815: loss = 0.0028666988632222693\n",
      "Iteration 816: loss = 0.002866537047657135\n",
      "Iteration 817: loss = 0.0028663753652929808\n",
      "Iteration 818: loss = 0.002866213816017862\n",
      "Iteration 819: loss = 0.002866052399719911\n",
      "Iteration 820: loss = 0.0028658911162873406\n",
      "Iteration 821: loss = 0.002865729965608443\n",
      "Iteration 822: loss = 0.00286556894757159\n",
      "Iteration 823: loss = 0.002865408062065232\n",
      "Iteration 824: loss = 0.0028652473089778997\n",
      "Iteration 825: loss = 0.0028650866881982026\n",
      "Iteration 826: loss = 0.0028649261996148284\n",
      "Iteration 827: loss = 0.0028647658431165458\n",
      "Iteration 828: loss = 0.0028646056185922023\n",
      "Iteration 829: loss = 0.0028644455259307228\n",
      "Iteration 830: loss = 0.0028642855650211144\n",
      "Iteration 831: loss = 0.0028641257357524606\n",
      "Iteration 832: loss = 0.0028639660380139257\n",
      "Iteration 833: loss = 0.002863806471694752\n",
      "Iteration 834: loss = 0.002863647036684261\n",
      "Iteration 835: loss = 0.0028634877328718544\n",
      "Iteration 836: loss = 0.0028633285601470104\n",
      "Iteration 837: loss = 0.002863169518399289\n",
      "Iteration 838: loss = 0.002863010607518327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 839: loss = 0.002862851827393841\n",
      "Iteration 840: loss = 0.002862693177915626\n",
      "Iteration 841: loss = 0.0028625346589735575\n",
      "Iteration 842: loss = 0.0028623762704575876\n",
      "Iteration 843: loss = 0.0028622180122577475\n",
      "Iteration 844: loss = 0.0028620598842641486\n",
      "Iteration 845: loss = 0.0028619018863669794\n",
      "Iteration 846: loss = 0.002861744018456508\n",
      "Iteration 847: loss = 0.0028615862804230808\n",
      "Iteration 848: loss = 0.002861428672157123\n",
      "Iteration 849: loss = 0.0028612711935491384\n",
      "Iteration 850: loss = 0.00286111384448971\n",
      "Iteration 851: loss = 0.0028609566248694986\n",
      "Iteration 852: loss = 0.002860799534579242\n",
      "Iteration 853: loss = 0.0028606425735097593\n",
      "Iteration 854: loss = 0.0028604857415519473\n",
      "Iteration 855: loss = 0.0028603290385967807\n",
      "Iteration 856: loss = 0.0028601724645353106\n",
      "Iteration 857: loss = 0.0028600160192586714\n",
      "Iteration 858: loss = 0.0028598597026580712\n",
      "Iteration 859: loss = 0.002859703514624799\n",
      "Iteration 860: loss = 0.0028595474550502206\n",
      "Iteration 861: loss = 0.002859391523825781\n",
      "Iteration 862: loss = 0.0028592357208430037\n",
      "Iteration 863: loss = 0.0028590800459934885\n",
      "Iteration 864: loss = 0.002858924499168916\n",
      "Iteration 865: loss = 0.0028587690802610428\n",
      "Iteration 866: loss = 0.002858613789161705\n",
      "Iteration 867: loss = 0.002858458625762816\n",
      "Iteration 868: loss = 0.002858303589956367\n",
      "Iteration 869: loss = 0.0028581486816344286\n",
      "Iteration 870: loss = 0.002857993900689147\n",
      "Iteration 871: loss = 0.002857839247012749\n",
      "Iteration 872: loss = 0.002857684720497538\n",
      "Iteration 873: loss = 0.002857530321035895\n",
      "Iteration 874: loss = 0.00285737604852028\n",
      "Iteration 875: loss = 0.002857221902843228\n",
      "Iteration 876: loss = 0.0028570678838973564\n",
      "Iteration 877: loss = 0.0028569139915753565\n",
      "Iteration 878: loss = 0.0028567602257699993\n",
      "Iteration 879: loss = 0.0028566065863741327\n",
      "Iteration 880: loss = 0.0028564530732806824\n",
      "Iteration 881: loss = 0.0028562996863826526\n",
      "Iteration 882: loss = 0.0028561464255731227\n",
      "Iteration 883: loss = 0.002855993290745253\n",
      "Iteration 884: loss = 0.0028558402817922793\n",
      "Iteration 885: loss = 0.002855687398607515\n",
      "Iteration 886: loss = 0.0028555346410843517\n",
      "Iteration 887: loss = 0.0028553820091162577\n",
      "Iteration 888: loss = 0.00285522950259678\n",
      "Iteration 889: loss = 0.0028550771214195412\n",
      "Iteration 890: loss = 0.002854924865478243\n",
      "Iteration 891: loss = 0.0028547727346666632\n",
      "Iteration 892: loss = 0.002854620728878658\n",
      "Iteration 893: loss = 0.0028544688480081595\n",
      "Iteration 894: loss = 0.002854317091949178\n",
      "Iteration 895: loss = 0.0028541654605958016\n",
      "Iteration 896: loss = 0.0028540139538421943\n",
      "Iteration 897: loss = 0.002853862571582597\n",
      "Iteration 898: loss = 0.0028537113137113313\n",
      "Iteration 899: loss = 0.0028535601801227893\n",
      "Iteration 900: loss = 0.002853409170711447\n",
      "Iteration 901: loss = 0.0028532582853718533\n",
      "Iteration 902: loss = 0.002853107523998636\n",
      "Iteration 903: loss = 0.0028529568864864973\n",
      "Iteration 904: loss = 0.0028528063727302202\n",
      "Iteration 905: loss = 0.002852655982624661\n",
      "Iteration 906: loss = 0.0028525057160647556\n",
      "Iteration 907: loss = 0.0028523555729455157\n",
      "Iteration 908: loss = 0.002852205553162029\n",
      "Iteration 909: loss = 0.0028520556566094604\n",
      "Iteration 910: loss = 0.002851905883183053\n",
      "Iteration 911: loss = 0.002851756232778124\n",
      "Iteration 912: loss = 0.0028516067052900703\n",
      "Iteration 913: loss = 0.0028514573006143628\n",
      "Iteration 914: loss = 0.0028513080186465516\n",
      "Iteration 915: loss = 0.00285115885928226\n",
      "Iteration 916: loss = 0.0028510098224171906\n",
      "Iteration 917: loss = 0.002850860907947122\n",
      "Iteration 918: loss = 0.002850712115767909\n",
      "Iteration 919: loss = 0.002850563445775483\n",
      "Iteration 920: loss = 0.0028504148978658514\n",
      "Iteration 921: loss = 0.002850266471935099\n",
      "Iteration 922: loss = 0.002850118167879385\n",
      "Iteration 923: loss = 0.0028499699855949476\n",
      "Iteration 924: loss = 0.0028498219249780987\n",
      "Iteration 925: loss = 0.0028496739859252295\n",
      "Iteration 926: loss = 0.002849526168332804\n",
      "Iteration 927: loss = 0.0028493784720973647\n",
      "Iteration 928: loss = 0.00284923089711553\n",
      "Iteration 929: loss = 0.0028490834432839934\n",
      "Iteration 930: loss = 0.002848936110499526\n",
      "Iteration 931: loss = 0.0028487888986589737\n",
      "Iteration 932: loss = 0.002848641807659259\n",
      "Iteration 933: loss = 0.0028484948373973806\n",
      "Iteration 934: loss = 0.002848347987770413\n",
      "Iteration 935: loss = 0.0028482012586755062\n",
      "Iteration 936: loss = 0.002848054650009887\n",
      "Iteration 937: loss = 0.0028479081616708574\n",
      "Iteration 938: loss = 0.0028477617935557956\n",
      "Iteration 939: loss = 0.0028476155455621553\n",
      "Iteration 940: loss = 0.002847469417587466\n",
      "Iteration 941: loss = 0.0028473234095293338\n",
      "Iteration 942: loss = 0.002847177521285439\n",
      "Iteration 943: loss = 0.00284703175275354\n",
      "Iteration 944: loss = 0.002846886103831467\n",
      "Iteration 945: loss = 0.00284674057441713\n",
      "Iteration 946: loss = 0.002846595164408512\n",
      "Iteration 947: loss = 0.0028464498737036724\n",
      "Iteration 948: loss = 0.002846304702200747\n",
      "Iteration 949: loss = 0.0028461596497979438\n",
      "Iteration 950: loss = 0.002846014716393551\n",
      "Iteration 951: loss = 0.002845869901885928\n",
      "Iteration 952: loss = 0.002845725206173512\n",
      "Iteration 953: loss = 0.0028455806291548163\n",
      "Iteration 954: loss = 0.0028454361707284264\n",
      "Iteration 955: loss = 0.0028452918307930054\n",
      "Iteration 956: loss = 0.002845147609247291\n",
      "Iteration 957: loss = 0.0028450035059900975\n",
      "Iteration 958: loss = 0.0028448595209203116\n",
      "Iteration 959: loss = 0.0028447156539368967\n",
      "Iteration 960: loss = 0.0028445719049388926\n",
      "Iteration 961: loss = 0.002844428273825413\n",
      "Iteration 962: loss = 0.0028442847604956455\n",
      "Iteration 963: loss = 0.0028441413648488544\n",
      "Iteration 964: loss = 0.0028439980867843782\n",
      "Iteration 965: loss = 0.002843854926201631\n",
      "Iteration 966: loss = 0.0028437118830001015\n",
      "Iteration 967: loss = 0.002843568957079352\n",
      "Iteration 968: loss = 0.0028434261483390234\n",
      "Iteration 969: loss = 0.0028432834566788265\n",
      "Iteration 970: loss = 0.0028431408819985502\n",
      "Iteration 971: loss = 0.002842998424198057\n",
      "Iteration 972: loss = 0.002842856083177286\n",
      "Iteration 973: loss = 0.0028427138588362484\n",
      "Iteration 974: loss = 0.0028425717510750298\n",
      "Iteration 975: loss = 0.0028424297597937926\n",
      "Iteration 976: loss = 0.002842287884892774\n",
      "Iteration 977: loss = 0.0028421461262722824\n",
      "Iteration 978: loss = 0.002842004483832705\n",
      "Iteration 979: loss = 0.0028418629574745004\n",
      "Iteration 980: loss = 0.0028417215470982033\n",
      "Iteration 981: loss = 0.0028415802526044216\n",
      "Iteration 982: loss = 0.002841439073893839\n",
      "Iteration 983: loss = 0.0028412980108672116\n",
      "Iteration 984: loss = 0.0028411570634253726\n",
      "Iteration 985: loss = 0.002841016231469226\n",
      "Iteration 986: loss = 0.002840875514899754\n",
      "Iteration 987: loss = 0.0028407349136180095\n",
      "Iteration 988: loss = 0.0028405944275251213\n",
      "Iteration 989: loss = 0.002840454056522293\n",
      "Iteration 990: loss = 0.0028403138005108\n",
      "Iteration 991: loss = 0.002840173659391994\n",
      "Iteration 992: loss = 0.0028400336330673\n",
      "Iteration 993: loss = 0.0028398937214382165\n",
      "Iteration 994: loss = 0.0028397539244063167\n",
      "Iteration 995: loss = 0.0028396142418732474\n",
      "Iteration 996: loss = 0.0028394746737407293\n",
      "Iteration 997: loss = 0.0028393352199105578\n",
      "Iteration 998: loss = 0.0028391958802846\n",
      "Iteration 999: loss = 0.002839056654764799\n",
      "Iteration 1000: loss = 0.0028389175432531714\n",
      "Iteration 1001: loss = 0.0028387785456518064\n",
      "Iteration 1002: loss = 0.0028386396618628683\n",
      "Iteration 1003: loss = 0.0028385008917885925\n",
      "Iteration 1004: loss = 0.0028383622353312914\n",
      "Iteration 1005: loss = 0.0028382236923933496\n",
      "Iteration 1006: loss = 0.0028380852628772236\n",
      "Iteration 1007: loss = 0.002837946946685446\n",
      "Iteration 1008: loss = 0.0028378087437206214\n",
      "Iteration 1009: loss = 0.002837670653885428\n",
      "Iteration 1010: loss = 0.002837532677082619\n",
      "Iteration 1011: loss = 0.0028373948132150184\n",
      "Iteration 1012: loss = 0.0028372570621855255\n",
      "Iteration 1013: loss = 0.0028371194238971118\n",
      "Iteration 1014: loss = 0.002836981898252823\n",
      "Iteration 1015: loss = 0.0028368444851557774\n",
      "Iteration 1016: loss = 0.0028367071845091666\n",
      "Iteration 1017: loss = 0.0028365699962162555\n",
      "Iteration 1018: loss = 0.0028364329201803823\n",
      "Iteration 1019: loss = 0.0028362959563049584\n",
      "Iteration 1020: loss = 0.0028361591044934673\n",
      "Iteration 1021: loss = 0.002836022364649467\n",
      "Iteration 1022: loss = 0.0028358857366765876\n",
      "Iteration 1023: loss = 0.0028357492204785315\n",
      "Iteration 1024: loss = 0.0028356128159590763\n",
      "Iteration 1025: loss = 0.0028354765230220705\n",
      "Iteration 1026: loss = 0.002835340341571436\n",
      "Iteration 1027: loss = 0.0028352042715111673\n",
      "Iteration 1028: loss = 0.0028350683127453323\n",
      "Iteration 1029: loss = 0.0028349324651780715\n",
      "Iteration 1030: loss = 0.002834796728713598\n",
      "Iteration 1031: loss = 0.002834661103256197\n",
      "Iteration 1032: loss = 0.0028345255887102273\n",
      "Iteration 1033: loss = 0.0028343901849801198\n",
      "Iteration 1034: loss = 0.0028342548919703774\n",
      "Iteration 1035: loss = 0.002834119709585578\n",
      "Iteration 1036: loss = 0.0028339846377303697\n",
      "Iteration 1037: loss = 0.0028338496763094727\n",
      "Iteration 1038: loss = 0.0028337148252276807\n",
      "Iteration 1039: loss = 0.0028335800843898606\n",
      "Iteration 1040: loss = 0.00283344545370095\n",
      "Iteration 1041: loss = 0.0028333109330659608\n",
      "Iteration 1042: loss = 0.0028331765223899743\n",
      "Iteration 1043: loss = 0.002833042221578147\n",
      "Iteration 1044: loss = 0.002832908030535706\n",
      "Iteration 1045: loss = 0.0028327739491679506\n",
      "Iteration 1046: loss = 0.0028326399773802536\n",
      "Iteration 1047: loss = 0.002832506115078058\n",
      "Iteration 1048: loss = 0.0028323723621668804\n",
      "Iteration 1049: loss = 0.002832238718552309\n",
      "Iteration 1050: loss = 0.002832105184140003\n",
      "Iteration 1051: loss = 0.0028319717588356956\n",
      "Iteration 1052: loss = 0.00283183844254519\n",
      "Iteration 1053: loss = 0.0028317052351743625\n",
      "Iteration 1054: loss = 0.0028315721366291616\n",
      "Iteration 1055: loss = 0.0028314391468156064\n",
      "Iteration 1056: loss = 0.0028313062656397875\n",
      "Iteration 1057: loss = 0.00283117349300787\n",
      "Iteration 1058: loss = 0.0028310408288260867\n",
      "Iteration 1059: loss = 0.0028309082730007453\n",
      "Iteration 1060: loss = 0.0028307758254382245\n",
      "Iteration 1061: loss = 0.0028306434860449737\n",
      "Iteration 1062: loss = 0.002830511254727514\n",
      "Iteration 1063: loss = 0.0028303791313924395\n",
      "Iteration 1064: loss = 0.002830247115946413\n",
      "Iteration 1065: loss = 0.0028301152082961735\n",
      "Iteration 1066: loss = 0.0028299834083485255\n",
      "Iteration 1067: loss = 0.0028298517160103495\n",
      "Iteration 1068: loss = 0.0028297201311885943\n",
      "Iteration 1069: loss = 0.0028295886537902833\n",
      "Iteration 1070: loss = 0.002829457283722508\n",
      "Iteration 1071: loss = 0.0028293260208924332\n",
      "Iteration 1072: loss = 0.002829194865207294\n",
      "Iteration 1073: loss = 0.0028290638165743972\n",
      "Iteration 1074: loss = 0.0028289328749011196\n",
      "Iteration 1075: loss = 0.0028288020400949114\n",
      "Iteration 1076: loss = 0.0028286713120632915\n",
      "Iteration 1077: loss = 0.0028285406907138507\n",
      "Iteration 1078: loss = 0.0028284101759542516\n",
      "Iteration 1079: loss = 0.0028282797676922266\n",
      "Iteration 1080: loss = 0.00282814946583558\n",
      "Iteration 1081: loss = 0.0028280192702921864\n",
      "Iteration 1082: loss = 0.0028278891809699904\n",
      "Iteration 1083: loss = 0.0028277591977770096\n",
      "Iteration 1084: loss = 0.0028276293206213306\n",
      "Iteration 1085: loss = 0.0028274995494111115\n",
      "Iteration 1086: loss = 0.0028273698840545806\n",
      "Iteration 1087: loss = 0.0028272403244600382\n",
      "Iteration 1088: loss = 0.002827110870535853\n",
      "Iteration 1089: loss = 0.0028269815221904668\n",
      "Iteration 1090: loss = 0.002826852279332389\n",
      "Iteration 1091: loss = 0.0028267231418702037\n",
      "Iteration 1092: loss = 0.0028265941097125606\n",
      "Iteration 1093: loss = 0.0028264651827681845\n",
      "Iteration 1094: loss = 0.002826336360945867\n",
      "Iteration 1095: loss = 0.002826207644154472\n",
      "Iteration 1096: loss = 0.002826079032302933\n",
      "Iteration 1097: loss = 0.0028259505253002545\n",
      "Iteration 1098: loss = 0.002825822123055511\n",
      "Iteration 1099: loss = 0.002825693825477846\n",
      "Iteration 1100: loss = 0.0028255656324764756\n",
      "Iteration 1101: loss = 0.002825437543960684\n",
      "Iteration 1102: loss = 0.0028253095598398266\n",
      "Iteration 1103: loss = 0.002825181680023329\n",
      "Iteration 1104: loss = 0.002825053904420685\n",
      "Iteration 1105: loss = 0.0028249262329414614\n",
      "Iteration 1106: loss = 0.002824798665495293\n",
      "Iteration 1107: loss = 0.0028246712019918845\n",
      "Iteration 1108: loss = 0.002824543842341011\n",
      "Iteration 1109: loss = 0.0028244165864525182\n",
      "Iteration 1110: loss = 0.0028242894342363203\n",
      "Iteration 1111: loss = 0.0028241623856024023\n",
      "Iteration 1112: loss = 0.002824035440460818\n",
      "Iteration 1113: loss = 0.0028239085987216916\n",
      "Iteration 1114: loss = 0.002823781860295217\n",
      "Iteration 1115: loss = 0.002823655225091658\n",
      "Iteration 1116: loss = 0.0028235286930213465\n",
      "Iteration 1117: loss = 0.0028234022639946864\n",
      "Iteration 1118: loss = 0.002823275937922149\n",
      "Iteration 1119: loss = 0.002823149714714276\n",
      "Iteration 1120: loss = 0.0028230235942816786\n",
      "Iteration 1121: loss = 0.002822897576535038\n",
      "Iteration 1122: loss = 0.0028227716613851025\n",
      "Iteration 1123: loss = 0.002822645848742693\n",
      "Iteration 1124: loss = 0.0028225201385186968\n",
      "Iteration 1125: loss = 0.0028223945306240722\n",
      "Iteration 1126: loss = 0.0028222690249698466\n",
      "Iteration 1127: loss = 0.0028221436214671165\n",
      "Iteration 1128: loss = 0.0028220183200270465\n",
      "Iteration 1129: loss = 0.002821893120560872\n",
      "Iteration 1130: loss = 0.002821768022979896\n",
      "Iteration 1131: loss = 0.0028216430271954915\n",
      "Iteration 1132: loss = 0.0028215181331191\n",
      "Iteration 1133: loss = 0.0028213933406622325\n",
      "Iteration 1134: loss = 0.002821268649736469\n",
      "Iteration 1135: loss = 0.0028211440602534574\n",
      "Iteration 1136: loss = 0.0028210195721249157\n",
      "Iteration 1137: loss = 0.0028208951852626307\n",
      "Iteration 1138: loss = 0.0028207708995784562\n",
      "Iteration 1139: loss = 0.002820646714984316\n",
      "Iteration 1140: loss = 0.0028205226313922035\n",
      "Iteration 1141: loss = 0.0028203986487141796\n",
      "Iteration 1142: loss = 0.002820274766862375\n",
      "Iteration 1143: loss = 0.0028201509857489868\n",
      "Iteration 1144: loss = 0.0028200273052862825\n",
      "Iteration 1145: loss = 0.002819903725386598\n",
      "Iteration 1146: loss = 0.0028197802459623378\n",
      "Iteration 1147: loss = 0.002819656866925973\n",
      "Iteration 1148: loss = 0.002819533588190046\n",
      "Iteration 1149: loss = 0.002819410409667165\n",
      "Iteration 1150: loss = 0.0028192873312700086\n",
      "Iteration 1151: loss = 0.002819164352911323\n",
      "Iteration 1152: loss = 0.0028190414745039205\n",
      "Iteration 1153: loss = 0.0028189186959606863\n",
      "Iteration 1154: loss = 0.002818796017194569\n",
      "Iteration 1155: loss = 0.002818673438118589\n",
      "Iteration 1156: loss = 0.002818550958645832\n",
      "Iteration 1157: loss = 0.002818428578689453\n",
      "Iteration 1158: loss = 0.0028183062981626764\n",
      "Iteration 1159: loss = 0.002818184116978792\n",
      "Iteration 1160: loss = 0.00281806203505116\n",
      "Iteration 1161: loss = 0.0028179400522932063\n",
      "Iteration 1162: loss = 0.0028178181686184275\n",
      "Iteration 1163: loss = 0.0028176963839403836\n",
      "Iteration 1164: loss = 0.002817574698172708\n",
      "Iteration 1165: loss = 0.0028174531112290973\n",
      "Iteration 1166: loss = 0.0028173316230233176\n",
      "Iteration 1167: loss = 0.002817210233469204\n",
      "Iteration 1168: loss = 0.002817088942480657\n",
      "Iteration 1169: loss = 0.0028169677499716456\n",
      "Iteration 1170: loss = 0.002816846655856207\n",
      "Iteration 1171: loss = 0.0028167256600484455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1172: loss = 0.0028166047624625322\n",
      "Iteration 1173: loss = 0.002816483963012707\n",
      "Iteration 1174: loss = 0.0028163632616132767\n",
      "Iteration 1175: loss = 0.0028162426581786145\n",
      "Iteration 1176: loss = 0.0028161221526231627\n",
      "Iteration 1177: loss = 0.0028160017448614297\n",
      "Iteration 1178: loss = 0.0028158814348079922\n",
      "Iteration 1179: loss = 0.002815761222377493\n",
      "Iteration 1180: loss = 0.0028156411074846425\n",
      "Iteration 1181: loss = 0.0028155210900442187\n",
      "Iteration 1182: loss = 0.0028154011699710667\n",
      "Iteration 1183: loss = 0.0028152813471800982\n",
      "Iteration 1184: loss = 0.002815161621586292\n",
      "Iteration 1185: loss = 0.0028150419931046947\n",
      "Iteration 1186: loss = 0.0028149224616504194\n",
      "Iteration 1187: loss = 0.002814803027138645\n",
      "Iteration 1188: loss = 0.00281468368948462\n",
      "Iteration 1189: loss = 0.002814564448603657\n",
      "Iteration 1190: loss = 0.002814445304411137\n",
      "Iteration 1191: loss = 0.002814326256822508\n",
      "Iteration 1192: loss = 0.002814207305753284\n",
      "Iteration 1193: loss = 0.0028140884511190446\n",
      "Iteration 1194: loss = 0.0028139696928354387\n",
      "Iteration 1195: loss = 0.00281385103081818\n",
      "Iteration 1196: loss = 0.00281373246498305\n",
      "Iteration 1197: loss = 0.0028136139952458965\n",
      "Iteration 1198: loss = 0.002813495621522632\n",
      "Iteration 1199: loss = 0.002813377343729238\n",
      "Iteration 1200: loss = 0.0028132591617817614\n",
      "Iteration 1201: loss = 0.0028131410755963143\n",
      "Iteration 1202: loss = 0.002813023085089078\n",
      "Iteration 1203: loss = 0.002812905190176298\n",
      "Iteration 1204: loss = 0.0028127873907742864\n",
      "Iteration 1205: loss = 0.002812669686799422\n",
      "Iteration 1206: loss = 0.0028125520781681498\n",
      "Iteration 1207: loss = 0.0028124345647969804\n",
      "Iteration 1208: loss = 0.0028123171466024922\n",
      "Iteration 1209: loss = 0.002812199823501327\n",
      "Iteration 1210: loss = 0.0028120825954101956\n",
      "Iteration 1211: loss = 0.0028119654622458723\n",
      "Iteration 1212: loss = 0.0028118484239251984\n",
      "Iteration 1213: loss = 0.0028117314803650824\n",
      "Iteration 1214: loss = 0.002811614631482498\n",
      "Iteration 1215: loss = 0.0028114978771944825\n",
      "Iteration 1216: loss = 0.002811381217418143\n",
      "Iteration 1217: loss = 0.0028112646520706483\n",
      "Iteration 1218: loss = 0.0028111481810692367\n",
      "Iteration 1219: loss = 0.00281103180433121\n",
      "Iteration 1220: loss = 0.002810915521773936\n",
      "Iteration 1221: loss = 0.002810799333314849\n",
      "Iteration 1222: loss = 0.0028106832388714487\n",
      "Iteration 1223: loss = 0.002810567238361299\n",
      "Iteration 1224: loss = 0.0028104513317020303\n",
      "Iteration 1225: loss = 0.00281033551881134\n",
      "Iteration 1226: loss = 0.0028102197996069883\n",
      "Iteration 1227: loss = 0.002810104174006802\n",
      "Iteration 1228: loss = 0.0028099886419286745\n",
      "Iteration 1229: loss = 0.002809873203290563\n",
      "Iteration 1230: loss = 0.0028097578580104894\n",
      "Iteration 1231: loss = 0.002809642606006543\n",
      "Iteration 1232: loss = 0.002809527447196877\n",
      "Iteration 1233: loss = 0.0028094123814997106\n",
      "Iteration 1234: loss = 0.0028092974088333267\n",
      "Iteration 1235: loss = 0.002809182529116074\n",
      "Iteration 1236: loss = 0.0028090677422663675\n",
      "Iteration 1237: loss = 0.002808953048202686\n",
      "Iteration 1238: loss = 0.0028088384468435736\n",
      "Iteration 1239: loss = 0.0028087239381076393\n",
      "Iteration 1240: loss = 0.0028086095219135567\n",
      "Iteration 1241: loss = 0.002808495198180065\n",
      "Iteration 1242: loss = 0.002808380966825968\n",
      "Iteration 1243: loss = 0.0028082668277701344\n",
      "Iteration 1244: loss = 0.002808152780931497\n",
      "Iteration 1245: loss = 0.0028080388262290536\n",
      "Iteration 1246: loss = 0.0028079249635818686\n",
      "Iteration 1247: loss = 0.0028078111929090675\n",
      "Iteration 1248: loss = 0.0028076975141298434\n",
      "Iteration 1249: loss = 0.0028075839271634534\n",
      "Iteration 1250: loss = 0.0028074704319292175\n",
      "Iteration 1251: loss = 0.0028073570283465217\n",
      "Iteration 1252: loss = 0.0028072437163348166\n",
      "Iteration 1253: loss = 0.002807130495813617\n",
      "Iteration 1254: loss = 0.002807017366702501\n",
      "Iteration 1255: loss = 0.0028069043289211124\n",
      "Iteration 1256: loss = 0.0028067913823891593\n",
      "Iteration 1257: loss = 0.0028066785270264133\n",
      "Iteration 1258: loss = 0.002806565762752711\n",
      "Iteration 1259: loss = 0.002806453089487952\n",
      "Iteration 1260: loss = 0.002806340507152101\n",
      "Iteration 1261: loss = 0.0028062280156651875\n",
      "Iteration 1262: loss = 0.0028061156149473037\n",
      "Iteration 1263: loss = 0.002806003304918607\n",
      "Iteration 1264: loss = 0.0028058910854993173\n",
      "Iteration 1265: loss = 0.00280577895660972\n",
      "Iteration 1266: loss = 0.0028056669181701632\n",
      "Iteration 1267: loss = 0.0028055549701010607\n",
      "Iteration 1268: loss = 0.0028054431123228883\n",
      "Iteration 1269: loss = 0.0028053313447561866\n",
      "Iteration 1270: loss = 0.0028052196673215594\n",
      "Iteration 1271: loss = 0.0028051080799396753\n",
      "Iteration 1272: loss = 0.0028049965825312647\n",
      "Iteration 1273: loss = 0.002804885175017123\n",
      "Iteration 1274: loss = 0.0028047738573181106\n",
      "Iteration 1275: loss = 0.002804662629355148\n",
      "Iteration 1276: loss = 0.0028045514910492223\n",
      "Iteration 1277: loss = 0.0028044404423213823\n",
      "Iteration 1278: loss = 0.002804329483092742\n",
      "Iteration 1279: loss = 0.002804218613284477\n",
      "Iteration 1280: loss = 0.0028041078328178274\n",
      "Iteration 1281: loss = 0.002803997141614096\n",
      "Iteration 1282: loss = 0.0028038865395946504\n",
      "Iteration 1283: loss = 0.0028037760266809188\n",
      "Iteration 1284: loss = 0.002803665602794395\n",
      "Iteration 1285: loss = 0.0028035552678566354\n",
      "Iteration 1286: loss = 0.002803445021789259\n",
      "Iteration 1287: loss = 0.002803334864513949\n",
      "Iteration 1288: loss = 0.0028032247959524498\n",
      "Iteration 1289: loss = 0.0028031148160265716\n",
      "Iteration 1290: loss = 0.002803004924658185\n",
      "Iteration 1291: loss = 0.002802895121769225\n",
      "Iteration 1292: loss = 0.0028027854072816883\n",
      "Iteration 1293: loss = 0.002802675781117637\n",
      "Iteration 1294: loss = 0.0028025662431991937\n",
      "Iteration 1295: loss = 0.0028024567934485435\n",
      "Iteration 1296: loss = 0.0028023474317879376\n",
      "Iteration 1297: loss = 0.0028022381581396852\n",
      "Iteration 1298: loss = 0.0028021289724261622\n",
      "Iteration 1299: loss = 0.0028020198745698057\n",
      "Iteration 1300: loss = 0.002801910864493114\n",
      "Iteration 1301: loss = 0.0028018019421186514\n",
      "Iteration 1302: loss = 0.0028016931073690417\n",
      "Iteration 1303: loss = 0.0028015843601669715\n",
      "Iteration 1304: loss = 0.002801475700435192\n",
      "Iteration 1305: loss = 0.002801367128096515\n",
      "Iteration 1306: loss = 0.002801258643073814\n",
      "Iteration 1307: loss = 0.0028011502452900275\n",
      "Iteration 1308: loss = 0.002801041934668154\n",
      "Iteration 1309: loss = 0.0028009337111312553\n",
      "Iteration 1310: loss = 0.0028008255746024553\n",
      "Iteration 1311: loss = 0.0028007175250049404\n",
      "Iteration 1312: loss = 0.0028006095622619583\n",
      "Iteration 1313: loss = 0.0028005016862968192\n",
      "Iteration 1314: loss = 0.002800393897032896\n",
      "Iteration 1315: loss = 0.0028002861943936226\n",
      "Iteration 1316: loss = 0.002800178578302497\n",
      "Iteration 1317: loss = 0.0028000710486830753\n",
      "Iteration 1318: loss = 0.0027999636054589798\n",
      "Iteration 1319: loss = 0.0027998562485538918\n",
      "Iteration 1320: loss = 0.0027997489778915563\n",
      "Iteration 1321: loss = 0.002799641793395778\n",
      "Iteration 1322: loss = 0.0027995346949904265\n",
      "Iteration 1323: loss = 0.0027994276825994294\n",
      "Iteration 1324: loss = 0.002799320756146779\n",
      "Iteration 1325: loss = 0.002799213915556528\n",
      "Iteration 1326: loss = 0.002799107160752791\n",
      "Iteration 1327: loss = 0.002799000491659744\n",
      "Iteration 1328: loss = 0.002798893908201624\n",
      "Iteration 1329: loss = 0.002798787410302731\n",
      "Iteration 1330: loss = 0.002798680997887425\n",
      "Iteration 1331: loss = 0.002798574670880129\n",
      "Iteration 1332: loss = 0.0027984684292053255\n",
      "Iteration 1333: loss = 0.0027983622727875595\n",
      "Iteration 1334: loss = 0.0027982562015514373\n",
      "Iteration 1335: loss = 0.002798150215421627\n",
      "Iteration 1336: loss = 0.0027980443143228555\n",
      "Iteration 1337: loss = 0.0027979384981799142\n",
      "Iteration 1338: loss = 0.002797832766917653\n",
      "Iteration 1339: loss = 0.0027977271204609857\n",
      "Iteration 1340: loss = 0.0027976215587348836\n",
      "Iteration 1341: loss = 0.0027975160816643817\n",
      "Iteration 1342: loss = 0.002797410689174576\n",
      "Iteration 1343: loss = 0.0027973053811906224\n",
      "Iteration 1344: loss = 0.0027972001576377376\n",
      "Iteration 1345: loss = 0.0027970950184412\n",
      "Iteration 1346: loss = 0.0027969899635263483\n",
      "Iteration 1347: loss = 0.0027968849928185833\n",
      "Iteration 1348: loss = 0.002796780106243364\n",
      "Iteration 1349: loss = 0.0027966753037262132\n",
      "Iteration 1350: loss = 0.002796570585192712\n",
      "Iteration 1351: loss = 0.002796465950568503\n",
      "Iteration 1352: loss = 0.0027963613997792898\n",
      "Iteration 1353: loss = 0.0027962569327508366\n",
      "Iteration 1354: loss = 0.0027961525494089676\n",
      "Iteration 1355: loss = 0.0027960482496795676\n",
      "Iteration 1356: loss = 0.002795944033488582\n",
      "Iteration 1357: loss = 0.002795839900762016\n",
      "Iteration 1358: loss = 0.0027957358514259364\n",
      "Iteration 1359: loss = 0.0027956318854064715\n",
      "Iteration 1360: loss = 0.002795528002629805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1361: loss = 0.0027954242030221866\n",
      "Iteration 1362: loss = 0.002795320486509923\n",
      "Iteration 1363: loss = 0.0027952168530193814\n",
      "Iteration 1364: loss = 0.0027951133024769897\n",
      "Iteration 1365: loss = 0.002795009834809237\n",
      "Iteration 1366: loss = 0.0027949064499426697\n",
      "Iteration 1367: loss = 0.0027948031478038965\n",
      "Iteration 1368: loss = 0.0027946999283195856\n",
      "Iteration 1369: loss = 0.0027945967914164652\n",
      "Iteration 1370: loss = 0.0027944937370213236\n",
      "Iteration 1371: loss = 0.002794390765061007\n",
      "Iteration 1372: loss = 0.0027942878754624255\n",
      "Iteration 1373: loss = 0.0027941850681525446\n",
      "Iteration 1374: loss = 0.0027940823430583927\n",
      "Iteration 1375: loss = 0.0027939797001070572\n",
      "Iteration 1376: loss = 0.002793877139225684\n",
      "Iteration 1377: loss = 0.00279377466034148\n",
      "Iteration 1378: loss = 0.0027936722633817115\n",
      "Iteration 1379: loss = 0.002793569948273704\n",
      "Iteration 1380: loss = 0.0027934677149448425\n",
      "Iteration 1381: loss = 0.0027933655633225717\n",
      "Iteration 1382: loss = 0.002793263493334396\n",
      "Iteration 1383: loss = 0.0027931615049078796\n",
      "Iteration 1384: loss = 0.002793059597970644\n",
      "Iteration 1385: loss = 0.002792957772450373\n",
      "Iteration 1386: loss = 0.002792856028274809\n",
      "Iteration 1387: loss = 0.00279275436537175\n",
      "Iteration 1388: loss = 0.0027926527836690594\n",
      "Iteration 1389: loss = 0.0027925512830946544\n",
      "Iteration 1390: loss = 0.0027924498635765152\n",
      "Iteration 1391: loss = 0.002792348525042679\n",
      "Iteration 1392: loss = 0.0027922472674212426\n",
      "Iteration 1393: loss = 0.0027921460906403612\n",
      "Iteration 1394: loss = 0.002792044994628251\n",
      "Iteration 1395: loss = 0.0027919439793131847\n",
      "Iteration 1396: loss = 0.0027918430446234955\n",
      "Iteration 1397: loss = 0.0027917421904875756\n",
      "Iteration 1398: loss = 0.002791641416833875\n",
      "Iteration 1399: loss = 0.0027915407235909032\n",
      "Iteration 1400: loss = 0.002791440110687228\n",
      "Iteration 1401: loss = 0.002791339578051477\n",
      "Iteration 1402: loss = 0.0027912391256123352\n",
      "Iteration 1403: loss = 0.002791138753298548\n",
      "Iteration 1404: loss = 0.0027910384610389163\n",
      "Iteration 1405: loss = 0.0027909382487623035\n",
      "Iteration 1406: loss = 0.0027908381163976284\n",
      "Iteration 1407: loss = 0.002790738063873871\n",
      "Iteration 1408: loss = 0.002790638091120067\n",
      "Iteration 1409: loss = 0.002790538198065312\n",
      "Iteration 1410: loss = 0.0027904383846387606\n",
      "Iteration 1411: loss = 0.0027903386507696242\n",
      "Iteration 1412: loss = 0.002790238996387174\n",
      "Iteration 1413: loss = 0.0027901394214207395\n",
      "Iteration 1414: loss = 0.0027900399257997062\n",
      "Iteration 1415: loss = 0.00278994050945352\n",
      "Iteration 1416: loss = 0.002789841172311685\n",
      "Iteration 1417: loss = 0.0027897419143037617\n",
      "Iteration 1418: loss = 0.002789642735359371\n",
      "Iteration 1419: loss = 0.0027895436354081903\n",
      "Iteration 1420: loss = 0.0027894446143799548\n",
      "Iteration 1421: loss = 0.0027893456722044587\n",
      "Iteration 1422: loss = 0.0027892468088115533\n",
      "Iteration 1423: loss = 0.0027891480241311493\n",
      "Iteration 1424: loss = 0.002789049318093213\n",
      "Iteration 1425: loss = 0.00278895069062777\n",
      "Iteration 1426: loss = 0.002788852141664903\n",
      "Iteration 1427: loss = 0.0027887536711347534\n",
      "Iteration 1428: loss = 0.0027886552789675202\n",
      "Iteration 1429: loss = 0.0027885569650934593\n",
      "Iteration 1430: loss = 0.002788458729442883\n",
      "Iteration 1431: loss = 0.0027883605719461646\n",
      "Iteration 1432: loss = 0.0027882624925337327\n",
      "Iteration 1433: loss = 0.0027881644911360733\n",
      "Iteration 1434: loss = 0.0027880665676837304\n",
      "Iteration 1435: loss = 0.0027879687221073067\n",
      "Iteration 1436: loss = 0.0027878709543374593\n",
      "Iteration 1437: loss = 0.0027877732643049056\n",
      "Iteration 1438: loss = 0.002787675651940418\n",
      "Iteration 1439: loss = 0.0027875781171748286\n",
      "Iteration 1440: loss = 0.002787480659939025\n",
      "Iteration 1441: loss = 0.0027873832801639524\n",
      "Iteration 1442: loss = 0.002787285977780613\n",
      "Iteration 1443: loss = 0.002787188752720067\n",
      "Iteration 1444: loss = 0.0027870916049134316\n",
      "Iteration 1445: loss = 0.0027869945342918795\n",
      "Iteration 1446: loss = 0.0027868975407866416\n",
      "Iteration 1447: loss = 0.002786800624329006\n",
      "Iteration 1448: loss = 0.0027867037848503177\n",
      "Iteration 1449: loss = 0.0027866070222819783\n",
      "Iteration 1450: loss = 0.0027865103365554453\n",
      "Iteration 1451: loss = 0.0027864137276022356\n",
      "Iteration 1452: loss = 0.0027863171953539205\n",
      "Iteration 1453: loss = 0.0027862207397421285\n",
      "Iteration 1454: loss = 0.0027861243606985466\n",
      "Iteration 1455: loss = 0.0027860280581549156\n",
      "Iteration 1456: loss = 0.0027859318320430366\n",
      "Iteration 1457: loss = 0.0027858356822947623\n",
      "Iteration 1458: loss = 0.0027857396088420065\n",
      "Iteration 1459: loss = 0.0027856436116167377\n",
      "Iteration 1460: loss = 0.0027855476905509816\n",
      "Iteration 1461: loss = 0.002785451845576819\n",
      "Iteration 1462: loss = 0.0027853560766263876\n",
      "Iteration 1463: loss = 0.002785260383631883\n",
      "Iteration 1464: loss = 0.0027851647665255554\n",
      "Iteration 1465: loss = 0.0027850692252397113\n",
      "Iteration 1466: loss = 0.002784973759706715\n",
      "Iteration 1467: loss = 0.0027848783698589848\n",
      "Iteration 1468: loss = 0.002784783055628998\n",
      "Iteration 1469: loss = 0.0027846878169492852\n",
      "Iteration 1470: loss = 0.002784592653752435\n",
      "Iteration 1471: loss = 0.002784497565971091\n",
      "Iteration 1472: loss = 0.0027844025535379542\n",
      "Iteration 1473: loss = 0.00278430761638578\n",
      "Iteration 1474: loss = 0.00278421275444738\n",
      "Iteration 1475: loss = 0.0027841179676556236\n",
      "Iteration 1476: loss = 0.002784023255943433\n",
      "Iteration 1477: loss = 0.0027839286192437892\n",
      "Iteration 1478: loss = 0.0027838340574897276\n",
      "Iteration 1479: loss = 0.0027837395706143387\n",
      "Iteration 1480: loss = 0.0027836451585507706\n",
      "Iteration 1481: loss = 0.0027835508212322247\n",
      "Iteration 1482: loss = 0.0027834565585919606\n",
      "Iteration 1483: loss = 0.0027833623705632916\n",
      "Iteration 1484: loss = 0.0027832682570795883\n",
      "Iteration 1485: loss = 0.0027831742180742746\n",
      "Iteration 1486: loss = 0.0027830802534808317\n",
      "Iteration 1487: loss = 0.0027829863632327953\n",
      "Iteration 1488: loss = 0.002782892547263758\n",
      "Iteration 1489: loss = 0.002782798805507365\n",
      "Iteration 1490: loss = 0.0027827051378973206\n",
      "Iteration 1491: loss = 0.002782611544367381\n",
      "Iteration 1492: loss = 0.0027825180248513604\n",
      "Iteration 1493: loss = 0.0027824245792831253\n",
      "Iteration 1494: loss = 0.002782331207596599\n",
      "Iteration 1495: loss = 0.0027822379097257625\n",
      "Iteration 1496: loss = 0.0027821446856046463\n",
      "Iteration 1497: loss = 0.0027820515351673412\n",
      "Iteration 1498: loss = 0.002781958458347991\n",
      "Iteration 1499: loss = 0.002781865455080793\n",
      "Iteration 1500: loss = 0.0027817725253000023\n",
      "Iteration 1501: loss = 0.002781679668939927\n",
      "Iteration 1502: loss = 0.0027815868859349304\n",
      "Iteration 1503: loss = 0.002781494176219432\n",
      "Iteration 1504: loss = 0.0027814015397279045\n",
      "Iteration 1505: loss = 0.0027813089763948757\n",
      "Iteration 1506: loss = 0.0027812164861549292\n",
      "Iteration 1507: loss = 0.002781124068942702\n",
      "Iteration 1508: loss = 0.002781031724692887\n",
      "Iteration 1509: loss = 0.00278093945334023\n",
      "Iteration 1510: loss = 0.0027808472548195334\n",
      "Iteration 1511: loss = 0.002780755129065653\n",
      "Iteration 1512: loss = 0.0027806630760134985\n",
      "Iteration 1513: loss = 0.002780571095598037\n",
      "Iteration 1514: loss = 0.0027804791877542867\n",
      "Iteration 1515: loss = 0.0027803873524173215\n",
      "Iteration 1516: loss = 0.0027802955895222694\n",
      "Iteration 1517: loss = 0.0027802038990043143\n",
      "Iteration 1518: loss = 0.0027801122807986915\n",
      "Iteration 1519: loss = 0.002780020734840694\n",
      "Iteration 1520: loss = 0.002779929261065667\n",
      "Iteration 1521: loss = 0.0027798378594090078\n",
      "Iteration 1522: loss = 0.0027797465298061725\n",
      "Iteration 1523: loss = 0.0027796552721926684\n",
      "Iteration 1524: loss = 0.0027795640865040574\n",
      "Iteration 1525: loss = 0.002779472972675956\n",
      "Iteration 1526: loss = 0.0027793819306440337\n",
      "Iteration 1527: loss = 0.0027792909603440146\n",
      "Iteration 1528: loss = 0.002779200061711676\n",
      "Iteration 1529: loss = 0.0027791092346828505\n",
      "Iteration 1530: loss = 0.0027790184791934235\n",
      "Iteration 1531: loss = 0.0027789277951793356\n",
      "Iteration 1532: loss = 0.0027788371825765777\n",
      "Iteration 1533: loss = 0.0027787466413211983\n",
      "Iteration 1534: loss = 0.0027786561713492976\n",
      "Iteration 1535: loss = 0.002778565772597031\n",
      "Iteration 1536: loss = 0.002778475445000605\n",
      "Iteration 1537: loss = 0.002778385188496282\n",
      "Iteration 1538: loss = 0.002778295003020376\n",
      "Iteration 1539: loss = 0.0027782048885092573\n",
      "Iteration 1540: loss = 0.002778114844899348\n",
      "Iteration 1541: loss = 0.002778024872127122\n",
      "Iteration 1542: loss = 0.0027779349701291096\n",
      "Iteration 1543: loss = 0.002777845138841893\n",
      "Iteration 1544: loss = 0.0027777553782021075\n",
      "Iteration 1545: loss = 0.002777665688146441\n",
      "Iteration 1546: loss = 0.002777576068611638\n",
      "Iteration 1547: loss = 0.0027774865195344927\n",
      "Iteration 1548: loss = 0.0027773970408518536\n",
      "Iteration 1549: loss = 0.0027773076325006227\n",
      "Iteration 1550: loss = 0.0027772182944177546\n",
      "Iteration 1551: loss = 0.0027771290265402575\n",
      "Iteration 1552: loss = 0.0027770398288051927\n",
      "Iteration 1553: loss = 0.0027769507011496733\n",
      "Iteration 1554: loss = 0.0027768616435108664\n",
      "Iteration 1555: loss = 0.0027767726558259923\n",
      "Iteration 1556: loss = 0.002776683738032323\n",
      "Iteration 1557: loss = 0.002776594890067185\n",
      "Iteration 1558: loss = 0.002776506111867956\n",
      "Iteration 1559: loss = 0.0027764174033720677\n",
      "Iteration 1560: loss = 0.002776328764517003\n",
      "Iteration 1561: loss = 0.0027762401952402993\n",
      "Iteration 1562: loss = 0.0027761516954795456\n",
      "Iteration 1563: loss = 0.0027760632651723837\n",
      "Iteration 1564: loss = 0.0027759749042565074\n",
      "Iteration 1565: loss = 0.0027758866126696654\n",
      "Iteration 1566: loss = 0.0027757983903496553\n",
      "Iteration 1567: loss = 0.00277571023723433\n",
      "Iteration 1568: loss = 0.0027756221532615945\n",
      "Iteration 1569: loss = 0.002775534138369404\n",
      "Iteration 1570: loss = 0.0027754461924957693\n",
      "Iteration 1571: loss = 0.002775358315578751\n",
      "Iteration 1572: loss = 0.002775270507556463\n",
      "Iteration 1573: loss = 0.0027751827683670715\n",
      "Iteration 1574: loss = 0.002775095097948794\n",
      "Iteration 1575: loss = 0.002775007496239903\n",
      "Iteration 1576: loss = 0.0027749199631787185\n",
      "Iteration 1577: loss = 0.002774832498703618\n",
      "Iteration 1578: loss = 0.0027747451027530256\n",
      "Iteration 1579: loss = 0.002774657775265421\n",
      "Iteration 1580: loss = 0.002774570516179336\n",
      "Iteration 1581: loss = 0.0027744833254333516\n",
      "Iteration 1582: loss = 0.0027743962029661037\n",
      "Iteration 1583: loss = 0.002774309148716279\n",
      "Iteration 1584: loss = 0.002774222162622615\n",
      "Iteration 1585: loss = 0.0027741352446239024\n",
      "Iteration 1586: loss = 0.002774048394658983\n",
      "Iteration 1587: loss = 0.0027739616126667504\n",
      "Iteration 1588: loss = 0.0027738748985861508\n",
      "Iteration 1589: loss = 0.0027737882523561803\n",
      "Iteration 1590: loss = 0.0027737016739158883\n",
      "Iteration 1591: loss = 0.002773615163204374\n",
      "Iteration 1592: loss = 0.002773528720160791\n",
      "Iteration 1593: loss = 0.002773442344724342\n",
      "Iteration 1594: loss = 0.0027733560368342807\n",
      "Iteration 1595: loss = 0.002773269796429915\n",
      "Iteration 1596: loss = 0.0027731836234506013\n",
      "Iteration 1597: loss = 0.00277309751783575\n",
      "Iteration 1598: loss = 0.00277301147952482\n",
      "Iteration 1599: loss = 0.0027729255084573243\n",
      "Iteration 1600: loss = 0.0027728396045728246\n",
      "Iteration 1601: loss = 0.002772753767810937\n",
      "Iteration 1602: loss = 0.0027726679981113244\n",
      "Iteration 1603: loss = 0.002772582295413705\n",
      "Iteration 1604: loss = 0.0027724966596578463\n",
      "Iteration 1605: loss = 0.0027724110907835662\n",
      "Iteration 1606: loss = 0.0027723255887307347\n",
      "Iteration 1607: loss = 0.002772240153439273\n",
      "Iteration 1608: loss = 0.002772154784849153\n",
      "Iteration 1609: loss = 0.002772069482900397\n",
      "Iteration 1610: loss = 0.0027719842475330777\n",
      "Iteration 1611: loss = 0.0027718990786873205\n",
      "Iteration 1612: loss = 0.002771813976303301\n",
      "Iteration 1613: loss = 0.0027717289403212443\n",
      "Iteration 1614: loss = 0.002771643970681427\n",
      "Iteration 1615: loss = 0.0027715590673241773\n",
      "Iteration 1616: loss = 0.0027714742301898727\n",
      "Iteration 1617: loss = 0.0027713894592189424\n",
      "Iteration 1618: loss = 0.0027713047543518657\n",
      "Iteration 1619: loss = 0.0027712201155291726\n",
      "Iteration 1620: loss = 0.002771135542691444\n",
      "Iteration 1621: loss = 0.0027710510357793097\n",
      "Iteration 1622: loss = 0.0027709665947334514\n",
      "Iteration 1623: loss = 0.0027708822194946023\n",
      "Iteration 1624: loss = 0.0027707979100035423\n",
      "Iteration 1625: loss = 0.002770713666201106\n",
      "Iteration 1626: loss = 0.0027706294880281757\n",
      "Iteration 1627: loss = 0.002770545375425685\n",
      "Iteration 1628: loss = 0.0027704613283346152\n",
      "Iteration 1629: loss = 0.0027703773466960027\n",
      "Iteration 1630: loss = 0.0027702934304509293\n",
      "Iteration 1631: loss = 0.0027702095795405293\n",
      "Iteration 1632: loss = 0.002770125793905988\n",
      "Iteration 1633: loss = 0.0027700420734885376\n",
      "Iteration 1634: loss = 0.0027699584182294627\n",
      "Iteration 1635: loss = 0.0027698748280700985\n",
      "Iteration 1636: loss = 0.002769791302951827\n",
      "Iteration 1637: loss = 0.002769707842816084\n",
      "Iteration 1638: loss = 0.002769624447604352\n",
      "Iteration 1639: loss = 0.0027695411172581653\n",
      "Iteration 1640: loss = 0.0027694578517191075\n",
      "Iteration 1641: loss = 0.002769374650928811\n",
      "Iteration 1642: loss = 0.002769291514828959\n",
      "Iteration 1643: loss = 0.002769208443361285\n",
      "Iteration 1644: loss = 0.0027691254364675703\n",
      "Iteration 1645: loss = 0.0027690424940896466\n",
      "Iteration 1646: loss = 0.002768959616169396\n",
      "Iteration 1647: loss = 0.00276887680264875\n",
      "Iteration 1648: loss = 0.0027687940534696876\n",
      "Iteration 1649: loss = 0.0027687113685742407\n",
      "Iteration 1650: loss = 0.002768628747904487\n",
      "Iteration 1651: loss = 0.0027685461914025568\n",
      "Iteration 1652: loss = 0.002768463699010628\n",
      "Iteration 1653: loss = 0.002768381270670928\n",
      "Iteration 1654: loss = 0.0027682989063257334\n",
      "Iteration 1655: loss = 0.0027682166059173705\n",
      "Iteration 1656: loss = 0.002768134369388215\n",
      "Iteration 1657: loss = 0.0027680521966806917\n",
      "Iteration 1658: loss = 0.0027679700877372734\n",
      "Iteration 1659: loss = 0.002767888042500484\n",
      "Iteration 1660: loss = 0.0027678060609128948\n",
      "Iteration 1661: loss = 0.002767724142917127\n",
      "Iteration 1662: loss = 0.00276764228845585\n",
      "Iteration 1663: loss = 0.0027675604974717843\n",
      "Iteration 1664: loss = 0.0027674787699076956\n",
      "Iteration 1665: loss = 0.002767397105706403\n",
      "Iteration 1666: loss = 0.00276731550481077\n",
      "Iteration 1667: loss = 0.002767233967163713\n",
      "Iteration 1668: loss = 0.0027671524927081947\n",
      "Iteration 1669: loss = 0.0027670710813872262\n",
      "Iteration 1670: loss = 0.00276698973314387\n",
      "Iteration 1671: loss = 0.0027669084479212337\n",
      "Iteration 1672: loss = 0.002766827225662477\n",
      "Iteration 1673: loss = 0.002766746066310806\n",
      "Iteration 1674: loss = 0.002766664969809476\n",
      "Iteration 1675: loss = 0.002766583936101791\n",
      "Iteration 1676: loss = 0.002766502965131104\n",
      "Iteration 1677: loss = 0.0027664220568408148\n",
      "Iteration 1678: loss = 0.002766341211174373\n",
      "Iteration 1679: loss = 0.0027662604280752775\n",
      "Iteration 1680: loss = 0.002766179707487072\n",
      "Iteration 1681: loss = 0.0027660990493533536\n",
      "Iteration 1682: loss = 0.002766018453617763\n",
      "Iteration 1683: loss = 0.002765937920223992\n",
      "Iteration 1684: loss = 0.0027658574491157798\n",
      "Iteration 1685: loss = 0.0027657770402369146\n",
      "Iteration 1686: loss = 0.002765696693531231\n",
      "Iteration 1687: loss = 0.002765616408942612\n",
      "Iteration 1688: loss = 0.0027655361864149905\n",
      "Iteration 1689: loss = 0.0027654560258923463\n",
      "Iteration 1690: loss = 0.002765375927318707\n",
      "Iteration 1691: loss = 0.0027652958906381475\n",
      "Iteration 1692: loss = 0.002765215915794793\n",
      "Iteration 1693: loss = 0.0027651360027328145\n",
      "Iteration 1694: loss = 0.0027650561513964315\n",
      "Iteration 1695: loss = 0.002764976361729912\n",
      "Iteration 1696: loss = 0.0027648966336775694\n",
      "Iteration 1697: loss = 0.002764816967183768\n",
      "Iteration 1698: loss = 0.0027647373621929195\n",
      "Iteration 1699: loss = 0.0027646578186494796\n",
      "Iteration 1700: loss = 0.002764578336497956\n",
      "Iteration 1701: loss = 0.002764498915682903\n",
      "Iteration 1702: loss = 0.00276441955614892\n",
      "Iteration 1703: loss = 0.002764340257840657\n",
      "Iteration 1704: loss = 0.00276426102070281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1705: loss = 0.002764181844680123\n",
      "Iteration 1706: loss = 0.0027641027297173864\n",
      "Iteration 1707: loss = 0.0027640236757594397\n",
      "Iteration 1708: loss = 0.0027639446827511685\n",
      "Iteration 1709: loss = 0.0027638657506375073\n",
      "Iteration 1710: loss = 0.0027637868793634348\n",
      "Iteration 1711: loss = 0.0027637080688739804\n",
      "Iteration 1712: loss = 0.0027636293191142186\n",
      "Iteration 1713: loss = 0.002763550630029272\n",
      "Iteration 1714: loss = 0.0027634720015643098\n",
      "Iteration 1715: loss = 0.00276339343366455\n",
      "Iteration 1716: loss = 0.002763314926275255\n",
      "Iteration 1717: loss = 0.002763236479341736\n",
      "Iteration 1718: loss = 0.002763158092809351\n",
      "Iteration 1719: loss = 0.0027630797666235046\n",
      "Iteration 1720: loss = 0.0027630015007296485\n",
      "Iteration 1721: loss = 0.0027629232950732822\n",
      "Iteration 1722: loss = 0.0027628451495999512\n",
      "Iteration 1723: loss = 0.002762767064255247\n",
      "Iteration 1724: loss = 0.0027626890389848093\n",
      "Iteration 1725: loss = 0.002762611073734325\n",
      "Iteration 1726: loss = 0.002762533168449526\n",
      "Iteration 1727: loss = 0.002762455323076192\n",
      "Iteration 1728: loss = 0.0027623775375601504\n",
      "Iteration 1729: loss = 0.002762299811847272\n",
      "Iteration 1730: loss = 0.0027622221458834775\n",
      "Iteration 1731: loss = 0.0027621445396147325\n",
      "Iteration 1732: loss = 0.00276206699298705\n",
      "Iteration 1733: loss = 0.0027619895059464885\n",
      "Iteration 1734: loss = 0.0027619120784391537\n",
      "Iteration 1735: loss = 0.0027618347104111985\n",
      "Iteration 1736: loss = 0.00276175740180882\n",
      "Iteration 1737: loss = 0.002761680152578263\n",
      "Iteration 1738: loss = 0.002761602962665819\n",
      "Iteration 1739: loss = 0.002761525832017825\n",
      "Iteration 1740: loss = 0.0027614487605806663\n",
      "Iteration 1741: loss = 0.00276137174830077\n",
      "Iteration 1742: loss = 0.002761294795124614\n",
      "Iteration 1743: loss = 0.00276121790099872\n",
      "Iteration 1744: loss = 0.002761141065869656\n",
      "Iteration 1745: loss = 0.002761064289684037\n",
      "Iteration 1746: loss = 0.002760987572388523\n",
      "Iteration 1747: loss = 0.0027609109139298204\n",
      "Iteration 1748: loss = 0.002760834314254681\n",
      "Iteration 1749: loss = 0.0027607577733099044\n",
      "Iteration 1750: loss = 0.002760681291042335\n",
      "Iteration 1751: loss = 0.0027606048673988613\n",
      "Iteration 1752: loss = 0.0027605285023264213\n",
      "Iteration 1753: loss = 0.002760452195771995\n",
      "Iteration 1754: loss = 0.002760375947682612\n",
      "Iteration 1755: loss = 0.0027602997580053437\n",
      "Iteration 1756: loss = 0.0027602236266873106\n",
      "Iteration 1757: loss = 0.002760147553675676\n",
      "Iteration 1758: loss = 0.0027600715389176516\n",
      "Iteration 1759: loss = 0.002759995582360492\n",
      "Iteration 1760: loss = 0.0027599196839515005\n",
      "Iteration 1761: loss = 0.002759843843638023\n",
      "Iteration 1762: loss = 0.002759768061367452\n",
      "Iteration 1763: loss = 0.002759692337087225\n",
      "Iteration 1764: loss = 0.002759616670744827\n",
      "Iteration 1765: loss = 0.0027595410622877864\n",
      "Iteration 1766: loss = 0.0027594655116636763\n",
      "Iteration 1767: loss = 0.0027593900188201165\n",
      "Iteration 1768: loss = 0.0027593145837047726\n",
      "Iteration 1769: loss = 0.0027592392062653548\n",
      "Iteration 1770: loss = 0.002759163886449617\n",
      "Iteration 1771: loss = 0.0027590886242053613\n",
      "Iteration 1772: loss = 0.002759013419480433\n",
      "Iteration 1773: loss = 0.002758938272222722\n",
      "Iteration 1774: loss = 0.002758863182380164\n",
      "Iteration 1775: loss = 0.002758788149900742\n",
      "Iteration 1776: loss = 0.00275871317473248\n",
      "Iteration 1777: loss = 0.002758638256823449\n",
      "Iteration 1778: loss = 0.002758563396121766\n",
      "Iteration 1779: loss = 0.0027584885925755905\n",
      "Iteration 1780: loss = 0.0027584138461331288\n",
      "Iteration 1781: loss = 0.0027583391567426317\n",
      "Iteration 1782: loss = 0.0027582645243523943\n",
      "Iteration 1783: loss = 0.002758189948910756\n",
      "Iteration 1784: loss = 0.002758115430366103\n",
      "Iteration 1785: loss = 0.0027580409686668635\n",
      "Iteration 1786: loss = 0.0027579665637615124\n",
      "Iteration 1787: loss = 0.002757892215598569\n",
      "Iteration 1788: loss = 0.002757817924126596\n",
      "Iteration 1789: loss = 0.002757743689294202\n",
      "Iteration 1790: loss = 0.002757669511050039\n",
      "Iteration 1791: loss = 0.0027575953893428052\n",
      "Iteration 1792: loss = 0.0027575213241212418\n",
      "Iteration 1793: loss = 0.0027574473153341335\n",
      "Iteration 1794: loss = 0.0027573733629303126\n",
      "Iteration 1795: loss = 0.002757299466858653\n",
      "Iteration 1796: loss = 0.0027572256270680744\n",
      "Iteration 1797: loss = 0.002757151843507539\n",
      "Iteration 1798: loss = 0.0027570781161260567\n",
      "Iteration 1799: loss = 0.0027570044448726773\n",
      "Iteration 1800: loss = 0.0027569308296964976\n",
      "Iteration 1801: loss = 0.0027568572705466586\n",
      "Iteration 1802: loss = 0.002756783767372345\n",
      "Iteration 1803: loss = 0.0027567103201227836\n",
      "Iteration 1804: loss = 0.0027566369287472495\n",
      "Iteration 1805: loss = 0.002756563593195057\n",
      "Iteration 1806: loss = 0.0027564903134155686\n",
      "Iteration 1807: loss = 0.0027564170893581877\n",
      "Iteration 1808: loss = 0.002756343920972364\n",
      "Iteration 1809: loss = 0.0027562708082075893\n",
      "Iteration 1810: loss = 0.0027561977510134\n",
      "Iteration 1811: loss = 0.0027561247493393766\n",
      "Iteration 1812: loss = 0.002756051803135142\n",
      "Iteration 1813: loss = 0.002755978912350366\n",
      "Iteration 1814: loss = 0.0027559060769347583\n",
      "Iteration 1815: loss = 0.0027558332968380746\n",
      "Iteration 1816: loss = 0.002755760572010114\n",
      "Iteration 1817: loss = 0.002755687902400719\n",
      "Iteration 1818: loss = 0.0027556152879597753\n",
      "Iteration 1819: loss = 0.0027555427286372128\n",
      "Iteration 1820: loss = 0.0027554702243830047\n",
      "Iteration 1821: loss = 0.0027553977751471675\n",
      "Iteration 1822: loss = 0.0027553253808797625\n",
      "Iteration 1823: loss = 0.002755253041530891\n",
      "Iteration 1824: loss = 0.002755180757050701\n",
      "Iteration 1825: loss = 0.002755108527389384\n",
      "Iteration 1826: loss = 0.002755036352497172\n",
      "Iteration 1827: loss = 0.0027549642323243433\n",
      "Iteration 1828: loss = 0.0027548921668212167\n",
      "Iteration 1829: loss = 0.002754820155938157\n",
      "Iteration 1830: loss = 0.0027547481996255704\n",
      "Iteration 1831: loss = 0.002754676297833906\n",
      "Iteration 1832: loss = 0.002754604450513658\n",
      "Iteration 1833: loss = 0.002754532657615361\n",
      "Iteration 1834: loss = 0.002754460919089596\n",
      "Iteration 1835: loss = 0.0027543892348869834\n",
      "Iteration 1836: loss = 0.002754317604958189\n",
      "Iteration 1837: loss = 0.002754246029253921\n",
      "Iteration 1838: loss = 0.0027541745077249305\n",
      "Iteration 1839: loss = 0.002754103040322011\n",
      "Iteration 1840: loss = 0.002754031626996\n",
      "Iteration 1841: loss = 0.002753960267697776\n",
      "Iteration 1842: loss = 0.002753888962378263\n",
      "Iteration 1843: loss = 0.0027538177109884244\n",
      "Iteration 1844: loss = 0.00275374651347927\n",
      "Iteration 1845: loss = 0.002753675369801849\n",
      "Iteration 1846: loss = 0.002753604279907255\n",
      "Iteration 1847: loss = 0.0027535332437466247\n",
      "Iteration 1848: loss = 0.0027534622612711357\n",
      "Iteration 1849: loss = 0.00275339133243201\n",
      "Iteration 1850: loss = 0.0027533204571805105\n",
      "Iteration 1851: loss = 0.002753249635467943\n",
      "Iteration 1852: loss = 0.002753178867245658\n",
      "Iteration 1853: loss = 0.002753108152465044\n",
      "Iteration 1854: loss = 0.002753037491077536\n",
      "Iteration 1855: loss = 0.0027529668830346098\n",
      "Iteration 1856: loss = 0.002752896328287783\n",
      "Iteration 1857: loss = 0.0027528258267886163\n",
      "Iteration 1858: loss = 0.0027527553784887126\n",
      "Iteration 1859: loss = 0.002752684983339716\n",
      "Iteration 1860: loss = 0.0027526146412933153\n",
      "Iteration 1861: loss = 0.0027525443523012394\n",
      "Iteration 1862: loss = 0.002752474116315258\n",
      "Iteration 1863: loss = 0.002752403933287187\n",
      "Iteration 1864: loss = 0.002752333803168881\n",
      "Iteration 1865: loss = 0.002752263725912238\n",
      "Iteration 1866: loss = 0.002752193701469197\n",
      "Iteration 1867: loss = 0.002752123729791741\n",
      "Iteration 1868: loss = 0.002752053810831893\n",
      "Iteration 1869: loss = 0.0027519839445417182\n",
      "Iteration 1870: loss = 0.0027519141308733246\n",
      "Iteration 1871: loss = 0.002751844369778861\n",
      "Iteration 1872: loss = 0.0027517746612105184\n",
      "Iteration 1873: loss = 0.0027517050051205304\n",
      "Iteration 1874: loss = 0.002751635401461172\n",
      "Iteration 1875: loss = 0.002751565850184758\n",
      "Iteration 1876: loss = 0.002751496351243648\n",
      "Iteration 1877: loss = 0.0027514269045902403\n",
      "Iteration 1878: loss = 0.002751357510176977\n",
      "Iteration 1879: loss = 0.0027512881679563413\n",
      "Iteration 1880: loss = 0.002751218877880857\n",
      "Iteration 1881: loss = 0.0027511496399030898\n",
      "Iteration 1882: loss = 0.0027510804539756487\n",
      "Iteration 1883: loss = 0.002751011320051182\n",
      "Iteration 1884: loss = 0.0027509422380823777\n",
      "Iteration 1885: loss = 0.0027508732080219708\n",
      "Iteration 1886: loss = 0.0027508042298227324\n",
      "Iteration 1887: loss = 0.002750735303437478\n",
      "Iteration 1888: loss = 0.002750666428819063\n",
      "Iteration 1889: loss = 0.002750597605920384\n",
      "Iteration 1890: loss = 0.00275052883469438\n",
      "Iteration 1891: loss = 0.0027504601150940286\n",
      "Iteration 1892: loss = 0.002750391447072353\n",
      "Iteration 1893: loss = 0.002750322830582413\n",
      "Iteration 1894: loss = 0.0027502542655773125\n",
      "Iteration 1895: loss = 0.002750185752010194\n",
      "Iteration 1896: loss = 0.0027501172898342437\n",
      "Iteration 1897: loss = 0.002750048879002687\n",
      "Iteration 1898: loss = 0.0027499805194687913\n",
      "Iteration 1899: loss = 0.002749912211185863\n",
      "Iteration 1900: loss = 0.002749843954107253\n",
      "Iteration 1901: loss = 0.002749775748186349\n",
      "Iteration 1902: loss = 0.0027497075933765824\n",
      "Iteration 1903: loss = 0.0027496394896314243\n",
      "Iteration 1904: loss = 0.0027495714369043874\n",
      "Iteration 1905: loss = 0.002749503435149023\n",
      "Iteration 1906: loss = 0.002749435484318926\n",
      "Iteration 1907: loss = 0.00274936758436773\n",
      "Iteration 1908: loss = 0.00274929973524911\n",
      "Iteration 1909: loss = 0.0027492319369167807\n",
      "Iteration 1910: loss = 0.0027491641893244998\n",
      "Iteration 1911: loss = 0.0027490964924260625\n",
      "Iteration 1912: loss = 0.002749028846175307\n",
      "Iteration 1913: loss = 0.00274896125052611\n",
      "Iteration 1914: loss = 0.00274889370543239\n",
      "Iteration 1915: loss = 0.0027488262108481053\n",
      "Iteration 1916: loss = 0.0027487587667272555\n",
      "Iteration 1917: loss = 0.002748691373023879\n",
      "Iteration 1918: loss = 0.0027486240296920564\n",
      "Iteration 1919: loss = 0.002748556736685907\n",
      "Iteration 1920: loss = 0.0027484894939595905\n",
      "Iteration 1921: loss = 0.002748422301467308\n",
      "Iteration 1922: loss = 0.0027483551591633007\n",
      "Iteration 1923: loss = 0.0027482880670018483\n",
      "Iteration 1924: loss = 0.002748221024937273\n",
      "Iteration 1925: loss = 0.002748154032923934\n",
      "Iteration 1926: loss = 0.0027480870909162344\n",
      "Iteration 1927: loss = 0.0027480201988686137\n",
      "Iteration 1928: loss = 0.0027479533567355548\n",
      "Iteration 1929: loss = 0.0027478865644715777\n",
      "Iteration 1930: loss = 0.002747819822031244\n",
      "Iteration 1931: loss = 0.002747753129369154\n",
      "Iteration 1932: loss = 0.002747686486439951\n",
      "Iteration 1933: loss = 0.002747619893198313\n",
      "Iteration 1934: loss = 0.002747553349598962\n",
      "Iteration 1935: loss = 0.0027474868555966586\n",
      "Iteration 1936: loss = 0.0027474204111462017\n",
      "Iteration 1937: loss = 0.0027473540162024335\n",
      "Iteration 1938: loss = 0.0027472876707202313\n",
      "Iteration 1939: loss = 0.0027472213746545164\n",
      "Iteration 1940: loss = 0.0027471551279602464\n",
      "Iteration 1941: loss = 0.0027470889305924204\n",
      "Iteration 1942: loss = 0.0027470227825060756\n",
      "Iteration 1943: loss = 0.002746956683656292\n",
      "Iteration 1944: loss = 0.002746890633998184\n",
      "Iteration 1945: loss = 0.00274682463348691\n",
      "Iteration 1946: loss = 0.002746758682077666\n",
      "Iteration 1947: loss = 0.0027466927797256866\n",
      "Iteration 1948: loss = 0.002746626926386248\n",
      "Iteration 1949: loss = 0.0027465611220146634\n",
      "Iteration 1950: loss = 0.002746495366566287\n",
      "Iteration 1951: loss = 0.002746429659996512\n",
      "Iteration 1952: loss = 0.0027463640022607695\n",
      "Iteration 1953: loss = 0.002746298393314532\n",
      "Iteration 1954: loss = 0.002746232833113309\n",
      "Iteration 1955: loss = 0.0027461673216126518\n",
      "Iteration 1956: loss = 0.0027461018587681477\n",
      "Iteration 1957: loss = 0.0027460364445354257\n",
      "Iteration 1958: loss = 0.002745971078870152\n",
      "Iteration 1959: loss = 0.0027459057617280343\n",
      "Iteration 1960: loss = 0.0027458404930648162\n",
      "Iteration 1961: loss = 0.0027457752728362824\n",
      "Iteration 1962: loss = 0.002745710100998256\n",
      "Iteration 1963: loss = 0.0027456449775065984\n",
      "Iteration 1964: loss = 0.0027455799023172124\n",
      "Iteration 1965: loss = 0.0027455148753860355\n",
      "Iteration 1966: loss = 0.0027454498966690472\n",
      "Iteration 1967: loss = 0.002745384966122265\n",
      "Iteration 1968: loss = 0.0027453200837017455\n",
      "Iteration 1969: loss = 0.002745255249363582\n",
      "Iteration 1970: loss = 0.0027451904630639104\n",
      "Iteration 1971: loss = 0.0027451257247589\n",
      "Iteration 1972: loss = 0.0027450610344047647\n",
      "Iteration 1973: loss = 0.0027449963919577514\n",
      "Iteration 1974: loss = 0.0027449317973741506\n",
      "Iteration 1975: loss = 0.0027448672506102875\n",
      "Iteration 1976: loss = 0.002744802751622527\n",
      "Iteration 1977: loss = 0.0027447383003672728\n",
      "Iteration 1978: loss = 0.002744673896800969\n",
      "Iteration 1979: loss = 0.0027446095408800927\n",
      "Iteration 1980: loss = 0.002744545232561166\n",
      "Iteration 1981: loss = 0.002744480971800745\n",
      "Iteration 1982: loss = 0.002744416758555424\n",
      "Iteration 1983: loss = 0.0027443525927818395\n",
      "Iteration 1984: loss = 0.0027442884744366615\n",
      "Iteration 1985: loss = 0.002744224403476601\n",
      "Iteration 1986: loss = 0.0027441603798584076\n",
      "Iteration 1987: loss = 0.0027440964035388674\n",
      "Iteration 1988: loss = 0.0027440324744748058\n",
      "Iteration 1989: loss = 0.0027439685926230846\n",
      "Iteration 1990: loss = 0.0027439047579406067\n",
      "Iteration 1991: loss = 0.0027438409703843102\n",
      "Iteration 1992: loss = 0.0027437772299111736\n",
      "Iteration 1993: loss = 0.0027437135364782104\n",
      "Iteration 1994: loss = 0.002743649890042475\n",
      "Iteration 1995: loss = 0.002743586290561059\n",
      "Iteration 1996: loss = 0.002743522737991091\n",
      "Iteration 1997: loss = 0.0027434592322897375\n",
      "Iteration 1998: loss = 0.0027433957734142034\n",
      "Iteration 1999: loss = 0.0027433323613217323\n",
      "Iteration 2000: loss = 0.0027432689959696038\n",
      "Iteration 2001: loss = 0.002743205677315136\n",
      "Iteration 2002: loss = 0.0027431424053156856\n",
      "Iteration 2003: loss = 0.0027430791799286456\n",
      "Iteration 2004: loss = 0.002743016001111447\n",
      "Iteration 2005: loss = 0.0027429528688215597\n",
      "Iteration 2006: loss = 0.002742889783016489\n",
      "Iteration 2007: loss = 0.0027428267436537795\n",
      "Iteration 2008: loss = 0.002742763750691013\n",
      "Iteration 2009: loss = 0.002742700804085808\n",
      "Iteration 2010: loss = 0.0027426379037958225\n",
      "Iteration 2011: loss = 0.0027425750497787494\n",
      "Iteration 2012: loss = 0.0027425122419923197\n",
      "Iteration 2013: loss = 0.002742449480394304\n",
      "Iteration 2014: loss = 0.0027423867649425068\n",
      "Iteration 2015: loss = 0.0027423240955947734\n",
      "Iteration 2016: loss = 0.0027422614723089825\n",
      "Iteration 2017: loss = 0.002742198895043054\n",
      "Iteration 2018: loss = 0.002742136363754943\n",
      "Iteration 2019: loss = 0.0027420738784026417\n",
      "Iteration 2020: loss = 0.00274201143894418\n",
      "Iteration 2021: loss = 0.002741949045337625\n",
      "Iteration 2022: loss = 0.0027418866975410803\n",
      "Iteration 2023: loss = 0.002741824395512688\n",
      "Iteration 2024: loss = 0.0027417621392106257\n",
      "Iteration 2025: loss = 0.0027416999285931084\n",
      "Iteration 2026: loss = 0.002741637763618389\n",
      "Iteration 2027: loss = 0.002741575644244756\n",
      "Iteration 2028: loss = 0.0027415135704305362\n",
      "Iteration 2029: loss = 0.002741451542134092\n",
      "Iteration 2030: loss = 0.0027413895593138243\n",
      "Iteration 2031: loss = 0.002741327621928169\n",
      "Iteration 2032: loss = 0.0027412657299356004\n",
      "Iteration 2033: loss = 0.002741203883294629\n",
      "Iteration 2034: loss = 0.002741142081963801\n",
      "Iteration 2035: loss = 0.002741080325901702\n",
      "Iteration 2036: loss = 0.002741018615066951\n",
      "Iteration 2037: loss = 0.0027409569494182056\n",
      "Iteration 2038: loss = 0.002740895328914161\n",
      "Iteration 2039: loss = 0.002740833753513546\n",
      "Iteration 2040: loss = 0.0027407722231751292\n",
      "Iteration 2041: loss = 0.0027407107378577145\n",
      "Iteration 2042: loss = 0.002740649297520141\n",
      "Iteration 2043: loss = 0.002740587902121285\n",
      "Iteration 2044: loss = 0.0027405265516200618\n",
      "Iteration 2045: loss = 0.002740465245975419\n",
      "Iteration 2046: loss = 0.0027404039851463436\n",
      "Iteration 2047: loss = 0.002740342769091858\n",
      "Iteration 2048: loss = 0.0027402815977710217\n",
      "Iteration 2049: loss = 0.0027402204711429283\n",
      "Iteration 2050: loss = 0.002740159389166711\n",
      "Iteration 2051: loss = 0.002740098351801535\n",
      "Iteration 2052: loss = 0.0027400373590066064\n",
      "Iteration 2053: loss = 0.0027399764107411645\n",
      "Iteration 2054: loss = 0.002739915506964485\n",
      "Iteration 2055: loss = 0.0027398546476358815\n",
      "Iteration 2056: loss = 0.002739793832714702\n",
      "Iteration 2057: loss = 0.00273973306216033\n",
      "Iteration 2058: loss = 0.0027396723359321877\n",
      "Iteration 2059: loss = 0.0027396116539897316\n",
      "Iteration 2060: loss = 0.002739551016292454\n",
      "Iteration 2061: loss = 0.0027394904227998835\n",
      "Iteration 2062: loss = 0.0027394298734715844\n",
      "Iteration 2063: loss = 0.0027393693682671583\n",
      "Iteration 2064: loss = 0.0027393089071462403\n",
      "Iteration 2065: loss = 0.0027392484900685037\n",
      "Iteration 2066: loss = 0.002739188116993656\n",
      "Iteration 2067: loss = 0.0027391277878814407\n",
      "Iteration 2068: loss = 0.0027390675026916387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2069: loss = 0.002739007261384064\n",
      "Iteration 2070: loss = 0.0027389470639185677\n",
      "Iteration 2071: loss = 0.0027388869102550383\n",
      "Iteration 2072: loss = 0.0027388268003533953\n",
      "Iteration 2073: loss = 0.002738766734173599\n",
      "Iteration 2074: loss = 0.002738706711675643\n",
      "Iteration 2075: loss = 0.0027386467328195546\n",
      "Iteration 2076: loss = 0.0027385867975654003\n",
      "Iteration 2077: loss = 0.002738526905873279\n",
      "Iteration 2078: loss = 0.002738467057703327\n",
      "Iteration 2079: loss = 0.002738407253015715\n",
      "Iteration 2080: loss = 0.002738347491770651\n",
      "Iteration 2081: loss = 0.0027382877739283752\n",
      "Iteration 2082: loss = 0.0027382280994491647\n",
      "Iteration 2083: loss = 0.002738168468293334\n",
      "Iteration 2084: loss = 0.0027381088804212285\n",
      "Iteration 2085: loss = 0.002738049335793233\n",
      "Iteration 2086: loss = 0.0027379898343697657\n",
      "Iteration 2087: loss = 0.0027379303761112797\n",
      "Iteration 2088: loss = 0.0027378709609782643\n",
      "Iteration 2089: loss = 0.0027378115889312433\n",
      "Iteration 2090: loss = 0.0027377522599307757\n",
      "Iteration 2091: loss = 0.0027376929739374558\n",
      "Iteration 2092: loss = 0.0027376337309119123\n",
      "Iteration 2093: loss = 0.0027375745308148105\n",
      "Iteration 2094: loss = 0.002737515373606849\n",
      "Iteration 2095: loss = 0.002737456259248762\n",
      "Iteration 2096: loss = 0.0027373971877013197\n",
      "Iteration 2097: loss = 0.0027373381589253245\n",
      "Iteration 2098: loss = 0.0027372791728816175\n",
      "Iteration 2099: loss = 0.002737220229531071\n",
      "Iteration 2100: loss = 0.002737161328834594\n",
      "Iteration 2101: loss = 0.0027371024707531317\n",
      "Iteration 2102: loss = 0.0027370436552476606\n",
      "Iteration 2103: loss = 0.002736984882279195\n",
      "Iteration 2104: loss = 0.0027369261518087815\n",
      "Iteration 2105: loss = 0.002736867463797504\n",
      "Iteration 2106: loss = 0.002736808818206478\n",
      "Iteration 2107: loss = 0.002736750214996858\n",
      "Iteration 2108: loss = 0.0027366916541298283\n",
      "Iteration 2109: loss = 0.0027366331355666105\n",
      "Iteration 2110: loss = 0.0027365746592684604\n",
      "Iteration 2111: loss = 0.002736516225196668\n",
      "Iteration 2112: loss = 0.002736457833312559\n",
      "Iteration 2113: loss = 0.0027363994835774906\n",
      "Iteration 2114: loss = 0.0027363411759528567\n",
      "Iteration 2115: loss = 0.0027362829104000874\n",
      "Iteration 2116: loss = 0.0027362246868806424\n",
      "Iteration 2117: loss = 0.00273616650535602\n",
      "Iteration 2118: loss = 0.002736108365787751\n",
      "Iteration 2119: loss = 0.0027360502681374\n",
      "Iteration 2120: loss = 0.002735992212366568\n",
      "Iteration 2121: loss = 0.0027359341984368873\n",
      "Iteration 2122: loss = 0.0027358762263100266\n",
      "Iteration 2123: loss = 0.0027358182959476888\n",
      "Iteration 2124: loss = 0.00273576040731161\n",
      "Iteration 2125: loss = 0.0027357025603635603\n",
      "Iteration 2126: loss = 0.002735644755065344\n",
      "Iteration 2127: loss = 0.0027355869913788015\n",
      "Iteration 2128: loss = 0.0027355292692658045\n",
      "Iteration 2129: loss = 0.0027354715886882598\n",
      "Iteration 2130: loss = 0.0027354139496081086\n",
      "Iteration 2131: loss = 0.0027353563519873247\n",
      "Iteration 2132: loss = 0.0027352987957879175\n",
      "Iteration 2133: loss = 0.002735241280971931\n",
      "Iteration 2134: loss = 0.002735183807501439\n",
      "Iteration 2135: loss = 0.0027351263753385537\n",
      "Iteration 2136: loss = 0.0027350689844454186\n",
      "Iteration 2137: loss = 0.0027350116347842125\n",
      "Iteration 2138: loss = 0.002734954326317146\n",
      "Iteration 2139: loss = 0.002734897059006465\n",
      "Iteration 2140: loss = 0.0027348398328144487\n",
      "Iteration 2141: loss = 0.0027347826477034107\n",
      "Iteration 2142: loss = 0.002734725503635697\n",
      "Iteration 2143: loss = 0.0027346684005736866\n",
      "Iteration 2144: loss = 0.0027346113384797955\n",
      "Iteration 2145: loss = 0.0027345543173164697\n",
      "Iteration 2146: loss = 0.0027344973370461894\n",
      "Iteration 2147: loss = 0.0027344403976314715\n",
      "Iteration 2148: loss = 0.0027343834990348607\n",
      "Iteration 2149: loss = 0.002734326641218941\n",
      "Iteration 2150: loss = 0.002734269824146326\n",
      "Iteration 2151: loss = 0.0027342130477796636\n",
      "Iteration 2152: loss = 0.0027341563120816367\n",
      "Iteration 2153: loss = 0.0027340996170149593\n",
      "Iteration 2154: loss = 0.00273404296254238\n",
      "Iteration 2155: loss = 0.0027339863486266795\n",
      "Iteration 2156: loss = 0.0027339297752306743\n",
      "Iteration 2157: loss = 0.0027338732423172114\n",
      "Iteration 2158: loss = 0.0027338167498491715\n",
      "Iteration 2159: loss = 0.0027337602977894706\n",
      "Iteration 2160: loss = 0.002733703886101055\n",
      "Iteration 2161: loss = 0.0027336475147469066\n",
      "Iteration 2162: loss = 0.002733591183690038\n",
      "Iteration 2163: loss = 0.0027335348928934976\n",
      "Iteration 2164: loss = 0.0027334786423203646\n",
      "Iteration 2165: loss = 0.002733422431933752\n",
      "Iteration 2166: loss = 0.0027333662616968062\n",
      "Iteration 2167: loss = 0.002733310131572706\n",
      "Iteration 2168: loss = 0.002733254041524663\n",
      "Iteration 2169: loss = 0.002733197991515923\n",
      "Iteration 2170: loss = 0.002733141981509763\n",
      "Iteration 2171: loss = 0.0027330860114694933\n",
      "Iteration 2172: loss = 0.0027330300813584586\n",
      "Iteration 2173: loss = 0.002732974191140034\n",
      "Iteration 2174: loss = 0.002732918340777629\n",
      "Iteration 2175: loss = 0.002732862530234685\n",
      "Iteration 2176: loss = 0.0027328067594746767\n",
      "Iteration 2177: loss = 0.002732751028461112\n",
      "Iteration 2178: loss = 0.002732695337157529\n",
      "Iteration 2179: loss = 0.002732639685527502\n",
      "Iteration 2180: loss = 0.0027325840735346356\n",
      "Iteration 2181: loss = 0.0027325285011425672\n",
      "Iteration 2182: loss = 0.0027324729683149665\n",
      "Iteration 2183: loss = 0.002732417475015538\n",
      "Iteration 2184: loss = 0.0027323620212080157\n",
      "Iteration 2185: loss = 0.002732306606856167\n",
      "Iteration 2186: loss = 0.0027322512319237933\n",
      "Iteration 2187: loss = 0.0027321958963747272\n",
      "Iteration 2188: loss = 0.0027321406001728334\n",
      "Iteration 2189: loss = 0.0027320853432820083\n",
      "Iteration 2190: loss = 0.002732030125666183\n",
      "Iteration 2191: loss = 0.0027319749472893198\n",
      "Iteration 2192: loss = 0.002731919808115412\n",
      "Iteration 2193: loss = 0.0027318647081084873\n",
      "Iteration 2194: loss = 0.0027318096472326034\n",
      "Iteration 2195: loss = 0.002731754625451853\n",
      "Iteration 2196: loss = 0.0027316996427303574\n",
      "Iteration 2197: loss = 0.0027316446990322736\n",
      "Iteration 2198: loss = 0.002731589794321789\n",
      "Iteration 2199: loss = 0.0027315349285631225\n",
      "Iteration 2200: loss = 0.0027314801017205258\n",
      "Iteration 2201: loss = 0.0027314253137582837\n",
      "Iteration 2202: loss = 0.0027313705646407117\n",
      "Iteration 2203: loss = 0.0027313158543321565\n",
      "Iteration 2204: loss = 0.0027312611827969998\n",
      "Iteration 2205: loss = 0.0027312065499996516\n",
      "Iteration 2206: loss = 0.002731151955904557\n",
      "Iteration 2207: loss = 0.0027310974004761904\n",
      "Iteration 2208: loss = 0.0027310428836790595\n",
      "Iteration 2209: loss = 0.0027309884054777032\n",
      "Iteration 2210: loss = 0.0027309339658366937\n",
      "Iteration 2211: loss = 0.002730879564720632\n",
      "Iteration 2212: loss = 0.0027308252020941553\n",
      "Iteration 2213: loss = 0.0027307708779219274\n",
      "Iteration 2214: loss = 0.0027307165921686472\n",
      "Iteration 2215: loss = 0.0027306623447990452\n",
      "Iteration 2216: loss = 0.0027306081357778814\n",
      "Iteration 2217: loss = 0.0027305539650699494\n",
      "Iteration 2218: loss = 0.0027304998326400736\n",
      "Iteration 2219: loss = 0.002730445738453111\n",
      "Iteration 2220: loss = 0.002730391682473948\n",
      "Iteration 2221: loss = 0.0027303376646675044\n",
      "Iteration 2222: loss = 0.0027302836849987305\n",
      "Iteration 2223: loss = 0.002730229743432609\n",
      "Iteration 2224: loss = 0.002730175839934154\n",
      "Iteration 2225: loss = 0.0027301219744684095\n",
      "Iteration 2226: loss = 0.002730068147000452\n",
      "Iteration 2227: loss = 0.00273001435749539\n",
      "Iteration 2228: loss = 0.002729960605918361\n",
      "Iteration 2229: loss = 0.002729906892234538\n",
      "Iteration 2230: loss = 0.0027298532164091198\n",
      "Iteration 2231: loss = 0.0027297995784073405\n",
      "Iteration 2232: loss = 0.002729745978194465\n",
      "Iteration 2233: loss = 0.002729692415735789\n",
      "Iteration 2234: loss = 0.0027296388909966367\n",
      "Iteration 2235: loss = 0.002729585403942368\n",
      "Iteration 2236: loss = 0.002729531954538371\n",
      "Iteration 2237: loss = 0.0027294785427500654\n",
      "Iteration 2238: loss = 0.002729425168542903\n",
      "Iteration 2239: loss = 0.002729371831882364\n",
      "Iteration 2240: loss = 0.002729318532733964\n",
      "Iteration 2241: loss = 0.002729265271063245\n",
      "Iteration 2242: loss = 0.0027292120468357826\n",
      "Iteration 2243: loss = 0.0027291588600171835\n",
      "Iteration 2244: loss = 0.002729105710573084\n",
      "Iteration 2245: loss = 0.002729052598469152\n",
      "Iteration 2246: loss = 0.0027289995236710863\n",
      "Iteration 2247: loss = 0.0027289464861446153\n",
      "Iteration 2248: loss = 0.002728893485855501\n",
      "Iteration 2249: loss = 0.0027288405227695327\n",
      "Iteration 2250: loss = 0.002728787596852534\n",
      "Iteration 2251: loss = 0.0027287347080703563\n",
      "Iteration 2252: loss = 0.0027286818563888836\n",
      "Iteration 2253: loss = 0.002728629041774029\n",
      "Iteration 2254: loss = 0.002728576264191738\n",
      "Iteration 2255: loss = 0.0027285235236079856\n",
      "Iteration 2256: loss = 0.002728470819988777\n",
      "Iteration 2257: loss = 0.0027284181533001493\n",
      "Iteration 2258: loss = 0.0027283655235081695\n",
      "Iteration 2259: loss = 0.002728312930578935\n",
      "Iteration 2260: loss = 0.0027282603744785746\n",
      "Iteration 2261: loss = 0.0027282078551732454\n",
      "Iteration 2262: loss = 0.002728155372629137\n",
      "Iteration 2263: loss = 0.0027281029268124693\n",
      "Iteration 2264: loss = 0.002728050517689491\n",
      "Iteration 2265: loss = 0.002727998145226484\n",
      "Iteration 2266: loss = 0.0027279458093897573\n",
      "Iteration 2267: loss = 0.0027278935101456517\n",
      "Iteration 2268: loss = 0.0027278412474605394\n",
      "Iteration 2269: loss = 0.002727789021300821\n",
      "Iteration 2270: loss = 0.002727736831632928\n",
      "Iteration 2271: loss = 0.0027276846784233233\n",
      "Iteration 2272: loss = 0.002727632561638499\n",
      "Iteration 2273: loss = 0.0027275804812449756\n",
      "Iteration 2274: loss = 0.0027275284372093073\n",
      "Iteration 2275: loss = 0.002727476429498075\n",
      "Iteration 2276: loss = 0.0027274244580778933\n",
      "Iteration 2277: loss = 0.0027273725229154033\n",
      "Iteration 2278: loss = 0.0027273206239772784\n",
      "Iteration 2279: loss = 0.002727268761230222\n",
      "Iteration 2280: loss = 0.002727216934640965\n",
      "Iteration 2281: loss = 0.002727165144176272\n",
      "Iteration 2282: loss = 0.0027271133898029345\n",
      "Iteration 2283: loss = 0.0027270616714877758\n",
      "Iteration 2284: loss = 0.0027270099891976477\n",
      "Iteration 2285: loss = 0.0027269583428994326\n",
      "Iteration 2286: loss = 0.0027269067325600433\n",
      "Iteration 2287: loss = 0.002726855158146421\n",
      "Iteration 2288: loss = 0.0027268036196255382\n",
      "Iteration 2289: loss = 0.0027267521169643963\n",
      "Iteration 2290: loss = 0.002726700650130026\n",
      "Iteration 2291: loss = 0.002726649219089489\n",
      "Iteration 2292: loss = 0.0027265978238098757\n",
      "Iteration 2293: loss = 0.0027265464642583065\n",
      "Iteration 2294: loss = 0.0027264951404019316\n",
      "Iteration 2295: loss = 0.0027264438522079304\n",
      "Iteration 2296: loss = 0.0027263925996435113\n",
      "Iteration 2297: loss = 0.0027263413826759146\n",
      "Iteration 2298: loss = 0.002726290201272408\n",
      "Iteration 2299: loss = 0.0027262390554002895\n",
      "Iteration 2300: loss = 0.0027261879450268863\n",
      "Iteration 2301: loss = 0.002726136870119554\n",
      "Iteration 2302: loss = 0.0027260858306456808\n",
      "Iteration 2303: loss = 0.0027260348265726814\n",
      "Iteration 2304: loss = 0.0027259838578680006\n",
      "Iteration 2305: loss = 0.0027259329244991135\n",
      "Iteration 2306: loss = 0.0027258820264335227\n",
      "Iteration 2307: loss = 0.002725831163638763\n",
      "Iteration 2308: loss = 0.0027257803360823956\n",
      "Iteration 2309: loss = 0.0027257295437320118\n",
      "Iteration 2310: loss = 0.0027256787865552337\n",
      "Iteration 2311: loss = 0.0027256280645197094\n",
      "Iteration 2312: loss = 0.0027255773775931203\n",
      "Iteration 2313: loss = 0.002725526725743174\n",
      "Iteration 2314: loss = 0.002725476108937608\n",
      "Iteration 2315: loss = 0.0027254255271441895\n",
      "Iteration 2316: loss = 0.002725374980330713\n",
      "Iteration 2317: loss = 0.002725324468465004\n",
      "Iteration 2318: loss = 0.0027252739915149173\n",
      "Iteration 2319: loss = 0.0027252235494483355\n",
      "Iteration 2320: loss = 0.002725173142233169\n",
      "Iteration 2321: loss = 0.002725122769837361\n",
      "Iteration 2322: loss = 0.002725072432228879\n",
      "Iteration 2323: loss = 0.002725022129375724\n",
      "Iteration 2324: loss = 0.0027249718612459223\n",
      "Iteration 2325: loss = 0.0027249216278075307\n",
      "Iteration 2326: loss = 0.002724871429028635\n",
      "Iteration 2327: loss = 0.002724821264877348\n",
      "Iteration 2328: loss = 0.0027247711353218153\n",
      "Iteration 2329: loss = 0.0027247210403302056\n",
      "Iteration 2330: loss = 0.0027246709798707212\n",
      "Iteration 2331: loss = 0.0027246209539115917\n",
      "Iteration 2332: loss = 0.002724570962421074\n",
      "Iteration 2333: loss = 0.0027245210053674558\n",
      "Iteration 2334: loss = 0.0027244710827190504\n",
      "Iteration 2335: loss = 0.002724421194444204\n",
      "Iteration 2336: loss = 0.0027243713405112877\n",
      "Iteration 2337: loss = 0.0027243215208887033\n",
      "Iteration 2338: loss = 0.00272427173554488\n",
      "Iteration 2339: loss = 0.002724221984448276\n",
      "Iteration 2340: loss = 0.002724172267567378\n",
      "Iteration 2341: loss = 0.0027241225848707016\n",
      "Iteration 2342: loss = 0.00272407293632679\n",
      "Iteration 2343: loss = 0.0027240233219042154\n",
      "Iteration 2344: loss = 0.002723973741571578\n",
      "Iteration 2345: loss = 0.002723924195297507\n",
      "Iteration 2346: loss = 0.002723874683050659\n",
      "Iteration 2347: loss = 0.0027238252047997197\n",
      "Iteration 2348: loss = 0.0027237757605134043\n",
      "Iteration 2349: loss = 0.0027237263501604537\n",
      "Iteration 2350: loss = 0.0027236769737096376\n",
      "Iteration 2351: loss = 0.0027236276311297567\n",
      "Iteration 2352: loss = 0.0027235783223896358\n",
      "Iteration 2353: loss = 0.0027235290474581304\n",
      "Iteration 2354: loss = 0.0027234798063041246\n",
      "Iteration 2355: loss = 0.00272343059889653\n",
      "Iteration 2356: loss = 0.002723381425204284\n",
      "Iteration 2357: loss = 0.0027233322851963567\n",
      "Iteration 2358: loss = 0.0027232831788417426\n",
      "Iteration 2359: loss = 0.0027232341061094656\n",
      "Iteration 2360: loss = 0.002723185066968576\n",
      "Iteration 2361: loss = 0.0027231360613881555\n",
      "Iteration 2362: loss = 0.0027230870893373118\n",
      "Iteration 2363: loss = 0.0027230381507851787\n",
      "Iteration 2364: loss = 0.0027229892457009213\n",
      "Iteration 2365: loss = 0.002722940374053731\n",
      "Iteration 2366: loss = 0.002722891535812827\n",
      "Iteration 2367: loss = 0.002722842730947456\n",
      "Iteration 2368: loss = 0.0027227939594268932\n",
      "Iteration 2369: loss = 0.0027227452212204425\n",
      "Iteration 2370: loss = 0.0027226965162974333\n",
      "Iteration 2371: loss = 0.002722647844627225\n",
      "Iteration 2372: loss = 0.0027225992061792026\n",
      "Iteration 2373: loss = 0.0027225506009227805\n",
      "Iteration 2374: loss = 0.0027225020288274013\n",
      "Iteration 2375: loss = 0.0027224534898625326\n",
      "Iteration 2376: loss = 0.002722404983997672\n",
      "Iteration 2377: loss = 0.002722356511202344\n",
      "Iteration 2378: loss = 0.002722308071446101\n",
      "Iteration 2379: loss = 0.002722259664698522\n",
      "Iteration 2380: loss = 0.0027222112909292147\n",
      "Iteration 2381: loss = 0.0027221629501078134\n",
      "Iteration 2382: loss = 0.0027221146422039804\n",
      "Iteration 2383: loss = 0.0027220663671874063\n",
      "Iteration 2384: loss = 0.002722018125027807\n",
      "Iteration 2385: loss = 0.002721969915694928\n",
      "Iteration 2386: loss = 0.002721921739158541\n",
      "Iteration 2387: loss = 0.002721873595388445\n",
      "Iteration 2388: loss = 0.0027218254843544678\n",
      "Iteration 2389: loss = 0.0027217774060264623\n",
      "Iteration 2390: loss = 0.0027217293603743102\n",
      "Iteration 2391: loss = 0.0027216813473679206\n",
      "Iteration 2392: loss = 0.0027216333669772296\n",
      "Iteration 2393: loss = 0.0027215854191721994\n",
      "Iteration 2394: loss = 0.002721537503922822\n",
      "Iteration 2395: loss = 0.0027214896211991135\n",
      "Iteration 2396: loss = 0.0027214417709711196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2397: loss = 0.002721393953208912\n",
      "Iteration 2398: loss = 0.0027213461678825897\n",
      "Iteration 2399: loss = 0.002721298414962279\n",
      "Iteration 2400: loss = 0.002721250694418133\n",
      "Iteration 2401: loss = 0.002721203006220332\n",
      "Iteration 2402: loss = 0.002721155350339084\n",
      "Iteration 2403: loss = 0.0027211077267446228\n",
      "Iteration 2404: loss = 0.0027210601354072093\n",
      "Iteration 2405: loss = 0.0027210125762971325\n",
      "Iteration 2406: loss = 0.002720965049384708\n",
      "Iteration 2407: loss = 0.0027209175546402766\n",
      "Iteration 2408: loss = 0.002720870092034208\n",
      "Iteration 2409: loss = 0.002720822661536899\n",
      "Iteration 2410: loss = 0.002720775263118773\n",
      "Iteration 2411: loss = 0.002720727896750276\n",
      "Iteration 2412: loss = 0.002720680562401888\n",
      "Iteration 2413: loss = 0.002720633260044111\n",
      "Iteration 2414: loss = 0.002720585989647475\n",
      "Iteration 2415: loss = 0.0027205387511825374\n",
      "Iteration 2416: loss = 0.00272049154461988\n",
      "Iteration 2417: loss = 0.0027204443699301137\n",
      "Iteration 2418: loss = 0.002720397227083876\n",
      "Iteration 2419: loss = 0.0027203501160518286\n",
      "Iteration 2420: loss = 0.0027203030368046634\n",
      "Iteration 2421: loss = 0.002720255989313096\n",
      "Iteration 2422: loss = 0.00272020897354787\n",
      "Iteration 2423: loss = 0.0027201619894797547\n",
      "Iteration 2424: loss = 0.002720115037079546\n",
      "Iteration 2425: loss = 0.0027200681163180684\n",
      "Iteration 2426: loss = 0.0027200212271661695\n",
      "Iteration 2427: loss = 0.0027199743695947255\n",
      "Iteration 2428: loss = 0.002719927543574639\n",
      "Iteration 2429: loss = 0.002719880749076838\n",
      "Iteration 2430: loss = 0.002719833986072278\n",
      "Iteration 2431: loss = 0.0027197872545319398\n",
      "Iteration 2432: loss = 0.002719740554426831\n",
      "Iteration 2433: loss = 0.002719693885727986\n",
      "Iteration 2434: loss = 0.002719647248406465\n",
      "Iteration 2435: loss = 0.0027196006424333547\n",
      "Iteration 2436: loss = 0.002719554067779768\n",
      "Iteration 2437: loss = 0.0027195075244168428\n",
      "Iteration 2438: loss = 0.002719461012315746\n",
      "Iteration 2439: loss = 0.0027194145314476683\n",
      "Iteration 2440: loss = 0.002719368081783828\n",
      "Iteration 2441: loss = 0.0027193216632954674\n",
      "Iteration 2442: loss = 0.0027192752759538576\n",
      "Iteration 2443: loss = 0.0027192289197302947\n",
      "Iteration 2444: loss = 0.0027191825945961\n",
      "Iteration 2445: loss = 0.0027191363005226224\n",
      "Iteration 2446: loss = 0.0027190900374812347\n",
      "Iteration 2447: loss = 0.0027190438054433387\n",
      "Iteration 2448: loss = 0.0027189976043803595\n",
      "Iteration 2449: loss = 0.0027189514342637494\n",
      "Iteration 2450: loss = 0.0027189052950649865\n",
      "Iteration 2451: loss = 0.0027188591867555755\n",
      "Iteration 2452: loss = 0.002718813109307045\n",
      "Iteration 2453: loss = 0.0027187670626909514\n",
      "Iteration 2454: loss = 0.0027187210468788757\n",
      "Iteration 2455: loss = 0.0027186750618424264\n",
      "Iteration 2456: loss = 0.0027186291075532356\n",
      "Iteration 2457: loss = 0.002718583183982964\n",
      "Iteration 2458: loss = 0.002718537291103294\n",
      "Iteration 2459: loss = 0.002718491428885937\n",
      "Iteration 2460: loss = 0.00271844559730263\n",
      "Iteration 2461: loss = 0.002718399796325135\n",
      "Iteration 2462: loss = 0.0027183540259252377\n",
      "Iteration 2463: loss = 0.002718308286074753\n",
      "Iteration 2464: loss = 0.0027182625767455193\n",
      "Iteration 2465: loss = 0.0027182168979094016\n",
      "Iteration 2466: loss = 0.0027181712495382886\n",
      "Iteration 2467: loss = 0.002718125631604097\n",
      "Iteration 2468: loss = 0.0027180800440787687\n",
      "Iteration 2469: loss = 0.0027180344869342683\n",
      "Iteration 2470: loss = 0.002717988960142589\n",
      "Iteration 2471: loss = 0.0027179434636757493\n",
      "Iteration 2472: loss = 0.0027178979975057906\n",
      "Iteration 2473: loss = 0.002717852561604783\n",
      "Iteration 2474: loss = 0.0027178071559448196\n",
      "Iteration 2475: loss = 0.0027177617804980195\n",
      "Iteration 2476: loss = 0.0027177164352365276\n",
      "Iteration 2477: loss = 0.0027176711201325147\n",
      "Iteration 2478: loss = 0.002717625835158175\n",
      "Iteration 2479: loss = 0.00271758058028573\n",
      "Iteration 2480: loss = 0.0027175353554874247\n",
      "Iteration 2481: loss = 0.0027174901607355307\n",
      "Iteration 2482: loss = 0.0027174449960023447\n",
      "Iteration 2483: loss = 0.0027173998612601887\n",
      "Iteration 2484: loss = 0.0027173547564814083\n",
      "Iteration 2485: loss = 0.0027173096816383757\n",
      "Iteration 2486: loss = 0.002717264636703489\n",
      "Iteration 2487: loss = 0.002717219621649169\n",
      "Iteration 2488: loss = 0.0027171746364478646\n",
      "Iteration 2489: loss = 0.002717129681072047\n",
      "Iteration 2490: loss = 0.002717084755494214\n",
      "Iteration 2491: loss = 0.002717039859686889\n",
      "Iteration 2492: loss = 0.0027169949936226177\n",
      "Iteration 2493: loss = 0.002716950157273974\n",
      "Iteration 2494: loss = 0.0027169053506135555\n",
      "Iteration 2495: loss = 0.002716860573613983\n",
      "Iteration 2496: loss = 0.0027168158262479056\n",
      "Iteration 2497: loss = 0.002716771108487995\n",
      "Iteration 2498: loss = 0.002716726420306948\n",
      "Iteration 2499: loss = 0.0027166817616774874\n",
      "Iteration 2500: loss = 0.002716637132572359\n",
      "Iteration 2501: loss = 0.0027165925329643357\n",
      "Iteration 2502: loss = 0.0027165479628262126\n",
      "Iteration 2503: loss = 0.002716503422130812\n",
      "Iteration 2504: loss = 0.00271645891085098\n",
      "Iteration 2505: loss = 0.0027164144289595856\n",
      "Iteration 2506: loss = 0.0027163699764295272\n",
      "Iteration 2507: loss = 0.0027163255532337218\n",
      "Iteration 2508: loss = 0.0027162811593451166\n",
      "Iteration 2509: loss = 0.002716236794736679\n",
      "Iteration 2510: loss = 0.002716192459381406\n",
      "Iteration 2511: loss = 0.0027161481532523126\n",
      "Iteration 2512: loss = 0.0027161038763224453\n",
      "Iteration 2513: loss = 0.0027160596285648697\n",
      "Iteration 2514: loss = 0.00271601540995268\n",
      "Iteration 2515: loss = 0.0027159712204589915\n",
      "Iteration 2516: loss = 0.002715927060056946\n",
      "Iteration 2517: loss = 0.00271588292871971\n",
      "Iteration 2518: loss = 0.0027158388264204737\n",
      "Iteration 2519: loss = 0.0027157947531324506\n",
      "Iteration 2520: loss = 0.0027157507088288817\n",
      "Iteration 2521: loss = 0.0027157066934830293\n",
      "Iteration 2522: loss = 0.0027156627070681824\n",
      "Iteration 2523: loss = 0.0027156187495576524\n",
      "Iteration 2524: loss = 0.0027155748209247753\n",
      "Iteration 2525: loss = 0.002715530921142913\n",
      "Iteration 2526: loss = 0.0027154870501854516\n",
      "Iteration 2527: loss = 0.002715443208025799\n",
      "Iteration 2528: loss = 0.0027153993946373895\n",
      "Iteration 2529: loss = 0.0027153556099936806\n",
      "Iteration 2530: loss = 0.002715311854068155\n",
      "Iteration 2531: loss = 0.002715268126834319\n",
      "Iteration 2532: loss = 0.002715224428265703\n",
      "Iteration 2533: loss = 0.0027151807583358613\n",
      "Iteration 2534: loss = 0.0027151371170183723\n",
      "Iteration 2535: loss = 0.00271509350428684\n",
      "Iteration 2536: loss = 0.0027150499201148903\n",
      "Iteration 2537: loss = 0.0027150063644761748\n",
      "Iteration 2538: loss = 0.002714962837344368\n",
      "Iteration 2539: loss = 0.002714919338693169\n",
      "Iteration 2540: loss = 0.002714875868496301\n",
      "Iteration 2541: loss = 0.002714832426727511\n",
      "Iteration 2542: loss = 0.002714789013360569\n",
      "Iteration 2543: loss = 0.002714745628369272\n",
      "Iteration 2544: loss = 0.002714702271727437\n",
      "Iteration 2545: loss = 0.002714658943408906\n",
      "Iteration 2546: loss = 0.0027146156433875473\n",
      "Iteration 2547: loss = 0.0027145723716372507\n",
      "Iteration 2548: loss = 0.00271452912813193\n",
      "Iteration 2549: loss = 0.0027144859128455223\n",
      "Iteration 2550: loss = 0.002714442725751991\n",
      "Iteration 2551: loss = 0.0027143995668253208\n",
      "Iteration 2552: loss = 0.0027143564360395216\n",
      "Iteration 2553: loss = 0.002714313333368626\n",
      "Iteration 2554: loss = 0.00271427025878669\n",
      "Iteration 2555: loss = 0.002714227212267795\n",
      "Iteration 2556: loss = 0.0027141841937860446\n",
      "Iteration 2557: loss = 0.0027141412033155664\n",
      "Iteration 2558: loss = 0.0027140982408305116\n",
      "Iteration 2559: loss = 0.0027140553063050555\n",
      "Iteration 2560: loss = 0.0027140123997133957\n",
      "Iteration 2561: loss = 0.002713969521029755\n",
      "Iteration 2562: loss = 0.0027139266702283786\n",
      "Iteration 2563: loss = 0.0027138838472835357\n",
      "Iteration 2564: loss = 0.0027138410521695194\n",
      "Iteration 2565: loss = 0.0027137982848606445\n",
      "Iteration 2566: loss = 0.002713755545331251\n",
      "Iteration 2567: loss = 0.0027137128335557024\n",
      "Iteration 2568: loss = 0.0027136701495083845\n",
      "Iteration 2569: loss = 0.002713627493163707\n",
      "Iteration 2570: loss = 0.0027135848644961025\n",
      "Iteration 2571: loss = 0.0027135422634800284\n",
      "Iteration 2572: loss = 0.0027134996900899646\n",
      "Iteration 2573: loss = 0.002713457144300413\n",
      "Iteration 2574: loss = 0.002713414626085901\n",
      "Iteration 2575: loss = 0.002713372135420977\n",
      "Iteration 2576: loss = 0.0027133296722802154\n",
      "Iteration 2577: loss = 0.0027132872366382116\n",
      "Iteration 2578: loss = 0.0027132448284695842\n",
      "Iteration 2579: loss = 0.0027132024477489773\n",
      "Iteration 2580: loss = 0.002713160094451055\n",
      "Iteration 2581: loss = 0.0027131177685505072\n",
      "Iteration 2582: loss = 0.0027130754700220457\n",
      "Iteration 2583: loss = 0.0027130331988404048\n",
      "Iteration 2584: loss = 0.0027129909549803437\n",
      "Iteration 2585: loss = 0.0027129487384166427\n",
      "Iteration 2586: loss = 0.0027129065491241066\n",
      "Iteration 2587: loss = 0.002712864387077562\n",
      "Iteration 2588: loss = 0.0027128222522518606\n",
      "Iteration 2589: loss = 0.0027127801446218736\n",
      "Iteration 2590: loss = 0.0027127380641624992\n",
      "Iteration 2591: loss = 0.0027126960108486554\n",
      "Iteration 2592: loss = 0.0027126539846552855\n",
      "Iteration 2593: loss = 0.002712611985557353\n",
      "Iteration 2594: loss = 0.0027125700135298466\n",
      "Iteration 2595: loss = 0.0027125280685477765\n",
      "Iteration 2596: loss = 0.0027124861505861773\n",
      "Iteration 2597: loss = 0.0027124442596201045\n",
      "Iteration 2598: loss = 0.002712402395624638\n",
      "Iteration 2599: loss = 0.002712360558574879\n",
      "Iteration 2600: loss = 0.0027123187484459537\n",
      "Iteration 2601: loss = 0.002712276965213008\n",
      "Iteration 2602: loss = 0.0027122352088512136\n",
      "Iteration 2603: loss = 0.0027121934793357615\n",
      "Iteration 2604: loss = 0.00271215177664187\n",
      "Iteration 2605: loss = 0.002712110100744775\n",
      "Iteration 2606: loss = 0.002712068451619739\n",
      "Iteration 2607: loss = 0.0027120268292420454\n",
      "Iteration 2608: loss = 0.0027119852335869997\n",
      "Iteration 2609: loss = 0.0027119436646299306\n",
      "Iteration 2610: loss = 0.0027119021223461904\n",
      "Iteration 2611: loss = 0.002711860606711152\n",
      "Iteration 2612: loss = 0.0027118191177002127\n",
      "Iteration 2613: loss = 0.0027117776552887905\n",
      "Iteration 2614: loss = 0.002711736219452327\n",
      "Iteration 2615: loss = 0.0027116948101662875\n",
      "Iteration 2616: loss = 0.002711653427406156\n",
      "Iteration 2617: loss = 0.0027116120711474434\n",
      "Iteration 2618: loss = 0.0027115707413656793\n",
      "Iteration 2619: loss = 0.0027115294380364173\n",
      "Iteration 2620: loss = 0.0027114881611352347\n",
      "Iteration 2621: loss = 0.002711446910637728\n",
      "Iteration 2622: loss = 0.0027114056865195197\n",
      "Iteration 2623: loss = 0.002711364488756251\n",
      "Iteration 2624: loss = 0.002711323317323588\n",
      "Iteration 2625: loss = 0.0027112821721972176\n",
      "Iteration 2626: loss = 0.0027112410533528496\n",
      "Iteration 2627: loss = 0.002711199960766217\n",
      "Iteration 2628: loss = 0.002711158894413073\n",
      "Iteration 2629: loss = 0.0027111178542691934\n",
      "Iteration 2630: loss = 0.0027110768403103775\n",
      "Iteration 2631: loss = 0.0027110358525124466\n",
      "Iteration 2632: loss = 0.002710994890851242\n",
      "Iteration 2633: loss = 0.00271095395530263\n",
      "Iteration 2634: loss = 0.002710913045842496\n",
      "Iteration 2635: loss = 0.0027108721624467514\n",
      "Iteration 2636: loss = 0.0027108313050913254\n",
      "Iteration 2637: loss = 0.0027107904737521714\n",
      "Iteration 2638: loss = 0.002710749668405265\n",
      "Iteration 2639: loss = 0.0027107088890266045\n",
      "Iteration 2640: loss = 0.002710668135592207\n",
      "Iteration 2641: loss = 0.0027106274080781153\n",
      "Iteration 2642: loss = 0.0027105867064603916\n",
      "Iteration 2643: loss = 0.002710546030715121\n",
      "Iteration 2644: loss = 0.0027105053808184104\n",
      "Iteration 2645: loss = 0.0027104647567463883\n",
      "Iteration 2646: loss = 0.0027104241584752064\n",
      "Iteration 2647: loss = 0.0027103835859810365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2648: loss = 0.0027103430392400726\n",
      "Iteration 2649: loss = 0.0027103025182285308\n",
      "Iteration 2650: loss = 0.00271026202292265\n",
      "Iteration 2651: loss = 0.002710221553298689\n",
      "Iteration 2652: loss = 0.0027101811093329297\n",
      "Iteration 2653: loss = 0.002710140691001675\n",
      "Iteration 2654: loss = 0.002710100298281249\n",
      "Iteration 2655: loss = 0.0027100599311479993\n",
      "Iteration 2656: loss = 0.0027100195895782943\n",
      "Iteration 2657: loss = 0.0027099792735485228\n",
      "Iteration 2658: loss = 0.002709938983035097\n",
      "Iteration 2659: loss = 0.00270989871801445\n",
      "Iteration 2660: loss = 0.0027098584784630356\n",
      "Iteration 2661: loss = 0.0027098182643573317\n",
      "Iteration 2662: loss = 0.0027097780756738347\n",
      "Iteration 2663: loss = 0.002709737912389065\n",
      "Iteration 2664: loss = 0.0027096977744795626\n",
      "Iteration 2665: loss = 0.002709657661921891\n",
      "Iteration 2666: loss = 0.002709617574692633\n",
      "Iteration 2667: loss = 0.002709577512768394\n",
      "Iteration 2668: loss = 0.0027095374761258017\n",
      "Iteration 2669: loss = 0.002709497464741503\n",
      "Iteration 2670: loss = 0.0027094574785921687\n",
      "Iteration 2671: loss = 0.002709417517654489\n",
      "Iteration 2672: loss = 0.002709377581905177\n",
      "Iteration 2673: loss = 0.0027093376713209663\n",
      "Iteration 2674: loss = 0.0027092977858786107\n",
      "Iteration 2675: loss = 0.002709257925554888\n",
      "Iteration 2676: loss = 0.002709218090326595\n",
      "Iteration 2677: loss = 0.0027091782801705513\n",
      "Iteration 2678: loss = 0.002709138495063596\n",
      "Iteration 2679: loss = 0.0027090987349825913\n",
      "Iteration 2680: loss = 0.0027090589999044204\n",
      "Iteration 2681: loss = 0.002709019289805986\n",
      "Iteration 2682: loss = 0.0027089796046642135\n",
      "Iteration 2683: loss = 0.0027089399444560494\n",
      "Iteration 2684: loss = 0.00270890030915846\n",
      "Iteration 2685: loss = 0.002708860698748434\n",
      "Iteration 2686: loss = 0.002708821113202982\n",
      "Iteration 2687: loss = 0.002708781552499134\n",
      "Iteration 2688: loss = 0.0027087420166139417\n",
      "Iteration 2689: loss = 0.002708702505524477\n",
      "Iteration 2690: loss = 0.0027086630192078346\n",
      "Iteration 2691: loss = 0.0027086235576411293\n",
      "Iteration 2692: loss = 0.0027085841208014964\n",
      "Iteration 2693: loss = 0.002708544708666092\n",
      "Iteration 2694: loss = 0.002708505321212096\n",
      "Iteration 2695: loss = 0.0027084659584167047\n",
      "Iteration 2696: loss = 0.0027084266202571385\n",
      "Iteration 2697: loss = 0.002708387306710639\n",
      "Iteration 2698: loss = 0.0027083480177544647\n",
      "Iteration 2699: loss = 0.002708308753365901\n",
      "Iteration 2700: loss = 0.0027082695135222487\n",
      "Iteration 2701: loss = 0.0027082302982008324\n",
      "Iteration 2702: loss = 0.0027081911073789963\n",
      "Iteration 2703: loss = 0.002708151941034107\n",
      "Iteration 2704: loss = 0.00270811279914355\n",
      "Iteration 2705: loss = 0.002708073681684732\n",
      "Iteration 2706: loss = 0.0027080345886350814\n",
      "Iteration 2707: loss = 0.002707995519972046\n",
      "Iteration 2708: loss = 0.002707956475673095\n",
      "Iteration 2709: loss = 0.002707917455715719\n",
      "Iteration 2710: loss = 0.002707878460077427\n",
      "Iteration 2711: loss = 0.0027078394887357516\n",
      "Iteration 2712: loss = 0.002707800541668243\n",
      "Iteration 2713: loss = 0.0027077616188524753\n",
      "Iteration 2714: loss = 0.0027077227202660405\n",
      "Iteration 2715: loss = 0.0027076838458865516\n",
      "Iteration 2716: loss = 0.002707644995691644\n",
      "Iteration 2717: loss = 0.0027076061696589705\n",
      "Iteration 2718: loss = 0.0027075673677662082\n",
      "Iteration 2719: loss = 0.002707528589991051\n",
      "Iteration 2720: loss = 0.0027074898363112156\n",
      "Iteration 2721: loss = 0.0027074511067044383\n",
      "Iteration 2722: loss = 0.002707412401148476\n",
      "Iteration 2723: loss = 0.002707373719621108\n",
      "Iteration 2724: loss = 0.0027073350621001297\n",
      "Iteration 2725: loss = 0.0027072964285633594\n",
      "Iteration 2726: loss = 0.0027072578189886367\n",
      "Iteration 2727: loss = 0.0027072192333538205\n",
      "Iteration 2728: loss = 0.002707180671636789\n",
      "Iteration 2729: loss = 0.002707142133815443\n",
      "Iteration 2730: loss = 0.002707103619867702\n",
      "Iteration 2731: loss = 0.002707065129771506\n",
      "Iteration 2732: loss = 0.002707026663504815\n",
      "Iteration 2733: loss = 0.00270698822104561\n",
      "Iteration 2734: loss = 0.0027069498023718917\n",
      "Iteration 2735: loss = 0.002706911407461682\n",
      "Iteration 2736: loss = 0.0027068730362930205\n",
      "Iteration 2737: loss = 0.00270683468884397\n",
      "Iteration 2738: loss = 0.0027067963650926124\n",
      "Iteration 2739: loss = 0.002706758065017048\n",
      "Iteration 2740: loss = 0.0027067197885953997\n",
      "Iteration 2741: loss = 0.002706681535805809\n",
      "Iteration 2742: loss = 0.0027066433066264385\n",
      "Iteration 2743: loss = 0.00270660510103547\n",
      "Iteration 2744: loss = 0.0027065669190111047\n",
      "Iteration 2745: loss = 0.002706528760531566\n",
      "Iteration 2746: loss = 0.0027064906255750954\n",
      "Iteration 2747: loss = 0.0027064525141199556\n",
      "Iteration 2748: loss = 0.002706414426144428\n",
      "Iteration 2749: loss = 0.0027063763616268157\n",
      "Iteration 2750: loss = 0.0027063383205454404\n",
      "Iteration 2751: loss = 0.0027063003028786436\n",
      "Iteration 2752: loss = 0.002706262308604787\n",
      "Iteration 2753: loss = 0.0027062243377022534\n",
      "Iteration 2754: loss = 0.0027061863901494437\n",
      "Iteration 2755: loss = 0.00270614846592478\n",
      "Iteration 2756: loss = 0.0027061105650067027\n",
      "Iteration 2757: loss = 0.0027060726873736735\n",
      "Iteration 2758: loss = 0.002706034833004173\n",
      "Iteration 2759: loss = 0.0027059970018767023\n",
      "Iteration 2760: loss = 0.002705959193969782\n",
      "Iteration 2761: loss = 0.002705921409261952\n",
      "Iteration 2762: loss = 0.0027058836477317715\n",
      "Iteration 2763: loss = 0.0027058459093578222\n",
      "Iteration 2764: loss = 0.0027058081941187017\n",
      "Iteration 2765: loss = 0.0027057705019930293\n",
      "Iteration 2766: loss = 0.002705732832959444\n",
      "Iteration 2767: loss = 0.002705695186996604\n",
      "Iteration 2768: loss = 0.0027056575640831875\n",
      "Iteration 2769: loss = 0.0027056199641978924\n",
      "Iteration 2770: loss = 0.0027055823873194346\n",
      "Iteration 2771: loss = 0.0027055448334265516\n",
      "Iteration 2772: loss = 0.0027055073024980004\n",
      "Iteration 2773: loss = 0.002705469794512555\n",
      "Iteration 2774: loss = 0.0027054323094490127\n",
      "Iteration 2775: loss = 0.002705394847286187\n",
      "Iteration 2776: loss = 0.0027053574080029125\n",
      "Iteration 2777: loss = 0.0027053199915780435\n",
      "Iteration 2778: loss = 0.0027052825979904526\n",
      "Iteration 2779: loss = 0.002705245227219033\n",
      "Iteration 2780: loss = 0.002705207879242696\n",
      "Iteration 2781: loss = 0.002705170554040374\n",
      "Iteration 2782: loss = 0.0027051332515910173\n",
      "Iteration 2783: loss = 0.0027050959718735966\n",
      "Iteration 2784: loss = 0.0027050587148671006\n",
      "Iteration 2785: loss = 0.002705021480550539\n",
      "Iteration 2786: loss = 0.00270498426890294\n",
      "Iteration 2787: loss = 0.00270494707990335\n",
      "Iteration 2788: loss = 0.0027049099135308375\n",
      "Iteration 2789: loss = 0.0027048727697644875\n",
      "Iteration 2790: loss = 0.0027048356485834054\n",
      "Iteration 2791: loss = 0.0027047985499667154\n",
      "Iteration 2792: loss = 0.0027047614738935624\n",
      "Iteration 2793: loss = 0.0027047244203431075\n",
      "Iteration 2794: loss = 0.0027046873892945344\n",
      "Iteration 2795: loss = 0.002704650380727043\n",
      "Iteration 2796: loss = 0.002704613394619855\n",
      "Iteration 2797: loss = 0.002704576430952209\n",
      "Iteration 2798: loss = 0.0027045394897033644\n",
      "Iteration 2799: loss = 0.0027045025708525975\n",
      "Iteration 2800: loss = 0.0027044656743792066\n",
      "Iteration 2801: loss = 0.002704428800262507\n",
      "Iteration 2802: loss = 0.002704391948481833\n",
      "Iteration 2803: loss = 0.0027043551190165382\n",
      "Iteration 2804: loss = 0.002704318311845998\n",
      "Iteration 2805: loss = 0.0027042815269496014\n",
      "Iteration 2806: loss = 0.0027042447643067604\n",
      "Iteration 2807: loss = 0.002704208023896905\n",
      "Iteration 2808: loss = 0.002704171305699483\n",
      "Iteration 2809: loss = 0.0027041346096939632\n",
      "Iteration 2810: loss = 0.002704097935859832\n",
      "Iteration 2811: loss = 0.0027040612841765935\n",
      "Iteration 2812: loss = 0.0027040246546237738\n",
      "Iteration 2813: loss = 0.002703988047180915\n",
      "Iteration 2814: loss = 0.0027039514618275797\n",
      "Iteration 2815: loss = 0.002703914898543348\n",
      "Iteration 2816: loss = 0.00270387835730782\n",
      "Iteration 2817: loss = 0.0027038418381006134\n",
      "Iteration 2818: loss = 0.0027038053409013667\n",
      "Iteration 2819: loss = 0.0027037688656897352\n",
      "Iteration 2820: loss = 0.0027037324124453933\n",
      "Iteration 2821: loss = 0.002703695981148034\n",
      "Iteration 2822: loss = 0.00270365957177737\n",
      "Iteration 2823: loss = 0.0027036231843131325\n",
      "Iteration 2824: loss = 0.0027035868187350698\n",
      "Iteration 2825: loss = 0.0027035504750229505\n",
      "Iteration 2826: loss = 0.0027035141531565614\n",
      "Iteration 2827: loss = 0.0027034778531157076\n",
      "Iteration 2828: loss = 0.0027034415748802133\n",
      "Iteration 2829: loss = 0.002703405318429921\n",
      "Iteration 2830: loss = 0.002703369083744692\n",
      "Iteration 2831: loss = 0.002703332870804405\n",
      "Iteration 2832: loss = 0.0027032966795889585\n",
      "Iteration 2833: loss = 0.00270326051007827\n",
      "Iteration 2834: loss = 0.002703224362252274\n",
      "Iteration 2835: loss = 0.002703188236090924\n",
      "Iteration 2836: loss = 0.002703152131574193\n",
      "Iteration 2837: loss = 0.00270311604868207\n",
      "Iteration 2838: loss = 0.002703079987394566\n",
      "Iteration 2839: loss = 0.0027030439476917073\n",
      "Iteration 2840: loss = 0.0027030079295535395\n",
      "Iteration 2841: loss = 0.0027029719329601273\n",
      "Iteration 2842: loss = 0.0027029359578915535\n",
      "Iteration 2843: loss = 0.0027029000043279185\n",
      "Iteration 2844: loss = 0.0027028640722493416\n",
      "Iteration 2845: loss = 0.0027028281616359616\n",
      "Iteration 2846: loss = 0.002702792272467932\n",
      "Iteration 2847: loss = 0.0027027564047254304\n",
      "Iteration 2848: loss = 0.0027027205583886457\n",
      "Iteration 2849: loss = 0.0027026847334377913\n",
      "Iteration 2850: loss = 0.0027026489298530945\n",
      "Iteration 2851: loss = 0.002702613147614803\n",
      "Iteration 2852: loss = 0.0027025773867031823\n",
      "Iteration 2853: loss = 0.002702541647098517\n",
      "Iteration 2854: loss = 0.002702505928781107\n",
      "Iteration 2855: loss = 0.0027024702317312733\n",
      "Iteration 2856: loss = 0.002702434555929353\n",
      "Iteration 2857: loss = 0.0027023989013557036\n",
      "Iteration 2858: loss = 0.002702363267990699\n",
      "Iteration 2859: loss = 0.002702327655814731\n",
      "Iteration 2860: loss = 0.0027022920648082102\n",
      "Iteration 2861: loss = 0.0027022564949515655\n",
      "Iteration 2862: loss = 0.0027022209462252435\n",
      "Iteration 2863: loss = 0.0027021854186097083\n",
      "Iteration 2864: loss = 0.0027021499120854424\n",
      "Iteration 2865: loss = 0.0027021144266329462\n",
      "Iteration 2866: loss = 0.00270207896223274\n",
      "Iteration 2867: loss = 0.0027020435188653577\n",
      "Iteration 2868: loss = 0.002702008096511356\n",
      "Iteration 2869: loss = 0.0027019726951513063\n",
      "Iteration 2870: loss = 0.0027019373147657987\n",
      "Iteration 2871: loss = 0.0027019019553354423\n",
      "Iteration 2872: loss = 0.002701866616840863\n",
      "Iteration 2873: loss = 0.0027018312992627036\n",
      "Iteration 2874: loss = 0.002701796002581627\n",
      "Iteration 2875: loss = 0.0027017607267783125\n",
      "Iteration 2876: loss = 0.0027017254718334576\n",
      "Iteration 2877: loss = 0.0027016902377277776\n",
      "Iteration 2878: loss = 0.0027016550244420056\n",
      "Iteration 2879: loss = 0.002701619831956893\n",
      "Iteration 2880: loss = 0.002701584660253207\n",
      "Iteration 2881: loss = 0.002701549509311735\n",
      "Iteration 2882: loss = 0.002701514379113281\n",
      "Iteration 2883: loss = 0.002701479269638666\n",
      "Iteration 2884: loss = 0.0027014441808687305\n",
      "Iteration 2885: loss = 0.0027014091127843306\n",
      "Iteration 2886: loss = 0.002701374065366341\n",
      "Iteration 2887: loss = 0.0027013390385956554\n",
      "Iteration 2888: loss = 0.002701304032453183\n",
      "Iteration 2889: loss = 0.002701269046919851\n",
      "Iteration 2890: loss = 0.0027012340819766055\n",
      "Iteration 2891: loss = 0.0027011991376044093\n",
      "Iteration 2892: loss = 0.0027011642137842414\n",
      "Iteration 2893: loss = 0.0027011293104971016\n",
      "Iteration 2894: loss = 0.0027010944277240045\n",
      "Iteration 2895: loss = 0.0027010595654459835\n",
      "Iteration 2896: loss = 0.002701024723644088\n",
      "Iteration 2897: loss = 0.002700989902299387\n",
      "Iteration 2898: loss = 0.002700955101392966\n",
      "Iteration 2899: loss = 0.0027009203209059276\n",
      "Iteration 2900: loss = 0.0027008855608193923\n",
      "Iteration 2901: loss = 0.002700850821114497\n",
      "Iteration 2902: loss = 0.0027008161017723978\n",
      "Iteration 2903: loss = 0.002700781402774267\n",
      "Iteration 2904: loss = 0.0027007467241012945\n",
      "Iteration 2905: loss = 0.002700712065734688\n",
      "Iteration 2906: loss = 0.0027006774276556716\n",
      "Iteration 2907: loss = 0.0027006428098454876\n",
      "Iteration 2908: loss = 0.002700608212285395\n",
      "Iteration 2909: loss = 0.0027005736349566705\n",
      "Iteration 2910: loss = 0.002700539077840608\n",
      "Iteration 2911: loss = 0.002700504540918518\n",
      "Iteration 2912: loss = 0.0027004700241717305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2913: loss = 0.0027004355275815897\n",
      "Iteration 2914: loss = 0.0027004010511294586\n",
      "Iteration 2915: loss = 0.002700366594796718\n",
      "Iteration 2916: loss = 0.002700332158564764\n",
      "Iteration 2917: loss = 0.0027002977424150122\n",
      "Iteration 2918: loss = 0.002700263346328893\n",
      "Iteration 2919: loss = 0.002700228970287857\n",
      "Iteration 2920: loss = 0.0027001946142733674\n",
      "Iteration 2921: loss = 0.0027001602782669087\n",
      "Iteration 2922: loss = 0.002700125962249981\n",
      "Iteration 2923: loss = 0.002700091666204101\n",
      "Iteration 2924: loss = 0.002700057390110803\n",
      "Iteration 2925: loss = 0.002700023133951638\n",
      "Iteration 2926: loss = 0.0026999888977081747\n",
      "Iteration 2927: loss = 0.002699954681361999\n",
      "Iteration 2928: loss = 0.0026999204848947114\n",
      "Iteration 2929: loss = 0.002699886308287933\n",
      "Iteration 2930: loss = 0.0026998521515232987\n",
      "Iteration 2931: loss = 0.0026998180145824625\n",
      "Iteration 2932: loss = 0.002699783897447095\n",
      "Iteration 2933: loss = 0.002699749800098882\n",
      "Iteration 2934: loss = 0.0026997157225195287\n",
      "Iteration 2935: loss = 0.002699681664690755\n",
      "Iteration 2936: loss = 0.0026996476265942998\n",
      "Iteration 2937: loss = 0.0026996136082119166\n",
      "Iteration 2938: loss = 0.0026995796095253773\n",
      "Iteration 2939: loss = 0.0026995456305164704\n",
      "Iteration 2940: loss = 0.002699511671167002\n",
      "Iteration 2941: loss = 0.002699477731458793\n",
      "Iteration 2942: loss = 0.0026994438113736815\n",
      "Iteration 2943: loss = 0.0026994099108935245\n",
      "Iteration 2944: loss = 0.0026993760300001936\n",
      "Iteration 2945: loss = 0.002699342168675578\n",
      "Iteration 2946: loss = 0.0026993083269015827\n",
      "Iteration 2947: loss = 0.0026992745046601316\n",
      "Iteration 2948: loss = 0.002699240701933163\n",
      "Iteration 2949: loss = 0.0026992069187026326\n",
      "Iteration 2950: loss = 0.0026991731549505142\n",
      "Iteration 2951: loss = 0.0026991394106587957\n",
      "Iteration 2952: loss = 0.0026991056858094832\n",
      "Iteration 2953: loss = 0.0026990719803846\n",
      "Iteration 2954: loss = 0.0026990382943661837\n",
      "Iteration 2955: loss = 0.0026990046277362923\n",
      "Iteration 2956: loss = 0.0026989709804769954\n",
      "Iteration 2957: loss = 0.0026989373525703837\n",
      "Iteration 2958: loss = 0.002698903743998562\n",
      "Iteration 2959: loss = 0.002698870154743652\n",
      "Iteration 2960: loss = 0.002698836584787792\n",
      "Iteration 2961: loss = 0.0026988030341131373\n",
      "Iteration 2962: loss = 0.0026987695027018603\n",
      "Iteration 2963: loss = 0.0026987359905361473\n",
      "Iteration 2964: loss = 0.002698702497598203\n",
      "Iteration 2965: loss = 0.0026986690238702494\n",
      "Iteration 2966: loss = 0.0026986355693345227\n",
      "Iteration 2967: loss = 0.0026986021339732765\n",
      "Iteration 2968: loss = 0.0026985687177687808\n",
      "Iteration 2969: loss = 0.0026985353207033234\n",
      "Iteration 2970: loss = 0.0026985019427592056\n",
      "Iteration 2971: loss = 0.002698468583918748\n",
      "Iteration 2972: loss = 0.002698435244164284\n",
      "Iteration 2973: loss = 0.002698401923478168\n",
      "Iteration 2974: loss = 0.0026983686218427665\n",
      "Iteration 2975: loss = 0.0026983353392404643\n",
      "Iteration 2976: loss = 0.002698302075653663\n",
      "Iteration 2977: loss = 0.002698268831064779\n",
      "Iteration 2978: loss = 0.002698235605456245\n",
      "Iteration 2979: loss = 0.0026982023988105118\n",
      "Iteration 2980: loss = 0.0026981692111100438\n",
      "Iteration 2981: loss = 0.002698136042337324\n",
      "Iteration 2982: loss = 0.00269810289247485\n",
      "Iteration 2983: loss = 0.002698069761505137\n",
      "Iteration 2984: loss = 0.002698036649410715\n",
      "Iteration 2985: loss = 0.0026980035561741302\n",
      "Iteration 2986: loss = 0.0026979704817779457\n",
      "Iteration 2987: loss = 0.002697937426204741\n",
      "Iteration 2988: loss = 0.0026979043894371103\n",
      "Iteration 2989: loss = 0.002697871371457666\n",
      "Iteration 2990: loss = 0.0026978383722490334\n",
      "Iteration 2991: loss = 0.002697805391793857\n",
      "Iteration 2992: loss = 0.0026977724300747967\n",
      "Iteration 2993: loss = 0.0026977394870745268\n",
      "Iteration 2994: loss = 0.0026977065627757386\n",
      "Iteration 2995: loss = 0.002697673657161141\n",
      "Iteration 2996: loss = 0.002697640770213455\n",
      "Iteration 2997: loss = 0.002697607901915422\n",
      "Iteration 2998: loss = 0.0026975750522497967\n",
      "Iteration 2999: loss = 0.0026975422211993503\n",
      "Iteration 3000: loss = 0.00269750940874687\n",
      "Iteration 3001: loss = 0.0026974766148751593\n",
      "Iteration 3002: loss = 0.002697443839567037\n",
      "Iteration 3003: loss = 0.002697411082805338\n",
      "Iteration 3004: loss = 0.002697378344572913\n",
      "Iteration 3005: loss = 0.0026973456248526283\n",
      "Iteration 3006: loss = 0.0026973129236273676\n",
      "Iteration 3007: loss = 0.0026972802408800284\n",
      "Iteration 3008: loss = 0.0026972475765935254\n",
      "Iteration 3009: loss = 0.002697214930750788\n",
      "Iteration 3010: loss = 0.002697182303334763\n",
      "Iteration 3011: loss = 0.00269714969432841\n",
      "Iteration 3012: loss = 0.002697117103714709\n",
      "Iteration 3013: loss = 0.002697084531476651\n",
      "Iteration 3014: loss = 0.002697051977597246\n",
      "Iteration 3015: loss = 0.0026970194420595176\n",
      "Iteration 3016: loss = 0.0026969869248465064\n",
      "Iteration 3017: loss = 0.002696954425941269\n",
      "Iteration 3018: loss = 0.002696921945326876\n",
      "Iteration 3019: loss = 0.002696889482986415\n",
      "Iteration 3020: loss = 0.0026968570389029885\n",
      "Iteration 3021: loss = 0.002696824613059717\n",
      "Iteration 3022: loss = 0.002696792205439732\n",
      "Iteration 3023: loss = 0.0026967598160261854\n",
      "Iteration 3024: loss = 0.002696727444802241\n",
      "Iteration 3025: loss = 0.0026966950917510814\n",
      "Iteration 3026: loss = 0.002696662756855901\n",
      "Iteration 3027: loss = 0.002696630440099914\n",
      "Iteration 3028: loss = 0.002696598141466347\n",
      "Iteration 3029: loss = 0.0026965658609384435\n",
      "Iteration 3030: loss = 0.0026965335984994615\n",
      "Iteration 3031: loss = 0.0026965013541326755\n",
      "Iteration 3032: loss = 0.002696469127821376\n",
      "Iteration 3033: loss = 0.0026964369195488663\n",
      "Iteration 3034: loss = 0.0026964047292984683\n",
      "Iteration 3035: loss = 0.002696372557053517\n",
      "Iteration 3036: loss = 0.0026963404027973647\n",
      "Iteration 3037: loss = 0.0026963082665133783\n",
      "Iteration 3038: loss = 0.0026962761481849396\n",
      "Iteration 3039: loss = 0.002696244047795446\n",
      "Iteration 3040: loss = 0.0026962119653283112\n",
      "Iteration 3041: loss = 0.002696179900766962\n",
      "Iteration 3042: loss = 0.0026961478540948433\n",
      "Iteration 3043: loss = 0.0026961158252954143\n",
      "Iteration 3044: loss = 0.002696083814352149\n",
      "Iteration 3045: loss = 0.0026960518212485367\n",
      "Iteration 3046: loss = 0.002696019845968082\n",
      "Iteration 3047: loss = 0.0026959878884943054\n",
      "Iteration 3048: loss = 0.002695955948810742\n",
      "Iteration 3049: loss = 0.0026959240269009443\n",
      "Iteration 3050: loss = 0.0026958921227484755\n",
      "Iteration 3051: loss = 0.0026958602363369184\n",
      "Iteration 3052: loss = 0.0026958283676498693\n",
      "Iteration 3053: loss = 0.002695796516670939\n",
      "Iteration 3054: loss = 0.002695764683383754\n",
      "Iteration 3055: loss = 0.002695732867771957\n",
      "Iteration 3056: loss = 0.0026957010698192042\n",
      "Iteration 3057: loss = 0.002695669289509168\n",
      "Iteration 3058: loss = 0.002695637526825536\n",
      "Iteration 3059: loss = 0.00269560578175201\n",
      "Iteration 3060: loss = 0.002695574054272308\n",
      "Iteration 3061: loss = 0.0026955423443701617\n",
      "Iteration 3062: loss = 0.0026955106520293198\n",
      "Iteration 3063: loss = 0.0026954789772335435\n",
      "Iteration 3064: loss = 0.002695447319966612\n",
      "Iteration 3065: loss = 0.0026954156802123166\n",
      "Iteration 3066: loss = 0.0026953840579544666\n",
      "Iteration 3067: loss = 0.002695352453176883\n",
      "Iteration 3068: loss = 0.002695320865863404\n",
      "Iteration 3069: loss = 0.0026952892959978838\n",
      "Iteration 3070: loss = 0.0026952577435641876\n",
      "Iteration 3071: loss = 0.002695226208546199\n",
      "Iteration 3072: loss = 0.002695194690927816\n",
      "Iteration 3073: loss = 0.0026951631906929506\n",
      "Iteration 3074: loss = 0.0026951317078255303\n",
      "Iteration 3075: loss = 0.002695100242309496\n",
      "Iteration 3076: loss = 0.0026950687941288062\n",
      "Iteration 3077: loss = 0.002695037363267433\n",
      "Iteration 3078: loss = 0.0026950059497093623\n",
      "Iteration 3079: loss = 0.002694974553438596\n",
      "Iteration 3080: loss = 0.0026949431744391506\n",
      "Iteration 3081: loss = 0.0026949118126950574\n",
      "Iteration 3082: loss = 0.002694880468190362\n",
      "Iteration 3083: loss = 0.002694849140909126\n",
      "Iteration 3084: loss = 0.002694817830835424\n",
      "Iteration 3085: loss = 0.0026947865379533475\n",
      "Iteration 3086: loss = 0.0026947552622470007\n",
      "Iteration 3087: loss = 0.0026947240037005038\n",
      "Iteration 3088: loss = 0.002694692762297991\n",
      "Iteration 3089: loss = 0.0026946615380236114\n",
      "Iteration 3090: loss = 0.00269463033086153\n",
      "Iteration 3091: loss = 0.0026945991407959233\n",
      "Iteration 3092: loss = 0.0026945679678109864\n",
      "Iteration 3093: loss = 0.0026945368118909277\n",
      "Iteration 3094: loss = 0.0026945056730199677\n",
      "Iteration 3095: loss = 0.0026944745511823438\n",
      "Iteration 3096: loss = 0.00269444344636231\n",
      "Iteration 3097: loss = 0.0026944123585441303\n",
      "Iteration 3098: loss = 0.002694381287712086\n",
      "Iteration 3099: loss = 0.002694350233850474\n",
      "Iteration 3100: loss = 0.002694319196943602\n",
      "Iteration 3101: loss = 0.002694288176975797\n",
      "Iteration 3102: loss = 0.002694257173931397\n",
      "Iteration 3103: loss = 0.0026942261877947555\n",
      "Iteration 3104: loss = 0.0026941952185502408\n",
      "Iteration 3105: loss = 0.0026941642661822357\n",
      "Iteration 3106: loss = 0.002694133330675137\n",
      "Iteration 3107: loss = 0.002694102412013357\n",
      "Iteration 3108: loss = 0.0026940715101813205\n",
      "Iteration 3109: loss = 0.0026940406251634687\n",
      "Iteration 3110: loss = 0.0026940097569442567\n",
      "Iteration 3111: loss = 0.0026939789055081534\n",
      "Iteration 3112: loss = 0.002693948070839642\n",
      "Iteration 3113: loss = 0.0026939172529232215\n",
      "Iteration 3114: loss = 0.002693886451743404\n",
      "Iteration 3115: loss = 0.0026938556672847155\n",
      "Iteration 3116: loss = 0.002693824899531699\n",
      "Iteration 3117: loss = 0.0026937941484689075\n",
      "Iteration 3118: loss = 0.002693763414080913\n",
      "Iteration 3119: loss = 0.002693732696352298\n",
      "Iteration 3120: loss = 0.0026937019952676623\n",
      "Iteration 3121: loss = 0.0026936713108116172\n",
      "Iteration 3122: loss = 0.0026936406429687912\n",
      "Iteration 3123: loss = 0.0026936099917238233\n",
      "Iteration 3124: loss = 0.0026935793570613703\n",
      "Iteration 3125: loss = 0.0026935487389661023\n",
      "Iteration 3126: loss = 0.002693518137422702\n",
      "Iteration 3127: loss = 0.0026934875524158683\n",
      "Iteration 3128: loss = 0.0026934569839303125\n",
      "Iteration 3129: loss = 0.002693426431950762\n",
      "Iteration 3130: loss = 0.002693395896461957\n",
      "Iteration 3131: loss = 0.0026933653774486527\n",
      "Iteration 3132: loss = 0.0026933348748956172\n",
      "Iteration 3133: loss = 0.0026933043887876337\n",
      "Iteration 3134: loss = 0.0026932739191095\n",
      "Iteration 3135: loss = 0.0026932434658460264\n",
      "Iteration 3136: loss = 0.002693213028982039\n",
      "Iteration 3137: loss = 0.002693182608502376\n",
      "Iteration 3138: loss = 0.0026931522043918926\n",
      "Iteration 3139: loss = 0.0026931218166354543\n",
      "Iteration 3140: loss = 0.0026930914452179436\n",
      "Iteration 3141: loss = 0.002693061090124256\n",
      "Iteration 3142: loss = 0.002693030751339302\n",
      "Iteration 3143: loss = 0.002693000428848003\n",
      "Iteration 3144: loss = 0.002692970122635299\n",
      "Iteration 3145: loss = 0.002692939832686139\n",
      "Iteration 3146: loss = 0.00269290955898549\n",
      "Iteration 3147: loss = 0.0026928793015183307\n",
      "Iteration 3148: loss = 0.002692849060269655\n",
      "Iteration 3149: loss = 0.00269281883522447\n",
      "Iteration 3150: loss = 0.002692788626367797\n",
      "Iteration 3151: loss = 0.00269275843368467\n",
      "Iteration 3152: loss = 0.0026927282571601397\n",
      "Iteration 3153: loss = 0.0026926980967792668\n",
      "Iteration 3154: loss = 0.00269266795252713\n",
      "Iteration 3155: loss = 0.002692637824388818\n",
      "Iteration 3156: loss = 0.0026926077123494365\n",
      "Iteration 3157: loss = 0.0026925776163941035\n",
      "Iteration 3158: loss = 0.0026925475365079505\n",
      "Iteration 3159: loss = 0.002692517472676123\n",
      "Iteration 3160: loss = 0.002692487424883781\n",
      "Iteration 3161: loss = 0.0026924573931160975\n",
      "Iteration 3162: loss = 0.002692427377358259\n",
      "Iteration 3163: loss = 0.002692397377595469\n",
      "Iteration 3164: loss = 0.002692367393812939\n",
      "Iteration 3165: loss = 0.0026923374259958982\n",
      "Iteration 3166: loss = 0.002692307474129589\n",
      "Iteration 3167: loss = 0.002692277538199266\n",
      "Iteration 3168: loss = 0.0026922476181901996\n",
      "Iteration 3169: loss = 0.002692217714087673\n",
      "Iteration 3170: loss = 0.002692187825876982\n",
      "Iteration 3171: loss = 0.0026921579535434366\n",
      "Iteration 3172: loss = 0.0026921280970723612\n",
      "Iteration 3173: loss = 0.0026920982564490946\n",
      "Iteration 3174: loss = 0.002692068431658986\n",
      "Iteration 3175: loss = 0.002692038622687401\n",
      "Iteration 3176: loss = 0.0026920088295197186\n",
      "Iteration 3177: loss = 0.0026919790521413296\n",
      "Iteration 3178: loss = 0.002691949290537639\n",
      "Iteration 3179: loss = 0.0026919195446940673\n",
      "Iteration 3180: loss = 0.0026918898145960463\n",
      "Iteration 3181: loss = 0.002691860100229022\n",
      "Iteration 3182: loss = 0.002691830401578454\n",
      "Iteration 3183: loss = 0.002691800718629816\n",
      "Iteration 3184: loss = 0.002691771051368593\n",
      "Iteration 3185: loss = 0.0026917413997802867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3186: loss = 0.0026917117638504094\n",
      "Iteration 3187: loss = 0.0026916821435644884\n",
      "Iteration 3188: loss = 0.002691652538908064\n",
      "Iteration 3189: loss = 0.0026916229498666904\n",
      "Iteration 3190: loss = 0.002691593376425934\n",
      "Iteration 3191: loss = 0.002691563818571376\n",
      "Iteration 3192: loss = 0.0026915342762886102\n",
      "Iteration 3193: loss = 0.0026915047495632435\n",
      "Iteration 3194: loss = 0.002691475238380897\n",
      "Iteration 3195: loss = 0.0026914457427272046\n",
      "Iteration 3196: loss = 0.0026914162625878146\n",
      "Iteration 3197: loss = 0.002691386797948386\n",
      "Iteration 3198: loss = 0.002691357348794593\n",
      "Iteration 3199: loss = 0.0026913279151121244\n",
      "Iteration 3200: loss = 0.0026912984968866798\n",
      "Iteration 3201: loss = 0.0026912690941039734\n",
      "Iteration 3202: loss = 0.002691239706749732\n",
      "Iteration 3203: loss = 0.002691210334809696\n",
      "Iteration 3204: loss = 0.0026911809782696186\n",
      "Iteration 3205: loss = 0.002691151637115268\n",
      "Iteration 3206: loss = 0.0026911223113324223\n",
      "Iteration 3207: loss = 0.002691093000906877\n",
      "Iteration 3208: loss = 0.002691063705824437\n",
      "Iteration 3209: loss = 0.002691034426070922\n",
      "Iteration 3210: loss = 0.0026910051616321658\n",
      "Iteration 3211: loss = 0.0026909759124940135\n",
      "Iteration 3212: loss = 0.0026909466786423247\n",
      "Iteration 3213: loss = 0.002690917460062971\n",
      "Iteration 3214: loss = 0.0026908882567418376\n",
      "Iteration 3215: loss = 0.0026908590686648246\n",
      "Iteration 3216: loss = 0.002690829895817842\n",
      "Iteration 3217: loss = 0.002690800738186814\n",
      "Iteration 3218: loss = 0.00269077159575768\n",
      "Iteration 3219: loss = 0.002690742468516389\n",
      "Iteration 3220: loss = 0.0026907133564489064\n",
      "Iteration 3221: loss = 0.0026906842595412084\n",
      "Iteration 3222: loss = 0.0026906551777792836\n",
      "Iteration 3223: loss = 0.0026906261111491374\n",
      "Iteration 3224: loss = 0.002690597059636784\n",
      "Iteration 3225: loss = 0.002690568023228252\n",
      "Iteration 3226: loss = 0.0026905390019095845\n",
      "Iteration 3227: loss = 0.0026905099956668357\n",
      "Iteration 3228: loss = 0.0026904810044860723\n",
      "Iteration 3229: loss = 0.002690452028353377\n",
      "Iteration 3230: loss = 0.0026904230672548414\n",
      "Iteration 3231: loss = 0.0026903941211765736\n",
      "Iteration 3232: loss = 0.002690365190104692\n",
      "Iteration 3233: loss = 0.0026903362740253295\n",
      "Iteration 3234: loss = 0.0026903073729246316\n",
      "Iteration 3235: loss = 0.002690278486788755\n",
      "Iteration 3236: loss = 0.0026902496156038726\n",
      "Iteration 3237: loss = 0.002690220759356167\n",
      "Iteration 3238: loss = 0.0026901919180318343\n",
      "Iteration 3239: loss = 0.0026901630916170856\n",
      "Iteration 3240: loss = 0.002690134280098141\n",
      "Iteration 3241: loss = 0.0026901054834612376\n",
      "Iteration 3242: loss = 0.002690076701692622\n",
      "Iteration 3243: loss = 0.0026900479347785554\n",
      "Iteration 3244: loss = 0.0026900191827053106\n",
      "Iteration 3245: loss = 0.0026899904454591745\n",
      "Iteration 3246: loss = 0.0026899617230264445\n",
      "Iteration 3247: loss = 0.0026899330153934346\n",
      "Iteration 3248: loss = 0.002689904322546467\n",
      "Iteration 3249: loss = 0.002689875644471879\n",
      "Iteration 3250: loss = 0.0026898469811560215\n",
      "Iteration 3251: loss = 0.002689818332585256\n",
      "Iteration 3252: loss = 0.0026897896987459575\n",
      "Iteration 3253: loss = 0.002689761079624514\n",
      "Iteration 3254: loss = 0.0026897324752073255\n",
      "Iteration 3255: loss = 0.0026897038854808058\n",
      "Iteration 3256: loss = 0.0026896753104313797\n",
      "Iteration 3257: loss = 0.002689646750045486\n",
      "Iteration 3258: loss = 0.002689618204309575\n",
      "Iteration 3259: loss = 0.0026895896732101105\n",
      "Iteration 3260: loss = 0.002689561156733569\n",
      "Iteration 3261: loss = 0.0026895326548664377\n",
      "Iteration 3262: loss = 0.0026895041675952186\n",
      "Iteration 3263: loss = 0.002689475694906425\n",
      "Iteration 3264: loss = 0.002689447236786585\n",
      "Iteration 3265: loss = 0.0026894187932222347\n",
      "Iteration 3266: loss = 0.002689390364199926\n",
      "Iteration 3267: loss = 0.0026893619497062237\n",
      "Iteration 3268: loss = 0.0026893335497277034\n",
      "Iteration 3269: loss = 0.002689305164250953\n",
      "Iteration 3270: loss = 0.002689276793262575\n",
      "Iteration 3271: loss = 0.002689248436749182\n",
      "Iteration 3272: loss = 0.0026892200946974005\n",
      "Iteration 3273: loss = 0.002689191767093869\n",
      "Iteration 3274: loss = 0.0026891634539252386\n",
      "Iteration 3275: loss = 0.002689135155178172\n",
      "Iteration 3276: loss = 0.0026891068708393456\n",
      "Iteration 3277: loss = 0.0026890786008954464\n",
      "Iteration 3278: loss = 0.0026890503453331762\n",
      "Iteration 3279: loss = 0.0026890221041392464\n",
      "Iteration 3280: loss = 0.002688993877300384\n",
      "Iteration 3281: loss = 0.002688965664803325\n",
      "Iteration 3282: loss = 0.0026889374666348195\n",
      "Iteration 3283: loss = 0.00268890928278163\n",
      "Iteration 3284: loss = 0.0026888811132305306\n",
      "Iteration 3285: loss = 0.0026888529579683087\n",
      "Iteration 3286: loss = 0.0026888248169817624\n",
      "Iteration 3287: loss = 0.0026887966902577032\n",
      "Iteration 3288: loss = 0.0026887685777829554\n",
      "Iteration 3289: loss = 0.0026887404795443545\n",
      "Iteration 3290: loss = 0.0026887123955287476\n",
      "Iteration 3291: loss = 0.0026886843257229957\n",
      "Iteration 3292: loss = 0.002688656270113972\n",
      "Iteration 3293: loss = 0.0026886282286885605\n",
      "Iteration 3294: loss = 0.0026886002014336577\n",
      "Iteration 3295: loss = 0.002688572188336173\n",
      "Iteration 3296: loss = 0.0026885441893830277\n",
      "Iteration 3297: loss = 0.002688516204561155\n",
      "Iteration 3298: loss = 0.002688488233857501\n",
      "Iteration 3299: loss = 0.0026884602772590228\n",
      "Iteration 3300: loss = 0.00268843233475269\n",
      "Iteration 3301: loss = 0.002688404406325485\n",
      "Iteration 3302: loss = 0.002688376491964402\n",
      "Iteration 3303: loss = 0.002688348591656447\n",
      "Iteration 3304: loss = 0.0026883207053886374\n",
      "Iteration 3305: loss = 0.0026882928331480043\n",
      "Iteration 3306: loss = 0.0026882649749215897\n",
      "Iteration 3307: loss = 0.002688237130696448\n",
      "Iteration 3308: loss = 0.002688209300459646\n",
      "Iteration 3309: loss = 0.0026881814841982613\n",
      "Iteration 3310: loss = 0.0026881536818993846\n",
      "Iteration 3311: loss = 0.002688125893550119\n",
      "Iteration 3312: loss = 0.002688098119137578\n",
      "Iteration 3313: loss = 0.0026880703586488895\n",
      "Iteration 3314: loss = 0.00268804261207119\n",
      "Iteration 3315: loss = 0.002688014879391631\n",
      "Iteration 3316: loss = 0.002687987160597375\n",
      "Iteration 3317: loss = 0.0026879594556755955\n",
      "Iteration 3318: loss = 0.0026879317646134786\n",
      "Iteration 3319: loss = 0.0026879040873982233\n",
      "Iteration 3320: loss = 0.002687876424017038\n",
      "Iteration 3321: loss = 0.0026878487744571465\n",
      "Iteration 3322: loss = 0.0026878211387057816\n",
      "Iteration 3323: loss = 0.0026877935167501886\n",
      "Iteration 3324: loss = 0.0026877659085776263\n",
      "Iteration 3325: loss = 0.0026877383141753627\n",
      "Iteration 3326: loss = 0.002687710733530679\n",
      "Iteration 3327: loss = 0.002687683166630869\n",
      "Iteration 3328: loss = 0.002687655613463238\n",
      "Iteration 3329: loss = 0.0026876280740151005\n",
      "Iteration 3330: loss = 0.002687600548273787\n",
      "Iteration 3331: loss = 0.002687573036226637\n",
      "Iteration 3332: loss = 0.002687545537861002\n",
      "Iteration 3333: loss = 0.0026875180531642467\n",
      "Iteration 3334: loss = 0.0026874905821237466\n",
      "Iteration 3335: loss = 0.002687463124726888\n",
      "Iteration 3336: loss = 0.0026874356809610707\n",
      "Iteration 3337: loss = 0.0026874082508137045\n",
      "Iteration 3338: loss = 0.0026873808342722134\n",
      "Iteration 3339: loss = 0.00268735343132403\n",
      "Iteration 3340: loss = 0.002687326041956601\n",
      "Iteration 3341: loss = 0.002687298666157384\n",
      "Iteration 3342: loss = 0.0026872713039138465\n",
      "Iteration 3343: loss = 0.0026872439552134714\n",
      "Iteration 3344: loss = 0.00268721662004375\n",
      "Iteration 3345: loss = 0.002687189298392187\n",
      "Iteration 3346: loss = 0.0026871619902462973\n",
      "Iteration 3347: loss = 0.0026871346955936095\n",
      "Iteration 3348: loss = 0.0026871074144216613\n",
      "Iteration 3349: loss = 0.002687080146718004\n",
      "Iteration 3350: loss = 0.002687052892470199\n",
      "Iteration 3351: loss = 0.0026870256516658207\n",
      "Iteration 3352: loss = 0.0026869984242924535\n",
      "Iteration 3353: loss = 0.002686971210337695\n",
      "Iteration 3354: loss = 0.0026869440097891536\n",
      "Iteration 3355: loss = 0.0026869168226344485\n",
      "Iteration 3356: loss = 0.002686889648861211\n",
      "Iteration 3357: loss = 0.0026868624884570846\n",
      "Iteration 3358: loss = 0.0026868353414097237\n",
      "Iteration 3359: loss = 0.0026868082077067933\n",
      "Iteration 3360: loss = 0.002686781087335972\n",
      "Iteration 3361: loss = 0.002686753980284947\n",
      "Iteration 3362: loss = 0.00268672688654142\n",
      "Iteration 3363: loss = 0.0026866998060931023\n",
      "Iteration 3364: loss = 0.002686672738927717\n",
      "Iteration 3365: loss = 0.0026866456850329987\n",
      "Iteration 3366: loss = 0.002686618644396693\n",
      "Iteration 3367: loss = 0.002686591617006557\n",
      "Iteration 3368: loss = 0.0026865646028503606\n",
      "Iteration 3369: loss = 0.0026865376019158827\n",
      "Iteration 3370: loss = 0.0026865106141909165\n",
      "Iteration 3371: loss = 0.0026864836396632633\n",
      "Iteration 3372: loss = 0.002686456678320738\n",
      "Iteration 3373: loss = 0.002686429730151166\n",
      "Iteration 3374: loss = 0.0026864027951423834\n",
      "Iteration 3375: loss = 0.00268637587328224\n",
      "Iteration 3376: loss = 0.0026863489645585947\n",
      "Iteration 3377: loss = 0.002686322068959318\n",
      "Iteration 3378: loss = 0.002686295186472292\n",
      "Iteration 3379: loss = 0.0026862683170854108\n",
      "Iteration 3380: loss = 0.002686241460786578\n",
      "Iteration 3381: loss = 0.0026862146175637104\n",
      "Iteration 3382: loss = 0.0026861877874047343\n",
      "Iteration 3383: loss = 0.0026861609702975887\n",
      "Iteration 3384: loss = 0.0026861341662302238\n",
      "Iteration 3385: loss = 0.002686107375190599\n",
      "Iteration 3386: loss = 0.0026860805971666876\n",
      "Iteration 3387: loss = 0.002686053832146472\n",
      "Iteration 3388: loss = 0.002686027080117947\n",
      "Iteration 3389: loss = 0.0026860003410691183\n",
      "Iteration 3390: loss = 0.0026859736149880023\n",
      "Iteration 3391: loss = 0.0026859469018626274\n",
      "Iteration 3392: loss = 0.0026859202016810313\n",
      "Iteration 3393: loss = 0.0026858935144312658\n",
      "Iteration 3394: loss = 0.0026858668401013924\n",
      "Iteration 3395: loss = 0.002685840178679482\n",
      "Iteration 3396: loss = 0.002685813530153619\n",
      "Iteration 3397: loss = 0.002685786894511898\n",
      "Iteration 3398: loss = 0.002685760271742424\n",
      "Iteration 3399: loss = 0.002685733661833315\n",
      "Iteration 3400: loss = 0.002685707064772698\n",
      "Iteration 3401: loss = 0.0026856804805487117\n",
      "Iteration 3402: loss = 0.002685653909149506\n",
      "Iteration 3403: loss = 0.002685627350563243\n",
      "Iteration 3404: loss = 0.0026856008047780934\n",
      "Iteration 3405: loss = 0.002685574271782241\n",
      "Iteration 3406: loss = 0.002685547751563879\n",
      "Iteration 3407: loss = 0.002685521244111213\n",
      "Iteration 3408: loss = 0.0026854947494124592\n",
      "Iteration 3409: loss = 0.002685468267455843\n",
      "Iteration 3410: loss = 0.0026854417982296046\n",
      "Iteration 3411: loss = 0.002685415341721991\n",
      "Iteration 3412: loss = 0.002685388897921263\n",
      "Iteration 3413: loss = 0.00268536246681569\n",
      "Iteration 3414: loss = 0.002685336048393555\n",
      "Iteration 3415: loss = 0.002685309642643149\n",
      "Iteration 3416: loss = 0.002685283249552778\n",
      "Iteration 3417: loss = 0.002685256869110754\n",
      "Iteration 3418: loss = 0.002685230501305403\n",
      "Iteration 3419: loss = 0.0026852041461250613\n",
      "Iteration 3420: loss = 0.0026851778035580755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3421: loss = 0.002685151473592803\n",
      "Iteration 3422: loss = 0.0026851251562176135\n",
      "Iteration 3423: loss = 0.002685098851420885\n",
      "Iteration 3424: loss = 0.002685072559191009\n",
      "Iteration 3425: loss = 0.002685046279516386\n",
      "Iteration 3426: loss = 0.002685020012385429\n",
      "Iteration 3427: loss = 0.002684993757786559\n",
      "Iteration 3428: loss = 0.0026849675157082096\n",
      "Iteration 3429: loss = 0.002684941286138826\n",
      "Iteration 3430: loss = 0.0026849150690668633\n",
      "Iteration 3431: loss = 0.0026848888644807863\n",
      "Iteration 3432: loss = 0.0026848626723690715\n",
      "Iteration 3433: loss = 0.0026848364927202066\n",
      "Iteration 3434: loss = 0.0026848103255226892\n",
      "Iteration 3435: loss = 0.0026847841707650284\n",
      "Iteration 3436: loss = 0.0026847580284357435\n",
      "Iteration 3437: loss = 0.0026847318985233634\n",
      "Iteration 3438: loss = 0.0026847057810164303\n",
      "Iteration 3439: loss = 0.002684679675903495\n",
      "Iteration 3440: loss = 0.002684653583173119\n",
      "Iteration 3441: loss = 0.0026846275028138754\n",
      "Iteration 3442: loss = 0.002684601434814348\n",
      "Iteration 3443: loss = 0.0026845753791631304\n",
      "Iteration 3444: loss = 0.0026845493358488267\n",
      "Iteration 3445: loss = 0.002684523304860053\n",
      "Iteration 3446: loss = 0.0026844972861854344\n",
      "Iteration 3447: loss = 0.0026844712798136075\n",
      "Iteration 3448: loss = 0.0026844452857332194\n",
      "Iteration 3449: loss = 0.002684419303932927\n",
      "Iteration 3450: loss = 0.002684393334401399\n",
      "Iteration 3451: loss = 0.0026843673771273146\n",
      "Iteration 3452: loss = 0.002684341432099362\n",
      "Iteration 3453: loss = 0.002684315499306241\n",
      "Iteration 3454: loss = 0.002684289578736663\n",
      "Iteration 3455: loss = 0.002684263670379348\n",
      "Iteration 3456: loss = 0.0026842377742230267\n",
      "Iteration 3457: loss = 0.0026842118902564425\n",
      "Iteration 3458: loss = 0.002684186018468345\n",
      "Iteration 3459: loss = 0.0026841601588475\n",
      "Iteration 3460: loss = 0.0026841343113826787\n",
      "Iteration 3461: loss = 0.002684108476062666\n",
      "Iteration 3462: loss = 0.0026840826528762548\n",
      "Iteration 3463: loss = 0.0026840568418122503\n",
      "Iteration 3464: loss = 0.0026840310428594676\n",
      "Iteration 3465: loss = 0.0026840052560067325\n",
      "Iteration 3466: loss = 0.0026839794812428793\n",
      "Iteration 3467: loss = 0.002683953718556756\n",
      "Iteration 3468: loss = 0.002683927967937218\n",
      "Iteration 3469: loss = 0.002683902229373132\n",
      "Iteration 3470: loss = 0.002683876502853377\n",
      "Iteration 3471: loss = 0.00268385078836684\n",
      "Iteration 3472: loss = 0.002683825085902418\n",
      "Iteration 3473: loss = 0.002683799395449021\n",
      "Iteration 3474: loss = 0.002683773716995566\n",
      "Iteration 3475: loss = 0.0026837480505309836\n",
      "Iteration 3476: loss = 0.002683722396044213\n",
      "Iteration 3477: loss = 0.002683696753524202\n",
      "Iteration 3478: loss = 0.0026836711229599134\n",
      "Iteration 3479: loss = 0.0026836455043403157\n",
      "Iteration 3480: loss = 0.0026836198976543896\n",
      "Iteration 3481: loss = 0.0026835943028911268\n",
      "Iteration 3482: loss = 0.0026835687200395266\n",
      "Iteration 3483: loss = 0.0026835431490886026\n",
      "Iteration 3484: loss = 0.0026835175900273745\n",
      "Iteration 3485: loss = 0.0026834920428448748\n",
      "Iteration 3486: loss = 0.0026834665075301458\n",
      "Iteration 3487: loss = 0.0026834409840722387\n",
      "Iteration 3488: loss = 0.002683415472460217\n",
      "Iteration 3489: loss = 0.0026833899726831528\n",
      "Iteration 3490: loss = 0.0026833644847301286\n",
      "Iteration 3491: loss = 0.002683339008590238\n",
      "Iteration 3492: loss = 0.002683313544252583\n",
      "Iteration 3493: loss = 0.0026832880917062782\n",
      "Iteration 3494: loss = 0.0026832626509404466\n",
      "Iteration 3495: loss = 0.002683237221944221\n",
      "Iteration 3496: loss = 0.002683211804706746\n",
      "Iteration 3497: loss = 0.0026831863992171747\n",
      "Iteration 3498: loss = 0.0026831610054646716\n",
      "Iteration 3499: loss = 0.0026831356234384095\n",
      "Iteration 3500: loss = 0.0026831102531275743\n",
      "Iteration 3501: loss = 0.0026830848945213585\n",
      "Iteration 3502: loss = 0.0026830595476089675\n",
      "Iteration 3503: loss = 0.0026830342123796147\n",
      "Iteration 3504: loss = 0.0026830088888225244\n",
      "Iteration 3505: loss = 0.0026829835769269315\n",
      "Iteration 3506: loss = 0.002682958276682081\n",
      "Iteration 3507: loss = 0.002682932988077226\n",
      "Iteration 3508: loss = 0.0026829077111016307\n",
      "Iteration 3509: loss = 0.0026828824457445716\n",
      "Iteration 3510: loss = 0.002682857191995331\n",
      "Iteration 3511: loss = 0.002682831949843205\n",
      "Iteration 3512: loss = 0.002682806719277496\n",
      "Iteration 3513: loss = 0.0026827815002875197\n",
      "Iteration 3514: loss = 0.0026827562928626015\n",
      "Iteration 3515: loss = 0.0026827310969920735\n",
      "Iteration 3516: loss = 0.0026827059126652807\n",
      "Iteration 3517: loss = 0.002682680739871578\n",
      "Iteration 3518: loss = 0.002682655578600328\n",
      "Iteration 3519: loss = 0.002682630428840906\n",
      "Iteration 3520: loss = 0.0026826052905826953\n",
      "Iteration 3521: loss = 0.0026825801638150903\n",
      "Iteration 3522: loss = 0.0026825550485274938\n",
      "Iteration 3523: loss = 0.002682529944709319\n",
      "Iteration 3524: loss = 0.00268250485234999\n",
      "Iteration 3525: loss = 0.0026824797714389407\n",
      "Iteration 3526: loss = 0.002682454701965613\n",
      "Iteration 3527: loss = 0.0026824296439194612\n",
      "Iteration 3528: loss = 0.002682404597289946\n",
      "Iteration 3529: loss = 0.002682379562066542\n",
      "Iteration 3530: loss = 0.002682354538238731\n",
      "Iteration 3531: loss = 0.0026823295257960046\n",
      "Iteration 3532: loss = 0.002682304524727865\n",
      "Iteration 3533: loss = 0.002682279535023825\n",
      "Iteration 3534: loss = 0.0026822545566734047\n",
      "Iteration 3535: loss = 0.0026822295896661357\n",
      "Iteration 3536: loss = 0.0026822046339915603\n",
      "Iteration 3537: loss = 0.0026821796896392283\n",
      "Iteration 3538: loss = 0.002682154756598701\n",
      "Iteration 3539: loss = 0.0026821298348595475\n",
      "Iteration 3540: loss = 0.0026821049244113494\n",
      "Iteration 3541: loss = 0.0026820800252436946\n",
      "Iteration 3542: loss = 0.002682055137346184\n",
      "Iteration 3543: loss = 0.002682030260708427\n",
      "Iteration 3544: loss = 0.00268200539532004\n",
      "Iteration 3545: loss = 0.0026819805411706548\n",
      "Iteration 3546: loss = 0.002681955698249908\n",
      "Iteration 3547: loss = 0.002681930866547447\n",
      "Iteration 3548: loss = 0.002681906046052929\n",
      "Iteration 3549: loss = 0.0026818812367560224\n",
      "Iteration 3550: loss = 0.0026818564386464037\n",
      "Iteration 3551: loss = 0.0026818316517137585\n",
      "Iteration 3552: loss = 0.002681806875947784\n",
      "Iteration 3553: loss = 0.002681782111338185\n",
      "Iteration 3554: loss = 0.002681757357874676\n",
      "Iteration 3555: loss = 0.002681732615546983\n",
      "Iteration 3556: loss = 0.00268170788434484\n",
      "Iteration 3557: loss = 0.0026816831642579907\n",
      "Iteration 3558: loss = 0.0026816584552761882\n",
      "Iteration 3559: loss = 0.0026816337573891966\n",
      "Iteration 3560: loss = 0.0026816090705867887\n",
      "Iteration 3561: loss = 0.002681584394858745\n",
      "Iteration 3562: loss = 0.002681559730194858\n",
      "Iteration 3563: loss = 0.0026815350765849295\n",
      "Iteration 3564: loss = 0.002681510434018769\n",
      "Iteration 3565: loss = 0.002681485802486198\n",
      "Iteration 3566: loss = 0.002681461181977046\n",
      "Iteration 3567: loss = 0.0026814365724811503\n",
      "Iteration 3568: loss = 0.002681411973988361\n",
      "Iteration 3569: loss = 0.002681387386488537\n",
      "Iteration 3570: loss = 0.002681362809971544\n",
      "Iteration 3571: loss = 0.0026813382444272602\n",
      "Iteration 3572: loss = 0.0026813136898455716\n",
      "Iteration 3573: loss = 0.002681289146216374\n",
      "Iteration 3574: loss = 0.0026812646135295727\n",
      "Iteration 3575: loss = 0.0026812400917750833\n",
      "Iteration 3576: loss = 0.002681215580942829\n",
      "Iteration 3577: loss = 0.0026811910810227426\n",
      "Iteration 3578: loss = 0.0026811665920047686\n",
      "Iteration 3579: loss = 0.0026811421138788582\n",
      "Iteration 3580: loss = 0.0026811176466349737\n",
      "Iteration 3581: loss = 0.0026810931902630855\n",
      "Iteration 3582: loss = 0.002681068744753173\n",
      "Iteration 3583: loss = 0.002681044310095229\n",
      "Iteration 3584: loss = 0.0026810198862792494\n",
      "Iteration 3585: loss = 0.002680995473295244\n",
      "Iteration 3586: loss = 0.0026809710711332294\n",
      "Iteration 3587: loss = 0.0026809466797832336\n",
      "Iteration 3588: loss = 0.0026809222992352933\n",
      "Iteration 3589: loss = 0.0026808979294794524\n",
      "Iteration 3590: loss = 0.002680873570505767\n",
      "Iteration 3591: loss = 0.002680849222304301\n",
      "Iteration 3592: loss = 0.0026808248848651274\n",
      "Iteration 3593: loss = 0.0026808005581783296\n",
      "Iteration 3594: loss = 0.0026807762422339987\n",
      "Iteration 3595: loss = 0.0026807519370222357\n",
      "Iteration 3596: loss = 0.0026807276425331516\n",
      "Iteration 3597: loss = 0.0026807033587568662\n",
      "Iteration 3598: loss = 0.0026806790856835076\n",
      "Iteration 3599: loss = 0.0026806548233032136\n",
      "Iteration 3600: loss = 0.002680630571606132\n",
      "Iteration 3601: loss = 0.002680606330582419\n",
      "Iteration 3602: loss = 0.0026805821002222406\n",
      "Iteration 3603: loss = 0.002680557880515771\n",
      "Iteration 3604: loss = 0.002680533671453194\n",
      "Iteration 3605: loss = 0.0026805094730247025\n",
      "Iteration 3606: loss = 0.0026804852852205\n",
      "Iteration 3607: loss = 0.002680461108030796\n",
      "Iteration 3608: loss = 0.002680436941445813\n",
      "Iteration 3609: loss = 0.002680412785455778\n",
      "Iteration 3610: loss = 0.0026803886400509312\n",
      "Iteration 3611: loss = 0.0026803645052215215\n",
      "Iteration 3612: loss = 0.002680340380957804\n",
      "Iteration 3613: loss = 0.0026803162672500453\n",
      "Iteration 3614: loss = 0.0026802921640885213\n",
      "Iteration 3615: loss = 0.0026802680714635148\n",
      "Iteration 3616: loss = 0.0026802439893653194\n",
      "Iteration 3617: loss = 0.0026802199177842377\n",
      "Iteration 3618: loss = 0.0026801958567105807\n",
      "Iteration 3619: loss = 0.002680171806134669\n",
      "Iteration 3620: loss = 0.0026801477660468318\n",
      "Iteration 3621: loss = 0.0026801237364374067\n",
      "Iteration 3622: loss = 0.002680099717296743\n",
      "Iteration 3623: loss = 0.0026800757086151957\n",
      "Iteration 3624: loss = 0.00268005171038313\n",
      "Iteration 3625: loss = 0.0026800277225909213\n",
      "Iteration 3626: loss = 0.0026800037452289517\n",
      "Iteration 3627: loss = 0.002679979778287615\n",
      "Iteration 3628: loss = 0.002679955821757312\n",
      "Iteration 3629: loss = 0.0026799318756284517\n",
      "Iteration 3630: loss = 0.0026799079398914546\n",
      "Iteration 3631: loss = 0.0026798840145367485\n",
      "Iteration 3632: loss = 0.0026798600995547707\n",
      "Iteration 3633: loss = 0.002679836194935967\n",
      "Iteration 3634: loss = 0.0026798123006707914\n",
      "Iteration 3635: loss = 0.002679788416749709\n",
      "Iteration 3636: loss = 0.0026797645431631922\n",
      "Iteration 3637: loss = 0.002679740679901722\n",
      "Iteration 3638: loss = 0.0026797168269557894\n",
      "Iteration 3639: loss = 0.002679692984315894\n",
      "Iteration 3640: loss = 0.002679669151972543\n",
      "Iteration 3641: loss = 0.0026796453299162536\n",
      "Iteration 3642: loss = 0.0026796215181375525\n",
      "Iteration 3643: loss = 0.0026795977166269745\n",
      "Iteration 3644: loss = 0.0026795739253750615\n",
      "Iteration 3645: loss = 0.002679550144372368\n",
      "Iteration 3646: loss = 0.0026795263736094535\n",
      "Iteration 3647: loss = 0.0026795026130768894\n",
      "Iteration 3648: loss = 0.0026794788627652532\n",
      "Iteration 3649: loss = 0.0026794551226651336\n",
      "Iteration 3650: loss = 0.002679431392767126\n",
      "Iteration 3651: loss = 0.0026794076730618363\n",
      "Iteration 3652: loss = 0.002679383963539878\n",
      "Iteration 3653: loss = 0.002679360264191874\n",
      "Iteration 3654: loss = 0.002679336575008456\n",
      "Iteration 3655: loss = 0.0026793128959802623\n",
      "Iteration 3656: loss = 0.0026792892270979442\n",
      "Iteration 3657: loss = 0.002679265568352158\n",
      "Iteration 3658: loss = 0.002679241919733571\n",
      "Iteration 3659: loss = 0.002679218281232857\n",
      "Iteration 3660: loss = 0.0026791946528407\n",
      "Iteration 3661: loss = 0.002679171034547793\n",
      "Iteration 3662: loss = 0.002679147426344837\n",
      "Iteration 3663: loss = 0.0026791238282225407\n",
      "Iteration 3664: loss = 0.0026791002401716246\n",
      "Iteration 3665: loss = 0.0026790766621828145\n",
      "Iteration 3666: loss = 0.0026790530942468463\n",
      "Iteration 3667: loss = 0.0026790295363544638\n",
      "Iteration 3668: loss = 0.002679005988496421\n",
      "Iteration 3669: loss = 0.002678982450663479\n",
      "Iteration 3670: loss = 0.0026789589228464086\n",
      "Iteration 3671: loss = 0.0026789354050359886\n",
      "Iteration 3672: loss = 0.002678911897223006\n",
      "Iteration 3673: loss = 0.002678888399398257\n",
      "Iteration 3674: loss = 0.002678864911552547\n",
      "Iteration 3675: loss = 0.0026788414336766887\n",
      "Iteration 3676: loss = 0.002678817965761504\n",
      "Iteration 3677: loss = 0.0026787945077978233\n",
      "Iteration 3678: loss = 0.0026787710597764853\n",
      "Iteration 3679: loss = 0.0026787476216883384\n",
      "Iteration 3680: loss = 0.002678724193524238\n",
      "Iteration 3681: loss = 0.002678700775275048\n",
      "Iteration 3682: loss = 0.0026786773669316426\n",
      "Iteration 3683: loss = 0.002678653968484903\n",
      "Iteration 3684: loss = 0.00267863057992572\n",
      "Iteration 3685: loss = 0.002678607201244991\n",
      "Iteration 3686: loss = 0.0026785838324336238\n",
      "Iteration 3687: loss = 0.0026785604734825344\n",
      "Iteration 3688: loss = 0.002678537124382646\n",
      "Iteration 3689: loss = 0.0026785137851248917\n",
      "Iteration 3690: loss = 0.0026784904557002123\n",
      "Iteration 3691: loss = 0.0026784671360995573\n",
      "Iteration 3692: loss = 0.0026784438263138853\n",
      "Iteration 3693: loss = 0.0026784205263341615\n",
      "Iteration 3694: loss = 0.0026783972361513614\n",
      "Iteration 3695: loss = 0.002678373955756468\n",
      "Iteration 3696: loss = 0.0026783506851404734\n",
      "Iteration 3697: loss = 0.0026783274242943764\n",
      "Iteration 3698: loss = 0.0026783041732091864\n",
      "Iteration 3699: loss = 0.002678280931875921\n",
      "Iteration 3700: loss = 0.0026782577002856037\n",
      "Iteration 3701: loss = 0.0026782344784292687\n",
      "Iteration 3702: loss = 0.002678211266297958\n",
      "Iteration 3703: loss = 0.0026781880638827226\n",
      "Iteration 3704: loss = 0.00267816487117462\n",
      "Iteration 3705: loss = 0.002678141688164718\n",
      "Iteration 3706: loss = 0.002678118514844091\n",
      "Iteration 3707: loss = 0.002678095351203825\n",
      "Iteration 3708: loss = 0.0026780721972350092\n",
      "Iteration 3709: loss = 0.002678049052928745\n",
      "Iteration 3710: loss = 0.0026780259182761414\n",
      "Iteration 3711: loss = 0.0026780027932683144\n",
      "Iteration 3712: loss = 0.0026779796778963905\n",
      "Iteration 3713: loss = 0.002677956572151502\n",
      "Iteration 3714: loss = 0.002677933476024791\n",
      "Iteration 3715: loss = 0.002677910389507408\n",
      "Iteration 3716: loss = 0.0026778873125905107\n",
      "Iteration 3717: loss = 0.002677864245265266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3718: loss = 0.002677841187522849\n",
      "Iteration 3719: loss = 0.0026778181393544414\n",
      "Iteration 3720: loss = 0.0026777951007512352\n",
      "Iteration 3721: loss = 0.0026777720717044307\n",
      "Iteration 3722: loss = 0.002677749052205234\n",
      "Iteration 3723: loss = 0.002677726042244862\n",
      "Iteration 3724: loss = 0.0026777030418145393\n",
      "Iteration 3725: loss = 0.002677680050905497\n",
      "Iteration 3726: loss = 0.002677657069508976\n",
      "Iteration 3727: loss = 0.0026776340976162253\n",
      "Iteration 3728: loss = 0.0026776111352185016\n",
      "Iteration 3729: loss = 0.002677588182307069\n",
      "Iteration 3730: loss = 0.002677565238873202\n",
      "Iteration 3731: loss = 0.002677542304908181\n",
      "Iteration 3732: loss = 0.002677519380403295\n",
      "Iteration 3733: loss = 0.0026774964653498426\n",
      "Iteration 3734: loss = 0.0026774735597391284\n",
      "Iteration 3735: loss = 0.0026774506635624676\n",
      "Iteration 3736: loss = 0.0026774277768111805\n",
      "Iteration 3737: loss = 0.0026774048994765974\n",
      "Iteration 3738: loss = 0.0026773820315500576\n",
      "Iteration 3739: loss = 0.002677359173022906\n",
      "Iteration 3740: loss = 0.0026773363238864977\n",
      "Iteration 3741: loss = 0.0026773134841321946\n",
      "Iteration 3742: loss = 0.0026772906537513674\n",
      "Iteration 3743: loss = 0.002677267832735393\n",
      "Iteration 3744: loss = 0.00267724502107566\n",
      "Iteration 3745: loss = 0.0026772222187635615\n",
      "Iteration 3746: loss = 0.0026771994257905004\n",
      "Iteration 3747: loss = 0.0026771766421478877\n",
      "Iteration 3748: loss = 0.0026771538678271416\n",
      "Iteration 3749: loss = 0.0026771311028196887\n",
      "Iteration 3750: loss = 0.0026771083471169633\n",
      "Iteration 3751: loss = 0.0026770856007104094\n",
      "Iteration 3752: loss = 0.0026770628635914757\n",
      "Iteration 3753: loss = 0.0026770401357516225\n",
      "Iteration 3754: loss = 0.002677017417182314\n",
      "Iteration 3755: loss = 0.002676994707875028\n",
      "Iteration 3756: loss = 0.002676972007821244\n",
      "Iteration 3757: loss = 0.0026769493170124537\n",
      "Iteration 3758: loss = 0.0026769266354401563\n",
      "Iteration 3759: loss = 0.002676903963095856\n",
      "Iteration 3760: loss = 0.002676881299971069\n",
      "Iteration 3761: loss = 0.0026768586460573164\n",
      "Iteration 3762: loss = 0.002676836001346128\n",
      "Iteration 3763: loss = 0.002676813365829043\n",
      "Iteration 3764: loss = 0.0026767907394976065\n",
      "Iteration 3765: loss = 0.002676768122343372\n",
      "Iteration 3766: loss = 0.0026767455143579017\n",
      "Iteration 3767: loss = 0.002676722915532765\n",
      "Iteration 3768: loss = 0.0026767003258595387\n",
      "Iteration 3769: loss = 0.0026766777453298093\n",
      "Iteration 3770: loss = 0.002676655173935169\n",
      "Iteration 3771: loss = 0.0026766326116672186\n",
      "Iteration 3772: loss = 0.002676610058517568\n",
      "Iteration 3773: loss = 0.002676587514477832\n",
      "Iteration 3774: loss = 0.002676564979539637\n",
      "Iteration 3775: loss = 0.0026765424536946145\n",
      "Iteration 3776: loss = 0.0026765199369344047\n",
      "Iteration 3777: loss = 0.002676497429250655\n",
      "Iteration 3778: loss = 0.002676474930635022\n",
      "Iteration 3779: loss = 0.002676452441079168\n",
      "Iteration 3780: loss = 0.0026764299605747653\n",
      "Iteration 3781: loss = 0.0026764074891134936\n",
      "Iteration 3782: loss = 0.0026763850266870383\n",
      "Iteration 3783: loss = 0.002676362573287094\n",
      "Iteration 3784: loss = 0.002676340128905363\n",
      "Iteration 3785: loss = 0.0026763176935335566\n",
      "Iteration 3786: loss = 0.002676295267163392\n",
      "Iteration 3787: loss = 0.0026762728497865948\n",
      "Iteration 3788: loss = 0.002676250441394898\n",
      "Iteration 3789: loss = 0.0026762280419800424\n",
      "Iteration 3790: loss = 0.0026762056515337775\n",
      "Iteration 3791: loss = 0.0026761832700478595\n",
      "Iteration 3792: loss = 0.0026761608975140517\n",
      "Iteration 3793: loss = 0.0026761385339241273\n",
      "Iteration 3794: loss = 0.0026761161792698647\n",
      "Iteration 3795: loss = 0.002676093833543052\n",
      "Iteration 3796: loss = 0.0026760714967354825\n",
      "Iteration 3797: loss = 0.0026760491688389604\n",
      "Iteration 3798: loss = 0.002676026849845294\n",
      "Iteration 3799: loss = 0.0026760045397463033\n",
      "Iteration 3800: loss = 0.0026759822385338124\n",
      "Iteration 3801: loss = 0.0026759599461996537\n",
      "Iteration 3802: loss = 0.0026759376627356695\n",
      "Iteration 3803: loss = 0.002675915388133708\n",
      "Iteration 3804: loss = 0.002675893122385624\n",
      "Iteration 3805: loss = 0.0026758708654832817\n",
      "Iteration 3806: loss = 0.0026758486174185523\n",
      "Iteration 3807: loss = 0.002675826378183314\n",
      "Iteration 3808: loss = 0.0026758041477694533\n",
      "Iteration 3809: loss = 0.002675781926168865\n",
      "Iteration 3810: loss = 0.0026757597133734486\n",
      "Iteration 3811: loss = 0.0026757375093751147\n",
      "Iteration 3812: loss = 0.0026757153141657797\n",
      "Iteration 3813: loss = 0.0026756931277373682\n",
      "Iteration 3814: loss = 0.00267567095008181\n",
      "Iteration 3815: loss = 0.002675648781191046\n",
      "Iteration 3816: loss = 0.0026756266210570213\n",
      "Iteration 3817: loss = 0.0026756044696716924\n",
      "Iteration 3818: loss = 0.0026755823270270197\n",
      "Iteration 3819: loss = 0.0026755601931149715\n",
      "Iteration 3820: loss = 0.0026755380679275268\n",
      "Iteration 3821: loss = 0.0026755159514566685\n",
      "Iteration 3822: loss = 0.002675493843694388\n",
      "Iteration 3823: loss = 0.0026754717446326854\n",
      "Iteration 3824: loss = 0.0026754496542635677\n",
      "Iteration 3825: loss = 0.0026754275725790473\n",
      "Iteration 3826: loss = 0.002675405499571147\n",
      "Iteration 3827: loss = 0.002675383435231897\n",
      "Iteration 3828: loss = 0.002675361379553331\n",
      "Iteration 3829: loss = 0.0026753393325274952\n",
      "Iteration 3830: loss = 0.0026753172941464403\n",
      "Iteration 3831: loss = 0.002675295264402225\n",
      "Iteration 3832: loss = 0.0026752732432869155\n",
      "Iteration 3833: loss = 0.002675251230792585\n",
      "Iteration 3834: loss = 0.002675229226911316\n",
      "Iteration 3835: loss = 0.002675207231635195\n",
      "Iteration 3836: loss = 0.002675185244956319\n",
      "Iteration 3837: loss = 0.0026751632668667916\n",
      "Iteration 3838: loss = 0.002675141297358722\n",
      "Iteration 3839: loss = 0.002675119336424229\n",
      "Iteration 3840: loss = 0.0026750973840554376\n",
      "Iteration 3841: loss = 0.0026750754402444805\n",
      "Iteration 3842: loss = 0.002675053504983498\n",
      "Iteration 3843: loss = 0.0026750315782646363\n",
      "Iteration 3844: loss = 0.0026750096600800517\n",
      "Iteration 3845: loss = 0.002674987750421906\n",
      "Iteration 3846: loss = 0.0026749658492823667\n",
      "Iteration 3847: loss = 0.0026749439566536126\n",
      "Iteration 3848: loss = 0.002674922072527826\n",
      "Iteration 3849: loss = 0.002674900196897199\n",
      "Iteration 3850: loss = 0.00267487832975393\n",
      "Iteration 3851: loss = 0.0026748564710902248\n",
      "Iteration 3852: loss = 0.002674834620898297\n",
      "Iteration 3853: loss = 0.0026748127791703658\n",
      "Iteration 3854: loss = 0.0026747909458986587\n",
      "Iteration 3855: loss = 0.0026747691210754127\n",
      "Iteration 3856: loss = 0.0026747473046928685\n",
      "Iteration 3857: loss = 0.002674725496743275\n",
      "Iteration 3858: loss = 0.002674703697218889\n",
      "Iteration 3859: loss = 0.002674681906111976\n",
      "Iteration 3860: loss = 0.002674660123414805\n",
      "Iteration 3861: loss = 0.002674638349119656\n",
      "Iteration 3862: loss = 0.0026746165832188125\n",
      "Iteration 3863: loss = 0.002674594825704569\n",
      "Iteration 3864: loss = 0.0026745730765692253\n",
      "Iteration 3865: loss = 0.002674551335805087\n",
      "Iteration 3866: loss = 0.0026745296034044698\n",
      "Iteration 3867: loss = 0.002674507879359695\n",
      "Iteration 3868: loss = 0.0026744861636630907\n",
      "Iteration 3869: loss = 0.0026744644563069936\n",
      "Iteration 3870: loss = 0.0026744427572837452\n",
      "Iteration 3871: loss = 0.0026744210665856976\n",
      "Iteration 3872: loss = 0.0026743993842052063\n",
      "Iteration 3873: loss = 0.0026743777101346368\n",
      "Iteration 3874: loss = 0.0026743560443663605\n",
      "Iteration 3875: loss = 0.0026743343868927557\n",
      "Iteration 3876: loss = 0.0026743127377062076\n",
      "Iteration 3877: loss = 0.00267429109679911\n",
      "Iteration 3878: loss = 0.002674269464163864\n",
      "Iteration 3879: loss = 0.002674247839792874\n",
      "Iteration 3880: loss = 0.002674226223678556\n",
      "Iteration 3881: loss = 0.002674204615813331\n",
      "Iteration 3882: loss = 0.002674183016189627\n",
      "Iteration 3883: loss = 0.0026741614247998797\n",
      "Iteration 3884: loss = 0.0026741398416365316\n",
      "Iteration 3885: loss = 0.0026741182666920327\n",
      "Iteration 3886: loss = 0.0026740966999588387\n",
      "Iteration 3887: loss = 0.002674075141429414\n",
      "Iteration 3888: loss = 0.0026740535910962284\n",
      "Iteration 3889: loss = 0.0026740320489517604\n",
      "Iteration 3890: loss = 0.002674010514988495\n",
      "Iteration 3891: loss = 0.002673988989198923\n",
      "Iteration 3892: loss = 0.002673967471575544\n",
      "Iteration 3893: loss = 0.002673945962110863\n",
      "Iteration 3894: loss = 0.002673924460797394\n",
      "Iteration 3895: loss = 0.0026739029676276557\n",
      "Iteration 3896: loss = 0.002673881482594175\n",
      "Iteration 3897: loss = 0.0026738600056894868\n",
      "Iteration 3898: loss = 0.0026738385369061302\n",
      "Iteration 3899: loss = 0.002673817076236654\n",
      "Iteration 3900: loss = 0.002673795623673612\n",
      "Iteration 3901: loss = 0.002673774179209567\n",
      "Iteration 3902: loss = 0.0026737527428370863\n",
      "Iteration 3903: loss = 0.0026737313145487464\n",
      "Iteration 3904: loss = 0.0026737098943371293\n",
      "Iteration 3905: loss = 0.0026736884821948245\n",
      "Iteration 3906: loss = 0.0026736670781144275\n",
      "Iteration 3907: loss = 0.002673645682088543\n",
      "Iteration 3908: loss = 0.0026736242941097802\n",
      "Iteration 3909: loss = 0.0026736029141707558\n",
      "Iteration 3910: loss = 0.0026735815422640947\n",
      "Iteration 3911: loss = 0.002673560178382427\n",
      "Iteration 3912: loss = 0.00267353882251839\n",
      "Iteration 3913: loss = 0.0026735174746646295\n",
      "Iteration 3914: loss = 0.0026734961348137964\n",
      "Iteration 3915: loss = 0.0026734748029585485\n",
      "Iteration 3916: loss = 0.0026734534790915517\n",
      "Iteration 3917: loss = 0.0026734321632054777\n",
      "Iteration 3918: loss = 0.002673410855293006\n",
      "Iteration 3919: loss = 0.0026733895553468206\n",
      "Iteration 3920: loss = 0.0026733682633596147\n",
      "Iteration 3921: loss = 0.0026733469793240893\n",
      "Iteration 3922: loss = 0.0026733257032329483\n",
      "Iteration 3923: loss = 0.0026733044350789065\n",
      "Iteration 3924: loss = 0.0026732831748546826\n",
      "Iteration 3925: loss = 0.0026732619225530037\n",
      "Iteration 3926: loss = 0.0026732406781666035\n",
      "Iteration 3927: loss = 0.0026732194416882204\n",
      "Iteration 3928: loss = 0.0026731982131106037\n",
      "Iteration 3929: loss = 0.002673176992426506\n",
      "Iteration 3930: loss = 0.0026731557796286884\n",
      "Iteration 3931: loss = 0.002673134574709917\n",
      "Iteration 3932: loss = 0.0026731133776629673\n",
      "Iteration 3933: loss = 0.0026730921884806193\n",
      "Iteration 3934: loss = 0.0026730710071556606\n",
      "Iteration 3935: loss = 0.002673049833680885\n",
      "Iteration 3936: loss = 0.0026730286680490944\n",
      "Iteration 3937: loss = 0.0026730075102530965\n",
      "Iteration 3938: loss = 0.0026729863602857054\n",
      "Iteration 3939: loss = 0.002672965218139742\n",
      "Iteration 3940: loss = 0.002672944083808034\n",
      "Iteration 3941: loss = 0.0026729229572834174\n",
      "Iteration 3942: loss = 0.0026729018385587317\n",
      "Iteration 3943: loss = 0.002672880727626826\n",
      "Iteration 3944: loss = 0.002672859624480554\n",
      "Iteration 3945: loss = 0.002672838529112778\n",
      "Iteration 3946: loss = 0.0026728174415163656\n",
      "Iteration 3947: loss = 0.002672796361684191\n",
      "Iteration 3948: loss = 0.0026727752896091366\n",
      "Iteration 3949: loss = 0.002672754225284089\n",
      "Iteration 3950: loss = 0.0026727331687019444\n",
      "Iteration 3951: loss = 0.0026727121198556027\n",
      "Iteration 3952: loss = 0.0026726910787379715\n",
      "Iteration 3953: loss = 0.0026726700453419665\n",
      "Iteration 3954: loss = 0.0026726490196605076\n",
      "Iteration 3955: loss = 0.002672628001686524\n",
      "Iteration 3956: loss = 0.0026726069914129497\n",
      "Iteration 3957: loss = 0.0026725859888327244\n",
      "Iteration 3958: loss = 0.0026725649939387964\n",
      "Iteration 3959: loss = 0.0026725440067241197\n",
      "Iteration 3960: loss = 0.002672523027181656\n",
      "Iteration 3961: loss = 0.0026725020553043707\n",
      "Iteration 3962: loss = 0.002672481091085239\n",
      "Iteration 3963: loss = 0.0026724601345172413\n",
      "Iteration 3964: loss = 0.002672439185593364\n",
      "Iteration 3965: loss = 0.002672418244306601\n",
      "Iteration 3966: loss = 0.0026723973106499523\n",
      "Iteration 3967: loss = 0.002672376384616425\n",
      "Iteration 3968: loss = 0.0026723554661990307\n",
      "Iteration 3969: loss = 0.002672334555390791\n",
      "Iteration 3970: loss = 0.0026723136521847306\n",
      "Iteration 3971: loss = 0.0026722927565738836\n",
      "Iteration 3972: loss = 0.002672271868551288\n",
      "Iteration 3973: loss = 0.00267225098810999\n",
      "Iteration 3974: loss = 0.002672230115243042\n",
      "Iteration 3975: loss = 0.002672209249943502\n",
      "Iteration 3976: loss = 0.0026721883922044367\n",
      "Iteration 3977: loss = 0.0026721675420189163\n",
      "Iteration 3978: loss = 0.0026721466993800194\n",
      "Iteration 3979: loss = 0.002672125864280831\n",
      "Iteration 3980: loss = 0.0026721050367144415\n",
      "Iteration 3981: loss = 0.0026720842166739495\n",
      "Iteration 3982: loss = 0.002672063404152458\n",
      "Iteration 3983: loss = 0.0026720425991430784\n",
      "Iteration 3984: loss = 0.0026720218016389264\n",
      "Iteration 3985: loss = 0.0026720010116331257\n",
      "Iteration 3986: loss = 0.002671980229118807\n",
      "Iteration 3987: loss = 0.002671959454089105\n",
      "Iteration 3988: loss = 0.002671938686537163\n",
      "Iteration 3989: loss = 0.00267191792645613\n",
      "Iteration 3990: loss = 0.002671897173839161\n",
      "Iteration 3991: loss = 0.0026718764286794185\n",
      "Iteration 3992: loss = 0.0026718556909700705\n",
      "Iteration 3993: loss = 0.002671834960704291\n",
      "Iteration 3994: loss = 0.002671814237875261\n",
      "Iteration 3995: loss = 0.002671793522476168\n",
      "Iteration 3996: loss = 0.0026717728145002058\n",
      "Iteration 3997: loss = 0.0026717521139405743\n",
      "Iteration 3998: loss = 0.0026717314207904794\n",
      "Iteration 3999: loss = 0.0026717107350431347\n",
      "Iteration 4000: loss = 0.002671690056691759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4001: loss = 0.0026716693857295765\n",
      "Iteration 4002: loss = 0.002671648722149821\n",
      "Iteration 4003: loss = 0.002671628065945729\n",
      "Iteration 4004: loss = 0.0026716074171105456\n",
      "Iteration 4005: loss = 0.0026715867756375213\n",
      "Iteration 4006: loss = 0.002671566141519913\n",
      "Iteration 4007: loss = 0.002671545514750984\n",
      "Iteration 4008: loss = 0.002671524895324004\n",
      "Iteration 4009: loss = 0.0026715042832322487\n",
      "Iteration 4010: loss = 0.0026714836784690007\n",
      "Iteration 4011: loss = 0.002671463081027548\n",
      "Iteration 4012: loss = 0.002671442490901185\n",
      "Iteration 4013: loss = 0.0026714219080832143\n",
      "Iteration 4014: loss = 0.0026714013325669413\n",
      "Iteration 4015: loss = 0.00267138076434568\n",
      "Iteration 4016: loss = 0.00267136020341275\n",
      "Iteration 4017: loss = 0.002671339649761478\n",
      "Iteration 4018: loss = 0.002671319103385196\n",
      "Iteration 4019: loss = 0.0026712985642772416\n",
      "Iteration 4020: loss = 0.0026712780324309607\n",
      "Iteration 4021: loss = 0.0026712575078397032\n",
      "Iteration 4022: loss = 0.002671236990496827\n",
      "Iteration 4023: loss = 0.002671216480395695\n",
      "Iteration 4024: loss = 0.0026711959775296765\n",
      "Iteration 4025: loss = 0.0026711754818921477\n",
      "Iteration 4026: loss = 0.0026711549934764905\n",
      "Iteration 4027: loss = 0.0026711345122760926\n",
      "Iteration 4028: loss = 0.0026711140382843484\n",
      "Iteration 4029: loss = 0.0026710935714946583\n",
      "Iteration 4030: loss = 0.002671073111900429\n",
      "Iteration 4031: loss = 0.0026710526594950733\n",
      "Iteration 4032: loss = 0.0026710322142720097\n",
      "Iteration 4033: loss = 0.0026710117762246645\n",
      "Iteration 4034: loss = 0.0026709913453464673\n",
      "Iteration 4035: loss = 0.002670970921630856\n",
      "Iteration 4036: loss = 0.0026709505050712757\n",
      "Iteration 4037: loss = 0.0026709300956611736\n",
      "Iteration 4038: loss = 0.002670909693394007\n",
      "Iteration 4039: loss = 0.002670889298263237\n",
      "Iteration 4040: loss = 0.0026708689102623315\n",
      "Iteration 4041: loss = 0.0026708485293847656\n",
      "Iteration 4042: loss = 0.002670828155624018\n",
      "Iteration 4043: loss = 0.0026708077889735764\n",
      "Iteration 4044: loss = 0.002670787429426932\n",
      "Iteration 4045: loss = 0.0026707670769775837\n",
      "Iteration 4046: loss = 0.0026707467316190365\n",
      "Iteration 4047: loss = 0.0026707263933447994\n",
      "Iteration 4048: loss = 0.0026707060621483916\n",
      "Iteration 4049: loss = 0.0026706857380233333\n",
      "Iteration 4050: loss = 0.0026706654209631545\n",
      "Iteration 4051: loss = 0.00267064511096139\n",
      "Iteration 4052: loss = 0.0026706248080115807\n",
      "Iteration 4053: loss = 0.0026706045121072724\n",
      "Iteration 4054: loss = 0.0026705842232420196\n",
      "Iteration 4055: loss = 0.00267056394140938\n",
      "Iteration 4056: loss = 0.002670543666602919\n",
      "Iteration 4057: loss = 0.0026705233988162077\n",
      "Iteration 4058: loss = 0.0026705031380428227\n",
      "Iteration 4059: loss = 0.002670482884276347\n",
      "Iteration 4060: loss = 0.0026704626375103706\n",
      "Iteration 4061: loss = 0.0026704423977384874\n",
      "Iteration 4062: loss = 0.0026704221649542985\n",
      "Iteration 4063: loss = 0.0026704019391514114\n",
      "Iteration 4064: loss = 0.0026703817203234384\n",
      "Iteration 4065: loss = 0.002670361508463998\n",
      "Iteration 4066: loss = 0.0026703413035667166\n",
      "Iteration 4067: loss = 0.0026703211056252232\n",
      "Iteration 4068: loss = 0.0026703009146331557\n",
      "Iteration 4069: loss = 0.002670280730584157\n",
      "Iteration 4070: loss = 0.0026702605534718745\n",
      "Iteration 4071: loss = 0.0026702403832899635\n",
      "Iteration 4072: loss = 0.0026702202200320846\n",
      "Iteration 4073: loss = 0.002670200063691904\n",
      "Iteration 4074: loss = 0.0026701799142630944\n",
      "Iteration 4075: loss = 0.0026701597717393344\n",
      "Iteration 4076: loss = 0.002670139636114307\n",
      "Iteration 4077: loss = 0.002670119507381703\n",
      "Iteration 4078: loss = 0.002670099385535218\n",
      "Iteration 4079: loss = 0.0026700792705685547\n",
      "Iteration 4080: loss = 0.0026700591624754196\n",
      "Iteration 4081: loss = 0.002670039061249528\n",
      "Iteration 4082: loss = 0.002670018966884598\n",
      "Iteration 4083: loss = 0.0026699988793743553\n",
      "Iteration 4084: loss = 0.002669978798712531\n",
      "Iteration 4085: loss = 0.0026699587248928632\n",
      "Iteration 4086: loss = 0.0026699386579090937\n",
      "Iteration 4087: loss = 0.0026699185977549717\n",
      "Iteration 4088: loss = 0.0026698985444242518\n",
      "Iteration 4089: loss = 0.002669878497910695\n",
      "Iteration 4090: loss = 0.0026698584582080668\n",
      "Iteration 4091: loss = 0.00266983842531014\n",
      "Iteration 4092: loss = 0.0026698183992106927\n",
      "Iteration 4093: loss = 0.002669798379903507\n",
      "Iteration 4094: loss = 0.0026697783673823748\n",
      "Iteration 4095: loss = 0.002669758361641089\n",
      "Iteration 4096: loss = 0.002669738362673454\n",
      "Iteration 4097: loss = 0.002669718370473274\n",
      "Iteration 4098: loss = 0.002669698385034363\n",
      "Iteration 4099: loss = 0.00266967840635054\n",
      "Iteration 4100: loss = 0.0026696584344156277\n",
      "Iteration 4101: loss = 0.002669638469223458\n",
      "Iteration 4102: loss = 0.0026696185107678645\n",
      "Iteration 4103: loss = 0.002669598559042692\n",
      "Iteration 4104: loss = 0.002669578614041785\n",
      "Iteration 4105: loss = 0.0026695586757589985\n",
      "Iteration 4106: loss = 0.00266953874418819\n",
      "Iteration 4107: loss = 0.0026695188193232256\n",
      "Iteration 4108: loss = 0.0026694989011579744\n",
      "Iteration 4109: loss = 0.0026694789896863132\n",
      "Iteration 4110: loss = 0.002669459084902123\n",
      "Iteration 4111: loss = 0.0026694391867992926\n",
      "Iteration 4112: loss = 0.002669419295371714\n",
      "Iteration 4113: loss = 0.0026693994106132867\n",
      "Iteration 4114: loss = 0.002669379532517916\n",
      "Iteration 4115: loss = 0.002669359661079511\n",
      "Iteration 4116: loss = 0.0026693397962919883\n",
      "Iteration 4117: loss = 0.0026693199381492695\n",
      "Iteration 4118: loss = 0.0026693000866452824\n",
      "Iteration 4119: loss = 0.0026692802417739602\n",
      "Iteration 4120: loss = 0.002669260403529241\n",
      "Iteration 4121: loss = 0.002669240571905069\n",
      "Iteration 4122: loss = 0.002669220746895396\n",
      "Iteration 4123: loss = 0.0026692009284941757\n",
      "Iteration 4124: loss = 0.0026691811166953705\n",
      "Iteration 4125: loss = 0.002669161311492947\n",
      "Iteration 4126: loss = 0.0026691415128808787\n",
      "Iteration 4127: loss = 0.002669121720853143\n",
      "Iteration 4128: loss = 0.0026691019354037238\n",
      "Iteration 4129: loss = 0.002669082156526611\n",
      "Iteration 4130: loss = 0.0026690623842158\n",
      "Iteration 4131: loss = 0.0026690426184652915\n",
      "Iteration 4132: loss = 0.0026690228592690917\n",
      "Iteration 4133: loss = 0.0026690031066212124\n",
      "Iteration 4134: loss = 0.0026689833605156718\n",
      "Iteration 4135: loss = 0.0026689636209464925\n",
      "Iteration 4136: loss = 0.002668943887907703\n",
      "Iteration 4137: loss = 0.0026689241613933397\n",
      "Iteration 4138: loss = 0.0026689044413974396\n",
      "Iteration 4139: loss = 0.0026688847279140496\n",
      "Iteration 4140: loss = 0.002668865020937221\n",
      "Iteration 4141: loss = 0.0026688453204610097\n",
      "Iteration 4142: loss = 0.0026688256264794786\n",
      "Iteration 4143: loss = 0.002668805938986695\n",
      "Iteration 4144: loss = 0.0026687862579767322\n",
      "Iteration 4145: loss = 0.00266876658344367\n",
      "Iteration 4146: loss = 0.0026687469153815913\n",
      "Iteration 4147: loss = 0.0026687272537845863\n",
      "Iteration 4148: loss = 0.0026687075986467504\n",
      "Iteration 4149: loss = 0.002668687949962185\n",
      "Iteration 4150: loss = 0.002668668307724997\n",
      "Iteration 4151: loss = 0.0026686486719292965\n",
      "Iteration 4152: loss = 0.0026686290425692026\n",
      "Iteration 4153: loss = 0.0026686094196388386\n",
      "Iteration 4154: loss = 0.002668589803132331\n",
      "Iteration 4155: loss = 0.0026685701930438143\n",
      "Iteration 4156: loss = 0.0026685505893674294\n",
      "Iteration 4157: loss = 0.0026685309920973198\n",
      "Iteration 4158: loss = 0.0026685114012276363\n",
      "Iteration 4159: loss = 0.0026684918167525344\n",
      "Iteration 4160: loss = 0.002668472238666176\n",
      "Iteration 4161: loss = 0.0026684526669627276\n",
      "Iteration 4162: loss = 0.0026684331016363604\n",
      "Iteration 4163: loss = 0.002668413542681254\n",
      "Iteration 4164: loss = 0.00266839399009159\n",
      "Iteration 4165: loss = 0.0026683744438615576\n",
      "Iteration 4166: loss = 0.00266835490398535\n",
      "Iteration 4167: loss = 0.0026683353704571675\n",
      "Iteration 4168: loss = 0.0026683158432712142\n",
      "Iteration 4169: loss = 0.002668296322421701\n",
      "Iteration 4170: loss = 0.0026682768079028434\n",
      "Iteration 4171: loss = 0.0026682572997088617\n",
      "Iteration 4172: loss = 0.002668237797833983\n",
      "Iteration 4173: loss = 0.002668218302272439\n",
      "Iteration 4174: loss = 0.0026681988130184673\n",
      "Iteration 4175: loss = 0.0026681793300663095\n",
      "Iteration 4176: loss = 0.0026681598534102147\n",
      "Iteration 4177: loss = 0.0026681403830444354\n",
      "Iteration 4178: loss = 0.002668120918963231\n",
      "Iteration 4179: loss = 0.0026681014611608656\n",
      "Iteration 4180: loss = 0.0026680820096316085\n",
      "Iteration 4181: loss = 0.0026680625643697347\n",
      "Iteration 4182: loss = 0.002668043125369524\n",
      "Iteration 4183: loss = 0.002668023692625262\n",
      "Iteration 4184: loss = 0.0026680042661312397\n",
      "Iteration 4185: loss = 0.0026679848458817535\n",
      "Iteration 4186: loss = 0.002667965431871105\n",
      "Iteration 4187: loss = 0.0026679460240936004\n",
      "Iteration 4188: loss = 0.002667926622543553\n",
      "Iteration 4189: loss = 0.0026679072272152785\n",
      "Iteration 4190: loss = 0.0026678878381031015\n",
      "Iteration 4191: loss = 0.0026678684552013496\n",
      "Iteration 4192: loss = 0.002667849078504356\n",
      "Iteration 4193: loss = 0.0026678297080064595\n",
      "Iteration 4194: loss = 0.0026678103437020044\n",
      "Iteration 4195: loss = 0.0026677909855853396\n",
      "Iteration 4196: loss = 0.00266777163365082\n",
      "Iteration 4197: loss = 0.002667752287892805\n",
      "Iteration 4198: loss = 0.00266773294830566\n",
      "Iteration 4199: loss = 0.0026677136148837554\n",
      "Iteration 4200: loss = 0.002667694287621467\n",
      "Iteration 4201: loss = 0.002667674966513176\n",
      "Iteration 4202: loss = 0.002667655651553268\n",
      "Iteration 4203: loss = 0.0026676363427361348\n",
      "Iteration 4204: loss = 0.002667617040056173\n",
      "Iteration 4205: loss = 0.0026675977435077834\n",
      "Iteration 4206: loss = 0.0026675784530853755\n",
      "Iteration 4207: loss = 0.0026675591687833596\n",
      "Iteration 4208: loss = 0.0026675398905961537\n",
      "Iteration 4209: loss = 0.0026675206185181816\n",
      "Iteration 4210: loss = 0.0026675013525438705\n",
      "Iteration 4211: loss = 0.002667482092667653\n",
      "Iteration 4212: loss = 0.0026674628388839686\n",
      "Iteration 4213: loss = 0.0026674435911872613\n",
      "Iteration 4214: loss = 0.0026674243495719784\n",
      "Iteration 4215: loss = 0.002667405114032575\n",
      "Iteration 4216: loss = 0.00266738588456351\n",
      "Iteration 4217: loss = 0.0026673666611592476\n",
      "Iteration 4218: loss = 0.002667347443814258\n",
      "Iteration 4219: loss = 0.0026673282325230157\n",
      "Iteration 4220: loss = 0.0026673090272799995\n",
      "Iteration 4221: loss = 0.0026672898280796957\n",
      "Iteration 4222: loss = 0.0026672706349165944\n",
      "Iteration 4223: loss = 0.0026672514477851903\n",
      "Iteration 4224: loss = 0.0026672322666799844\n",
      "Iteration 4225: loss = 0.0026672130915954824\n",
      "Iteration 4226: loss = 0.002667193922526195\n",
      "Iteration 4227: loss = 0.002667174759466637\n",
      "Iteration 4228: loss = 0.0026671556024113314\n",
      "Iteration 4229: loss = 0.0026671364513548034\n",
      "Iteration 4230: loss = 0.0026671173062915833\n",
      "Iteration 4231: loss = 0.002667098167216209\n",
      "Iteration 4232: loss = 0.0026670790341232213\n",
      "Iteration 4233: loss = 0.002667059907007167\n",
      "Iteration 4234: loss = 0.002667040785862598\n",
      "Iteration 4235: loss = 0.0026670216706840702\n",
      "Iteration 4236: loss = 0.002667002561466146\n",
      "Iteration 4237: loss = 0.0026669834582033923\n",
      "Iteration 4238: loss = 0.0026669643608903826\n",
      "Iteration 4239: loss = 0.002666945269521691\n",
      "Iteration 4240: loss = 0.0026669261840919023\n",
      "Iteration 4241: loss = 0.0026669071045956018\n",
      "Iteration 4242: loss = 0.0026668880310273834\n",
      "Iteration 4243: loss = 0.002666868963381844\n",
      "Iteration 4244: loss = 0.0026668499016535853\n",
      "Iteration 4245: loss = 0.0026668308458372156\n",
      "Iteration 4246: loss = 0.0026668117959273473\n",
      "Iteration 4247: loss = 0.0026667927519185967\n",
      "Iteration 4248: loss = 0.0026667737138055883\n",
      "Iteration 4249: loss = 0.0026667546815829484\n",
      "Iteration 4250: loss = 0.00266673565524531\n",
      "Iteration 4251: loss = 0.0026667166347873104\n",
      "Iteration 4252: loss = 0.0026666976202035925\n",
      "Iteration 4253: loss = 0.0026666786114888045\n",
      "Iteration 4254: loss = 0.0026666596086375983\n",
      "Iteration 4255: loss = 0.0026666406116446313\n",
      "Iteration 4256: loss = 0.002666621620504567\n",
      "Iteration 4257: loss = 0.002666602635212073\n",
      "Iteration 4258: loss = 0.0026665836557618213\n",
      "Iteration 4259: loss = 0.002666564682148489\n",
      "Iteration 4260: loss = 0.0026665457143667604\n",
      "Iteration 4261: loss = 0.002666526752411322\n",
      "Iteration 4262: loss = 0.002666507796276866\n",
      "Iteration 4263: loss = 0.00266648884595809\n",
      "Iteration 4264: loss = 0.002666469901449697\n",
      "Iteration 4265: loss = 0.002666450962746394\n",
      "Iteration 4266: loss = 0.0026664320298428936\n",
      "Iteration 4267: loss = 0.0026664131027339124\n",
      "Iteration 4268: loss = 0.002666394181414173\n",
      "Iteration 4269: loss = 0.002666375265878403\n",
      "Iteration 4270: loss = 0.0026663563561213346\n",
      "Iteration 4271: loss = 0.0026663374521377035\n",
      "Iteration 4272: loss = 0.0026663185539222527\n",
      "Iteration 4273: loss = 0.0026662996614697284\n",
      "Iteration 4274: loss = 0.002666280774774883\n",
      "Iteration 4275: loss = 0.0026662618938324725\n",
      "Iteration 4276: loss = 0.0026662430186372593\n",
      "Iteration 4277: loss = 0.002666224149184009\n",
      "Iteration 4278: loss = 0.002666205285467493\n",
      "Iteration 4279: loss = 0.0026661864274824883\n",
      "Iteration 4280: loss = 0.0026661675752237745\n",
      "Iteration 4281: loss = 0.0026661487286861398\n",
      "Iteration 4282: loss = 0.0026661298878643733\n",
      "Iteration 4283: loss = 0.0026661110527532715\n",
      "Iteration 4284: loss = 0.0026660922233476343\n",
      "Iteration 4285: loss = 0.0026660733996422678\n",
      "Iteration 4286: loss = 0.0026660545816319823\n",
      "Iteration 4287: loss = 0.002666035769311593\n",
      "Iteration 4288: loss = 0.002666016962675919\n",
      "Iteration 4289: loss = 0.0026659981617197866\n",
      "Iteration 4290: loss = 0.0026659793664380246\n",
      "Iteration 4291: loss = 0.002665960576825468\n",
      "Iteration 4292: loss = 0.002665941792876955\n",
      "Iteration 4293: loss = 0.002665923014587331\n",
      "Iteration 4294: loss = 0.0026659042419514447\n",
      "Iteration 4295: loss = 0.00266588547496415\n",
      "Iteration 4296: loss = 0.0026658667136203055\n",
      "Iteration 4297: loss = 0.002665847957914774\n",
      "Iteration 4298: loss = 0.002665829207842424\n",
      "Iteration 4299: loss = 0.002665810463398129\n",
      "Iteration 4300: loss = 0.0026657917245767673\n",
      "Iteration 4301: loss = 0.00266577299137322\n",
      "Iteration 4302: loss = 0.0026657542637823755\n",
      "Iteration 4303: loss = 0.0026657355417991258\n",
      "Iteration 4304: loss = 0.0026657168254183676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4305: loss = 0.002665698114635003\n",
      "Iteration 4306: loss = 0.0026656794094439386\n",
      "Iteration 4307: loss = 0.002665660709840085\n",
      "Iteration 4308: loss = 0.0026656420158183585\n",
      "Iteration 4309: loss = 0.00266562332737368\n",
      "Iteration 4310: loss = 0.0026656046445009754\n",
      "Iteration 4311: loss = 0.0026655859671951736\n",
      "Iteration 4312: loss = 0.002665567295451211\n",
      "Iteration 4313: loss = 0.0026655486292640265\n",
      "Iteration 4314: loss = 0.002665529968628565\n",
      "Iteration 4315: loss = 0.0026655113135397755\n",
      "Iteration 4316: loss = 0.002665492663992612\n",
      "Iteration 4317: loss = 0.0026654740199820335\n",
      "Iteration 4318: loss = 0.0026654553815030017\n",
      "Iteration 4319: loss = 0.0026654367485504865\n",
      "Iteration 4320: loss = 0.00266541812111946\n",
      "Iteration 4321: loss = 0.002665399499204899\n",
      "Iteration 4322: loss = 0.002665380882801787\n",
      "Iteration 4323: loss = 0.0026653622719051092\n",
      "Iteration 4324: loss = 0.002665343666509859\n",
      "Iteration 4325: loss = 0.0026653250666110306\n",
      "Iteration 4326: loss = 0.0026653064722036266\n",
      "Iteration 4327: loss = 0.002665287883282651\n",
      "Iteration 4328: loss = 0.0026652692998431152\n",
      "Iteration 4329: loss = 0.0026652507218800337\n",
      "Iteration 4330: loss = 0.0026652321493884257\n",
      "Iteration 4331: loss = 0.0026652135823633165\n",
      "Iteration 4332: loss = 0.0026651950207997326\n",
      "Iteration 4333: loss = 0.0026651764646927105\n",
      "Iteration 4334: loss = 0.0026651579140372864\n",
      "Iteration 4335: loss = 0.0026651393688285037\n",
      "Iteration 4336: loss = 0.00266512082906141\n",
      "Iteration 4337: loss = 0.0026651022947310563\n",
      "Iteration 4338: loss = 0.0026650837658325002\n",
      "Iteration 4339: loss = 0.002665065242360803\n",
      "Iteration 4340: loss = 0.0026650467243110303\n",
      "Iteration 4341: loss = 0.0026650282116782525\n",
      "Iteration 4342: loss = 0.002665009704457545\n",
      "Iteration 4343: loss = 0.0026649912026439878\n",
      "Iteration 4344: loss = 0.0026649727062326646\n",
      "Iteration 4345: loss = 0.0026649542152186643\n",
      "Iteration 4346: loss = 0.0026649357295970805\n",
      "Iteration 4347: loss = 0.0026649172493630126\n",
      "Iteration 4348: loss = 0.002664898774511561\n",
      "Iteration 4349: loss = 0.002664880305037835\n",
      "Iteration 4350: loss = 0.0026648618409369452\n",
      "Iteration 4351: loss = 0.0026648433822040085\n",
      "Iteration 4352: loss = 0.002664824928834146\n",
      "Iteration 4353: loss = 0.002664806480822483\n",
      "Iteration 4354: loss = 0.0026647880381641493\n",
      "Iteration 4355: loss = 0.0026647696008542795\n",
      "Iteration 4356: loss = 0.0026647511688880133\n",
      "Iteration 4357: loss = 0.0026647327422604948\n",
      "Iteration 4358: loss = 0.002664714320966871\n",
      "Iteration 4359: loss = 0.0026646959050022957\n",
      "Iteration 4360: loss = 0.0026646774943619262\n",
      "Iteration 4361: loss = 0.0026646590890409237\n",
      "Iteration 4362: loss = 0.002664640689034455\n",
      "Iteration 4363: loss = 0.002664622294337691\n",
      "Iteration 4364: loss = 0.002664603904945808\n",
      "Iteration 4365: loss = 0.0026645855208539846\n",
      "Iteration 4366: loss = 0.0026645671420574054\n",
      "Iteration 4367: loss = 0.0026645487685512596\n",
      "Iteration 4368: loss = 0.002664530400330741\n",
      "Iteration 4369: loss = 0.0026645120373910472\n",
      "Iteration 4370: loss = 0.0026644936797273803\n",
      "Iteration 4371: loss = 0.0026644753273349477\n",
      "Iteration 4372: loss = 0.002664456980208961\n",
      "Iteration 4373: loss = 0.0026644386383446357\n",
      "Iteration 4374: loss = 0.0026644203017371915\n",
      "Iteration 4375: loss = 0.002664401970381854\n",
      "Iteration 4376: loss = 0.002664383644273853\n",
      "Iteration 4377: loss = 0.0026643653234084204\n",
      "Iteration 4378: loss = 0.002664347007780796\n",
      "Iteration 4379: loss = 0.0026643286973862218\n",
      "Iteration 4380: loss = 0.002664310392219945\n",
      "Iteration 4381: loss = 0.0026642920922772175\n",
      "Iteration 4382: loss = 0.0026642737975532942\n",
      "Iteration 4383: loss = 0.002664255508043437\n",
      "Iteration 4384: loss = 0.002664237223742909\n",
      "Iteration 4385: loss = 0.002664218944646981\n",
      "Iteration 4386: loss = 0.0026642006707509255\n",
      "Iteration 4387: loss = 0.002664182402050022\n",
      "Iteration 4388: loss = 0.002664164138539552\n",
      "Iteration 4389: loss = 0.002664145880214802\n",
      "Iteration 4390: loss = 0.002664127627071063\n",
      "Iteration 4391: loss = 0.002664109379103633\n",
      "Iteration 4392: loss = 0.00266409113630781\n",
      "Iteration 4393: loss = 0.0026640728986789\n",
      "Iteration 4394: loss = 0.0026640546662122103\n",
      "Iteration 4395: loss = 0.0026640364389030557\n",
      "Iteration 4396: loss = 0.0026640182167467523\n",
      "Iteration 4397: loss = 0.002663999999738624\n",
      "Iteration 4398: loss = 0.002663981787873996\n",
      "Iteration 4399: loss = 0.002663963581148199\n",
      "Iteration 4400: loss = 0.002663945379556569\n",
      "Iteration 4401: loss = 0.0026639271830944445\n",
      "Iteration 4402: loss = 0.0026639089917571703\n",
      "Iteration 4403: loss = 0.0026638908055400938\n",
      "Iteration 4404: loss = 0.002663872624438569\n",
      "Iteration 4405: loss = 0.0026638544484479504\n",
      "Iteration 4406: loss = 0.0026638362775636016\n",
      "Iteration 4407: loss = 0.002663818111780887\n",
      "Iteration 4408: loss = 0.002663799951095177\n",
      "Iteration 4409: loss = 0.0026637817955018454\n",
      "Iteration 4410: loss = 0.0026637636449962712\n",
      "Iteration 4411: loss = 0.0026637454995738375\n",
      "Iteration 4412: loss = 0.002663727359229931\n",
      "Iteration 4413: loss = 0.0026637092239599424\n",
      "Iteration 4414: loss = 0.0026636910937592696\n",
      "Iteration 4415: loss = 0.0026636729686233115\n",
      "Iteration 4416: loss = 0.0026636548485474724\n",
      "Iteration 4417: loss = 0.0026636367335271614\n",
      "Iteration 4418: loss = 0.0026636186235577913\n",
      "Iteration 4419: loss = 0.00266360051863478\n",
      "Iteration 4420: loss = 0.002663582418753548\n",
      "Iteration 4421: loss = 0.002663564323909523\n",
      "Iteration 4422: loss = 0.002663546234098133\n",
      "Iteration 4423: loss = 0.002663528149314814\n",
      "Iteration 4424: loss = 0.0026635100695550033\n",
      "Iteration 4425: loss = 0.0026634919948141456\n",
      "Iteration 4426: loss = 0.0026634739250876865\n",
      "Iteration 4427: loss = 0.0026634558603710785\n",
      "Iteration 4428: loss = 0.002663437800659778\n",
      "Iteration 4429: loss = 0.0026634197459492422\n",
      "Iteration 4430: loss = 0.0026634016962349383\n",
      "Iteration 4431: loss = 0.002663383651512333\n",
      "Iteration 4432: loss = 0.0026633656117768995\n",
      "Iteration 4433: loss = 0.002663347577024116\n",
      "Iteration 4434: loss = 0.002663329547249461\n",
      "Iteration 4435: loss = 0.0026633115224484216\n",
      "Iteration 4436: loss = 0.0026632935026164872\n",
      "Iteration 4437: loss = 0.0026632754877491515\n",
      "Iteration 4438: loss = 0.0026632574778419128\n",
      "Iteration 4439: loss = 0.002663239472890273\n",
      "Iteration 4440: loss = 0.002663221472889738\n",
      "Iteration 4441: loss = 0.00266320347783582\n",
      "Iteration 4442: loss = 0.0026631854877240324\n",
      "Iteration 4443: loss = 0.002663167502549895\n",
      "Iteration 4444: loss = 0.002663149522308931\n",
      "Iteration 4445: loss = 0.0026631315469966667\n",
      "Iteration 4446: loss = 0.0026631135766086353\n",
      "Iteration 4447: loss = 0.0026630956111403717\n",
      "Iteration 4448: loss = 0.0026630776505874153\n",
      "Iteration 4449: loss = 0.002663059694945311\n",
      "Iteration 4450: loss = 0.002663041744209608\n",
      "Iteration 4451: loss = 0.002663023798375857\n",
      "Iteration 4452: loss = 0.0026630058574396145\n",
      "Iteration 4453: loss = 0.0026629879213964426\n",
      "Iteration 4454: loss = 0.0026629699902419057\n",
      "Iteration 4455: loss = 0.002662952063971573\n",
      "Iteration 4456: loss = 0.0026629341425810165\n",
      "Iteration 4457: loss = 0.0026629162260658145\n",
      "Iteration 4458: loss = 0.002662898314421549\n",
      "Iteration 4459: loss = 0.002662880407643805\n",
      "Iteration 4460: loss = 0.002662862505728172\n",
      "Iteration 4461: loss = 0.002662844608670243\n",
      "Iteration 4462: loss = 0.002662826716465618\n",
      "Iteration 4463: loss = 0.0026628088291098975\n",
      "Iteration 4464: loss = 0.0026627909465986883\n",
      "Iteration 4465: loss = 0.0026627730689276004\n",
      "Iteration 4466: loss = 0.0026627551960922485\n",
      "Iteration 4467: loss = 0.002662737328088251\n",
      "Iteration 4468: loss = 0.00266271946491123\n",
      "Iteration 4469: loss = 0.0026627016065568125\n",
      "Iteration 4470: loss = 0.0026626837530206297\n",
      "Iteration 4471: loss = 0.0026626659042983166\n",
      "Iteration 4472: loss = 0.002662648060385511\n",
      "Iteration 4473: loss = 0.0026626302212778563\n",
      "Iteration 4474: loss = 0.002662612386971\n",
      "Iteration 4475: loss = 0.002662594557460593\n",
      "Iteration 4476: loss = 0.0026625767327422907\n",
      "Iteration 4477: loss = 0.002662558912811752\n",
      "Iteration 4478: loss = 0.0026625410976646407\n",
      "Iteration 4479: loss = 0.0026625232872966235\n",
      "Iteration 4480: loss = 0.002662505481703373\n",
      "Iteration 4481: loss = 0.002662487680880563\n",
      "Iteration 4482: loss = 0.0026624698848238745\n",
      "Iteration 4483: loss = 0.0026624520935289906\n",
      "Iteration 4484: loss = 0.0026624343069915984\n",
      "Iteration 4485: loss = 0.00266241652520739\n",
      "Iteration 4486: loss = 0.0026623987481720615\n",
      "Iteration 4487: loss = 0.0026623809758813117\n",
      "Iteration 4488: loss = 0.002662363208330844\n",
      "Iteration 4489: loss = 0.0026623454455163673\n",
      "Iteration 4490: loss = 0.0026623276874335926\n",
      "Iteration 4491: loss = 0.0026623099340782365\n",
      "Iteration 4492: loss = 0.0026622921854460176\n",
      "Iteration 4493: loss = 0.0026622744415326595\n",
      "Iteration 4494: loss = 0.0026622567023338916\n",
      "Iteration 4495: loss = 0.002662238967845444\n",
      "Iteration 4496: loss = 0.002662221238063053\n",
      "Iteration 4497: loss = 0.002662203512982458\n",
      "Iteration 4498: loss = 0.0026621857925994033\n",
      "Iteration 4499: loss = 0.0026621680769096366\n",
      "Iteration 4500: loss = 0.002662150365908908\n",
      "Iteration 4501: loss = 0.002662132659592975\n",
      "Iteration 4502: loss = 0.0026621149579575966\n",
      "Iteration 4503: loss = 0.002662097260998536\n",
      "Iteration 4504: loss = 0.002662079568711561\n",
      "Iteration 4505: loss = 0.002662061881092443\n",
      "Iteration 4506: loss = 0.002662044198136957\n",
      "Iteration 4507: loss = 0.0026620265198408826\n",
      "Iteration 4508: loss = 0.002662008846200004\n",
      "Iteration 4509: loss = 0.0026619911772101075\n",
      "Iteration 4510: loss = 0.002661973512866984\n",
      "Iteration 4511: loss = 0.0026619558531664296\n",
      "Iteration 4512: loss = 0.0026619381981042426\n",
      "Iteration 4513: loss = 0.0026619205476762263\n",
      "Iteration 4514: loss = 0.002661902901878187\n",
      "Iteration 4515: loss = 0.002661885260705937\n",
      "Iteration 4516: loss = 0.0026618676241552887\n",
      "Iteration 4517: loss = 0.002661849992222063\n",
      "Iteration 4518: loss = 0.0026618323649020815\n",
      "Iteration 4519: loss = 0.002661814742191171\n",
      "Iteration 4520: loss = 0.002661797124085161\n",
      "Iteration 4521: loss = 0.0026617795105798865\n",
      "Iteration 4522: loss = 0.0026617619016711855\n",
      "Iteration 4523: loss = 0.0026617442973549004\n",
      "Iteration 4524: loss = 0.002661726697626876\n",
      "Iteration 4525: loss = 0.0026617091024829635\n",
      "Iteration 4526: loss = 0.0026616915119190157\n",
      "Iteration 4527: loss = 0.002661673925930891\n",
      "Iteration 4528: loss = 0.0026616563445144487\n",
      "Iteration 4529: loss = 0.002661638767665557\n",
      "Iteration 4530: loss = 0.0026616211953800835\n",
      "Iteration 4531: loss = 0.0026616036276539017\n",
      "Iteration 4532: loss = 0.0026615860644828874\n",
      "Iteration 4533: loss = 0.002661568505862923\n",
      "Iteration 4534: loss = 0.002661550951789892\n",
      "Iteration 4535: loss = 0.002661533402259683\n",
      "Iteration 4536: loss = 0.0026615158572681884\n",
      "Iteration 4537: loss = 0.002661498316811305\n",
      "Iteration 4538: loss = 0.0026614807808849313\n",
      "Iteration 4539: loss = 0.002661463249484973\n",
      "Iteration 4540: loss = 0.0026614457226073356\n",
      "Iteration 4541: loss = 0.0026614282002479315\n",
      "Iteration 4542: loss = 0.0026614106824026764\n",
      "Iteration 4543: loss = 0.002661393169067489\n",
      "Iteration 4544: loss = 0.0026613756602382925\n",
      "Iteration 4545: loss = 0.002661358155911013\n",
      "Iteration 4546: loss = 0.002661340656081581\n",
      "Iteration 4547: loss = 0.0026613231607459315\n",
      "Iteration 4548: loss = 0.002661305669900002\n",
      "Iteration 4549: loss = 0.0026612881835397354\n",
      "Iteration 4550: loss = 0.0026612707016610755\n",
      "Iteration 4551: loss = 0.002661253224259974\n",
      "Iteration 4552: loss = 0.0026612357513323835\n",
      "Iteration 4553: loss = 0.0026612182828742604\n",
      "Iteration 4554: loss = 0.002661200818881565\n",
      "Iteration 4555: loss = 0.002661183359350264\n",
      "Iteration 4556: loss = 0.0026611659042763245\n",
      "Iteration 4557: loss = 0.0026611484536557184\n",
      "Iteration 4558: loss = 0.0026611310074844217\n",
      "Iteration 4559: loss = 0.002661113565758415\n",
      "Iteration 4560: loss = 0.002661096128473681\n",
      "Iteration 4561: loss = 0.002661078695626207\n",
      "Iteration 4562: loss = 0.002661061267211984\n",
      "Iteration 4563: loss = 0.002661043843227007\n",
      "Iteration 4564: loss = 0.0026610264236672735\n",
      "Iteration 4565: loss = 0.002661009008528787\n",
      "Iteration 4566: loss = 0.0026609915978075527\n",
      "Iteration 4567: loss = 0.00266097419149958\n",
      "Iteration 4568: loss = 0.0026609567896008824\n",
      "Iteration 4569: loss = 0.0026609393921074777\n",
      "Iteration 4570: loss = 0.0026609219990153866\n",
      "Iteration 4571: loss = 0.0026609046103206325\n",
      "Iteration 4572: loss = 0.0026608872260192444\n",
      "Iteration 4573: loss = 0.002660869846107255\n",
      "Iteration 4574: loss = 0.0026608524705806985\n",
      "Iteration 4575: loss = 0.0026608350994356154\n",
      "Iteration 4576: loss = 0.0026608177326680484\n",
      "Iteration 4577: loss = 0.002660800370274045\n",
      "Iteration 4578: loss = 0.0026607830122496547\n",
      "Iteration 4579: loss = 0.0026607656585909316\n",
      "Iteration 4580: loss = 0.0026607483092939348\n",
      "Iteration 4581: loss = 0.002660730964354724\n",
      "Iteration 4582: loss = 0.002660713623769367\n",
      "Iteration 4583: loss = 0.0026606962875339304\n",
      "Iteration 4584: loss = 0.0026606789556444872\n",
      "Iteration 4585: loss = 0.0026606616280971146\n",
      "Iteration 4586: loss = 0.0026606443048878916\n",
      "Iteration 4587: loss = 0.0026606269860129025\n",
      "Iteration 4588: loss = 0.0026606096714682343\n",
      "Iteration 4589: loss = 0.0026605923612499777\n",
      "Iteration 4590: loss = 0.002660575055354227\n",
      "Iteration 4591: loss = 0.0026605577537770816\n",
      "Iteration 4592: loss = 0.002660540456514642\n",
      "Iteration 4593: loss = 0.002660523163563014\n",
      "Iteration 4594: loss = 0.0026605058749183067\n",
      "Iteration 4595: loss = 0.0026604885905766344\n",
      "Iteration 4596: loss = 0.0026604713105341116\n",
      "Iteration 4597: loss = 0.0026604540347868583\n",
      "Iteration 4598: loss = 0.002660436763331\n",
      "Iteration 4599: loss = 0.0026604194961626624\n",
      "Iteration 4600: loss = 0.0026604022332779764\n",
      "Iteration 4601: loss = 0.002660384974673077\n",
      "Iteration 4602: loss = 0.0026603677203441024\n",
      "Iteration 4603: loss = 0.0026603504702871947\n",
      "Iteration 4604: loss = 0.0026603332244984988\n",
      "Iteration 4605: loss = 0.0026603159829741633\n",
      "Iteration 4606: loss = 0.002660298745710341\n",
      "Iteration 4607: loss = 0.0026602815127031885\n",
      "Iteration 4608: loss = 0.0026602642839488647\n",
      "Iteration 4609: loss = 0.002660247059443534\n",
      "Iteration 4610: loss = 0.002660229839183362\n",
      "Iteration 4611: loss = 0.0026602126231645203\n",
      "Iteration 4612: loss = 0.0026601954113831825\n",
      "Iteration 4613: loss = 0.0026601782038355266\n",
      "Iteration 4614: loss = 0.0026601610005177337\n",
      "Iteration 4615: loss = 0.0026601438014259883\n",
      "Iteration 4616: loss = 0.0026601266065564787\n",
      "Iteration 4617: loss = 0.0026601094159053973\n",
      "Iteration 4618: loss = 0.0026600922294689397\n",
      "Iteration 4619: loss = 0.0026600750472433043\n",
      "Iteration 4620: loss = 0.0026600578692246935\n",
      "Iteration 4621: loss = 0.0026600406954093146\n",
      "Iteration 4622: loss = 0.002660023525793376\n",
      "Iteration 4623: loss = 0.002660006360373092\n",
      "Iteration 4624: loss = 0.002659989199144679\n",
      "Iteration 4625: loss = 0.0026599720421043565\n",
      "Iteration 4626: loss = 0.0026599548892483494\n",
      "Iteration 4627: loss = 0.0026599377405728845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4628: loss = 0.002659920596074193\n",
      "Iteration 4629: loss = 0.002659903455748509\n",
      "Iteration 4630: loss = 0.0026598863195920713\n",
      "Iteration 4631: loss = 0.0026598691876011204\n",
      "Iteration 4632: loss = 0.002659852059771901\n",
      "Iteration 4633: loss = 0.002659834936100663\n",
      "Iteration 4634: loss = 0.0026598178165836567\n",
      "Iteration 4635: loss = 0.002659800701217138\n",
      "Iteration 4636: loss = 0.002659783589997367\n",
      "Iteration 4637: loss = 0.0026597664829206055\n",
      "Iteration 4638: loss = 0.0026597493799831186\n",
      "Iteration 4639: loss = 0.002659732281181177\n",
      "Iteration 4640: loss = 0.0026597151865110536\n",
      "Iteration 4641: loss = 0.002659698095969024\n",
      "Iteration 4642: loss = 0.002659681009551368\n",
      "Iteration 4643: loss = 0.0026596639272543697\n",
      "Iteration 4644: loss = 0.002659646849074317\n",
      "Iteration 4645: loss = 0.002659629775007497\n",
      "Iteration 4646: loss = 0.0026596127050502066\n",
      "Iteration 4647: loss = 0.002659595639198741\n",
      "Iteration 4648: loss = 0.0026595785774494026\n",
      "Iteration 4649: loss = 0.0026595615197984954\n",
      "Iteration 4650: loss = 0.002659544466242326\n",
      "Iteration 4651: loss = 0.0026595274167772053\n",
      "Iteration 4652: loss = 0.0026595103713994484\n",
      "Iteration 4653: loss = 0.002659493330105374\n",
      "Iteration 4654: loss = 0.002659476292891303\n",
      "Iteration 4655: loss = 0.0026594592597535597\n",
      "Iteration 4656: loss = 0.002659442230688473\n",
      "Iteration 4657: loss = 0.0026594252056923737\n",
      "Iteration 4658: loss = 0.0026594081847615978\n",
      "Iteration 4659: loss = 0.0026593911678924834\n",
      "Iteration 4660: loss = 0.002659374155081374\n",
      "Iteration 4661: loss = 0.002659357146324613\n",
      "Iteration 4662: loss = 0.00265934014161855\n",
      "Iteration 4663: loss = 0.0026593231409595373\n",
      "Iteration 4664: loss = 0.00265930614434393\n",
      "Iteration 4665: loss = 0.002659289151768089\n",
      "Iteration 4666: loss = 0.0026592721632283747\n",
      "Iteration 4667: loss = 0.002659255178721154\n",
      "Iteration 4668: loss = 0.0026592381982427945\n",
      "Iteration 4669: loss = 0.002659221221789671\n",
      "Iteration 4670: loss = 0.002659204249358159\n",
      "Iteration 4671: loss = 0.0026591872809446373\n",
      "Iteration 4672: loss = 0.0026591703165454897\n",
      "Iteration 4673: loss = 0.0026591533561571012\n",
      "Iteration 4674: loss = 0.002659136399775861\n",
      "Iteration 4675: loss = 0.0026591194473981642\n",
      "Iteration 4676: loss = 0.002659102499020405\n",
      "Iteration 4677: loss = 0.0026590855546389843\n",
      "Iteration 4678: loss = 0.002659068614250305\n",
      "Iteration 4679: loss = 0.002659051677850772\n",
      "Iteration 4680: loss = 0.0026590347454367976\n",
      "Iteration 4681: loss = 0.002659017817004793\n",
      "Iteration 4682: loss = 0.0026590008925511744\n",
      "Iteration 4683: loss = 0.0026589839720723638\n",
      "Iteration 4684: loss = 0.002658967055564782\n",
      "Iteration 4685: loss = 0.0026589501430248574\n",
      "Iteration 4686: loss = 0.0026589332344490184\n",
      "Iteration 4687: loss = 0.002658916329833699\n",
      "Iteration 4688: loss = 0.0026588994291753356\n",
      "Iteration 4689: loss = 0.0026588825324703673\n",
      "Iteration 4690: loss = 0.002658865639715239\n",
      "Iteration 4691: loss = 0.002658848750906395\n",
      "Iteration 4692: loss = 0.0026588318660402863\n",
      "Iteration 4693: loss = 0.002658814985113366\n",
      "Iteration 4694: loss = 0.0026587981081220903\n",
      "Iteration 4695: loss = 0.0026587812350629195\n",
      "Iteration 4696: loss = 0.0026587643659323162\n",
      "Iteration 4697: loss = 0.002658747500726747\n",
      "Iteration 4698: loss = 0.0026587306394426804\n",
      "Iteration 4699: loss = 0.002658713782076591\n",
      "Iteration 4700: loss = 0.0026586969286249546\n",
      "Iteration 4701: loss = 0.0026586800790842505\n",
      "Iteration 4702: loss = 0.0026586632334509616\n",
      "Iteration 4703: loss = 0.002658646391721574\n",
      "Iteration 4704: loss = 0.0026586295538925777\n",
      "Iteration 4705: loss = 0.0026586127199604644\n",
      "Iteration 4706: loss = 0.00265859588992173\n",
      "Iteration 4707: loss = 0.0026585790637728758\n",
      "Iteration 4708: loss = 0.0026585622415104017\n",
      "Iteration 4709: loss = 0.002658545423130815\n",
      "Iteration 4710: loss = 0.0026585286086306245\n",
      "Iteration 4711: loss = 0.0026585117980063426\n",
      "Iteration 4712: loss = 0.0026584949912544843\n",
      "Iteration 4713: loss = 0.002658478188371569\n",
      "Iteration 4714: loss = 0.002658461389354118\n",
      "Iteration 4715: loss = 0.002658444594198658\n",
      "Iteration 4716: loss = 0.002658427802901717\n",
      "Iteration 4717: loss = 0.0026584110154598264\n",
      "Iteration 4718: loss = 0.002658394231869521\n",
      "Iteration 4719: loss = 0.0026583774521273407\n",
      "Iteration 4720: loss = 0.0026583606762298254\n",
      "Iteration 4721: loss = 0.00265834390417352\n",
      "Iteration 4722: loss = 0.0026583271359549732\n",
      "Iteration 4723: loss = 0.0026583103715707365\n",
      "Iteration 4724: loss = 0.002658293611017364\n",
      "Iteration 4725: loss = 0.002658276854291412\n",
      "Iteration 4726: loss = 0.0026582601013894432\n",
      "Iteration 4727: loss = 0.0026582433523080215\n",
      "Iteration 4728: loss = 0.0026582266070437136\n",
      "Iteration 4729: loss = 0.00265820986559309\n",
      "Iteration 4730: loss = 0.0026581931279527245\n",
      "Iteration 4731: loss = 0.0026581763941191943\n",
      "Iteration 4732: loss = 0.0026581596640890795\n",
      "Iteration 4733: loss = 0.0026581429378589633\n",
      "Iteration 4734: loss = 0.002658126215425432\n",
      "Iteration 4735: loss = 0.002658109496785076\n",
      "Iteration 4736: loss = 0.002658092781934487\n",
      "Iteration 4737: loss = 0.0026580760708702625\n",
      "Iteration 4738: loss = 0.0026580593635890004\n",
      "Iteration 4739: loss = 0.0026580426600873045\n",
      "Iteration 4740: loss = 0.002658025960361779\n",
      "Iteration 4741: loss = 0.0026580092644090335\n",
      "Iteration 4742: loss = 0.0026579925722256797\n",
      "Iteration 4743: loss = 0.002657975883808333\n",
      "Iteration 4744: loss = 0.002657959199153611\n",
      "Iteration 4745: loss = 0.0026579425182581356\n",
      "Iteration 4746: loss = 0.002657925841118532\n",
      "Iteration 4747: loss = 0.002657909167731427\n",
      "Iteration 4748: loss = 0.0026578924980934513\n",
      "Iteration 4749: loss = 0.00265787583220124\n",
      "Iteration 4750: loss = 0.00265785917005143\n",
      "Iteration 4751: loss = 0.002657842511640661\n",
      "Iteration 4752: loss = 0.0026578258569655766\n",
      "Iteration 4753: loss = 0.002657809206022824\n",
      "Iteration 4754: loss = 0.0026577925588090524\n",
      "Iteration 4755: loss = 0.0026577759153209152\n",
      "Iteration 4756: loss = 0.002657759275555068\n",
      "Iteration 4757: loss = 0.0026577426395081698\n",
      "Iteration 4758: loss = 0.0026577260071768824\n",
      "Iteration 4759: loss = 0.002657709378557873\n",
      "Iteration 4760: loss = 0.0026576927536478084\n",
      "Iteration 4761: loss = 0.0026576761324433604\n",
      "Iteration 4762: loss = 0.0026576595149412044\n",
      "Iteration 4763: loss = 0.0026576429011380175\n",
      "Iteration 4764: loss = 0.0026576262910304803\n",
      "Iteration 4765: loss = 0.0026576096846152777\n",
      "Iteration 4766: loss = 0.002657593081889097\n",
      "Iteration 4767: loss = 0.002657576482848627\n",
      "Iteration 4768: loss = 0.0026575598874905627\n",
      "Iteration 4769: loss = 0.0026575432958115992\n",
      "Iteration 4770: loss = 0.0026575267078084368\n",
      "Iteration 4771: loss = 0.002657510123477777\n",
      "Iteration 4772: loss = 0.002657493542816327\n",
      "Iteration 4773: loss = 0.0026574769658207946\n",
      "Iteration 4774: loss = 0.002657460392487891\n",
      "Iteration 4775: loss = 0.0026574438228143325\n",
      "Iteration 4776: loss = 0.0026574272567968355\n",
      "Iteration 4777: loss = 0.002657410694432122\n",
      "Iteration 4778: loss = 0.002657394135716916\n",
      "Iteration 4779: loss = 0.0026573775806479445\n",
      "Iteration 4780: loss = 0.002657361029221938\n",
      "Iteration 4781: loss = 0.0026573444814356286\n",
      "Iteration 4782: loss = 0.002657327937285754\n",
      "Iteration 4783: loss = 0.0026573113967690524\n",
      "Iteration 4784: loss = 0.002657294859882267\n",
      "Iteration 4785: loss = 0.002657278326622143\n",
      "Iteration 4786: loss = 0.0026572617969854283\n",
      "Iteration 4787: loss = 0.0026572452709688753\n",
      "Iteration 4788: loss = 0.002657228748569238\n",
      "Iteration 4789: loss = 0.002657212229783275\n",
      "Iteration 4790: loss = 0.002657195714607745\n",
      "Iteration 4791: loss = 0.0026571792030394125\n",
      "Iteration 4792: loss = 0.002657162695075045\n",
      "Iteration 4793: loss = 0.0026571461907114115\n",
      "Iteration 4794: loss = 0.0026571296899452843\n",
      "Iteration 4795: loss = 0.002657113192773439\n",
      "Iteration 4796: loss = 0.002657096699192656\n",
      "Iteration 4797: loss = 0.0026570802091997153\n",
      "Iteration 4798: loss = 0.0026570637227914026\n",
      "Iteration 4799: loss = 0.0026570472399645045\n",
      "Iteration 4800: loss = 0.0026570307607158127\n",
      "Iteration 4801: loss = 0.0026570142850421206\n",
      "Iteration 4802: loss = 0.0026569978129402255\n",
      "Iteration 4803: loss = 0.0026569813444069267\n",
      "Iteration 4804: loss = 0.0026569648794390263\n",
      "Iteration 4805: loss = 0.002656948418033331\n",
      "Iteration 4806: loss = 0.0026569319601866498\n",
      "Iteration 4807: loss = 0.0026569155058957924\n",
      "Iteration 4808: loss = 0.0026568990551575754\n",
      "Iteration 4809: loss = 0.0026568826079688165\n",
      "Iteration 4810: loss = 0.002656866164326335\n",
      "Iteration 4811: loss = 0.0026568497242269546\n",
      "Iteration 4812: loss = 0.0026568332876675026\n",
      "Iteration 4813: loss = 0.0026568168546448083\n",
      "Iteration 4814: loss = 0.002656800425155704\n",
      "Iteration 4815: loss = 0.002656783999197025\n",
      "Iteration 4816: loss = 0.00265676757676561\n",
      "Iteration 4817: loss = 0.0026567511578583004\n",
      "Iteration 4818: loss = 0.00265673474247194\n",
      "Iteration 4819: loss = 0.002656718330603376\n",
      "Iteration 4820: loss = 0.0026567019222494596\n",
      "Iteration 4821: loss = 0.0026566855174070428\n",
      "Iteration 4822: loss = 0.0026566691160729823\n",
      "Iteration 4823: loss = 0.002656652718244137\n",
      "Iteration 4824: loss = 0.0026566363239173686\n",
      "Iteration 4825: loss = 0.0026566199330895425\n",
      "Iteration 4826: loss = 0.002656603545757525\n",
      "Iteration 4827: loss = 0.002656587161918189\n",
      "Iteration 4828: loss = 0.002656570781568407\n",
      "Iteration 4829: loss = 0.0026565544047050553\n",
      "Iteration 4830: loss = 0.0026565380313250137\n",
      "Iteration 4831: loss = 0.0026565216614251653\n",
      "Iteration 4832: loss = 0.0026565052950023936\n",
      "Iteration 4833: loss = 0.0026564889320535886\n",
      "Iteration 4834: loss = 0.0026564725725756405\n",
      "Iteration 4835: loss = 0.0026564562165654446\n",
      "Iteration 4836: loss = 0.002656439864019895\n",
      "Iteration 4837: loss = 0.002656423514935895\n",
      "Iteration 4838: loss = 0.0026564071693103448\n",
      "Iteration 4839: loss = 0.0026563908271401517\n",
      "Iteration 4840: loss = 0.002656374488422223\n",
      "Iteration 4841: loss = 0.002656358153153471\n",
      "Iteration 4842: loss = 0.0026563418213308085\n",
      "Iteration 4843: loss = 0.002656325492951155\n",
      "Iteration 4844: loss = 0.002656309168011428\n",
      "Iteration 4845: loss = 0.0026562928465085523\n",
      "Iteration 4846: loss = 0.0026562765284394536\n",
      "Iteration 4847: loss = 0.0026562602138010595\n",
      "Iteration 4848: loss = 0.002656243902590302\n",
      "Iteration 4849: loss = 0.002656227594804116\n",
      "Iteration 4850: loss = 0.002656211290439438\n",
      "Iteration 4851: loss = 0.002656194989493209\n",
      "Iteration 4852: loss = 0.002656178691962371\n",
      "Iteration 4853: loss = 0.0026561623978438704\n",
      "Iteration 4854: loss = 0.0026561461071346565\n",
      "Iteration 4855: loss = 0.00265612981983168\n",
      "Iteration 4856: loss = 0.0026561135359318955\n",
      "Iteration 4857: loss = 0.00265609725543226\n",
      "Iteration 4858: loss = 0.002656080978329734\n",
      "Iteration 4859: loss = 0.0026560647046212802\n",
      "Iteration 4860: loss = 0.002656048434303865\n",
      "Iteration 4861: loss = 0.0026560321673744555\n",
      "Iteration 4862: loss = 0.0026560159038300246\n",
      "Iteration 4863: loss = 0.002655999643667546\n",
      "Iteration 4864: loss = 0.002655983386883997\n",
      "Iteration 4865: loss = 0.002655967133476357\n",
      "Iteration 4866: loss = 0.00265595088344161\n",
      "Iteration 4867: loss = 0.00265593463677674\n",
      "Iteration 4868: loss = 0.002655918393478735\n",
      "Iteration 4869: loss = 0.0026559021535445884\n",
      "Iteration 4870: loss = 0.002655885916971293\n",
      "Iteration 4871: loss = 0.0026558696837558453\n",
      "Iteration 4872: loss = 0.0026558534538952444\n",
      "Iteration 4873: loss = 0.0026558372273864945\n",
      "Iteration 4874: loss = 0.002655821004226599\n",
      "Iteration 4875: loss = 0.0026558047844125673\n",
      "Iteration 4876: loss = 0.0026557885679414097\n",
      "Iteration 4877: loss = 0.002655772354810139\n",
      "Iteration 4878: loss = 0.002655756145015772\n",
      "Iteration 4879: loss = 0.0026557399385553292\n",
      "Iteration 4880: loss = 0.002655723735425831\n",
      "Iteration 4881: loss = 0.002655707535624302\n",
      "Iteration 4882: loss = 0.002655691339147772\n",
      "Iteration 4883: loss = 0.002655675145993268\n",
      "Iteration 4884: loss = 0.0026556589561578248\n",
      "Iteration 4885: loss = 0.0026556427696384785\n",
      "Iteration 4886: loss = 0.0026556265864322672\n",
      "Iteration 4887: loss = 0.0026556104065362327\n",
      "Iteration 4888: loss = 0.002655594229947418\n",
      "Iteration 4889: loss = 0.002655578056662872\n",
      "Iteration 4890: loss = 0.0026555618866796423\n",
      "Iteration 4891: loss = 0.0026555457199947826\n",
      "Iteration 4892: loss = 0.002655529556605347\n",
      "Iteration 4893: loss = 0.002655513396508395\n",
      "Iteration 4894: loss = 0.0026554972397009858\n",
      "Iteration 4895: loss = 0.0026554810861801835\n",
      "Iteration 4896: loss = 0.0026554649359430533\n",
      "Iteration 4897: loss = 0.0026554487889866654\n",
      "Iteration 4898: loss = 0.0026554326453080914\n",
      "Iteration 4899: loss = 0.002655416504904404\n",
      "Iteration 4900: loss = 0.002655400367772682\n",
      "Iteration 4901: loss = 0.0026553842339100053\n",
      "Iteration 4902: loss = 0.002655368103313455\n",
      "Iteration 4903: loss = 0.0026553519759801177\n",
      "Iteration 4904: loss = 0.002655335851907081\n",
      "Iteration 4905: loss = 0.0026553197310914345\n",
      "Iteration 4906: loss = 0.002655303613530274\n",
      "Iteration 4907: loss = 0.002655287499220693\n",
      "Iteration 4908: loss = 0.0026552713881597928\n",
      "Iteration 4909: loss = 0.0026552552803446737\n",
      "Iteration 4910: loss = 0.0026552391757724398\n",
      "Iteration 4911: loss = 0.0026552230744401988\n",
      "Iteration 4912: loss = 0.0026552069763450593\n",
      "Iteration 4913: loss = 0.002655190881484135\n",
      "Iteration 4914: loss = 0.0026551747898545404\n",
      "Iteration 4915: loss = 0.0026551587014533927\n",
      "Iteration 4916: loss = 0.002655142616277814\n",
      "Iteration 4917: loss = 0.002655126534324926\n",
      "Iteration 4918: loss = 0.0026551104555918546\n",
      "Iteration 4919: loss = 0.0026550943800757293\n",
      "Iteration 4920: loss = 0.00265507830777368\n",
      "Iteration 4921: loss = 0.0026550622386828423\n",
      "Iteration 4922: loss = 0.0026550461728003507\n",
      "Iteration 4923: loss = 0.002655030110123346\n",
      "Iteration 4924: loss = 0.0026550140506489696\n",
      "Iteration 4925: loss = 0.002654997994374367\n",
      "Iteration 4926: loss = 0.0026549819412966832\n",
      "Iteration 4927: loss = 0.00265496589141307\n",
      "Iteration 4928: loss = 0.00265494984472068\n",
      "Iteration 4929: loss = 0.002654933801216668\n",
      "Iteration 4930: loss = 0.002654917760898191\n",
      "Iteration 4931: loss = 0.002654901723762411\n",
      "Iteration 4932: loss = 0.0026548856898064906\n",
      "Iteration 4933: loss = 0.0026548696590275953\n",
      "Iteration 4934: loss = 0.002654853631422894\n",
      "Iteration 4935: loss = 0.002654837606989558\n",
      "Iteration 4936: loss = 0.002654821585724761\n",
      "Iteration 4937: loss = 0.002654805567625679\n",
      "Iteration 4938: loss = 0.0026547895526894925\n",
      "Iteration 4939: loss = 0.0026547735409133814\n",
      "Iteration 4940: loss = 0.0026547575322945307\n",
      "Iteration 4941: loss = 0.002654741526830127\n",
      "Iteration 4942: loss = 0.0026547255245173602\n",
      "Iteration 4943: loss = 0.0026547095253534234\n",
      "Iteration 4944: loss = 0.0026546935293355104\n",
      "Iteration 4945: loss = 0.0026546775364608187\n",
      "Iteration 4946: loss = 0.0026546615467265486\n",
      "Iteration 4947: loss = 0.0026546455601299027\n",
      "Iteration 4948: loss = 0.002654629576668086\n",
      "Iteration 4949: loss = 0.0026546135963383074\n",
      "Iteration 4950: loss = 0.0026545976191377765\n",
      "Iteration 4951: loss = 0.0026545816450637065\n",
      "Iteration 4952: loss = 0.0026545656741133136\n",
      "Iteration 4953: loss = 0.0026545497062838157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4954: loss = 0.0026545337415724343\n",
      "Iteration 4955: loss = 0.002654517779976392\n",
      "Iteration 4956: loss = 0.0026545018214929154\n",
      "Iteration 4957: loss = 0.002654485866119234\n",
      "Iteration 4958: loss = 0.002654469913852578\n",
      "Iteration 4959: loss = 0.002654453964690181\n",
      "Iteration 4960: loss = 0.0026544380186292807\n",
      "Iteration 4961: loss = 0.002654422075667115\n",
      "Iteration 4962: loss = 0.0026544061358009263\n",
      "Iteration 4963: loss = 0.0026543901990279586\n",
      "Iteration 4964: loss = 0.0026543742653454584\n",
      "Iteration 4965: loss = 0.002654358334750675\n",
      "Iteration 4966: loss = 0.002654342407240861\n",
      "Iteration 4967: loss = 0.0026543264828132705\n",
      "Iteration 4968: loss = 0.0026543105614651604\n",
      "Iteration 4969: loss = 0.00265429464319379\n",
      "Iteration 4970: loss = 0.0026542787279964226\n",
      "Iteration 4971: loss = 0.0026542628158703215\n",
      "Iteration 4972: loss = 0.002654246906812755\n",
      "Iteration 4973: loss = 0.002654231000820992\n",
      "Iteration 4974: loss = 0.0026542150978923055\n",
      "Iteration 4975: loss = 0.002654199198023971\n",
      "Iteration 4976: loss = 0.0026541833012132645\n",
      "Iteration 4977: loss = 0.0026541674074574673\n",
      "Iteration 4978: loss = 0.002654151516753861\n",
      "Iteration 4979: loss = 0.002654135629099732\n",
      "Iteration 4980: loss = 0.002654119744492367\n",
      "Iteration 4981: loss = 0.0026541038629290556\n",
      "Iteration 4982: loss = 0.0026540879844070913\n",
      "Iteration 4983: loss = 0.0026540721089237697\n",
      "Iteration 4984: loss = 0.002654056236476387\n",
      "Iteration 4985: loss = 0.002654040367062245\n",
      "Iteration 4986: loss = 0.0026540245006786466\n",
      "Iteration 4987: loss = 0.0026540086373228956\n",
      "Iteration 4988: loss = 0.002653992776992301\n",
      "Iteration 4989: loss = 0.0026539769196841727\n",
      "Iteration 4990: loss = 0.0026539610653958236\n",
      "Iteration 4991: loss = 0.0026539452141245696\n",
      "Iteration 4992: loss = 0.0026539293658677274\n",
      "Iteration 4993: loss = 0.0026539135206226183\n",
      "Iteration 4994: loss = 0.002653897678386565\n",
      "Iteration 4995: loss = 0.0026538818391568927\n",
      "Iteration 4996: loss = 0.0026538660029309293\n",
      "Iteration 4997: loss = 0.0026538501697060046\n",
      "Iteration 4998: loss = 0.002653834339479453\n",
      "Iteration 4999: loss = 0.0026538185122486086\n",
      "Iteration 5000: loss = 0.0026538026880108096\n",
      "Iteration 5001: loss = 0.002653786866763396\n",
      "Iteration 5002: loss = 0.0026537710485037115\n",
      "Iteration 5003: loss = 0.0026537552332291\n",
      "Iteration 5004: loss = 0.0026537394209369103\n",
      "Iteration 5005: loss = 0.002653723611624492\n",
      "Iteration 5006: loss = 0.002653707805289199\n",
      "Iteration 5007: loss = 0.0026536920019283854\n",
      "Iteration 5008: loss = 0.0026536762015394086\n",
      "Iteration 5009: loss = 0.0026536604041196296\n",
      "Iteration 5010: loss = 0.002653644609666411\n",
      "Iteration 5011: loss = 0.002653628818177117\n",
      "Iteration 5012: loss = 0.002653613029649116\n",
      "Iteration 5013: loss = 0.0026535972440797774\n",
      "Iteration 5014: loss = 0.002653581461466474\n",
      "Iteration 5015: loss = 0.0026535656818065796\n",
      "Iteration 5016: loss = 0.0026535499050974734\n",
      "Iteration 5017: loss = 0.0026535341313365336\n",
      "Iteration 5018: loss = 0.0026535183605211434\n",
      "Iteration 5019: loss = 0.0026535025926486874\n",
      "Iteration 5020: loss = 0.0026534868277165516\n",
      "Iteration 5021: loss = 0.002653471065722127\n",
      "Iteration 5022: loss = 0.002653455306662805\n",
      "Iteration 5023: loss = 0.0026534395505359793\n",
      "Iteration 5024: loss = 0.002653423797339048\n",
      "Iteration 5025: loss = 0.00265340804706941\n",
      "Iteration 5026: loss = 0.0026533922997244662\n",
      "Iteration 5027: loss = 0.002653376555301622\n",
      "Iteration 5028: loss = 0.0026533608137982835\n",
      "Iteration 5029: loss = 0.0026533450752118594\n",
      "Iteration 5030: loss = 0.0026533293395397607\n",
      "Iteration 5031: loss = 0.002653313606779402\n",
      "Iteration 5032: loss = 0.0026532978769282004\n",
      "Iteration 5033: loss = 0.0026532821499835723\n",
      "Iteration 5034: loss = 0.00265326642594294\n",
      "Iteration 5035: loss = 0.0026532507048037278\n",
      "Iteration 5036: loss = 0.0026532349865633596\n",
      "Iteration 5037: loss = 0.0026532192712192654\n",
      "Iteration 5038: loss = 0.002653203558768875\n",
      "Iteration 5039: loss = 0.0026531878492096223\n",
      "Iteration 5040: loss = 0.002653172142538942\n",
      "Iteration 5041: loss = 0.0026531564387542723\n",
      "Iteration 5042: loss = 0.002653140737853053\n",
      "Iteration 5043: loss = 0.0026531250398327275\n",
      "Iteration 5044: loss = 0.002653109344690741\n",
      "Iteration 5045: loss = 0.0026530936524245393\n",
      "Iteration 5046: loss = 0.002653077963031574\n",
      "Iteration 5047: loss = 0.002653062276509296\n",
      "Iteration 5048: loss = 0.0026530465928551613\n",
      "Iteration 5049: loss = 0.0026530309120666256\n",
      "Iteration 5050: loss = 0.0026530152341411486\n",
      "Iteration 5051: loss = 0.002652999559076192\n",
      "Iteration 5052: loss = 0.0026529838868692207\n",
      "Iteration 5053: loss = 0.002652968217517699\n",
      "Iteration 5054: loss = 0.0026529525510190985\n",
      "Iteration 5055: loss = 0.002652936887370888\n",
      "Iteration 5056: loss = 0.002652921226570542\n",
      "Iteration 5057: loss = 0.0026529055686155367\n",
      "Iteration 5058: loss = 0.0026528899135033496\n",
      "Iteration 5059: loss = 0.0026528742612314618\n",
      "Iteration 5060: loss = 0.002652858611797356\n",
      "Iteration 5061: loss = 0.0026528429651985173\n",
      "Iteration 5062: loss = 0.0026528273214324345\n",
      "Iteration 5063: loss = 0.0026528116804965963\n",
      "Iteration 5064: loss = 0.0026527960423884953\n",
      "Iteration 5065: loss = 0.0026527804071056265\n",
      "Iteration 5066: loss = 0.002652764774645487\n",
      "Iteration 5067: loss = 0.0026527491450055755\n",
      "Iteration 5068: loss = 0.0026527335181833938\n",
      "Iteration 5069: loss = 0.0026527178941764466\n",
      "Iteration 5070: loss = 0.002652702272982239\n",
      "Iteration 5071: loss = 0.0026526866545982823\n",
      "Iteration 5072: loss = 0.0026526710390220843\n",
      "Iteration 5073: loss = 0.00265265542625116\n",
      "Iteration 5074: loss = 0.0026526398162830256\n",
      "Iteration 5075: loss = 0.002652624209115197\n",
      "Iteration 5076: loss = 0.0026526086047451965\n",
      "Iteration 5077: loss = 0.002652593003170546\n",
      "Iteration 5078: loss = 0.00265257740438877\n",
      "Iteration 5079: loss = 0.0026525618083973967\n",
      "Iteration 5080: loss = 0.0026525462151939542\n",
      "Iteration 5081: loss = 0.002652530624775975\n",
      "Iteration 5082: loss = 0.002652515037140994\n",
      "Iteration 5083: loss = 0.0026524994522865468\n",
      "Iteration 5084: loss = 0.0026524838702101723\n",
      "Iteration 5085: loss = 0.002652468290909412\n",
      "Iteration 5086: loss = 0.002652452714381809\n",
      "Iteration 5087: loss = 0.002652437140624908\n",
      "Iteration 5088: loss = 0.0026524215696362586\n",
      "Iteration 5089: loss = 0.002652406001413409\n",
      "Iteration 5090: loss = 0.0026523904359539133\n",
      "Iteration 5091: loss = 0.002652374873255326\n",
      "Iteration 5092: loss = 0.0026523593133152036\n",
      "Iteration 5093: loss = 0.0026523437561311062\n",
      "Iteration 5094: loss = 0.002652328201700595\n",
      "Iteration 5095: loss = 0.0026523126500212334\n",
      "Iteration 5096: loss = 0.002652297101090589\n",
      "Iteration 5097: loss = 0.002652281554906228\n",
      "Iteration 5098: loss = 0.0026522660114657237\n",
      "Iteration 5099: loss = 0.002652250470766647\n",
      "Iteration 5100: loss = 0.0026522349328065744\n",
      "Iteration 5101: loss = 0.002652219397583083\n",
      "Iteration 5102: loss = 0.0026522038650937533\n",
      "Iteration 5103: loss = 0.0026521883353361657\n",
      "Iteration 5104: loss = 0.0026521728083079054\n",
      "Iteration 5105: loss = 0.0026521572840065596\n",
      "Iteration 5106: loss = 0.002652141762429716\n",
      "Iteration 5107: loss = 0.0026521262435749665\n",
      "Iteration 5108: loss = 0.002652110727439904\n",
      "Iteration 5109: loss = 0.002652095214022124\n",
      "Iteration 5110: loss = 0.002652079703319224\n",
      "Iteration 5111: loss = 0.0026520641953288043\n",
      "Iteration 5112: loss = 0.0026520486900484678\n",
      "Iteration 5113: loss = 0.0026520331874758177\n",
      "Iteration 5114: loss = 0.002652017687608462\n",
      "Iteration 5115: loss = 0.0026520021904440096\n",
      "Iteration 5116: loss = 0.0026519866959800716\n",
      "Iteration 5117: loss = 0.00265197120421426\n",
      "Iteration 5118: loss = 0.0026519557151441927\n",
      "Iteration 5119: loss = 0.0026519402287674856\n",
      "Iteration 5120: loss = 0.002651924745081761\n",
      "Iteration 5121: loss = 0.002651909264084639\n",
      "Iteration 5122: loss = 0.0026518937857737458\n",
      "Iteration 5123: loss = 0.002651878310146707\n",
      "Iteration 5124: loss = 0.002651862837201152\n",
      "Iteration 5125: loss = 0.0026518473669347132\n",
      "Iteration 5126: loss = 0.002651831899345022\n",
      "Iteration 5127: loss = 0.0026518164344297157\n",
      "Iteration 5128: loss = 0.0026518009721864305\n",
      "Iteration 5129: loss = 0.0026517855126128084\n",
      "Iteration 5130: loss = 0.0026517700557064897\n",
      "Iteration 5131: loss = 0.002651754601465121\n",
      "Iteration 5132: loss = 0.0026517391498863466\n",
      "Iteration 5133: loss = 0.002651723700967817\n",
      "Iteration 5134: loss = 0.002651708254707182\n",
      "Iteration 5135: loss = 0.0026516928111020967\n",
      "Iteration 5136: loss = 0.002651677370150214\n",
      "Iteration 5137: loss = 0.002651661931849194\n",
      "Iteration 5138: loss = 0.002651646496196695\n",
      "Iteration 5139: loss = 0.0026516310631903794\n",
      "Iteration 5140: loss = 0.0026516156328279116\n",
      "Iteration 5141: loss = 0.002651600205106957\n",
      "Iteration 5142: loss = 0.0026515847800251853\n",
      "Iteration 5143: loss = 0.002651569357580266\n",
      "Iteration 5144: loss = 0.0026515539377698727\n",
      "Iteration 5145: loss = 0.002651538520591681\n",
      "Iteration 5146: loss = 0.0026515231060433666\n",
      "Iteration 5147: loss = 0.0026515076941226105\n",
      "Iteration 5148: loss = 0.002651492284827093\n",
      "Iteration 5149: loss = 0.0026514768781544986\n",
      "Iteration 5150: loss = 0.0026514614741025132\n",
      "Iteration 5151: loss = 0.002651446072668824\n",
      "Iteration 5152: loss = 0.002651430673851122\n",
      "Iteration 5153: loss = 0.0026514152776470987\n",
      "Iteration 5154: loss = 0.0026513998840544495\n",
      "Iteration 5155: loss = 0.0026513844930708707\n",
      "Iteration 5156: loss = 0.002651369104694062\n",
      "Iteration 5157: loss = 0.0026513537189217216\n",
      "Iteration 5158: loss = 0.002651338335751556\n",
      "Iteration 5159: loss = 0.002651322955181268\n",
      "Iteration 5160: loss = 0.0026513075772085664\n",
      "Iteration 5161: loss = 0.00265129220183116\n",
      "Iteration 5162: loss = 0.0026512768290467606\n",
      "Iteration 5163: loss = 0.002651261458853082\n",
      "Iteration 5164: loss = 0.0026512460912478403\n",
      "Iteration 5165: loss = 0.0026512307262287534\n",
      "Iteration 5166: loss = 0.002651215363793541\n",
      "Iteration 5167: loss = 0.002651200003939927\n",
      "Iteration 5168: loss = 0.002651184646665634\n",
      "Iteration 5169: loss = 0.00265116929196839\n",
      "Iteration 5170: loss = 0.0026511539398459223\n",
      "Iteration 5171: loss = 0.0026511385902959626\n",
      "Iteration 5172: loss = 0.0026511232433162443\n",
      "Iteration 5173: loss = 0.002651107898904501\n",
      "Iteration 5174: loss = 0.002651092557058471\n",
      "Iteration 5175: loss = 0.002651077217775894\n",
      "Iteration 5176: loss = 0.0026510618810545103\n",
      "Iteration 5177: loss = 0.002651046546892063\n",
      "Iteration 5178: loss = 0.0026510312152862986\n",
      "Iteration 5179: loss = 0.002651015886234965\n",
      "Iteration 5180: loss = 0.0026510005597358117\n",
      "Iteration 5181: loss = 0.00265098523578659\n",
      "Iteration 5182: loss = 0.0026509699143850546\n",
      "Iteration 5183: loss = 0.0026509545955289613\n",
      "Iteration 5184: loss = 0.0026509392792160693\n",
      "Iteration 5185: loss = 0.002650923965444137\n",
      "Iteration 5186: loss = 0.0026509086542109276\n",
      "Iteration 5187: loss = 0.0026508933455142063\n",
      "Iteration 5188: loss = 0.002650878039351739\n",
      "Iteration 5189: loss = 0.0026508627357212946\n",
      "Iteration 5190: loss = 0.0026508474346206433\n",
      "Iteration 5191: loss = 0.002650832136047558\n",
      "Iteration 5192: loss = 0.002650816839999814\n",
      "Iteration 5193: loss = 0.0026508015464751875\n",
      "Iteration 5194: loss = 0.0026507862554714583\n",
      "Iteration 5195: loss = 0.002650770966986408\n",
      "Iteration 5196: loss = 0.0026507556810178175\n",
      "Iteration 5197: loss = 0.0026507403975634747\n",
      "Iteration 5198: loss = 0.0026507251166211655\n",
      "Iteration 5199: loss = 0.0026507098381886797\n",
      "Iteration 5200: loss = 0.002650694562263808\n",
      "Iteration 5201: loss = 0.0026506792888443442\n",
      "Iteration 5202: loss = 0.002650664017928085\n",
      "Iteration 5203: loss = 0.0026506487495128266\n",
      "Iteration 5204: loss = 0.0026506334835963694\n",
      "Iteration 5205: loss = 0.0026506182201765146\n",
      "Iteration 5206: loss = 0.002650602959251066\n",
      "Iteration 5207: loss = 0.00265058770081783\n",
      "Iteration 5208: loss = 0.002650572444874614\n",
      "Iteration 5209: loss = 0.002650557191419228\n",
      "Iteration 5210: loss = 0.0026505419404494845\n",
      "Iteration 5211: loss = 0.002650526691963196\n",
      "Iteration 5212: loss = 0.0026505114459581805\n",
      "Iteration 5213: loss = 0.0026504962024322546\n",
      "Iteration 5214: loss = 0.002650480961383239\n",
      "Iteration 5215: loss = 0.0026504657228089553\n",
      "Iteration 5216: loss = 0.0026504504867072287\n",
      "Iteration 5217: loss = 0.0026504352530758845\n",
      "Iteration 5218: loss = 0.002650420021912752\n",
      "Iteration 5219: loss = 0.00265040479321566\n",
      "Iteration 5220: loss = 0.002650389566982442\n",
      "Iteration 5221: loss = 0.002650374343210931\n",
      "Iteration 5222: loss = 0.0026503591218989646\n",
      "Iteration 5223: loss = 0.002650343903044381\n",
      "Iteration 5224: loss = 0.00265032868664502\n",
      "Iteration 5225: loss = 0.0026503134726987244\n",
      "Iteration 5226: loss = 0.002650298261203338\n",
      "Iteration 5227: loss = 0.0026502830521567084\n",
      "Iteration 5228: loss = 0.0026502678455566833\n",
      "Iteration 5229: loss = 0.002650252641401113\n",
      "Iteration 5230: loss = 0.0026502374396878497\n",
      "Iteration 5231: loss = 0.002650222240414749\n",
      "Iteration 5232: loss = 0.0026502070435796654\n",
      "Iteration 5233: loss = 0.002650191849180459\n",
      "Iteration 5234: loss = 0.0026501766572149905\n",
      "Iteration 5235: loss = 0.0026501614676811202\n",
      "Iteration 5236: loss = 0.0026501462805767146\n",
      "Iteration 5237: loss = 0.002650131095899639\n",
      "Iteration 5238: loss = 0.0026501159136477622\n",
      "Iteration 5239: loss = 0.002650100733818955\n",
      "Iteration 5240: loss = 0.00265008555641109\n",
      "Iteration 5241: loss = 0.00265007038142204\n",
      "Iteration 5242: loss = 0.002650055208849683\n",
      "Iteration 5243: loss = 0.002650040038691896\n",
      "Iteration 5244: loss = 0.00265002487094656\n",
      "Iteration 5245: loss = 0.0026500097056115575\n",
      "Iteration 5246: loss = 0.0026499945426847723\n",
      "Iteration 5247: loss = 0.0026499793821640914\n",
      "Iteration 5248: loss = 0.002649964224047402\n",
      "Iteration 5249: loss = 0.0026499490683325956\n",
      "Iteration 5250: loss = 0.0026499339150175627\n",
      "Iteration 5251: loss = 0.0026499187641001987\n",
      "Iteration 5252: loss = 0.0026499036155783993\n",
      "Iteration 5253: loss = 0.0026498884694500626\n",
      "Iteration 5254: loss = 0.002649873325713088\n",
      "Iteration 5255: loss = 0.0026498581843653786\n",
      "Iteration 5256: loss = 0.0026498430454048373\n",
      "Iteration 5257: loss = 0.0026498279088293707\n",
      "Iteration 5258: loss = 0.002649812774636887\n",
      "Iteration 5259: loss = 0.0026497976428252945\n",
      "Iteration 5260: loss = 0.002649782513392506\n",
      "Iteration 5261: loss = 0.0026497673863364346\n",
      "Iteration 5262: loss = 0.002649752261654997\n",
      "Iteration 5263: loss = 0.0026497371393461103\n",
      "Iteration 5264: loss = 0.0026497220194076935\n",
      "Iteration 5265: loss = 0.0026497069018376683\n",
      "Iteration 5266: loss = 0.002649691786633958\n",
      "Iteration 5267: loss = 0.0026496766737944886\n",
      "Iteration 5268: loss = 0.002649661563317187\n",
      "Iteration 5269: loss = 0.002649646455199982\n",
      "Iteration 5270: loss = 0.0026496313494408057\n",
      "Iteration 5271: loss = 0.00264961624603759\n",
      "Iteration 5272: loss = 0.0026496011449882706\n",
      "Iteration 5273: loss = 0.0026495860462907844\n",
      "Iteration 5274: loss = 0.0026495709499430703\n",
      "Iteration 5275: loss = 0.0026495558559430695\n",
      "Iteration 5276: loss = 0.0026495407642887227\n",
      "Iteration 5277: loss = 0.002649525674977977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5278: loss = 0.002649510588008778\n",
      "Iteration 5279: loss = 0.002649495503379074\n",
      "Iteration 5280: loss = 0.0026494804210868152\n",
      "Iteration 5281: loss = 0.0026494653411299543\n",
      "Iteration 5282: loss = 0.002649450263506445\n",
      "Iteration 5283: loss = 0.0026494351882142436\n",
      "Iteration 5284: loss = 0.002649420115251309\n",
      "Iteration 5285: loss = 0.0026494050446155995\n",
      "Iteration 5286: loss = 0.0026493899763050783\n",
      "Iteration 5287: loss = 0.0026493749103177083\n",
      "Iteration 5288: loss = 0.0026493598466514555\n",
      "Iteration 5289: loss = 0.002649344785304287\n",
      "Iteration 5290: loss = 0.002649329726274173\n",
      "Iteration 5291: loss = 0.0026493146695590842\n",
      "Iteration 5292: loss = 0.0026492996151569934\n",
      "Iteration 5293: loss = 0.002649284563065877\n",
      "Iteration 5294: loss = 0.0026492695132837103\n",
      "Iteration 5295: loss = 0.002649254465808474\n",
      "Iteration 5296: loss = 0.002649239420638147\n",
      "Iteration 5297: loss = 0.0026492243777707135\n",
      "Iteration 5298: loss = 0.0026492093372041566\n",
      "Iteration 5299: loss = 0.0026491942989364642\n",
      "Iteration 5300: loss = 0.0026491792629656234\n",
      "Iteration 5301: loss = 0.0026491642292896253\n",
      "Iteration 5302: loss = 0.0026491491979064603\n",
      "Iteration 5303: loss = 0.0026491341688141246\n",
      "Iteration 5304: loss = 0.0026491191420106117\n",
      "Iteration 5305: loss = 0.002649104117493921\n",
      "Iteration 5306: loss = 0.002649089095262051\n",
      "Iteration 5307: loss = 0.0026490740753130036\n",
      "Iteration 5308: loss = 0.0026490590576447814\n",
      "Iteration 5309: loss = 0.0026490440422553907\n",
      "Iteration 5310: loss = 0.0026490290291428372\n",
      "Iteration 5311: loss = 0.0026490140183051303\n",
      "Iteration 5312: loss = 0.00264899900974028\n",
      "Iteration 5313: loss = 0.0026489840034463006\n",
      "Iteration 5314: loss = 0.002648968999421205\n",
      "Iteration 5315: loss = 0.0026489539976630094\n",
      "Iteration 5316: loss = 0.002648938998169732\n",
      "Iteration 5317: loss = 0.002648924000939394\n",
      "Iteration 5318: loss = 0.002648909005970016\n",
      "Iteration 5319: loss = 0.002648894013259621\n",
      "Iteration 5320: loss = 0.002648879022806236\n",
      "Iteration 5321: loss = 0.0026488640346078876\n",
      "Iteration 5322: loss = 0.0026488490486626043\n",
      "Iteration 5323: loss = 0.0026488340649684183\n",
      "Iteration 5324: loss = 0.0026488190835233624\n",
      "Iteration 5325: loss = 0.0026488041043254704\n",
      "Iteration 5326: loss = 0.0026487891273727798\n",
      "Iteration 5327: loss = 0.0026487741526633277\n",
      "Iteration 5328: loss = 0.0026487591801951545\n",
      "Iteration 5329: loss = 0.002648744209966303\n",
      "Iteration 5330: loss = 0.002648729241974817\n",
      "Iteration 5331: loss = 0.002648714276218742\n",
      "Iteration 5332: loss = 0.0026486993126961247\n",
      "Iteration 5333: loss = 0.0026486843514050154\n",
      "Iteration 5334: loss = 0.0026486693923434643\n",
      "Iteration 5335: loss = 0.0026486544355095246\n",
      "Iteration 5336: loss = 0.002648639480901252\n",
      "Iteration 5337: loss = 0.002648624528516702\n",
      "Iteration 5338: loss = 0.002648609578353933\n",
      "Iteration 5339: loss = 0.0026485946304110053\n",
      "Iteration 5340: loss = 0.002648579684685982\n",
      "Iteration 5341: loss = 0.0026485647411769245\n",
      "Iteration 5342: loss = 0.0026485497998819008\n",
      "Iteration 5343: loss = 0.0026485348607989774\n",
      "Iteration 5344: loss = 0.0026485199239262235\n",
      "Iteration 5345: loss = 0.0026485049892617093\n",
      "Iteration 5346: loss = 0.0026484900568035093\n",
      "Iteration 5347: loss = 0.0026484751265496965\n",
      "Iteration 5348: loss = 0.0026484601984983485\n",
      "Iteration 5349: loss = 0.002648445272647543\n",
      "Iteration 5350: loss = 0.0026484303489953597\n",
      "Iteration 5351: loss = 0.002648415427539881\n",
      "Iteration 5352: loss = 0.00264840050827919\n",
      "Iteration 5353: loss = 0.002648385591211372\n",
      "Iteration 5354: loss = 0.0026483706763345155\n",
      "Iteration 5355: loss = 0.0026483557636467073\n",
      "Iteration 5356: loss = 0.00264834085314604\n",
      "Iteration 5357: loss = 0.002648325944830604\n",
      "Iteration 5358: loss = 0.0026483110386984958\n",
      "Iteration 5359: loss = 0.00264829613474781\n",
      "Iteration 5360: loss = 0.0026482812329766446\n",
      "Iteration 5361: loss = 0.0026482663333831\n",
      "Iteration 5362: loss = 0.002648251435965277\n",
      "Iteration 5363: loss = 0.0026482365407212785\n",
      "Iteration 5364: loss = 0.0026482216476492104\n",
      "Iteration 5365: loss = 0.002648206756747178\n",
      "Iteration 5366: loss = 0.002648191868013291\n",
      "Iteration 5367: loss = 0.0026481769814456586\n",
      "Iteration 5368: loss = 0.002648162097042394\n",
      "Iteration 5369: loss = 0.0026481472148016092\n",
      "Iteration 5370: loss = 0.0026481323347214212\n",
      "Iteration 5371: loss = 0.0026481174567999466\n",
      "Iteration 5372: loss = 0.0026481025810353048\n",
      "Iteration 5373: loss = 0.0026480877074256163\n",
      "Iteration 5374: loss = 0.0026480728359690032\n",
      "Iteration 5375: loss = 0.0026480579666635904\n",
      "Iteration 5376: loss = 0.002648043099507504\n",
      "Iteration 5377: loss = 0.0026480282344988715\n",
      "Iteration 5378: loss = 0.0026480133716358222\n",
      "Iteration 5379: loss = 0.002647998510916488\n",
      "Iteration 5380: loss = 0.002647983652339\n",
      "Iteration 5381: loss = 0.0026479687959014954\n",
      "Iteration 5382: loss = 0.0026479539416021094\n",
      "Iteration 5383: loss = 0.002647939089438981\n",
      "Iteration 5384: loss = 0.0026479242394102497\n",
      "Iteration 5385: loss = 0.0026479093915140564\n",
      "Iteration 5386: loss = 0.0026478945457485458\n",
      "Iteration 5387: loss = 0.0026478797021118623\n",
      "Iteration 5388: loss = 0.002647864860602153\n",
      "Iteration 5389: loss = 0.0026478500212175665\n",
      "Iteration 5390: loss = 0.0026478351839562533\n",
      "Iteration 5391: loss = 0.0026478203488163655\n",
      "Iteration 5392: loss = 0.002647805515796056\n",
      "Iteration 5393: loss = 0.0026477906848934815\n",
      "Iteration 5394: loss = 0.0026477758561067985\n",
      "Iteration 5395: loss = 0.0026477610294341663\n",
      "Iteration 5396: loss = 0.0026477462048737454\n",
      "Iteration 5397: loss = 0.002647731382423698\n",
      "Iteration 5398: loss = 0.0026477165620821886\n",
      "Iteration 5399: loss = 0.002647701743847383\n",
      "Iteration 5400: loss = 0.0026476869277174482\n",
      "Iteration 5401: loss = 0.0026476721136905546\n",
      "Iteration 5402: loss = 0.0026476573017648716\n",
      "Iteration 5403: loss = 0.0026476424919385723\n",
      "Iteration 5404: loss = 0.002647627684209832\n",
      "Iteration 5405: loss = 0.002647612878576826\n",
      "Iteration 5406: loss = 0.002647598075037732\n",
      "Iteration 5407: loss = 0.0026475832735907284\n",
      "Iteration 5408: loss = 0.0026475684742339987\n",
      "Iteration 5409: loss = 0.0026475536769657243\n",
      "Iteration 5410: loss = 0.0026475388817840894\n",
      "Iteration 5411: loss = 0.002647524088687281\n",
      "Iteration 5412: loss = 0.002647509297673487\n",
      "Iteration 5413: loss = 0.002647494508740897\n",
      "Iteration 5414: loss = 0.002647479721887702\n",
      "Iteration 5415: loss = 0.0026474649371120945\n",
      "Iteration 5416: loss = 0.0026474501544122703\n",
      "Iteration 5417: loss = 0.0026474353737864255\n",
      "Iteration 5418: loss = 0.0026474205952327573\n",
      "Iteration 5419: loss = 0.0026474058187494656\n",
      "Iteration 5420: loss = 0.0026473910443347527\n",
      "Iteration 5421: loss = 0.0026473762719868214\n",
      "Iteration 5422: loss = 0.002647361501703876\n",
      "Iteration 5423: loss = 0.0026473467334841225\n",
      "Iteration 5424: loss = 0.00264733196732577\n",
      "Iteration 5425: loss = 0.0026473172032270282\n",
      "Iteration 5426: loss = 0.002647302441186108\n",
      "Iteration 5427: loss = 0.0026472876812012216\n",
      "Iteration 5428: loss = 0.0026472729232705863\n",
      "Iteration 5429: loss = 0.0026472581673924163\n",
      "Iteration 5430: loss = 0.002647243413564931\n",
      "Iteration 5431: loss = 0.002647228661786349\n",
      "Iteration 5432: loss = 0.002647213912054893\n",
      "Iteration 5433: loss = 0.002647199164368785\n",
      "Iteration 5434: loss = 0.00264718441872625\n",
      "Iteration 5435: loss = 0.002647169675125515\n",
      "Iteration 5436: loss = 0.002647154933564807\n",
      "Iteration 5437: loss = 0.002647140194042358\n",
      "Iteration 5438: loss = 0.0026471254565563957\n",
      "Iteration 5439: loss = 0.002647110721105156\n",
      "Iteration 5440: loss = 0.002647095987686872\n",
      "Iteration 5441: loss = 0.002647081256299781\n",
      "Iteration 5442: loss = 0.00264706652694212\n",
      "Iteration 5443: loss = 0.0026470517996121296\n",
      "Iteration 5444: loss = 0.0026470370743080507\n",
      "Iteration 5445: loss = 0.002647022351028125\n",
      "Iteration 5446: loss = 0.002647007629770599\n",
      "Iteration 5447: loss = 0.0026469929105337165\n",
      "Iteration 5448: loss = 0.002646978193315728\n",
      "Iteration 5449: loss = 0.0026469634781148806\n",
      "Iteration 5450: loss = 0.0026469487649294263\n",
      "Iteration 5451: loss = 0.002646934053757618\n",
      "Iteration 5452: loss = 0.002646919344597709\n",
      "Iteration 5453: loss = 0.0026469046374479567\n",
      "Iteration 5454: loss = 0.0026468899323066177\n",
      "Iteration 5455: loss = 0.0026468752291719504\n",
      "Iteration 5456: loss = 0.002646860528042217\n",
      "Iteration 5457: loss = 0.00264684582891568\n",
      "Iteration 5458: loss = 0.0026468311317906015\n",
      "Iteration 5459: loss = 0.0026468164366652497\n",
      "Iteration 5460: loss = 0.002646801743537891\n",
      "Iteration 5461: loss = 0.0026467870524067927\n",
      "Iteration 5462: loss = 0.0026467723632702276\n",
      "Iteration 5463: loss = 0.002646757676126466\n",
      "Iteration 5464: loss = 0.0026467429909737827\n",
      "Iteration 5465: loss = 0.002646728307810453\n",
      "Iteration 5466: loss = 0.002646713626634753\n",
      "Iteration 5467: loss = 0.0026466989474449626\n",
      "Iteration 5468: loss = 0.0026466842702393607\n",
      "Iteration 5469: loss = 0.0026466695950162302\n",
      "Iteration 5470: loss = 0.002646654921773853\n",
      "Iteration 5471: loss = 0.0026466402505105156\n",
      "Iteration 5472: loss = 0.002646625581224503\n",
      "Iteration 5473: loss = 0.002646610913914105\n",
      "Iteration 5474: loss = 0.00264659624857761\n",
      "Iteration 5475: loss = 0.0026465815852133104\n",
      "Iteration 5476: loss = 0.0026465669238194983\n",
      "Iteration 5477: loss = 0.0026465522643944694\n",
      "Iteration 5478: loss = 0.002646537606936518\n",
      "Iteration 5479: loss = 0.0026465229514439433\n",
      "Iteration 5480: loss = 0.002646508297915044\n",
      "Iteration 5481: loss = 0.002646493646348121\n",
      "Iteration 5482: loss = 0.0026464789967414774\n",
      "Iteration 5483: loss = 0.0026464643490934163\n",
      "Iteration 5484: loss = 0.002646449703402244\n",
      "Iteration 5485: loss = 0.0026464350596662675\n",
      "Iteration 5486: loss = 0.002646420417883795\n",
      "Iteration 5487: loss = 0.0026464057780531377\n",
      "Iteration 5488: loss = 0.0026463911401726076\n",
      "Iteration 5489: loss = 0.0026463765042405176\n",
      "Iteration 5490: loss = 0.0026463618702551833\n",
      "Iteration 5491: loss = 0.0026463472382149216\n",
      "Iteration 5492: loss = 0.0026463326081180498\n",
      "Iteration 5493: loss = 0.002646317979962888\n",
      "Iteration 5494: loss = 0.0026463033537477585\n",
      "Iteration 5495: loss = 0.002646288729470983\n",
      "Iteration 5496: loss = 0.0026462741071308863\n",
      "Iteration 5497: loss = 0.0026462594867257944\n",
      "Iteration 5498: loss = 0.002646244868254036\n",
      "Iteration 5499: loss = 0.0026462302517139386\n",
      "Iteration 5500: loss = 0.0026462156371038345\n",
      "Iteration 5501: loss = 0.0026462010244220546\n",
      "Iteration 5502: loss = 0.0026461864136669336\n",
      "Iteration 5503: loss = 0.0026461718048368067\n",
      "Iteration 5504: loss = 0.002646157197930011\n",
      "Iteration 5505: loss = 0.0026461425929448844\n",
      "Iteration 5506: loss = 0.0026461279898797674\n",
      "Iteration 5507: loss = 0.0026461133887330022\n",
      "Iteration 5508: loss = 0.0026460987895029313\n",
      "Iteration 5509: loss = 0.0026460841921878983\n",
      "Iteration 5510: loss = 0.0026460695967862515\n",
      "Iteration 5511: loss = 0.002646055003296337\n",
      "Iteration 5512: loss = 0.002646040411716505\n",
      "Iteration 5513: loss = 0.0026460258220451067\n",
      "Iteration 5514: loss = 0.002646011234280493\n",
      "Iteration 5515: loss = 0.0026459966484210194\n",
      "Iteration 5516: loss = 0.0026459820644650412\n",
      "Iteration 5517: loss = 0.002645967482410914\n",
      "Iteration 5518: loss = 0.002645952902256998\n",
      "Iteration 5519: loss = 0.0026459383240016515\n",
      "Iteration 5520: loss = 0.0026459237476432374\n",
      "Iteration 5521: loss = 0.0026459091731801192\n",
      "Iteration 5522: loss = 0.0026458946006106604\n",
      "Iteration 5523: loss = 0.0026458800299332275\n",
      "Iteration 5524: loss = 0.002645865461146188\n",
      "Iteration 5525: loss = 0.0026458508942479113\n",
      "Iteration 5526: loss = 0.0026458363292367684\n",
      "Iteration 5527: loss = 0.0026458217661111316\n",
      "Iteration 5528: loss = 0.002645807204869374\n",
      "Iteration 5529: loss = 0.0026457926455098714\n",
      "Iteration 5530: loss = 0.002645778088031\n",
      "Iteration 5531: loss = 0.002645763532431139\n",
      "Iteration 5532: loss = 0.0026457489787086673\n",
      "Iteration 5533: loss = 0.0026457344268619675\n",
      "Iteration 5534: loss = 0.0026457198768894213\n",
      "Iteration 5535: loss = 0.0026457053287894127\n",
      "Iteration 5536: loss = 0.002645690782560329\n",
      "Iteration 5537: loss = 0.002645676238200556\n",
      "Iteration 5538: loss = 0.0026456616957084844\n",
      "Iteration 5539: loss = 0.002645647155082502\n",
      "Iteration 5540: loss = 0.0026456326163210035\n",
      "Iteration 5541: loss = 0.0026456180794223795\n",
      "Iteration 5542: loss = 0.0026456035443850274\n",
      "Iteration 5543: loss = 0.0026455890112073418\n",
      "Iteration 5544: loss = 0.002645574479887721\n",
      "Iteration 5545: loss = 0.0026455599504245646\n",
      "Iteration 5546: loss = 0.0026455454228162733\n",
      "Iteration 5547: loss = 0.002645530897061249\n",
      "Iteration 5548: loss = 0.002645516373157897\n",
      "Iteration 5549: loss = 0.002645501851104621\n",
      "Iteration 5550: loss = 0.0026454873308998277\n",
      "Iteration 5551: loss = 0.002645472812541927\n",
      "Iteration 5552: loss = 0.002645458296029327\n",
      "Iteration 5553: loss = 0.0026454437813604394\n",
      "Iteration 5554: loss = 0.0026454292685336774\n",
      "Iteration 5555: loss = 0.0026454147575474545\n",
      "Iteration 5556: loss = 0.0026454002484001867\n",
      "Iteration 5557: loss = 0.0026453857410902917\n",
      "Iteration 5558: loss = 0.002645371235616187\n",
      "Iteration 5559: loss = 0.002645356731976294\n",
      "Iteration 5560: loss = 0.0026453422301690325\n",
      "Iteration 5561: loss = 0.0026453277301928275\n",
      "Iteration 5562: loss = 0.002645313232046103\n",
      "Iteration 5563: loss = 0.0026452987357272833\n",
      "Iteration 5564: loss = 0.002645284241234798\n",
      "Iteration 5565: loss = 0.002645269748567075\n",
      "Iteration 5566: loss = 0.0026452552577225447\n",
      "Iteration 5567: loss = 0.002645240768699639\n",
      "Iteration 5568: loss = 0.002645226281496792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5569: loss = 0.0026452117961124374\n",
      "Iteration 5570: loss = 0.0026451973125450113\n",
      "Iteration 5571: loss = 0.0026451828307929525\n",
      "Iteration 5572: loss = 0.0026451683508546993\n",
      "Iteration 5573: loss = 0.0026451538727286926\n",
      "Iteration 5574: loss = 0.002645139396413374\n",
      "Iteration 5575: loss = 0.0026451249219071873\n",
      "Iteration 5576: loss = 0.002645110449208577\n",
      "Iteration 5577: loss = 0.002645095978315991\n",
      "Iteration 5578: loss = 0.0026450815092278752\n",
      "Iteration 5579: loss = 0.00264506704194268\n",
      "Iteration 5580: loss = 0.002645052576458856\n",
      "Iteration 5581: loss = 0.0026450381127748556\n",
      "Iteration 5582: loss = 0.002645023650889132\n",
      "Iteration 5583: loss = 0.0026450091908001403\n",
      "Iteration 5584: loss = 0.0026449947325063367\n",
      "Iteration 5585: loss = 0.00264498027600618\n",
      "Iteration 5586: loss = 0.002644965821298129\n",
      "Iteration 5587: loss = 0.0026449513683806446\n",
      "Iteration 5588: loss = 0.0026449369172521888\n",
      "Iteration 5589: loss = 0.0026449224679112255\n",
      "Iteration 5590: loss = 0.0026449080203562202\n",
      "Iteration 5591: loss = 0.0026448935745856388\n",
      "Iteration 5592: loss = 0.0026448791305979497\n",
      "Iteration 5593: loss = 0.0026448646883916226\n",
      "Iteration 5594: loss = 0.002644850247965127\n",
      "Iteration 5595: loss = 0.0026448358093169363\n",
      "Iteration 5596: loss = 0.0026448213724455244\n",
      "Iteration 5597: loss = 0.0026448069373493656\n",
      "Iteration 5598: loss = 0.002644792504026937\n",
      "Iteration 5599: loss = 0.0026447780724767156\n",
      "Iteration 5600: loss = 0.002644763642697182\n",
      "Iteration 5601: loss = 0.002644749214686816\n",
      "Iteration 5602: loss = 0.0026447347884441\n",
      "Iteration 5603: loss = 0.0026447203639675186\n",
      "Iteration 5604: loss = 0.0026447059412555555\n",
      "Iteration 5605: loss = 0.0026446915203066977\n",
      "Iteration 5606: loss = 0.0026446771011194334\n",
      "Iteration 5607: loss = 0.0026446626836922503\n",
      "Iteration 5608: loss = 0.0026446482680236414\n",
      "Iteration 5609: loss = 0.002644633854112097\n",
      "Iteration 5610: loss = 0.002644619441956111\n",
      "Iteration 5611: loss = 0.0026446050315541785\n",
      "Iteration 5612: loss = 0.002644590622904796\n",
      "Iteration 5613: loss = 0.0026445762160064606\n",
      "Iteration 5614: loss = 0.0026445618108576715\n",
      "Iteration 5615: loss = 0.0026445474074569286\n",
      "Iteration 5616: loss = 0.0026445330058027356\n",
      "Iteration 5617: loss = 0.0026445186058935938\n",
      "Iteration 5618: loss = 0.0026445042077280095\n",
      "Iteration 5619: loss = 0.0026444898113044874\n",
      "Iteration 5620: loss = 0.0026444754166215353\n",
      "Iteration 5621: loss = 0.002644461023677663\n",
      "Iteration 5622: loss = 0.002644446632471379\n",
      "Iteration 5623: loss = 0.002644432243001196\n",
      "Iteration 5624: loss = 0.0026444178552656275\n",
      "Iteration 5625: loss = 0.0026444034692631865\n",
      "Iteration 5626: loss = 0.00264438908499239\n",
      "Iteration 5627: loss = 0.002644374702451755\n",
      "Iteration 5628: loss = 0.002644360321639799\n",
      "Iteration 5629: loss = 0.002644345942555043\n",
      "Iteration 5630: loss = 0.002644331565196008\n",
      "Iteration 5631: loss = 0.0026443171895612163\n",
      "Iteration 5632: loss = 0.002644302815649193\n",
      "Iteration 5633: loss = 0.0026442884434584616\n",
      "Iteration 5634: loss = 0.002644274072987551\n",
      "Iteration 5635: loss = 0.002644259704234988\n",
      "Iteration 5636: loss = 0.0026442453371993027\n",
      "Iteration 5637: loss = 0.002644230971879026\n",
      "Iteration 5638: loss = 0.002644216608272691\n",
      "Iteration 5639: loss = 0.0026442022463788292\n",
      "Iteration 5640: loss = 0.002644187886195977\n",
      "Iteration 5641: loss = 0.0026441735277226708\n",
      "Iteration 5642: loss = 0.002644159170957449\n",
      "Iteration 5643: loss = 0.0026441448158988487\n",
      "Iteration 5644: loss = 0.0026441304625454123\n",
      "Iteration 5645: loss = 0.002644116110895681\n",
      "Iteration 5646: loss = 0.0026441017609481975\n",
      "Iteration 5647: loss = 0.002644087412701507\n",
      "Iteration 5648: loss = 0.0026440730661541553\n",
      "Iteration 5649: loss = 0.002644058721304689\n",
      "Iteration 5650: loss = 0.002644044378151658\n",
      "Iteration 5651: loss = 0.0026440300366936103\n",
      "Iteration 5652: loss = 0.0026440156969290986\n",
      "Iteration 5653: loss = 0.0026440013588566766\n",
      "Iteration 5654: loss = 0.0026439870224748965\n",
      "Iteration 5655: loss = 0.0026439726877823134\n",
      "Iteration 5656: loss = 0.002643958354777486\n",
      "Iteration 5657: loss = 0.002643944023458971\n",
      "Iteration 5658: loss = 0.002643929693825327\n",
      "Iteration 5659: loss = 0.002643915365875116\n",
      "Iteration 5660: loss = 0.0026439010396069004\n",
      "Iteration 5661: loss = 0.002643886715019243\n",
      "Iteration 5662: loss = 0.002643872392110708\n",
      "Iteration 5663: loss = 0.002643858070879862\n",
      "Iteration 5664: loss = 0.0026438437513252736\n",
      "Iteration 5665: loss = 0.0026438294334455094\n",
      "Iteration 5666: loss = 0.0026438151172391406\n",
      "Iteration 5667: loss = 0.002643800802704739\n",
      "Iteration 5668: loss = 0.0026437864898408766\n",
      "Iteration 5669: loss = 0.002643772178646128\n",
      "Iteration 5670: loss = 0.002643757869119068\n",
      "Iteration 5671: loss = 0.0026437435612582747\n",
      "Iteration 5672: loss = 0.002643729255062324\n",
      "Iteration 5673: loss = 0.0026437149505297974\n",
      "Iteration 5674: loss = 0.0026437006476592744\n",
      "Iteration 5675: loss = 0.002643686346449338\n",
      "Iteration 5676: loss = 0.0026436720468985707\n",
      "Iteration 5677: loss = 0.002643657749005557\n",
      "Iteration 5678: loss = 0.0026436434527688834\n",
      "Iteration 5679: loss = 0.002643629158187138\n",
      "Iteration 5680: loss = 0.002643614865258908\n",
      "Iteration 5681: loss = 0.002643600573982783\n",
      "Iteration 5682: loss = 0.002643586284357356\n",
      "Iteration 5683: loss = 0.0026435719963812192\n",
      "Iteration 5684: loss = 0.002643557710052965\n",
      "Iteration 5685: loss = 0.0026435434253711913\n",
      "Iteration 5686: loss = 0.002643529142334492\n",
      "Iteration 5687: loss = 0.002643514860941466\n",
      "Iteration 5688: loss = 0.0026435005811907115\n",
      "Iteration 5689: loss = 0.0026434863030808304\n",
      "Iteration 5690: loss = 0.002643472026610424\n",
      "Iteration 5691: loss = 0.0026434577517780947\n",
      "Iteration 5692: loss = 0.0026434434785824476\n",
      "Iteration 5693: loss = 0.002643429207022088\n",
      "Iteration 5694: loss = 0.002643414937095622\n",
      "Iteration 5695: loss = 0.002643400668801659\n",
      "Iteration 5696: loss = 0.002643386402138808\n",
      "Iteration 5697: loss = 0.00264337213710568\n",
      "Iteration 5698: loss = 0.0026433578737008866\n",
      "Iteration 5699: loss = 0.0026433436119230424\n",
      "Iteration 5700: loss = 0.0026433293517707608\n",
      "Iteration 5701: loss = 0.002643315093242659\n",
      "Iteration 5702: loss = 0.0026433008363373534\n",
      "Iteration 5703: loss = 0.002643286581053462\n",
      "Iteration 5704: loss = 0.002643272327389606\n",
      "Iteration 5705: loss = 0.0026432580753444064\n",
      "Iteration 5706: loss = 0.0026432438249164845\n",
      "Iteration 5707: loss = 0.002643229576104465\n",
      "Iteration 5708: loss = 0.002643215328906973\n",
      "Iteration 5709: loss = 0.002643201083322633\n",
      "Iteration 5710: loss = 0.002643186839350075\n",
      "Iteration 5711: loss = 0.002643172596987926\n",
      "Iteration 5712: loss = 0.0026431583562348177\n",
      "Iteration 5713: loss = 0.0026431441170893803\n",
      "Iteration 5714: loss = 0.002643129879550246\n",
      "Iteration 5715: loss = 0.0026431156436160503\n",
      "Iteration 5716: loss = 0.0026431014092854273\n",
      "Iteration 5717: loss = 0.0026430871765570144\n",
      "Iteration 5718: loss = 0.002643072945429448\n",
      "Iteration 5719: loss = 0.0026430587159013675\n",
      "Iteration 5720: loss = 0.0026430444879714144\n",
      "Iteration 5721: loss = 0.002643030261638229\n",
      "Iteration 5722: loss = 0.002643016036900454\n",
      "Iteration 5723: loss = 0.0026430018137567346\n",
      "Iteration 5724: loss = 0.0026429875922057153\n",
      "Iteration 5725: loss = 0.002642973372246043\n",
      "Iteration 5726: loss = 0.0026429591538763646\n",
      "Iteration 5727: loss = 0.002642944937095331\n",
      "Iteration 5728: loss = 0.0026429307219015905\n",
      "Iteration 5729: loss = 0.0026429165082937967\n",
      "Iteration 5730: loss = 0.0026429022962706017\n",
      "Iteration 5731: loss = 0.002642888085830659\n",
      "Iteration 5732: loss = 0.0026428738769726247\n",
      "Iteration 5733: loss = 0.0026428596696951557\n",
      "Iteration 5734: loss = 0.002642845463996909\n",
      "Iteration 5735: loss = 0.0026428312598765456\n",
      "Iteration 5736: loss = 0.002642817057332723\n",
      "Iteration 5737: loss = 0.0026428028563641056\n",
      "Iteration 5738: loss = 0.0026427886569693544\n",
      "Iteration 5739: loss = 0.0026427744591471345\n",
      "Iteration 5740: loss = 0.0026427602628961108\n",
      "Iteration 5741: loss = 0.0026427460682149507\n",
      "Iteration 5742: loss = 0.002642731875102321\n",
      "Iteration 5743: loss = 0.002642717683556892\n",
      "Iteration 5744: loss = 0.0026427034935773332\n",
      "Iteration 5745: loss = 0.0026426893051623166\n",
      "Iteration 5746: loss = 0.0026426751183105143\n",
      "Iteration 5747: loss = 0.002642660933020601\n",
      "Iteration 5748: loss = 0.002642646749291252\n",
      "Iteration 5749: loss = 0.002642632567121144\n",
      "Iteration 5750: loss = 0.002642618386508954\n",
      "Iteration 5751: loss = 0.002642604207453362\n",
      "Iteration 5752: loss = 0.002642590029953047\n",
      "Iteration 5753: loss = 0.002642575854006692\n",
      "Iteration 5754: loss = 0.0026425616796129786\n",
      "Iteration 5755: loss = 0.002642547506770591\n",
      "Iteration 5756: loss = 0.002642533335478214\n",
      "Iteration 5757: loss = 0.002642519165734534\n",
      "Iteration 5758: loss = 0.0026425049975382395\n",
      "Iteration 5759: loss = 0.0026424908308880176\n",
      "Iteration 5760: loss = 0.0026424766657825605\n",
      "Iteration 5761: loss = 0.0026424625022205576\n",
      "Iteration 5762: loss = 0.0026424483402007026\n",
      "Iteration 5763: loss = 0.0026424341797216884\n",
      "Iteration 5764: loss = 0.0026424200207822105\n",
      "Iteration 5765: loss = 0.0026424058633809644\n",
      "Iteration 5766: loss = 0.0026423917075166473\n",
      "Iteration 5767: loss = 0.002642377553187958\n",
      "Iteration 5768: loss = 0.0026423634003935968\n",
      "Iteration 5769: loss = 0.0026423492491322647\n",
      "Iteration 5770: loss = 0.002642335099402663\n",
      "Iteration 5771: loss = 0.0026423209512034954\n",
      "Iteration 5772: loss = 0.002642306804533467\n",
      "Iteration 5773: loss = 0.002642292659391282\n",
      "Iteration 5774: loss = 0.00264227851577565\n",
      "Iteration 5775: loss = 0.0026422643736852777\n",
      "Iteration 5776: loss = 0.002642250233118874\n",
      "Iteration 5777: loss = 0.002642236094075151\n",
      "Iteration 5778: loss = 0.002642221956552819\n",
      "Iteration 5779: loss = 0.002642207820550591\n",
      "Iteration 5780: loss = 0.002642193686067183\n",
      "Iteration 5781: loss = 0.0026421795531013084\n",
      "Iteration 5782: loss = 0.0026421654216516855\n",
      "Iteration 5783: loss = 0.0026421512917170304\n",
      "Iteration 5784: loss = 0.0026421371632960634\n",
      "Iteration 5785: loss = 0.002642123036387505\n",
      "Iteration 5786: loss = 0.0026421089109900753\n",
      "Iteration 5787: loss = 0.002642094787102497\n",
      "Iteration 5788: loss = 0.0026420806647234943\n",
      "Iteration 5789: loss = 0.0026420665438517924\n",
      "Iteration 5790: loss = 0.002642052424486117\n",
      "Iteration 5791: loss = 0.002642038306625196\n",
      "Iteration 5792: loss = 0.002642024190267757\n",
      "Iteration 5793: loss = 0.00264201007541253\n",
      "Iteration 5794: loss = 0.0026419959620582463\n",
      "Iteration 5795: loss = 0.0026419818502036374\n",
      "Iteration 5796: loss = 0.002641967739847437\n",
      "Iteration 5797: loss = 0.0026419536309883794\n",
      "Iteration 5798: loss = 0.0026419395236252\n",
      "Iteration 5799: loss = 0.002641925417756636\n",
      "Iteration 5800: loss = 0.0026419113133814244\n",
      "Iteration 5801: loss = 0.0026418972104983057\n",
      "Iteration 5802: loss = 0.002641883109106019\n",
      "Iteration 5803: loss = 0.0026418690092033063\n",
      "Iteration 5804: loss = 0.00264185491078891\n",
      "Iteration 5805: loss = 0.0026418408138615744\n",
      "Iteration 5806: loss = 0.002641826718420044\n",
      "Iteration 5807: loss = 0.002641812624463065\n",
      "Iteration 5808: loss = 0.002641798531989385\n",
      "Iteration 5809: loss = 0.002641784440997752\n",
      "Iteration 5810: loss = 0.0026417703514869163\n",
      "Iteration 5811: loss = 0.002641756263455628\n",
      "Iteration 5812: loss = 0.0026417421769026395\n",
      "Iteration 5813: loss = 0.002641728091826704\n",
      "Iteration 5814: loss = 0.0026417140082265755\n",
      "Iteration 5815: loss = 0.0026416999261010096\n",
      "Iteration 5816: loss = 0.002641685845448764\n",
      "Iteration 5817: loss = 0.0026416717662685937\n",
      "Iteration 5818: loss = 0.0026416576885592603\n",
      "Iteration 5819: loss = 0.0026416436123195227\n",
      "Iteration 5820: loss = 0.0026416295375481427\n",
      "Iteration 5821: loss = 0.002641615464243882\n",
      "Iteration 5822: loss = 0.0026416013924055047\n",
      "Iteration 5823: loss = 0.002641587322031775\n",
      "Iteration 5824: loss = 0.00264157325312146\n",
      "Iteration 5825: loss = 0.0026415591856733247\n",
      "Iteration 5826: loss = 0.0026415451196861385\n",
      "Iteration 5827: loss = 0.0026415310551586705\n",
      "Iteration 5828: loss = 0.0026415169920896917\n",
      "Iteration 5829: loss = 0.002641502930477973\n",
      "Iteration 5830: loss = 0.002641488870322287\n",
      "Iteration 5831: loss = 0.0026414748116214077\n",
      "Iteration 5832: loss = 0.00264146075437411\n",
      "Iteration 5833: loss = 0.0026414466985791706\n",
      "Iteration 5834: loss = 0.002641432644235367\n",
      "Iteration 5835: loss = 0.0026414185913414766\n",
      "Iteration 5836: loss = 0.0026414045398962796\n",
      "Iteration 5837: loss = 0.002641390489898556\n",
      "Iteration 5838: loss = 0.0026413764413470886\n",
      "Iteration 5839: loss = 0.0026413623942406596\n",
      "Iteration 5840: loss = 0.0026413483485780544\n",
      "Iteration 5841: loss = 0.002641334304358057\n",
      "Iteration 5842: loss = 0.002641320261579454\n",
      "Iteration 5843: loss = 0.0026413062202410324\n",
      "Iteration 5844: loss = 0.0026412921803415823\n",
      "Iteration 5845: loss = 0.002641278141879892\n",
      "Iteration 5846: loss = 0.002641264104854753\n",
      "Iteration 5847: loss = 0.0026412500692649574\n",
      "Iteration 5848: loss = 0.002641236035109298\n",
      "Iteration 5849: loss = 0.0026412220023865696\n",
      "Iteration 5850: loss = 0.002641207971095567\n",
      "Iteration 5851: loss = 0.0026411939412350873\n",
      "Iteration 5852: loss = 0.002641179912803927\n",
      "Iteration 5853: loss = 0.0026411658858008858\n",
      "Iteration 5854: loss = 0.002641151860224764\n",
      "Iteration 5855: loss = 0.0026411378360743617\n",
      "Iteration 5856: loss = 0.0026411238133484806\n",
      "Iteration 5857: loss = 0.0026411097920459245\n",
      "Iteration 5858: loss = 0.0026410957721654978\n",
      "Iteration 5859: loss = 0.0026410817537060064\n",
      "Iteration 5860: loss = 0.002641067736666256\n",
      "Iteration 5861: loss = 0.002641053721045055\n",
      "Iteration 5862: loss = 0.0026410397068412114\n",
      "Iteration 5863: loss = 0.002641025694053535\n",
      "Iteration 5864: loss = 0.002641011682680837\n",
      "Iteration 5865: loss = 0.0026409976727219306\n",
      "Iteration 5866: loss = 0.002640983664175628\n",
      "Iteration 5867: loss = 0.0026409696570407425\n",
      "Iteration 5868: loss = 0.0026409556513160913\n",
      "Iteration 5869: loss = 0.00264094164700049\n",
      "Iteration 5870: loss = 0.002640927644092757\n",
      "Iteration 5871: loss = 0.0026409136425917094\n",
      "Iteration 5872: loss = 0.002640899642496169\n",
      "Iteration 5873: loss = 0.002640885643804955\n",
      "Iteration 5874: loss = 0.0026408716465168903\n",
      "Iteration 5875: loss = 0.0026408576506307974\n",
      "Iteration 5876: loss = 0.002640843656145502\n",
      "Iteration 5877: loss = 0.0026408296630598274\n",
      "Iteration 5878: loss = 0.002640815671372602\n",
      "Iteration 5879: loss = 0.0026408016810826507\n",
      "Iteration 5880: loss = 0.002640787692188804\n",
      "Iteration 5881: loss = 0.002640773704689892\n",
      "Iteration 5882: loss = 0.0026407597185847437\n",
      "Iteration 5883: loss = 0.0026407457338721916\n",
      "Iteration 5884: loss = 0.002640731750551069\n",
      "Iteration 5885: loss = 0.00264071776862021\n",
      "Iteration 5886: loss = 0.0026407037880784496\n",
      "Iteration 5887: loss = 0.0026406898089246233\n",
      "Iteration 5888: loss = 0.0026406758311575693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5889: loss = 0.0026406618547761248\n",
      "Iteration 5890: loss = 0.00264064787977913\n",
      "Iteration 5891: loss = 0.002640633906165426\n",
      "Iteration 5892: loss = 0.0026406199339338533\n",
      "Iteration 5893: loss = 0.0026406059630832554\n",
      "Iteration 5894: loss = 0.0026405919936124756\n",
      "Iteration 5895: loss = 0.002640578025520358\n",
      "Iteration 5896: loss = 0.00264056405880575\n",
      "Iteration 5897: loss = 0.0026405500934674974\n",
      "Iteration 5898: loss = 0.0026405361295044495\n",
      "Iteration 5899: loss = 0.0026405221669154534\n",
      "Iteration 5900: loss = 0.002640508205699361\n",
      "Iteration 5901: loss = 0.002640494245855023\n",
      "Iteration 5902: loss = 0.0026404802873812917\n",
      "Iteration 5903: loss = 0.00264046633027702\n",
      "Iteration 5904: loss = 0.0026404523745410637\n",
      "Iteration 5905: loss = 0.002640438420172277\n",
      "Iteration 5906: loss = 0.002640424467169517\n",
      "Iteration 5907: loss = 0.0026404105155316423\n",
      "Iteration 5908: loss = 0.00264039656525751\n",
      "Iteration 5909: loss = 0.00264038261634598\n",
      "Iteration 5910: loss = 0.002640368668795914\n",
      "Iteration 5911: loss = 0.0026403547226061743\n",
      "Iteration 5912: loss = 0.0026403407777756223\n",
      "Iteration 5913: loss = 0.0026403268343031232\n",
      "Iteration 5914: loss = 0.002640312892187542\n",
      "Iteration 5915: loss = 0.002640298951427744\n",
      "Iteration 5916: loss = 0.0026402850120225974\n",
      "Iteration 5917: loss = 0.00264027107397097\n",
      "Iteration 5918: loss = 0.002640257137271731\n",
      "Iteration 5919: loss = 0.0026402432019237507\n",
      "Iteration 5920: loss = 0.0026402292679259015\n",
      "Iteration 5921: loss = 0.002640215335277054\n",
      "Iteration 5922: loss = 0.002640201403976083\n",
      "Iteration 5923: loss = 0.002640187474021863\n",
      "Iteration 5924: loss = 0.0026401735454132692\n",
      "Iteration 5925: loss = 0.0026401596181491785\n",
      "Iteration 5926: loss = 0.0026401456922284685\n",
      "Iteration 5927: loss = 0.0026401317676500173\n",
      "Iteration 5928: loss = 0.002640117844412706\n",
      "Iteration 5929: loss = 0.0026401039225154143\n",
      "Iteration 5930: loss = 0.0026400900019570247\n",
      "Iteration 5931: loss = 0.00264007608273642\n",
      "Iteration 5932: loss = 0.002640062164852484\n",
      "Iteration 5933: loss = 0.0026400482483041015\n",
      "Iteration 5934: loss = 0.0026400343330901586\n",
      "Iteration 5935: loss = 0.002640020419209543\n",
      "Iteration 5936: loss = 0.0026400065066611413\n",
      "Iteration 5937: loss = 0.0026399925954438447\n",
      "Iteration 5938: loss = 0.002639978685556541\n",
      "Iteration 5939: loss = 0.0026399647769981237\n",
      "Iteration 5940: loss = 0.002639950869767483\n",
      "Iteration 5941: loss = 0.002639936963863514\n",
      "Iteration 5942: loss = 0.002639923059285109\n",
      "Iteration 5943: loss = 0.0026399091560311656\n",
      "Iteration 5944: loss = 0.002639895254100578\n",
      "Iteration 5945: loss = 0.0026398813534922443\n",
      "Iteration 5946: loss = 0.002639867454205064\n",
      "Iteration 5947: loss = 0.002639853556237935\n",
      "Iteration 5948: loss = 0.0026398396595897584\n",
      "Iteration 5949: loss = 0.0026398257642594354\n",
      "Iteration 5950: loss = 0.002639811870245869\n",
      "Iteration 5951: loss = 0.002639797977547962\n",
      "Iteration 5952: loss = 0.0026397840861646198\n",
      "Iteration 5953: loss = 0.0026397701960947477\n",
      "Iteration 5954: loss = 0.0026397563073372516\n",
      "Iteration 5955: loss = 0.002639742419891039\n",
      "Iteration 5956: loss = 0.0026397285337550203\n",
      "Iteration 5957: loss = 0.0026397146489281027\n",
      "Iteration 5958: loss = 0.0026397007654091987\n",
      "Iteration 5959: loss = 0.0026396868831972185\n",
      "Iteration 5960: loss = 0.002639673002291076\n",
      "Iteration 5961: loss = 0.0026396591226896843\n",
      "Iteration 5962: loss = 0.0026396452443919584\n",
      "Iteration 5963: loss = 0.002639631367396813\n",
      "Iteration 5964: loss = 0.002639617491703166\n",
      "Iteration 5965: loss = 0.0026396036173099344\n",
      "Iteration 5966: loss = 0.0026395897442160372\n",
      "Iteration 5967: loss = 0.002639575872420394\n",
      "Iteration 5968: loss = 0.0026395620019219255\n",
      "Iteration 5969: loss = 0.002639548132719554\n",
      "Iteration 5970: loss = 0.002639534264812201\n",
      "Iteration 5971: loss = 0.0026395203981987914\n",
      "Iteration 5972: loss = 0.0026395065328782496\n",
      "Iteration 5973: loss = 0.002639492668849501\n",
      "Iteration 5974: loss = 0.002639478806111473\n",
      "Iteration 5975: loss = 0.002639464944663092\n",
      "Iteration 5976: loss = 0.0026394510845032886\n",
      "Iteration 5977: loss = 0.002639437225630991\n",
      "Iteration 5978: loss = 0.002639423368045131\n",
      "Iteration 5979: loss = 0.0026394095117446394\n",
      "Iteration 5980: loss = 0.00263939565672845\n",
      "Iteration 5981: loss = 0.002639381802995495\n",
      "Iteration 5982: loss = 0.00263936795054471\n",
      "Iteration 5983: loss = 0.002639354099375031\n",
      "Iteration 5984: loss = 0.0026393402494853946\n",
      "Iteration 5985: loss = 0.002639326400874738\n",
      "Iteration 5986: loss = 0.002639312553542\n",
      "Iteration 5987: loss = 0.00263929870748612\n",
      "Iteration 5988: loss = 0.002639284862706039\n",
      "Iteration 5989: loss = 0.0026392710192006986\n",
      "Iteration 5990: loss = 0.0026392571769690413\n",
      "Iteration 5991: loss = 0.002639243336010011\n",
      "Iteration 5992: loss = 0.0026392294963225524\n",
      "Iteration 5993: loss = 0.0026392156579056096\n",
      "Iteration 5994: loss = 0.002639201820758131\n",
      "Iteration 5995: loss = 0.0026391879848790627\n",
      "Iteration 5996: loss = 0.0026391741502673543\n",
      "Iteration 5997: loss = 0.002639160316921955\n",
      "Iteration 5998: loss = 0.002639146484841814\n",
      "Iteration 5999: loss = 0.0026391326540258844\n",
      "Iteration 6000: loss = 0.0026391188244731184\n",
      "Iteration 6001: loss = 0.0026391049961824683\n",
      "Iteration 6002: loss = 0.0026390911691528893\n",
      "Iteration 6003: loss = 0.0026390773433833366\n",
      "Iteration 6004: loss = 0.002639063518872766\n",
      "Iteration 6005: loss = 0.0026390496956201354\n",
      "Iteration 6006: loss = 0.002639035873624403\n",
      "Iteration 6007: loss = 0.002639022052884527\n",
      "Iteration 6008: loss = 0.002639008233399469\n",
      "Iteration 6009: loss = 0.0026389944151681893\n",
      "Iteration 6010: loss = 0.0026389805981896497\n",
      "Iteration 6011: loss = 0.002638966782462815\n",
      "Iteration 6012: loss = 0.002638952967986647\n",
      "Iteration 6013: loss = 0.0026389391547601126\n",
      "Iteration 6014: loss = 0.0026389253427821766\n",
      "Iteration 6015: loss = 0.002638911532051806\n",
      "Iteration 6016: loss = 0.002638897722567968\n",
      "Iteration 6017: loss = 0.002638883914329633\n",
      "Iteration 6018: loss = 0.002638870107335771\n",
      "Iteration 6019: loss = 0.002638856301585351\n",
      "Iteration 6020: loss = 0.0026388424970773456\n",
      "Iteration 6021: loss = 0.002638828693810728\n",
      "Iteration 6022: loss = 0.002638814891784471\n",
      "Iteration 6023: loss = 0.0026388010909975494\n",
      "Iteration 6024: loss = 0.002638787291448939\n",
      "Iteration 6025: loss = 0.0026387734931376163\n",
      "Iteration 6026: loss = 0.002638759696062558\n",
      "Iteration 6027: loss = 0.0026387459002227436\n",
      "Iteration 6028: loss = 0.002638732105617152\n",
      "Iteration 6029: loss = 0.002638718312244763\n",
      "Iteration 6030: loss = 0.0026387045201045583\n",
      "Iteration 6031: loss = 0.0026386907291955208\n",
      "Iteration 6032: loss = 0.0026386769395166324\n",
      "Iteration 6033: loss = 0.002638663151066878\n",
      "Iteration 6034: loss = 0.002638649363845242\n",
      "Iteration 6035: loss = 0.002638635577850711\n",
      "Iteration 6036: loss = 0.0026386217930822716\n",
      "Iteration 6037: loss = 0.0026386080095389115\n",
      "Iteration 6038: loss = 0.00263859422721962\n",
      "Iteration 6039: loss = 0.0026385804461233867\n",
      "Iteration 6040: loss = 0.002638566666249202\n",
      "Iteration 6041: loss = 0.002638552887596057\n",
      "Iteration 6042: loss = 0.002638539110162946\n",
      "Iteration 6043: loss = 0.0026385253339488604\n",
      "Iteration 6044: loss = 0.002638511558952796\n",
      "Iteration 6045: loss = 0.002638497785173748\n",
      "Iteration 6046: loss = 0.002638484012610713\n",
      "Iteration 6047: loss = 0.0026384702412626864\n",
      "Iteration 6048: loss = 0.0026384564711286686\n",
      "Iteration 6049: loss = 0.002638442702207658\n",
      "Iteration 6050: loss = 0.002638428934498654\n",
      "Iteration 6051: loss = 0.002638415168000659\n",
      "Iteration 6052: loss = 0.0026384014027126725\n",
      "Iteration 6053: loss = 0.002638387638633699\n",
      "Iteration 6054: loss = 0.002638373875762743\n",
      "Iteration 6055: loss = 0.002638360114098808\n",
      "Iteration 6056: loss = 0.002638346353640899\n",
      "Iteration 6057: loss = 0.002638332594388024\n",
      "Iteration 6058: loss = 0.0026383188363391886\n",
      "Iteration 6059: loss = 0.0026383050794934037\n",
      "Iteration 6060: loss = 0.0026382913238496767\n",
      "Iteration 6061: loss = 0.0026382775694070177\n",
      "Iteration 6062: loss = 0.002638263816164439\n",
      "Iteration 6063: loss = 0.0026382500641209524\n",
      "Iteration 6064: loss = 0.002638236313275571\n",
      "Iteration 6065: loss = 0.002638222563627307\n",
      "Iteration 6066: loss = 0.002638208815175177\n",
      "Iteration 6067: loss = 0.002638195067918197\n",
      "Iteration 6068: loss = 0.0026381813218553816\n",
      "Iteration 6069: loss = 0.0026381675769857503\n",
      "Iteration 6070: loss = 0.0026381538333083215\n",
      "Iteration 6071: loss = 0.0026381400908221126\n",
      "Iteration 6072: loss = 0.002638126349526146\n",
      "Iteration 6073: loss = 0.0026381126094194426\n",
      "Iteration 6074: loss = 0.0026380988705010234\n",
      "Iteration 6075: loss = 0.0026380851327699128\n",
      "Iteration 6076: loss = 0.002638071396225134\n",
      "Iteration 6077: loss = 0.002638057660865712\n",
      "Iteration 6078: loss = 0.0026380439266906718\n",
      "Iteration 6079: loss = 0.002638030193699041\n",
      "Iteration 6080: loss = 0.002638016461889847\n",
      "Iteration 6081: loss = 0.002638002731262118\n",
      "Iteration 6082: loss = 0.0026379890018148836\n",
      "Iteration 6083: loss = 0.002637975273547174\n",
      "Iteration 6084: loss = 0.0026379615464580206\n",
      "Iteration 6085: loss = 0.002637947820546455\n",
      "Iteration 6086: loss = 0.002637934095811511\n",
      "Iteration 6087: loss = 0.002637920372252221\n",
      "Iteration 6088: loss = 0.0026379066498676213\n",
      "Iteration 6089: loss = 0.0026378929286567463\n",
      "Iteration 6090: loss = 0.002637879208618634\n",
      "Iteration 6091: loss = 0.0026378654897523204\n",
      "Iteration 6092: loss = 0.002637851772056845\n",
      "Iteration 6093: loss = 0.0026378380555312463\n",
      "Iteration 6094: loss = 0.002637824340174565\n",
      "Iteration 6095: loss = 0.0026378106259858417\n",
      "Iteration 6096: loss = 0.0026377969129641183\n",
      "Iteration 6097: loss = 0.0026377832011084384\n",
      "Iteration 6098: loss = 0.0026377694904178447\n",
      "Iteration 6099: loss = 0.0026377557808913827\n",
      "Iteration 6100: loss = 0.002637742072528097\n",
      "Iteration 6101: loss = 0.002637728365327035\n",
      "Iteration 6102: loss = 0.002637714659287242\n",
      "Iteration 6103: loss = 0.0026377009544077683\n",
      "Iteration 6104: loss = 0.002637687250687663\n",
      "Iteration 6105: loss = 0.002637673548125974\n",
      "Iteration 6106: loss = 0.002637659846721754\n",
      "Iteration 6107: loss = 0.002637646146474054\n",
      "Iteration 6108: loss = 0.0026376324473819258\n",
      "Iteration 6109: loss = 0.002637618749444424\n",
      "Iteration 6110: loss = 0.0026376050526606023\n",
      "Iteration 6111: loss = 0.002637591357029517\n",
      "Iteration 6112: loss = 0.0026375776625502225\n",
      "Iteration 6113: loss = 0.0026375639692217765\n",
      "Iteration 6114: loss = 0.0026375502770432375\n",
      "Iteration 6115: loss = 0.002637536586013663\n",
      "Iteration 6116: loss = 0.002637522896132114\n",
      "Iteration 6117: loss = 0.00263750920739765\n",
      "Iteration 6118: loss = 0.0026374955198093324\n",
      "Iteration 6119: loss = 0.0026374818333662234\n",
      "Iteration 6120: loss = 0.0026374681480673865\n",
      "Iteration 6121: loss = 0.0026374544639118855\n",
      "Iteration 6122: loss = 0.002637440780898785\n",
      "Iteration 6123: loss = 0.002637427099027151\n",
      "Iteration 6124: loss = 0.002637413418296049\n",
      "Iteration 6125: loss = 0.002637399738704548\n",
      "Iteration 6126: loss = 0.002637386060251716\n",
      "Iteration 6127: loss = 0.0026373723829366215\n",
      "Iteration 6128: loss = 0.002637358706758335\n",
      "Iteration 6129: loss = 0.002637345031715927\n",
      "Iteration 6130: loss = 0.0026373313578084696\n",
      "Iteration 6131: loss = 0.002637317685035035\n",
      "Iteration 6132: loss = 0.0026373040133946977\n",
      "Iteration 6133: loss = 0.002637290342886531\n",
      "Iteration 6134: loss = 0.00263727667350961\n",
      "Iteration 6135: loss = 0.002637263005263012\n",
      "Iteration 6136: loss = 0.0026372493381458125\n",
      "Iteration 6137: loss = 0.0026372356721570908\n",
      "Iteration 6138: loss = 0.0026372220072959237\n",
      "Iteration 6139: loss = 0.0026372083435613923\n",
      "Iteration 6140: loss = 0.0026371946809525763\n",
      "Iteration 6141: loss = 0.002637181019468557\n",
      "Iteration 6142: loss = 0.0026371673591084168\n",
      "Iteration 6143: loss = 0.0026371536998712375\n",
      "Iteration 6144: loss = 0.0026371400417561043\n",
      "Iteration 6145: loss = 0.0026371263847621007\n",
      "Iteration 6146: loss = 0.002637112728888313\n",
      "Iteration 6147: loss = 0.0026370990741338266\n",
      "Iteration 6148: loss = 0.0026370854204977295\n",
      "Iteration 6149: loss = 0.0026370717679791095\n",
      "Iteration 6150: loss = 0.0026370581165770556\n",
      "Iteration 6151: loss = 0.002637044466290657\n",
      "Iteration 6152: loss = 0.002637030817119005\n",
      "Iteration 6153: loss = 0.00263701716906119\n",
      "Iteration 6154: loss = 0.0026370035221163057\n",
      "Iteration 6155: loss = 0.0026369898762834435\n",
      "Iteration 6156: loss = 0.002636976231561699\n",
      "Iteration 6157: loss = 0.0026369625879501665\n",
      "Iteration 6158: loss = 0.0026369489454479405\n",
      "Iteration 6159: loss = 0.0026369353040541182\n",
      "Iteration 6160: loss = 0.0026369216637677976\n",
      "Iteration 6161: loss = 0.002636908024588076\n",
      "Iteration 6162: loss = 0.0026368943865140524\n",
      "Iteration 6163: loss = 0.0026368807495448275\n",
      "Iteration 6164: loss = 0.002636867113679501\n",
      "Iteration 6165: loss = 0.0026368534789171743\n",
      "Iteration 6166: loss = 0.0026368398452569506\n",
      "Iteration 6167: loss = 0.0026368262126979327\n",
      "Iteration 6168: loss = 0.002636812581239225\n",
      "Iteration 6169: loss = 0.002636798950879931\n",
      "Iteration 6170: loss = 0.002636785321619157\n",
      "Iteration 6171: loss = 0.0026367716934560105\n",
      "Iteration 6172: loss = 0.0026367580663895977\n",
      "Iteration 6173: loss = 0.002636744440419027\n",
      "Iteration 6174: loss = 0.0026367308155434074\n",
      "Iteration 6175: loss = 0.0026367171917618492\n",
      "Iteration 6176: loss = 0.0026367035690734618\n",
      "Iteration 6177: loss = 0.0026366899474773585\n",
      "Iteration 6178: loss = 0.00263667632697265\n",
      "Iteration 6179: loss = 0.0026366627075584508\n",
      "Iteration 6180: loss = 0.002636649089233873\n",
      "Iteration 6181: loss = 0.002636635471998033\n",
      "Iteration 6182: loss = 0.002636621855850046\n",
      "Iteration 6183: loss = 0.002636608240789028\n",
      "Iteration 6184: loss = 0.002636594626814096\n",
      "Iteration 6185: loss = 0.0026365810139243693\n",
      "Iteration 6186: loss = 0.0026365674021189655\n",
      "Iteration 6187: loss = 0.002636553791397005\n",
      "Iteration 6188: loss = 0.002636540181757608\n",
      "Iteration 6189: loss = 0.002636526573199896\n",
      "Iteration 6190: loss = 0.002636512965722991\n",
      "Iteration 6191: loss = 0.002636499359326016\n",
      "Iteration 6192: loss = 0.0026364857540080947\n",
      "Iteration 6193: loss = 0.0026364721497683526\n",
      "Iteration 6194: loss = 0.002636458546605914\n",
      "Iteration 6195: loss = 0.0026364449445199054\n",
      "Iteration 6196: loss = 0.002636431343509454\n",
      "Iteration 6197: loss = 0.0026364177435736864\n",
      "Iteration 6198: loss = 0.002636404144711733\n",
      "Iteration 6199: loss = 0.0026363905469227234\n",
      "Iteration 6200: loss = 0.0026363769502057853\n",
      "Iteration 6201: loss = 0.0026363633545600533\n",
      "Iteration 6202: loss = 0.0026363497599846573\n",
      "Iteration 6203: loss = 0.00263633616647873\n",
      "Iteration 6204: loss = 0.0026363225740414047\n",
      "Iteration 6205: loss = 0.0026363089826718167\n",
      "Iteration 6206: loss = 0.0026362953923691006\n",
      "Iteration 6207: loss = 0.002636281803132393\n",
      "Iteration 6208: loss = 0.002636268214960829\n",
      "Iteration 6209: loss = 0.002636254627853547\n",
      "Iteration 6210: loss = 0.002636241041809686\n",
      "Iteration 6211: loss = 0.0026362274568283834\n",
      "Iteration 6212: loss = 0.002636213872908781\n",
      "Iteration 6213: loss = 0.002636200290050019\n",
      "Iteration 6214: loss = 0.002636186708251238\n",
      "Iteration 6215: loss = 0.0026361731275115815\n",
      "Iteration 6216: loss = 0.0026361595478301917\n",
      "Iteration 6217: loss = 0.002636145969206213\n",
      "Iteration 6218: loss = 0.00263613239163879\n",
      "Iteration 6219: loss = 0.002636118815127069\n",
      "Iteration 6220: loss = 0.002636105239670195\n",
      "Iteration 6221: loss = 0.0026360916652673153\n",
      "Iteration 6222: loss = 0.002636078091917578\n",
      "Iteration 6223: loss = 0.0026360645196201325\n",
      "Iteration 6224: loss = 0.0026360509483741275\n",
      "Iteration 6225: loss = 0.0026360373781787127\n",
      "Iteration 6226: loss = 0.002636023809033041\n",
      "Iteration 6227: loss = 0.002636010240936262\n",
      "Iteration 6228: loss = 0.0026359966738875302\n",
      "Iteration 6229: loss = 0.002635983107885998\n",
      "Iteration 6230: loss = 0.0026359695429308203\n",
      "Iteration 6231: loss = 0.0026359559790211503\n",
      "Iteration 6232: loss = 0.002635942416156146\n",
      "Iteration 6233: loss = 0.002635928854334963\n",
      "Iteration 6234: loss = 0.0026359152935567587\n",
      "Iteration 6235: loss = 0.0026359017338206912\n",
      "Iteration 6236: loss = 0.0026358881751259193\n",
      "Iteration 6237: loss = 0.0026358746174716032\n",
      "Iteration 6238: loss = 0.002635861060856903\n",
      "Iteration 6239: loss = 0.0026358475052809796\n",
      "Iteration 6240: loss = 0.002635833950742995\n",
      "Iteration 6241: loss = 0.0026358203972421126\n",
      "Iteration 6242: loss = 0.002635806844777496\n",
      "Iteration 6243: loss = 0.002635793293348309\n",
      "Iteration 6244: loss = 0.0026357797429537176\n",
      "Iteration 6245: loss = 0.002635766193592887\n",
      "Iteration 6246: loss = 0.0026357526452649837\n",
      "Iteration 6247: loss = 0.0026357390979691755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6248: loss = 0.0026357255517046306\n",
      "Iteration 6249: loss = 0.0026357120064705183\n",
      "Iteration 6250: loss = 0.002635698462266008\n",
      "Iteration 6251: loss = 0.0026356849190902706\n",
      "Iteration 6252: loss = 0.0026356713769424777\n",
      "Iteration 6253: loss = 0.0026356578358218\n",
      "Iteration 6254: loss = 0.0026356442957274117\n",
      "Iteration 6255: loss = 0.002635630756658486\n",
      "Iteration 6256: loss = 0.0026356172186141977\n",
      "Iteration 6257: loss = 0.002635603681593721\n",
      "Iteration 6258: loss = 0.0026355901455962323\n",
      "Iteration 6259: loss = 0.0026355766106209095\n",
      "Iteration 6260: loss = 0.0026355630766669276\n",
      "Iteration 6261: loss = 0.002635549543733467\n",
      "Iteration 6262: loss = 0.0026355360118197053\n",
      "Iteration 6263: loss = 0.002635522480924824\n",
      "Iteration 6264: loss = 0.002635508951048001\n",
      "Iteration 6265: loss = 0.002635495422188421\n",
      "Iteration 6266: loss = 0.002635481894345263\n",
      "Iteration 6267: loss = 0.002635468367517711\n",
      "Iteration 6268: loss = 0.002635454841704948\n",
      "Iteration 6269: loss = 0.00263544131690616\n",
      "Iteration 6270: loss = 0.00263542779312053\n",
      "Iteration 6271: loss = 0.002635414270347244\n",
      "Iteration 6272: loss = 0.0026354007485854913\n",
      "Iteration 6273: loss = 0.002635387227834456\n",
      "Iteration 6274: loss = 0.0026353737080933273\n",
      "Iteration 6275: loss = 0.002635360189361295\n",
      "Iteration 6276: loss = 0.002635346671637547\n",
      "Iteration 6277: loss = 0.002635333154921275\n",
      "Iteration 6278: loss = 0.00263531963921167\n",
      "Iteration 6279: loss = 0.0026353061245079235\n",
      "Iteration 6280: loss = 0.0026352926108092283\n",
      "Iteration 6281: loss = 0.002635279098114777\n",
      "Iteration 6282: loss = 0.0026352655864237654\n",
      "Iteration 6283: loss = 0.0026352520757353865\n",
      "Iteration 6284: loss = 0.002635238566048837\n",
      "Iteration 6285: loss = 0.002635225057363313\n",
      "Iteration 6286: loss = 0.002635211549678012\n",
      "Iteration 6287: loss = 0.002635198042992131\n",
      "Iteration 6288: loss = 0.002635184537304869\n",
      "Iteration 6289: loss = 0.002635171032615426\n",
      "Iteration 6290: loss = 0.0026351575289230013\n",
      "Iteration 6291: loss = 0.0026351440262267958\n",
      "Iteration 6292: loss = 0.002635130524526012\n",
      "Iteration 6293: loss = 0.00263511702381985\n",
      "Iteration 6294: loss = 0.002635103524107516\n",
      "Iteration 6295: loss = 0.0026350900253882107\n",
      "Iteration 6296: loss = 0.0026350765276611406\n",
      "Iteration 6297: loss = 0.0026350630309255104\n",
      "Iteration 6298: loss = 0.002635049535180526\n",
      "Iteration 6299: loss = 0.0026350360404253947\n",
      "Iteration 6300: loss = 0.002635022546659323\n",
      "Iteration 6301: loss = 0.0026350090538815206\n",
      "Iteration 6302: loss = 0.002634995562091195\n",
      "Iteration 6303: loss = 0.002634982071287556\n",
      "Iteration 6304: loss = 0.0026349685814698156\n",
      "Iteration 6305: loss = 0.002634955092637183\n",
      "Iteration 6306: loss = 0.002634941604788871\n",
      "Iteration 6307: loss = 0.0026349281179240927\n",
      "Iteration 6308: loss = 0.0026349146320420605\n",
      "Iteration 6309: loss = 0.0026349011471419894\n",
      "Iteration 6310: loss = 0.002634887663223094\n",
      "Iteration 6311: loss = 0.002634874180284589\n",
      "Iteration 6312: loss = 0.0026348606983256917\n",
      "Iteration 6313: loss = 0.002634847217345619\n",
      "Iteration 6314: loss = 0.0026348337373435877\n",
      "Iteration 6315: loss = 0.002634820258318817\n",
      "Iteration 6316: loss = 0.002634806780270526\n",
      "Iteration 6317: loss = 0.0026347933031979354\n",
      "Iteration 6318: loss = 0.002634779827100264\n",
      "Iteration 6319: loss = 0.002634766351976735\n",
      "Iteration 6320: loss = 0.002634752877826569\n",
      "Iteration 6321: loss = 0.00263473940464899\n",
      "Iteration 6322: loss = 0.0026347259324432212\n",
      "Iteration 6323: loss = 0.0026347124612084863\n",
      "Iteration 6324: loss = 0.002634698990944011\n",
      "Iteration 6325: loss = 0.0026346855216490207\n",
      "Iteration 6326: loss = 0.002634672053322742\n",
      "Iteration 6327: loss = 0.0026346585859644014\n",
      "Iteration 6328: loss = 0.002634645119573227\n",
      "Iteration 6329: loss = 0.0026346316541484478\n",
      "Iteration 6330: loss = 0.002634618189689293\n",
      "Iteration 6331: loss = 0.0026346047261949926\n",
      "Iteration 6332: loss = 0.0026345912636647766\n",
      "Iteration 6333: loss = 0.002634577802097877\n",
      "Iteration 6334: loss = 0.0026345643414935264\n",
      "Iteration 6335: loss = 0.002634550881850957\n",
      "Iteration 6336: loss = 0.0026345374231694025\n",
      "Iteration 6337: loss = 0.002634523965448097\n",
      "Iteration 6338: loss = 0.002634510508686276\n",
      "Iteration 6339: loss = 0.0026344970528831744\n",
      "Iteration 6340: loss = 0.00263448359803803\n",
      "Iteration 6341: loss = 0.0026344701441500785\n",
      "Iteration 6342: loss = 0.0026344566912185584\n",
      "Iteration 6343: loss = 0.0026344432392427083\n",
      "Iteration 6344: loss = 0.0026344297882217667\n",
      "Iteration 6345: loss = 0.002634416338154975\n",
      "Iteration 6346: loss = 0.0026344028890415722\n",
      "Iteration 6347: loss = 0.002634389440880801\n",
      "Iteration 6348: loss = 0.002634375993671903\n",
      "Iteration 6349: loss = 0.0026343625474141204\n",
      "Iteration 6350: loss = 0.002634349102106698\n",
      "Iteration 6351: loss = 0.002634335657748878\n",
      "Iteration 6352: loss = 0.002634322214339907\n",
      "Iteration 6353: loss = 0.00263430877187903\n",
      "Iteration 6354: loss = 0.0026342953303654937\n",
      "Iteration 6355: loss = 0.0026342818897985443\n",
      "Iteration 6356: loss = 0.00263426845017743\n",
      "Iteration 6357: loss = 0.002634255011501399\n",
      "Iteration 6358: loss = 0.0026342415737697\n",
      "Iteration 6359: loss = 0.0026342281369815834\n",
      "Iteration 6360: loss = 0.0026342147011363002\n",
      "Iteration 6361: loss = 0.0026342012662331003\n",
      "Iteration 6362: loss = 0.0026341878322712364\n",
      "Iteration 6363: loss = 0.0026341743992499608\n",
      "Iteration 6364: loss = 0.0026341609671685262\n",
      "Iteration 6365: loss = 0.0026341475360261873\n",
      "Iteration 6366: loss = 0.002634134105822199\n",
      "Iteration 6367: loss = 0.002634120676555816\n",
      "Iteration 6368: loss = 0.0026341072482262942\n",
      "Iteration 6369: loss = 0.0026340938208328905\n",
      "Iteration 6370: loss = 0.002634080394374863\n",
      "Iteration 6371: loss = 0.0026340669688514687\n",
      "Iteration 6372: loss = 0.002634053544261967\n",
      "Iteration 6373: loss = 0.0026340401206056173\n",
      "Iteration 6374: loss = 0.00263402669788168\n",
      "Iteration 6375: loss = 0.002634013276089415\n",
      "Iteration 6376: loss = 0.0026339998552280855\n",
      "Iteration 6377: loss = 0.0026339864352969526\n",
      "Iteration 6378: loss = 0.0026339730162952793\n",
      "Iteration 6379: loss = 0.002633959598222329\n",
      "Iteration 6380: loss = 0.002633946181077366\n",
      "Iteration 6381: loss = 0.0026339327648596555\n",
      "Iteration 6382: loss = 0.0026339193495684638\n",
      "Iteration 6383: loss = 0.0026339059352030562\n",
      "Iteration 6384: loss = 0.002633892521762699\n",
      "Iteration 6385: loss = 0.0026338791092466625\n",
      "Iteration 6386: loss = 0.0026338656976542135\n",
      "Iteration 6387: loss = 0.00263385228698462\n",
      "Iteration 6388: loss = 0.0026338388772371535\n",
      "Iteration 6389: loss = 0.0026338254684110827\n",
      "Iteration 6390: loss = 0.0026338120605056813\n",
      "Iteration 6391: loss = 0.0026337986535202185\n",
      "Iteration 6392: loss = 0.0026337852474539675\n",
      "Iteration 6393: loss = 0.0026337718423062022\n",
      "Iteration 6394: loss = 0.002633758438076195\n",
      "Iteration 6395: loss = 0.0026337450347632212\n",
      "Iteration 6396: loss = 0.002633731632366556\n",
      "Iteration 6397: loss = 0.0026337182308854754\n",
      "Iteration 6398: loss = 0.0026337048303192554\n",
      "Iteration 6399: loss = 0.002633691430667173\n",
      "Iteration 6400: loss = 0.0026336780319285063\n",
      "Iteration 6401: loss = 0.002633664634102534\n",
      "Iteration 6402: loss = 0.0026336512371885347\n",
      "Iteration 6403: loss = 0.002633637841185789\n",
      "Iteration 6404: loss = 0.0026336244460935768\n",
      "Iteration 6405: loss = 0.0026336110519111795\n",
      "Iteration 6406: loss = 0.0026335976586378787\n",
      "Iteration 6407: loss = 0.0026335842662729573\n",
      "Iteration 6408: loss = 0.002633570874815698\n",
      "Iteration 6409: loss = 0.0026335574842653855\n",
      "Iteration 6410: loss = 0.0026335440946213032\n",
      "Iteration 6411: loss = 0.0026335307058827366\n",
      "Iteration 6412: loss = 0.002633517318048972\n",
      "Iteration 6413: loss = 0.002633503931119296\n",
      "Iteration 6414: loss = 0.0026334905450929942\n",
      "Iteration 6415: loss = 0.0026334771599693567\n",
      "Iteration 6416: loss = 0.00263346377574767\n",
      "Iteration 6417: loss = 0.0026334503924272247\n",
      "Iteration 6418: loss = 0.0026334370100073094\n",
      "Iteration 6419: loss = 0.0026334236284872156\n",
      "Iteration 6420: loss = 0.0026334102478662337\n",
      "Iteration 6421: loss = 0.0026333968681436556\n",
      "Iteration 6422: loss = 0.002633383489318774\n",
      "Iteration 6423: loss = 0.0026333701113908817\n",
      "Iteration 6424: loss = 0.0026333567343592725\n",
      "Iteration 6425: loss = 0.0026333433582232417\n",
      "Iteration 6426: loss = 0.0026333299829820827\n",
      "Iteration 6427: loss = 0.002633316608635092\n",
      "Iteration 6428: loss = 0.0026333032351815665\n",
      "Iteration 6429: loss = 0.0026332898626208025\n",
      "Iteration 6430: loss = 0.002633276490952098\n",
      "Iteration 6431: loss = 0.0026332631201747506\n",
      "Iteration 6432: loss = 0.002633249750288061\n",
      "Iteration 6433: loss = 0.002633236381291327\n",
      "Iteration 6434: loss = 0.0026332230131838495\n",
      "Iteration 6435: loss = 0.0026332096459649305\n",
      "Iteration 6436: loss = 0.0026331962796338694\n",
      "Iteration 6437: loss = 0.0026331829141899707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6438: loss = 0.0026331695496325355\n",
      "Iteration 6439: loss = 0.0026331561859608676\n",
      "Iteration 6440: loss = 0.0026331428231742724\n",
      "Iteration 6441: loss = 0.0026331294612720537\n",
      "Iteration 6442: loss = 0.0026331161002535173\n",
      "Iteration 6443: loss = 0.002633102740117969\n",
      "Iteration 6444: loss = 0.0026330893808647155\n",
      "Iteration 6445: loss = 0.002633076022493065\n",
      "Iteration 6446: loss = 0.0026330626650023243\n",
      "Iteration 6447: loss = 0.002633049308391803\n",
      "Iteration 6448: loss = 0.0026330359526608098\n",
      "Iteration 6449: loss = 0.002633022597808655\n",
      "Iteration 6450: loss = 0.00263300924383465\n",
      "Iteration 6451: loss = 0.0026329958907381043\n",
      "Iteration 6452: loss = 0.002632982538518331\n",
      "Iteration 6453: loss = 0.0026329691871746423\n",
      "Iteration 6454: loss = 0.002632955836706352\n",
      "Iteration 6455: loss = 0.0026329424871127717\n",
      "Iteration 6456: loss = 0.0026329291383932188\n",
      "Iteration 6457: loss = 0.0026329157905470068\n",
      "Iteration 6458: loss = 0.0026329024435734504\n",
      "Iteration 6459: loss = 0.002632889097471868\n",
      "Iteration 6460: loss = 0.0026328757522415757\n",
      "Iteration 6461: loss = 0.0026328624078818916\n",
      "Iteration 6462: loss = 0.0026328490643921323\n",
      "Iteration 6463: loss = 0.002632835721771618\n",
      "Iteration 6464: loss = 0.0026328223800196685\n",
      "Iteration 6465: loss = 0.0026328090391356035\n",
      "Iteration 6466: loss = 0.0026327956991187436\n",
      "Iteration 6467: loss = 0.00263278235996841\n",
      "Iteration 6468: loss = 0.0026327690216839256\n",
      "Iteration 6469: loss = 0.002632755684264612\n",
      "Iteration 6470: loss = 0.0026327423477097932\n",
      "Iteration 6471: loss = 0.0026327290120187925\n",
      "Iteration 6472: loss = 0.0026327156771909347\n",
      "Iteration 6473: loss = 0.0026327023432255463\n",
      "Iteration 6474: loss = 0.0026326890101219507\n",
      "Iteration 6475: loss = 0.0026326756778794758\n",
      "Iteration 6476: loss = 0.0026326623464974484\n",
      "Iteration 6477: loss = 0.002632649015975197\n",
      "Iteration 6478: loss = 0.0026326356863120483\n",
      "Iteration 6479: loss = 0.0026326223575073317\n",
      "Iteration 6480: loss = 0.0026326090295603777\n",
      "Iteration 6481: loss = 0.002632595702470515\n",
      "Iteration 6482: loss = 0.0026325823762370754\n",
      "Iteration 6483: loss = 0.0026325690508593905\n",
      "Iteration 6484: loss = 0.0026325557263367915\n",
      "Iteration 6485: loss = 0.002632542402668612\n",
      "Iteration 6486: loss = 0.0026325290798541842\n",
      "Iteration 6487: loss = 0.002632515757892843\n",
      "Iteration 6488: loss = 0.0026325024367839216\n",
      "Iteration 6489: loss = 0.0026324891165267566\n",
      "Iteration 6490: loss = 0.0026324757971206833\n",
      "Iteration 6491: loss = 0.0026324624785650373\n",
      "Iteration 6492: loss = 0.0026324491608591565\n",
      "Iteration 6493: loss = 0.0026324358440023778\n",
      "Iteration 6494: loss = 0.00263242252799404\n",
      "Iteration 6495: loss = 0.002632409212833481\n",
      "Iteration 6496: loss = 0.0026323958985200414\n",
      "Iteration 6497: loss = 0.0026323825850530606\n",
      "Iteration 6498: loss = 0.0026323692724318794\n",
      "Iteration 6499: loss = 0.002632355960655839\n",
      "Iteration 6500: loss = 0.002632342649724281\n",
      "Iteration 6501: loss = 0.002632329339636549\n",
      "Iteration 6502: loss = 0.0026323160303919845\n",
      "Iteration 6503: loss = 0.0026323027219899327\n",
      "Iteration 6504: loss = 0.002632289414429737\n",
      "Iteration 6505: loss = 0.0026322761077107425\n",
      "Iteration 6506: loss = 0.0026322628018322952\n",
      "Iteration 6507: loss = 0.0026322494967937404\n",
      "Iteration 6508: loss = 0.0026322361925944253\n",
      "Iteration 6509: loss = 0.0026322228892336976\n",
      "Iteration 6510: loss = 0.002632209586710904\n",
      "Iteration 6511: loss = 0.0026321962850253955\n",
      "Iteration 6512: loss = 0.0026321829841765183\n",
      "Iteration 6513: loss = 0.0026321696841636252\n",
      "Iteration 6514: loss = 0.002632156384986064\n",
      "Iteration 6515: loss = 0.0026321430866431865\n",
      "Iteration 6516: loss = 0.002632129789134345\n",
      "Iteration 6517: loss = 0.0026321164924588913\n",
      "Iteration 6518: loss = 0.0026321031966161784\n",
      "Iteration 6519: loss = 0.0026320899016055582\n",
      "Iteration 6520: loss = 0.0026320766074263867\n",
      "Iteration 6521: loss = 0.0026320633140780184\n",
      "Iteration 6522: loss = 0.002632050021559807\n",
      "Iteration 6523: loss = 0.002632036729871109\n",
      "Iteration 6524: loss = 0.0026320234390112807\n",
      "Iteration 6525: loss = 0.0026320101489796795\n",
      "Iteration 6526: loss = 0.0026319968597756623\n",
      "Iteration 6527: loss = 0.0026319835713985882\n",
      "Iteration 6528: loss = 0.002631970283847815\n",
      "Iteration 6529: loss = 0.002631956997122703\n",
      "Iteration 6530: loss = 0.0026319437112226115\n",
      "Iteration 6531: loss = 0.002631930426146901\n",
      "Iteration 6532: loss = 0.002631917141894933\n",
      "Iteration 6533: loss = 0.002631903858466069\n",
      "Iteration 6534: loss = 0.0026318905758596716\n",
      "Iteration 6535: loss = 0.002631877294075104\n",
      "Iteration 6536: loss = 0.0026318640131117285\n",
      "Iteration 6537: loss = 0.0026318507329689106\n",
      "Iteration 6538: loss = 0.0026318374536460146\n",
      "Iteration 6539: loss = 0.002631824175142405\n",
      "Iteration 6540: loss = 0.0026318108974574487\n",
      "Iteration 6541: loss = 0.0026317976205905112\n",
      "Iteration 6542: loss = 0.0026317843445409607\n",
      "Iteration 6543: loss = 0.002631771069308164\n",
      "Iteration 6544: loss = 0.0026317577948914896\n",
      "Iteration 6545: loss = 0.0026317445212903066\n",
      "Iteration 6546: loss = 0.002631731248503983\n",
      "Iteration 6547: loss = 0.002631717976531891\n",
      "Iteration 6548: loss = 0.0026317047053733998\n",
      "Iteration 6549: loss = 0.0026316914350278803\n",
      "Iteration 6550: loss = 0.0026316781654947053\n",
      "Iteration 6551: loss = 0.002631664896773246\n",
      "Iteration 6552: loss = 0.0026316516288628763\n",
      "Iteration 6553: loss = 0.002631638361762969\n",
      "Iteration 6554: loss = 0.0026316250954728985\n",
      "Iteration 6555: loss = 0.002631611829992039\n",
      "Iteration 6556: loss = 0.002631598565319767\n",
      "Iteration 6557: loss = 0.0026315853014554566\n",
      "Iteration 6558: loss = 0.002631572038398485\n",
      "Iteration 6559: loss = 0.0026315587761482294\n",
      "Iteration 6560: loss = 0.002631545514704067\n",
      "Iteration 6561: loss = 0.002631532254065376\n",
      "Iteration 6562: loss = 0.002631518994231535\n",
      "Iteration 6563: loss = 0.0026315057352019238\n",
      "Iteration 6564: loss = 0.0026314924769759215\n",
      "Iteration 6565: loss = 0.0026314792195529083\n",
      "Iteration 6566: loss = 0.0026314659629322664\n",
      "Iteration 6567: loss = 0.0026314527071133764\n",
      "Iteration 6568: loss = 0.002631439452095621\n",
      "Iteration 6569: loss = 0.0026314261978783826\n",
      "Iteration 6570: loss = 0.002631412944461045\n",
      "Iteration 6571: loss = 0.002631399691842991\n",
      "Iteration 6572: loss = 0.0026313864400236055\n",
      "Iteration 6573: loss = 0.0026313731890022746\n",
      "Iteration 6574: loss = 0.0026313599387783825\n",
      "Iteration 6575: loss = 0.002631346689351316\n",
      "Iteration 6576: loss = 0.0026313334407204615\n",
      "Iteration 6577: loss = 0.0026313201928852064\n",
      "Iteration 6578: loss = 0.002631306945844939\n",
      "Iteration 6579: loss = 0.002631293699599047\n",
      "Iteration 6580: loss = 0.00263128045414692\n",
      "Iteration 6581: loss = 0.0026312672094879466\n",
      "Iteration 6582: loss = 0.0026312539656215186\n",
      "Iteration 6583: loss = 0.002631240722547025\n",
      "Iteration 6584: loss = 0.002631227480263858\n",
      "Iteration 6585: loss = 0.002631214238771409\n",
      "Iteration 6586: loss = 0.0026312009980690707\n",
      "Iteration 6587: loss = 0.0026311877581562354\n",
      "Iteration 6588: loss = 0.002631174519032298\n",
      "Iteration 6589: loss = 0.0026311612806966516\n",
      "Iteration 6590: loss = 0.0026311480431486906\n",
      "Iteration 6591: loss = 0.0026311348063878105\n",
      "Iteration 6592: loss = 0.002631121570413407\n",
      "Iteration 6593: loss = 0.002631108335224877\n",
      "Iteration 6594: loss = 0.0026310951008216166\n",
      "Iteration 6595: loss = 0.0026310818672030238\n",
      "Iteration 6596: loss = 0.0026310686343684964\n",
      "Iteration 6597: loss = 0.002631055402317433\n",
      "Iteration 6598: loss = 0.0026310421710492317\n",
      "Iteration 6599: loss = 0.002631028940563294\n",
      "Iteration 6600: loss = 0.002631015710859019\n",
      "Iteration 6601: loss = 0.002631002481935808\n",
      "Iteration 6602: loss = 0.002630989253793062\n",
      "Iteration 6603: loss = 0.0026309760264301824\n",
      "Iteration 6604: loss = 0.002630962799846573\n",
      "Iteration 6605: loss = 0.0026309495740416355\n",
      "Iteration 6606: loss = 0.0026309363490147745\n",
      "Iteration 6607: loss = 0.002630923124765393\n",
      "Iteration 6608: loss = 0.0026309099012928965\n",
      "Iteration 6609: loss = 0.0026308966785966903\n",
      "Iteration 6610: loss = 0.0026308834566761796\n",
      "Iteration 6611: loss = 0.002630870235530771\n",
      "Iteration 6612: loss = 0.0026308570151598715\n",
      "Iteration 6613: loss = 0.002630843795562888\n",
      "Iteration 6614: loss = 0.00263083057673923\n",
      "Iteration 6615: loss = 0.0026308173586883036\n",
      "Iteration 6616: loss = 0.0026308041414095204\n",
      "Iteration 6617: loss = 0.0026307909249022876\n",
      "Iteration 6618: loss = 0.0026307777091660175\n",
      "Iteration 6619: loss = 0.002630764494200119\n",
      "Iteration 6620: loss = 0.002630751280004005\n",
      "Iteration 6621: loss = 0.0026307380665770864\n",
      "Iteration 6622: loss = 0.0026307248539187756\n",
      "Iteration 6623: loss = 0.0026307116420284855\n",
      "Iteration 6624: loss = 0.0026306984309056296\n",
      "Iteration 6625: loss = 0.002630685220549622\n",
      "Iteration 6626: loss = 0.0026306720109598777\n",
      "Iteration 6627: loss = 0.0026306588021358106\n",
      "Iteration 6628: loss = 0.002630645594076837\n",
      "Iteration 6629: loss = 0.0026306323867823734\n",
      "Iteration 6630: loss = 0.002630619180251836\n",
      "Iteration 6631: loss = 0.0026306059744846417\n",
      "Iteration 6632: loss = 0.002630592769480209\n",
      "Iteration 6633: loss = 0.0026305795652379563\n",
      "Iteration 6634: loss = 0.002630566361757302\n",
      "Iteration 6635: loss = 0.0026305531590376653\n",
      "Iteration 6636: loss = 0.002630539957078467\n",
      "Iteration 6637: loss = 0.002630526755879127\n",
      "Iteration 6638: loss = 0.0026305135554390664\n",
      "Iteration 6639: loss = 0.002630500355757706\n",
      "Iteration 6640: loss = 0.002630487156834469\n",
      "Iteration 6641: loss = 0.002630473958668778\n",
      "Iteration 6642: loss = 0.0026304607612600553\n",
      "Iteration 6643: loss = 0.0026304475646077253\n",
      "Iteration 6644: loss = 0.0026304343687112122\n",
      "Iteration 6645: loss = 0.00263042117356994\n",
      "Iteration 6646: loss = 0.0026304079791833355\n",
      "Iteration 6647: loss = 0.002630394785550823\n",
      "Iteration 6648: loss = 0.00263038159267183\n",
      "Iteration 6649: loss = 0.0026303684005457824\n",
      "Iteration 6650: loss = 0.002630355209172108\n",
      "Iteration 6651: loss = 0.002630342018550235\n",
      "Iteration 6652: loss = 0.002630328828679592\n",
      "Iteration 6653: loss = 0.0026303156395596074\n",
      "Iteration 6654: loss = 0.002630302451189711\n",
      "Iteration 6655: loss = 0.0026302892635693335\n",
      "Iteration 6656: loss = 0.002630276076697905\n",
      "Iteration 6657: loss = 0.002630262890574856\n",
      "Iteration 6658: loss = 0.0026302497051996195\n",
      "Iteration 6659: loss = 0.0026302365205716264\n",
      "Iteration 6660: loss = 0.0026302233366903107\n",
      "Iteration 6661: loss = 0.002630210153555104\n",
      "Iteration 6662: loss = 0.0026301969711654415\n",
      "Iteration 6663: loss = 0.0026301837895207573\n",
      "Iteration 6664: loss = 0.002630170608620486\n",
      "Iteration 6665: loss = 0.002630157428464062\n",
      "Iteration 6666: loss = 0.0026301442490509223\n",
      "Iteration 6667: loss = 0.0026301310703805035\n",
      "Iteration 6668: loss = 0.002630117892452242\n",
      "Iteration 6669: loss = 0.0026301047152655745\n",
      "Iteration 6670: loss = 0.0026300915388199407\n",
      "Iteration 6671: loss = 0.002630078363114778\n",
      "Iteration 6672: loss = 0.0026300651881495246\n",
      "Iteration 6673: loss = 0.0026300520139236217\n",
      "Iteration 6674: loss = 0.002630038840436508\n",
      "Iteration 6675: loss = 0.002630025667687625\n",
      "Iteration 6676: loss = 0.0026300124956764133\n",
      "Iteration 6677: loss = 0.002629999324402315\n",
      "Iteration 6678: loss = 0.0026299861538647707\n",
      "Iteration 6679: loss = 0.002629972984063225\n",
      "Iteration 6680: loss = 0.00262995981499712\n",
      "Iteration 6681: loss = 0.002629946646665899\n",
      "Iteration 6682: loss = 0.0026299334790690072\n",
      "Iteration 6683: loss = 0.0026299203122058883\n",
      "Iteration 6684: loss = 0.0026299071460759884\n",
      "Iteration 6685: loss = 0.0026298939806787525\n",
      "Iteration 6686: loss = 0.002629880816013628\n",
      "Iteration 6687: loss = 0.00262986765208006\n",
      "Iteration 6688: loss = 0.002629854488877497\n",
      "Iteration 6689: loss = 0.0026298413264053857\n",
      "Iteration 6690: loss = 0.002629828164663176\n",
      "Iteration 6691: loss = 0.002629815003650315\n",
      "Iteration 6692: loss = 0.002629801843366253\n",
      "Iteration 6693: loss = 0.0026297886838104394\n",
      "Iteration 6694: loss = 0.0026297755249823245\n",
      "Iteration 6695: loss = 0.00262976236688136\n",
      "Iteration 6696: loss = 0.002629749209506996\n",
      "Iteration 6697: loss = 0.002629736052858685\n",
      "Iteration 6698: loss = 0.0026297228969358794\n",
      "Iteration 6699: loss = 0.0026297097417380326\n",
      "Iteration 6700: loss = 0.0026296965872645963\n",
      "Iteration 6701: loss = 0.0026296834335150262\n",
      "Iteration 6702: loss = 0.002629670280488776\n",
      "Iteration 6703: loss = 0.0026296571281853\n",
      "Iteration 6704: loss = 0.0026296439766040547\n",
      "Iteration 6705: loss = 0.0026296308257444953\n",
      "Iteration 6706: loss = 0.002629617675606078\n",
      "Iteration 6707: loss = 0.002629604526188261\n",
      "Iteration 6708: loss = 0.0026295913774905\n",
      "Iteration 6709: loss = 0.0026295782295122543\n",
      "Iteration 6710: loss = 0.0026295650822529817\n",
      "Iteration 6711: loss = 0.002629551935712141\n",
      "Iteration 6712: loss = 0.0026295387898891917\n",
      "Iteration 6713: loss = 0.0026295256447835948\n",
      "Iteration 6714: loss = 0.002629512500394809\n",
      "Iteration 6715: loss = 0.0026294993567222962\n",
      "Iteration 6716: loss = 0.0026294862137655178\n",
      "Iteration 6717: loss = 0.0026294730715239354\n",
      "Iteration 6718: loss = 0.0026294599299970123\n",
      "Iteration 6719: loss = 0.0026294467891842103\n",
      "Iteration 6720: loss = 0.002629433649084994\n",
      "Iteration 6721: loss = 0.002629420509698826\n",
      "Iteration 6722: loss = 0.002629407371025171\n",
      "Iteration 6723: loss = 0.002629394233063495\n",
      "Iteration 6724: loss = 0.0026293810958132632\n",
      "Iteration 6725: loss = 0.0026293679592739403\n",
      "Iteration 6726: loss = 0.0026293548234449936\n",
      "Iteration 6727: loss = 0.0026293416883258897\n",
      "Iteration 6728: loss = 0.002629328553916097\n",
      "Iteration 6729: loss = 0.0026293154202150816\n",
      "Iteration 6730: loss = 0.0026293022872223137\n",
      "Iteration 6731: loss = 0.0026292891549372604\n",
      "Iteration 6732: loss = 0.0026292760233593924\n",
      "Iteration 6733: loss = 0.0026292628924881795\n",
      "Iteration 6734: loss = 0.002629249762323091\n",
      "Iteration 6735: loss = 0.002629236632863599\n",
      "Iteration 6736: loss = 0.002629223504109174\n",
      "Iteration 6737: loss = 0.002629210376059288\n",
      "Iteration 6738: loss = 0.002629197248713414\n",
      "Iteration 6739: loss = 0.0026291841220710236\n",
      "Iteration 6740: loss = 0.002629170996131591\n",
      "Iteration 6741: loss = 0.00262915787089459\n",
      "Iteration 6742: loss = 0.002629144746359494\n",
      "Iteration 6743: loss = 0.0026291316225257786\n",
      "Iteration 6744: loss = 0.0026291184993929187\n",
      "Iteration 6745: loss = 0.0026291053769603896\n",
      "Iteration 6746: loss = 0.0026290922552276684\n",
      "Iteration 6747: loss = 0.0026290791341942324\n",
      "Iteration 6748: loss = 0.0026290660138595565\n",
      "Iteration 6749: loss = 0.0026290528942231207\n",
      "Iteration 6750: loss = 0.002629039775284401\n",
      "Iteration 6751: loss = 0.0026290266570428785\n",
      "Iteration 6752: loss = 0.00262901353949803\n",
      "Iteration 6753: loss = 0.0026290004226493364\n",
      "Iteration 6754: loss = 0.0026289873064962773\n",
      "Iteration 6755: loss = 0.0026289741910383337\n",
      "Iteration 6756: loss = 0.0026289610762749863\n",
      "Iteration 6757: loss = 0.0026289479622057165\n",
      "Iteration 6758: loss = 0.002628934848830007\n",
      "Iteration 6759: loss = 0.00262892173614734\n",
      "Iteration 6760: loss = 0.0026289086241571974\n",
      "Iteration 6761: loss = 0.0026288955128590643\n",
      "Iteration 6762: loss = 0.0026288824022524244\n",
      "Iteration 6763: loss = 0.0026288692923367605\n",
      "Iteration 6764: loss = 0.002628856183111559\n",
      "Iteration 6765: loss = 0.0026288430745763053\n",
      "Iteration 6766: loss = 0.0026288299667304844\n",
      "Iteration 6767: loss = 0.002628816859573583\n",
      "Iteration 6768: loss = 0.002628803753105088\n",
      "Iteration 6769: loss = 0.0026287906473244868\n",
      "Iteration 6770: loss = 0.002628777542231267\n",
      "Iteration 6771: loss = 0.002628764437824916\n",
      "Iteration 6772: loss = 0.0026287513341049233\n",
      "Iteration 6773: loss = 0.0026287382310707787\n",
      "Iteration 6774: loss = 0.002628725128721971\n",
      "Iteration 6775: loss = 0.0026287120270579905\n",
      "Iteration 6776: loss = 0.0026286989260783274\n",
      "Iteration 6777: loss = 0.0026286858257824736\n",
      "Iteration 6778: loss = 0.00262867272616992\n",
      "Iteration 6779: loss = 0.002628659627240159\n",
      "Iteration 6780: loss = 0.0026286465289926822\n",
      "Iteration 6781: loss = 0.002628633431426984\n",
      "Iteration 6782: loss = 0.0026286203345425563\n",
      "Iteration 6783: loss = 0.0026286072383388936\n",
      "Iteration 6784: loss = 0.002628594142815491\n",
      "Iteration 6785: loss = 0.0026285810479718423\n",
      "Iteration 6786: loss = 0.002628567953807443\n",
      "Iteration 6787: loss = 0.0026285548603217895\n",
      "Iteration 6788: loss = 0.002628541767514377\n",
      "Iteration 6789: loss = 0.002628528675384703\n",
      "Iteration 6790: loss = 0.002628515583932264\n",
      "Iteration 6791: loss = 0.0026285024931565586\n",
      "Iteration 6792: loss = 0.0026284894030570837\n",
      "Iteration 6793: loss = 0.0026284763136333383\n",
      "Iteration 6794: loss = 0.0026284632248848223\n",
      "Iteration 6795: loss = 0.0026284501368110343\n",
      "Iteration 6796: loss = 0.0026284370494114744\n",
      "Iteration 6797: loss = 0.002628423962685643\n",
      "Iteration 6798: loss = 0.0026284108766330408\n",
      "Iteration 6799: loss = 0.002628397791253169\n",
      "Iteration 6800: loss = 0.0026283847065455305\n",
      "Iteration 6801: loss = 0.0026283716225096263\n",
      "Iteration 6802: loss = 0.0026283585391449595\n",
      "Iteration 6803: loss = 0.002628345456451034\n",
      "Iteration 6804: loss = 0.0026283323744273517\n",
      "Iteration 6805: loss = 0.002628319293073419\n",
      "Iteration 6806: loss = 0.002628306212388738\n",
      "Iteration 6807: loss = 0.0026282931323728166\n",
      "Iteration 6808: loss = 0.0026282800530251582\n",
      "Iteration 6809: loss = 0.0026282669743452687\n",
      "Iteration 6810: loss = 0.0026282538963326553\n",
      "Iteration 6811: loss = 0.0026282408189868245\n",
      "Iteration 6812: loss = 0.0026282277423072837\n",
      "Iteration 6813: loss = 0.002628214666293541\n",
      "Iteration 6814: loss = 0.0026282015909451046\n",
      "Iteration 6815: loss = 0.0026281885162614815\n",
      "Iteration 6816: loss = 0.002628175442242184\n",
      "Iteration 6817: loss = 0.0026281623688867183\n",
      "Iteration 6818: loss = 0.0026281492961945973\n",
      "Iteration 6819: loss = 0.002628136224165329\n",
      "Iteration 6820: loss = 0.002628123152798427\n",
      "Iteration 6821: loss = 0.0026281100820934\n",
      "Iteration 6822: loss = 0.0026280970120497625\n",
      "Iteration 6823: loss = 0.002628083942667025\n",
      "Iteration 6824: loss = 0.0026280708739447\n",
      "Iteration 6825: loss = 0.0026280578058823023\n",
      "Iteration 6826: loss = 0.0026280447384793445\n",
      "Iteration 6827: loss = 0.0026280316717353404\n",
      "Iteration 6828: loss = 0.0026280186056498057\n",
      "Iteration 6829: loss = 0.002628005540222255\n",
      "Iteration 6830: loss = 0.002627992475452203\n",
      "Iteration 6831: loss = 0.0026279794113391666\n",
      "Iteration 6832: loss = 0.0026279663478826617\n",
      "Iteration 6833: loss = 0.002627953285082205\n",
      "Iteration 6834: loss = 0.002627940222937314\n",
      "Iteration 6835: loss = 0.0026279271614475064\n",
      "Iteration 6836: loss = 0.0026279141006122996\n",
      "Iteration 6837: loss = 0.002627901040431214\n",
      "Iteration 6838: loss = 0.0026278879809037677\n",
      "Iteration 6839: loss = 0.002627874922029479\n",
      "Iteration 6840: loss = 0.0026278618638078694\n",
      "Iteration 6841: loss = 0.0026278488062384587\n",
      "Iteration 6842: loss = 0.002627835749320767\n",
      "Iteration 6843: loss = 0.0026278226930543176\n",
      "Iteration 6844: loss = 0.00262780963743863\n",
      "Iteration 6845: loss = 0.002627796582473228\n",
      "Iteration 6846: loss = 0.0026277835281576334\n",
      "Iteration 6847: loss = 0.002627770474491369\n",
      "Iteration 6848: loss = 0.0026277574214739587\n",
      "Iteration 6849: loss = 0.0026277443691049267\n",
      "Iteration 6850: loss = 0.002627731317383797\n",
      "Iteration 6851: loss = 0.002627718266310094\n",
      "Iteration 6852: loss = 0.0026277052158833435\n",
      "Iteration 6853: loss = 0.002627692166103071\n",
      "Iteration 6854: loss = 0.002627679116968803\n",
      "Iteration 6855: loss = 0.002627666068480066\n",
      "Iteration 6856: loss = 0.002627653020636386\n",
      "Iteration 6857: loss = 0.002627639973437292\n",
      "Iteration 6858: loss = 0.0026276269268823106\n",
      "Iteration 6859: loss = 0.0026276138809709707\n",
      "Iteration 6860: loss = 0.0026276008357028007\n",
      "Iteration 6861: loss = 0.00262758779107733\n",
      "Iteration 6862: loss = 0.002627574747094089\n",
      "Iteration 6863: loss = 0.0026275617037526066\n",
      "Iteration 6864: loss = 0.0026275486610524137\n",
      "Iteration 6865: loss = 0.0026275356189930416\n",
      "Iteration 6866: loss = 0.002627522577574021\n",
      "Iteration 6867: loss = 0.0026275095367948842\n",
      "Iteration 6868: loss = 0.002627496496655163\n",
      "Iteration 6869: loss = 0.0026274834571543906\n",
      "Iteration 6870: loss = 0.0026274704182920995\n",
      "Iteration 6871: loss = 0.002627457380067824\n",
      "Iteration 6872: loss = 0.0026274443424810978\n",
      "Iteration 6873: loss = 0.0026274313055314547\n",
      "Iteration 6874: loss = 0.0026274182692184304\n",
      "Iteration 6875: loss = 0.0026274052335415595\n",
      "Iteration 6876: loss = 0.0026273921985003777\n",
      "Iteration 6877: loss = 0.0026273791640944216\n",
      "Iteration 6878: loss = 0.0026273661303232274\n",
      "Iteration 6879: loss = 0.0026273530971863326\n",
      "Iteration 6880: loss = 0.002627340064683274\n",
      "Iteration 6881: loss = 0.002627327032813589\n",
      "Iteration 6882: loss = 0.002627314001576817\n",
      "Iteration 6883: loss = 0.002627300970972496\n",
      "Iteration 6884: loss = 0.002627287941000166\n",
      "Iteration 6885: loss = 0.0026272749116593654\n",
      "Iteration 6886: loss = 0.0026272618829496345\n",
      "Iteration 6887: loss = 0.0026272488548705142\n",
      "Iteration 6888: loss = 0.002627235827421544\n",
      "Iteration 6889: loss = 0.002627222800602267\n",
      "Iteration 6890: loss = 0.0026272097744122237\n",
      "Iteration 6891: loss = 0.0026271967488509574\n",
      "Iteration 6892: loss = 0.002627183723918009\n",
      "Iteration 6893: loss = 0.0026271706996129217\n",
      "Iteration 6894: loss = 0.00262715767593524\n",
      "Iteration 6895: loss = 0.002627144652884507\n",
      "Iteration 6896: loss = 0.0026271316304602665\n",
      "Iteration 6897: loss = 0.0026271186086620642\n",
      "Iteration 6898: loss = 0.002627105587489444\n",
      "Iteration 6899: loss = 0.002627092566941952\n",
      "Iteration 6900: loss = 0.0026270795470191345\n",
      "Iteration 6901: loss = 0.002627066527720537\n",
      "Iteration 6902: loss = 0.0026270535090457065\n",
      "Iteration 6903: loss = 0.002627040490994191\n",
      "Iteration 6904: loss = 0.0026270274735655367\n",
      "Iteration 6905: loss = 0.0026270144567592923\n",
      "Iteration 6906: loss = 0.002627001440575006\n",
      "Iteration 6907: loss = 0.0026269884250122275\n",
      "Iteration 6908: loss = 0.002626975410070505\n",
      "Iteration 6909: loss = 0.0026269623957493886\n",
      "Iteration 6910: loss = 0.0026269493820484284\n",
      "Iteration 6911: loss = 0.0026269363689671752\n",
      "Iteration 6912: loss = 0.002626923356505179\n",
      "Iteration 6913: loss = 0.0026269103446619926\n",
      "Iteration 6914: loss = 0.002626897333437166\n",
      "Iteration 6915: loss = 0.0026268843228302525\n",
      "Iteration 6916: loss = 0.0026268713128408047\n",
      "Iteration 6917: loss = 0.002626858303468375\n",
      "Iteration 6918: loss = 0.0026268452947125175\n",
      "Iteration 6919: loss = 0.0026268322865727854\n",
      "Iteration 6920: loss = 0.0026268192790487345\n",
      "Iteration 6921: loss = 0.002626806272139917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6922: loss = 0.0026267932658458892\n",
      "Iteration 6923: loss = 0.0026267802601662063\n",
      "Iteration 6924: loss = 0.0026267672551004246\n",
      "Iteration 6925: loss = 0.002626754250648101\n",
      "Iteration 6926: loss = 0.0026267412468087914\n",
      "Iteration 6927: loss = 0.002626728243582052\n",
      "Iteration 6928: loss = 0.002626715240967442\n",
      "Iteration 6929: loss = 0.002626702238964519\n",
      "Iteration 6930: loss = 0.0026266892375728405\n",
      "Iteration 6931: loss = 0.002626676236791966\n",
      "Iteration 6932: loss = 0.002626663236621455\n",
      "Iteration 6933: loss = 0.0026266502370608648\n",
      "Iteration 6934: loss = 0.0026266372381097587\n",
      "Iteration 6935: loss = 0.0026266242397676954\n",
      "Iteration 6936: loss = 0.002626611242034235\n",
      "Iteration 6937: loss = 0.0026265982449089403\n",
      "Iteration 6938: loss = 0.002626585248391372\n",
      "Iteration 6939: loss = 0.0026265722524810922\n",
      "Iteration 6940: loss = 0.0026265592571776634\n",
      "Iteration 6941: loss = 0.0026265462624806488\n",
      "Iteration 6942: loss = 0.002626533268389611\n",
      "Iteration 6943: loss = 0.0026265202749041136\n",
      "Iteration 6944: loss = 0.0026265072820237214\n",
      "Iteration 6945: loss = 0.002626494289747998\n",
      "Iteration 6946: loss = 0.0026264812980765087\n",
      "Iteration 6947: loss = 0.002626468307008819\n",
      "Iteration 6948: loss = 0.002626455316544494\n",
      "Iteration 6949: loss = 0.0026264423266831\n",
      "Iteration 6950: loss = 0.0026264293374242036\n",
      "Iteration 6951: loss = 0.002626416348767372\n",
      "Iteration 6952: loss = 0.0026264033607121717\n",
      "Iteration 6953: loss = 0.0026263903732581706\n",
      "Iteration 6954: loss = 0.0026263773864049367\n",
      "Iteration 6955: loss = 0.002626364400152039\n",
      "Iteration 6956: loss = 0.002626351414499045\n",
      "Iteration 6957: loss = 0.0026263384294455266\n",
      "Iteration 6958: loss = 0.002626325444991051\n",
      "Iteration 6959: loss = 0.002626312461135189\n",
      "Iteration 6960: loss = 0.0026262994778775113\n",
      "Iteration 6961: loss = 0.0026262864952175885\n",
      "Iteration 6962: loss = 0.0026262735131549917\n",
      "Iteration 6963: loss = 0.0026262605316892933\n",
      "Iteration 6964: loss = 0.002626247550820065\n",
      "Iteration 6965: loss = 0.0026262345705468783\n",
      "Iteration 6966: loss = 0.0026262215908693072\n",
      "Iteration 6967: loss = 0.0026262086117869246\n",
      "Iteration 6968: loss = 0.0026261956332993045\n",
      "Iteration 6969: loss = 0.00262618265540602\n",
      "Iteration 6970: loss = 0.0026261696781066466\n",
      "Iteration 6971: loss = 0.002626156701400758\n",
      "Iteration 6972: loss = 0.00262614372528793\n",
      "Iteration 6973: loss = 0.0026261307497677383\n",
      "Iteration 6974: loss = 0.002626117774839759\n",
      "Iteration 6975: loss = 0.002626104800503568\n",
      "Iteration 6976: loss = 0.0026260918267587427\n",
      "Iteration 6977: loss = 0.00262607885360486\n",
      "Iteration 6978: loss = 0.0026260658810414966\n",
      "Iteration 6979: loss = 0.0026260529090682315\n",
      "Iteration 6980: loss = 0.0026260399376846436\n",
      "Iteration 6981: loss = 0.00262602696689031\n",
      "Iteration 6982: loss = 0.0026260139966848115\n",
      "Iteration 6983: loss = 0.0026260010270677254\n",
      "Iteration 6984: loss = 0.0026259880580386343\n",
      "Iteration 6985: loss = 0.0026259750895971158\n",
      "Iteration 6986: loss = 0.002625962121742753\n",
      "Iteration 6987: loss = 0.0026259491544751252\n",
      "Iteration 6988: loss = 0.002625936187793815\n",
      "Iteration 6989: loss = 0.002625923221698404\n",
      "Iteration 6990: loss = 0.002625910256188474\n",
      "Iteration 6991: loss = 0.0026258972912636073\n",
      "Iteration 6992: loss = 0.002625884326923388\n",
      "Iteration 6993: loss = 0.002625871363167399\n",
      "Iteration 6994: loss = 0.002625858399995224\n",
      "Iteration 6995: loss = 0.002625845437406447\n",
      "Iteration 6996: loss = 0.002625832475400653\n",
      "Iteration 6997: loss = 0.0026258195139774267\n",
      "Iteration 6998: loss = 0.002625806553136353\n",
      "Iteration 6999: loss = 0.0026257935928770183\n",
      "Iteration 7000: loss = 0.0026257806331990083\n",
      "Iteration 7001: loss = 0.00262576767410191\n",
      "Iteration 7002: loss = 0.0026257547155853094\n",
      "Iteration 7003: loss = 0.002625741757648794\n",
      "Iteration 7004: loss = 0.0026257288002919515\n",
      "Iteration 7005: loss = 0.00262571584351437\n",
      "Iteration 7006: loss = 0.002625702887315638\n",
      "Iteration 7007: loss = 0.0026256899316953438\n",
      "Iteration 7008: loss = 0.002625676976653077\n",
      "Iteration 7009: loss = 0.0026256640221884267\n",
      "Iteration 7010: loss = 0.002625651068300983\n",
      "Iteration 7011: loss = 0.0026256381149903367\n",
      "Iteration 7012: loss = 0.002625625162256078\n",
      "Iteration 7013: loss = 0.0026256122100977975\n",
      "Iteration 7014: loss = 0.002625599258515087\n",
      "Iteration 7015: loss = 0.002625586307507538\n",
      "Iteration 7016: loss = 0.002625573357074744\n",
      "Iteration 7017: loss = 0.002625560407216295\n",
      "Iteration 7018: loss = 0.0026255474579317865\n",
      "Iteration 7019: loss = 0.002625534509220811\n",
      "Iteration 7020: loss = 0.002625521561082962\n",
      "Iteration 7021: loss = 0.002625508613517833\n",
      "Iteration 7022: loss = 0.002625495666525019\n",
      "Iteration 7023: loss = 0.002625482720104115\n",
      "Iteration 7024: loss = 0.0026254697742547155\n",
      "Iteration 7025: loss = 0.002625456828976417\n",
      "Iteration 7026: loss = 0.0026254438842688143\n",
      "Iteration 7027: loss = 0.0026254309401315045\n",
      "Iteration 7028: loss = 0.0026254179965640844\n",
      "Iteration 7029: loss = 0.0026254050535661506\n",
      "Iteration 7030: loss = 0.0026253921111373006\n",
      "Iteration 7031: loss = 0.0026253791692771324\n",
      "Iteration 7032: loss = 0.0026253662279852436\n",
      "Iteration 7033: loss = 0.002625353287261234\n",
      "Iteration 7034: loss = 0.002625340347104701\n",
      "Iteration 7035: loss = 0.0026253274075152453\n",
      "Iteration 7036: loss = 0.002625314468492465\n",
      "Iteration 7037: loss = 0.0026253015300359618\n",
      "Iteration 7038: loss = 0.0026252885921453344\n",
      "Iteration 7039: loss = 0.002625275654820185\n",
      "Iteration 7040: loss = 0.0026252627180601137\n",
      "Iteration 7041: loss = 0.002625249781864723\n",
      "Iteration 7042: loss = 0.002625236846233614\n",
      "Iteration 7043: loss = 0.0026252239111663896\n",
      "Iteration 7044: loss = 0.002625210976662651\n",
      "Iteration 7045: loss = 0.002625198042722003\n",
      "Iteration 7046: loss = 0.0026251851093440476\n",
      "Iteration 7047: loss = 0.00262517217652839\n",
      "Iteration 7048: loss = 0.0026251592442746324\n",
      "Iteration 7049: loss = 0.00262514631258238\n",
      "Iteration 7050: loss = 0.002625133381451238\n",
      "Iteration 7051: loss = 0.0026251204508808117\n",
      "Iteration 7052: loss = 0.0026251075208707066\n",
      "Iteration 7053: loss = 0.0026250945914205277\n",
      "Iteration 7054: loss = 0.002625081662529882\n",
      "Iteration 7055: loss = 0.0026250687341983756\n",
      "Iteration 7056: loss = 0.002625055806425617\n",
      "Iteration 7057: loss = 0.002625042879211212\n",
      "Iteration 7058: loss = 0.002625029952554768\n",
      "Iteration 7059: loss = 0.0026250170264558947\n",
      "Iteration 7060: loss = 0.0026250041009141995\n",
      "Iteration 7061: loss = 0.0026249911759292912\n",
      "Iteration 7062: loss = 0.00262497825150078\n",
      "Iteration 7063: loss = 0.0026249653276282743\n",
      "Iteration 7064: loss = 0.002624952404311384\n",
      "Iteration 7065: loss = 0.00262493948154972\n",
      "Iteration 7066: loss = 0.002624926559342893\n",
      "Iteration 7067: loss = 0.002624913637690513\n",
      "Iteration 7068: loss = 0.002624900716592192\n",
      "Iteration 7069: loss = 0.0026248877960475423\n",
      "Iteration 7070: loss = 0.0026248748760561757\n",
      "Iteration 7071: loss = 0.0026248619566177035\n",
      "Iteration 7072: loss = 0.0026248490377317395\n",
      "Iteration 7073: loss = 0.0026248361193978963\n",
      "Iteration 7074: loss = 0.002624823201615788\n",
      "Iteration 7075: loss = 0.002624810284385028\n",
      "Iteration 7076: loss = 0.002624797367705231\n",
      "Iteration 7077: loss = 0.0026247844515760108\n",
      "Iteration 7078: loss = 0.0026247715359969835\n",
      "Iteration 7079: loss = 0.002624758620967763\n",
      "Iteration 7080: loss = 0.0026247457064879657\n",
      "Iteration 7081: loss = 0.002624732792557207\n",
      "Iteration 7082: loss = 0.0026247198791751044\n",
      "Iteration 7083: loss = 0.002624706966341274\n",
      "Iteration 7084: loss = 0.002624694054055332\n",
      "Iteration 7085: loss = 0.0026246811423168977\n",
      "Iteration 7086: loss = 0.002624668231125587\n",
      "Iteration 7087: loss = 0.0026246553204810185\n",
      "Iteration 7088: loss = 0.0026246424103828114\n",
      "Iteration 7089: loss = 0.002624629500830584\n",
      "Iteration 7090: loss = 0.002624616591823956\n",
      "Iteration 7091: loss = 0.0026246036833625455\n",
      "Iteration 7092: loss = 0.002624590775445974\n",
      "Iteration 7093: loss = 0.002624577868073861\n",
      "Iteration 7094: loss = 0.0026245649612458275\n",
      "Iteration 7095: loss = 0.0026245520549614937\n",
      "Iteration 7096: loss = 0.0026245391492204814\n",
      "Iteration 7097: loss = 0.0026245262440224124\n",
      "Iteration 7098: loss = 0.002624513339366908\n",
      "Iteration 7099: loss = 0.002624500435253591\n",
      "Iteration 7100: loss = 0.002624487531682084\n",
      "Iteration 7101: loss = 0.002624474628652011\n",
      "Iteration 7102: loss = 0.0026244617261629937\n",
      "Iteration 7103: loss = 0.0026244488242146567\n",
      "Iteration 7104: loss = 0.0026244359228066237\n",
      "Iteration 7105: loss = 0.0026244230219385203\n",
      "Iteration 7106: loss = 0.00262441012160997\n",
      "Iteration 7107: loss = 0.002624397221820598\n",
      "Iteration 7108: loss = 0.0026243843225700306\n",
      "Iteration 7109: loss = 0.0026243714238578927\n",
      "Iteration 7110: loss = 0.002624358525683811\n",
      "Iteration 7111: loss = 0.002624345628047412\n",
      "Iteration 7112: loss = 0.0026243327309483228\n",
      "Iteration 7113: loss = 0.002624319834386169\n",
      "Iteration 7114: loss = 0.0026243069383605805\n",
      "Iteration 7115: loss = 0.0026242940428711837\n",
      "Iteration 7116: loss = 0.0026242811479176074\n",
      "Iteration 7117: loss = 0.00262426825349948\n",
      "Iteration 7118: loss = 0.00262425535961643\n",
      "Iteration 7119: loss = 0.002624242466268088\n",
      "Iteration 7120: loss = 0.002624229573454082\n",
      "Iteration 7121: loss = 0.0026242166811740425\n",
      "Iteration 7122: loss = 0.0026242037894276\n",
      "Iteration 7123: loss = 0.0026241908982143855\n",
      "Iteration 7124: loss = 0.00262417800753403\n",
      "Iteration 7125: loss = 0.0026241651173861628\n",
      "Iteration 7126: loss = 0.0026241522277704182\n",
      "Iteration 7127: loss = 0.002624139338686427\n",
      "Iteration 7128: loss = 0.0026241264501338213\n",
      "Iteration 7129: loss = 0.0026241135621122343\n",
      "Iteration 7130: loss = 0.0026241006746212992\n",
      "Iteration 7131: loss = 0.002624087787660649\n",
      "Iteration 7132: loss = 0.0026240749012299168\n",
      "Iteration 7133: loss = 0.0026240620153287378\n",
      "Iteration 7134: loss = 0.0026240491299567457\n",
      "Iteration 7135: loss = 0.0026240362451135754\n",
      "Iteration 7136: loss = 0.0026240233607988618\n",
      "Iteration 7137: loss = 0.002624010477012241\n",
      "Iteration 7138: loss = 0.0026239975937533467\n",
      "Iteration 7139: loss = 0.0026239847110218176\n",
      "Iteration 7140: loss = 0.002623971828817289\n",
      "Iteration 7141: loss = 0.002623958947139397\n",
      "Iteration 7142: loss = 0.002623946065987779\n",
      "Iteration 7143: loss = 0.002623933185362072\n",
      "Iteration 7144: loss = 0.0026239203052619156\n",
      "Iteration 7145: loss = 0.002623907425686946\n",
      "Iteration 7146: loss = 0.0026238945466368025\n",
      "Iteration 7147: loss = 0.0026238816681111235\n",
      "Iteration 7148: loss = 0.0026238687901095482\n",
      "Iteration 7149: loss = 0.002623855912631716\n",
      "Iteration 7150: loss = 0.0026238430356772665\n",
      "Iteration 7151: loss = 0.0026238301592458393\n",
      "Iteration 7152: loss = 0.0026238172833370765\n",
      "Iteration 7153: loss = 0.0026238044079506177\n",
      "Iteration 7154: loss = 0.0026237915330861034\n",
      "Iteration 7155: loss = 0.0026237786587431757\n",
      "Iteration 7156: loss = 0.002623765784921477\n",
      "Iteration 7157: loss = 0.002623752911620648\n",
      "Iteration 7158: loss = 0.002623740038840332\n",
      "Iteration 7159: loss = 0.0026237271665801713\n",
      "Iteration 7160: loss = 0.0026237142948398098\n",
      "Iteration 7161: loss = 0.00262370142361889\n",
      "Iteration 7162: loss = 0.0026236885529170558\n",
      "Iteration 7163: loss = 0.002623675682733951\n",
      "Iteration 7164: loss = 0.0026236628130692215\n",
      "Iteration 7165: loss = 0.0026236499439225096\n",
      "Iteration 7166: loss = 0.0026236370752934623\n",
      "Iteration 7167: loss = 0.0026236242071817238\n",
      "Iteration 7168: loss = 0.002623611339586941\n",
      "Iteration 7169: loss = 0.0026235984725087587\n",
      "Iteration 7170: loss = 0.0026235856059468243\n",
      "Iteration 7171: loss = 0.0026235727399007837\n",
      "Iteration 7172: loss = 0.0026235598743702838\n",
      "Iteration 7173: loss = 0.0026235470093549725\n",
      "Iteration 7174: loss = 0.0026235341448544976\n",
      "Iteration 7175: loss = 0.002623521280868506\n",
      "Iteration 7176: loss = 0.0026235084173966477\n",
      "Iteration 7177: loss = 0.0026234955544385697\n",
      "Iteration 7178: loss = 0.002623482691993922\n",
      "Iteration 7179: loss = 0.0026234698300623537\n",
      "Iteration 7180: loss = 0.002623456968643514\n",
      "Iteration 7181: loss = 0.002623444107737053\n",
      "Iteration 7182: loss = 0.002623431247342621\n",
      "Iteration 7183: loss = 0.002623418387459869\n",
      "Iteration 7184: loss = 0.002623405528088447\n",
      "Iteration 7185: loss = 0.0026233926692280076\n",
      "Iteration 7186: loss = 0.002623379810878201\n",
      "Iteration 7187: loss = 0.0026233669530386795\n",
      "Iteration 7188: loss = 0.0026233540957090956\n",
      "Iteration 7189: loss = 0.0026233412388891016\n",
      "Iteration 7190: loss = 0.00262332838257835\n",
      "Iteration 7191: loss = 0.0026233155267764946\n",
      "Iteration 7192: loss = 0.0026233026714831886\n",
      "Iteration 7193: loss = 0.002623289816698086\n",
      "Iteration 7194: loss = 0.00262327696242084\n",
      "Iteration 7195: loss = 0.0026232641086511065\n",
      "Iteration 7196: loss = 0.002623251255388539\n",
      "Iteration 7197: loss = 0.0026232384026327934\n",
      "Iteration 7198: loss = 0.002623225550383524\n",
      "Iteration 7199: loss = 0.0026232126986403884\n",
      "Iteration 7200: loss = 0.0026231998474030406\n",
      "Iteration 7201: loss = 0.0026231869966711384\n",
      "Iteration 7202: loss = 0.002623174146444338\n",
      "Iteration 7203: loss = 0.002623161296722296\n",
      "Iteration 7204: loss = 0.0026231484475046705\n",
      "Iteration 7205: loss = 0.002623135598791119\n",
      "Iteration 7206: loss = 0.002623122750581298\n",
      "Iteration 7207: loss = 0.002623109902874868\n",
      "Iteration 7208: loss = 0.0026230970556714857\n",
      "Iteration 7209: loss = 0.0026230842089708106\n",
      "Iteration 7210: loss = 0.0026230713627725024\n",
      "Iteration 7211: loss = 0.00262305851707622\n",
      "Iteration 7212: loss = 0.002623045671881624\n",
      "Iteration 7213: loss = 0.0026230328271883738\n",
      "Iteration 7214: loss = 0.0026230199829961292\n",
      "Iteration 7215: loss = 0.002623007139304553\n",
      "Iteration 7216: loss = 0.002622994296113305\n",
      "Iteration 7217: loss = 0.002622981453422046\n",
      "Iteration 7218: loss = 0.0026229686112304385\n",
      "Iteration 7219: loss = 0.002622955769538145\n",
      "Iteration 7220: loss = 0.0026229429283448273\n",
      "Iteration 7221: loss = 0.002622930087650148\n",
      "Iteration 7222: loss = 0.0026229172474537704\n",
      "Iteration 7223: loss = 0.002622904407755357\n",
      "Iteration 7224: loss = 0.002622891568554572\n",
      "Iteration 7225: loss = 0.0026228787298510792\n",
      "Iteration 7226: loss = 0.002622865891644543\n",
      "Iteration 7227: loss = 0.0026228530539346277\n",
      "Iteration 7228: loss = 0.002622840216720998\n",
      "Iteration 7229: loss = 0.002622827380003319\n",
      "Iteration 7230: loss = 0.002622814543781257\n",
      "Iteration 7231: loss = 0.0026228017080544763\n",
      "Iteration 7232: loss = 0.002622788872822644\n",
      "Iteration 7233: loss = 0.0026227760380854266\n",
      "Iteration 7234: loss = 0.0026227632038424904\n",
      "Iteration 7235: loss = 0.0026227503700935024\n",
      "Iteration 7236: loss = 0.0026227375368381295\n",
      "Iteration 7237: loss = 0.00262272470407604\n",
      "Iteration 7238: loss = 0.002622711871806901\n",
      "Iteration 7239: loss = 0.0026226990400303823\n",
      "Iteration 7240: loss = 0.002622686208746151\n",
      "Iteration 7241: loss = 0.002622673377953876\n",
      "Iteration 7242: loss = 0.0026226605476532268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7243: loss = 0.002622647717843873\n",
      "Iteration 7244: loss = 0.0026226348885254836\n",
      "Iteration 7245: loss = 0.00262262205969773\n",
      "Iteration 7246: loss = 0.0026226092313602813\n",
      "Iteration 7247: loss = 0.0026225964035128087\n",
      "Iteration 7248: loss = 0.0026225835761549835\n",
      "Iteration 7249: loss = 0.002622570749286476\n",
      "Iteration 7250: loss = 0.0026225579229069585\n",
      "Iteration 7251: loss = 0.0026225450970161026\n",
      "Iteration 7252: loss = 0.0026225322716135814\n",
      "Iteration 7253: loss = 0.002622519446699066\n",
      "Iteration 7254: loss = 0.0026225066222722304\n",
      "Iteration 7255: loss = 0.0026224937983327463\n",
      "Iteration 7256: loss = 0.002622480974880288\n",
      "Iteration 7257: loss = 0.00262246815191453\n",
      "Iteration 7258: loss = 0.002622455329435144\n",
      "Iteration 7259: loss = 0.002622442507441807\n",
      "Iteration 7260: loss = 0.0026224296859341917\n",
      "Iteration 7261: loss = 0.002622416864911974\n",
      "Iteration 7262: loss = 0.002622404044374828\n",
      "Iteration 7263: loss = 0.0026223912243224304\n",
      "Iteration 7264: loss = 0.0026223784047544564\n",
      "Iteration 7265: loss = 0.002622365585670582\n",
      "Iteration 7266: loss = 0.002622352767070484\n",
      "Iteration 7267: loss = 0.002622339948953839\n",
      "Iteration 7268: loss = 0.002622327131320324\n",
      "Iteration 7269: loss = 0.0026223143141696163\n",
      "Iteration 7270: loss = 0.0026223014975013934\n",
      "Iteration 7271: loss = 0.0026222886813153327\n",
      "Iteration 7272: loss = 0.0026222758656111135\n",
      "Iteration 7273: loss = 0.0026222630503884136\n",
      "Iteration 7274: loss = 0.0026222502356469117\n",
      "Iteration 7275: loss = 0.002622237421386287\n",
      "Iteration 7276: loss = 0.002622224607606219\n",
      "Iteration 7277: loss = 0.0026222117943063878\n",
      "Iteration 7278: loss = 0.0026221989814864725\n",
      "Iteration 7279: loss = 0.002622186169146153\n",
      "Iteration 7280: loss = 0.002622173357285112\n",
      "Iteration 7281: loss = 0.002622160545903028\n",
      "Iteration 7282: loss = 0.002622147734999584\n",
      "Iteration 7283: loss = 0.00262213492457446\n",
      "Iteration 7284: loss = 0.0026221221146273386\n",
      "Iteration 7285: loss = 0.002622109305157901\n",
      "Iteration 7286: loss = 0.002622096496165831\n",
      "Iteration 7287: loss = 0.0026220836876508094\n",
      "Iteration 7288: loss = 0.0026220708796125206\n",
      "Iteration 7289: loss = 0.0026220580720506476\n",
      "Iteration 7290: loss = 0.002622045264964873\n",
      "Iteration 7291: loss = 0.0026220324583548812\n",
      "Iteration 7292: loss = 0.0026220196522203565\n",
      "Iteration 7293: loss = 0.002622006846560983\n",
      "Iteration 7294: loss = 0.0026219940413764456\n",
      "Iteration 7295: loss = 0.002621981236666429\n",
      "Iteration 7296: loss = 0.002621968432430618\n",
      "Iteration 7297: loss = 0.0026219556286686987\n",
      "Iteration 7298: loss = 0.0026219428253803573\n",
      "Iteration 7299: loss = 0.0026219300225652797\n",
      "Iteration 7300: loss = 0.0026219172202231523\n",
      "Iteration 7301: loss = 0.0026219044183536603\n",
      "Iteration 7302: loss = 0.002621891616956493\n",
      "Iteration 7303: loss = 0.002621878816031337\n",
      "Iteration 7304: loss = 0.002621866015577879\n",
      "Iteration 7305: loss = 0.002621853215595808\n",
      "Iteration 7306: loss = 0.0026218404160848114\n",
      "Iteration 7307: loss = 0.0026218276170445775\n",
      "Iteration 7308: loss = 0.002621814818474796\n",
      "Iteration 7309: loss = 0.0026218020203751555\n",
      "Iteration 7310: loss = 0.0026217892227453445\n",
      "Iteration 7311: loss = 0.0026217764255850526\n",
      "Iteration 7312: loss = 0.002621763628893971\n",
      "Iteration 7313: loss = 0.0026217508326717895\n",
      "Iteration 7314: loss = 0.0026217380369181974\n",
      "Iteration 7315: loss = 0.0026217252416328865\n",
      "Iteration 7316: loss = 0.002621712446815548\n",
      "Iteration 7317: loss = 0.0026216996524658725\n",
      "Iteration 7318: loss = 0.0026216868585835513\n",
      "Iteration 7319: loss = 0.0026216740651682775\n",
      "Iteration 7320: loss = 0.0026216612722197424\n",
      "Iteration 7321: loss = 0.002621648479737638\n",
      "Iteration 7322: loss = 0.002621635687721658\n",
      "Iteration 7323: loss = 0.002621622896171495\n",
      "Iteration 7324: loss = 0.0026216101050868424\n",
      "Iteration 7325: loss = 0.0026215973144673938\n",
      "Iteration 7326: loss = 0.002621584524312843\n",
      "Iteration 7327: loss = 0.0026215717346228843\n",
      "Iteration 7328: loss = 0.0026215589453972123\n",
      "Iteration 7329: loss = 0.0026215461566355203\n",
      "Iteration 7330: loss = 0.002621533368337505\n",
      "Iteration 7331: loss = 0.002621520580502861\n",
      "Iteration 7332: loss = 0.002621507793131284\n",
      "Iteration 7333: loss = 0.002621495006222469\n",
      "Iteration 7334: loss = 0.0026214822197761136\n",
      "Iteration 7335: loss = 0.0026214694337919134\n",
      "Iteration 7336: loss = 0.002621456648269565\n",
      "Iteration 7337: loss = 0.0026214438632087655\n",
      "Iteration 7338: loss = 0.0026214310786092116\n",
      "Iteration 7339: loss = 0.002621418294470602\n",
      "Iteration 7340: loss = 0.0026214055107926337\n",
      "Iteration 7341: loss = 0.002621392727575005\n",
      "Iteration 7342: loss = 0.002621379944817414\n",
      "Iteration 7343: loss = 0.0026213671625195593\n",
      "Iteration 7344: loss = 0.0026213543806811405\n",
      "Iteration 7345: loss = 0.0026213415993018565\n",
      "Iteration 7346: loss = 0.0026213288183814065\n",
      "Iteration 7347: loss = 0.00262131603791949\n",
      "Iteration 7348: loss = 0.002621303257915807\n",
      "Iteration 7349: loss = 0.0026212904783700585\n",
      "Iteration 7350: loss = 0.0026212776992819453\n",
      "Iteration 7351: loss = 0.002621264920651167\n",
      "Iteration 7352: loss = 0.0026212521424774254\n",
      "Iteration 7353: loss = 0.0026212393647604224\n",
      "Iteration 7354: loss = 0.002621226587499859\n",
      "Iteration 7355: loss = 0.0026212138106954373\n",
      "Iteration 7356: loss = 0.0026212010343468596\n",
      "Iteration 7357: loss = 0.0026211882584538286\n",
      "Iteration 7358: loss = 0.0026211754830160468\n",
      "Iteration 7359: loss = 0.0026211627080332174\n",
      "Iteration 7360: loss = 0.0026211499335050442\n",
      "Iteration 7361: loss = 0.00262113715943123\n",
      "Iteration 7362: loss = 0.002621124385811478\n",
      "Iteration 7363: loss = 0.0026211116126454944\n",
      "Iteration 7364: loss = 0.002621098839932982\n",
      "Iteration 7365: loss = 0.0026210860676736465\n",
      "Iteration 7366: loss = 0.0026210732958671925\n",
      "Iteration 7367: loss = 0.0026210605245133244\n",
      "Iteration 7368: loss = 0.0026210477536117494\n",
      "Iteration 7369: loss = 0.002621034983162172\n",
      "Iteration 7370: loss = 0.002621022213164299\n",
      "Iteration 7371: loss = 0.002621009443617836\n",
      "Iteration 7372: loss = 0.00262099667452249\n",
      "Iteration 7373: loss = 0.0026209839058779676\n",
      "Iteration 7374: loss = 0.002620971137683977\n",
      "Iteration 7375: loss = 0.002620958369940224\n",
      "Iteration 7376: loss = 0.002620945602646418\n",
      "Iteration 7377: loss = 0.002620932835802265\n",
      "Iteration 7378: loss = 0.0026209200694074747\n",
      "Iteration 7379: loss = 0.0026209073034617557\n",
      "Iteration 7380: loss = 0.0026208945379648154\n",
      "Iteration 7381: loss = 0.0026208817729163644\n",
      "Iteration 7382: loss = 0.0026208690083161113\n",
      "Iteration 7383: loss = 0.0026208562441637656\n",
      "Iteration 7384: loss = 0.0026208434804590366\n",
      "Iteration 7385: loss = 0.002620830717201635\n",
      "Iteration 7386: loss = 0.002620817954391272\n",
      "Iteration 7387: loss = 0.002620805192027657\n",
      "Iteration 7388: loss = 0.0026207924301105017\n",
      "Iteration 7389: loss = 0.0026207796686395163\n",
      "Iteration 7390: loss = 0.0026207669076144125\n",
      "Iteration 7391: loss = 0.0026207541470349037\n",
      "Iteration 7392: loss = 0.002620741386900699\n",
      "Iteration 7393: loss = 0.0026207286272115137\n",
      "Iteration 7394: loss = 0.0026207158679670576\n",
      "Iteration 7395: loss = 0.0026207031091670455\n",
      "Iteration 7396: loss = 0.002620690350811189\n",
      "Iteration 7397: loss = 0.0026206775928992025\n",
      "Iteration 7398: loss = 0.0026206648354307983\n",
      "Iteration 7399: loss = 0.002620652078405692\n",
      "Iteration 7400: loss = 0.002620639321823596\n",
      "Iteration 7401: loss = 0.0026206265656842256\n",
      "Iteration 7402: loss = 0.002620613809987295\n",
      "Iteration 7403: loss = 0.002620601054732519\n",
      "Iteration 7404: loss = 0.002620588299919614\n",
      "Iteration 7405: loss = 0.0026205755455482936\n",
      "Iteration 7406: loss = 0.0026205627916182747\n",
      "Iteration 7407: loss = 0.0026205500381292724\n",
      "Iteration 7408: loss = 0.002620537285081004\n",
      "Iteration 7409: loss = 0.0026205245324731844\n",
      "Iteration 7410: loss = 0.002620511780305532\n",
      "Iteration 7411: loss = 0.002620499028577763\n",
      "Iteration 7412: loss = 0.002620486277289595\n",
      "Iteration 7413: loss = 0.002620473526440744\n",
      "Iteration 7414: loss = 0.0026204607760309303\n",
      "Iteration 7415: loss = 0.0026204480260598696\n",
      "Iteration 7416: loss = 0.0026204352765272815\n",
      "Iteration 7417: loss = 0.0026204225274328845\n",
      "Iteration 7418: loss = 0.0026204097787763973\n",
      "Iteration 7419: loss = 0.002620397030557538\n",
      "Iteration 7420: loss = 0.0026203842827760273\n",
      "Iteration 7421: loss = 0.002620371535431584\n",
      "Iteration 7422: loss = 0.0026203587885239286\n",
      "Iteration 7423: loss = 0.00262034604205278\n",
      "Iteration 7424: loss = 0.0026203332960178603\n",
      "Iteration 7425: loss = 0.0026203205504188886\n",
      "Iteration 7426: loss = 0.0026203078052555863\n",
      "Iteration 7427: loss = 0.002620295060527675\n",
      "Iteration 7428: loss = 0.002620282316234875\n",
      "Iteration 7429: loss = 0.002620269572376909\n",
      "Iteration 7430: loss = 0.0026202568289534992\n",
      "Iteration 7431: loss = 0.002620244085964366\n",
      "Iteration 7432: loss = 0.002620231343409234\n",
      "Iteration 7433: loss = 0.002620218601287824\n",
      "Iteration 7434: loss = 0.00262020585959986\n",
      "Iteration 7435: loss = 0.0026201931183450653\n",
      "Iteration 7436: loss = 0.0026201803775231626\n",
      "Iteration 7437: loss = 0.002620167637133877\n",
      "Iteration 7438: loss = 0.0026201548971769295\n",
      "Iteration 7439: loss = 0.002620142157652048\n",
      "Iteration 7440: loss = 0.0026201294185589545\n",
      "Iteration 7441: loss = 0.0026201166798973745\n",
      "Iteration 7442: loss = 0.0026201039416670338\n",
      "Iteration 7443: loss = 0.0026200912038676556\n",
      "Iteration 7444: loss = 0.0026200784664989667\n",
      "Iteration 7445: loss = 0.002620065729560693\n",
      "Iteration 7446: loss = 0.0026200529930525606\n",
      "Iteration 7447: loss = 0.0026200402569742944\n",
      "Iteration 7448: loss = 0.0026200275213256225\n",
      "Iteration 7449: loss = 0.0026200147861062707\n",
      "Iteration 7450: loss = 0.0026200020513159664\n",
      "Iteration 7451: loss = 0.0026199893169544363\n",
      "Iteration 7452: loss = 0.002619976583021409\n",
      "Iteration 7453: loss = 0.002619963849516611\n",
      "Iteration 7454: loss = 0.002619951116439771\n",
      "Iteration 7455: loss = 0.0026199383837906174\n",
      "Iteration 7456: loss = 0.0026199256515688782\n",
      "Iteration 7457: loss = 0.002619912919774282\n",
      "Iteration 7458: loss = 0.0026199001884065595\n",
      "Iteration 7459: loss = 0.0026198874574654375\n",
      "Iteration 7460: loss = 0.002619874726950647\n",
      "Iteration 7461: loss = 0.0026198619968619175\n",
      "Iteration 7462: loss = 0.002619849267198979\n",
      "Iteration 7463: loss = 0.0026198365379615615\n",
      "Iteration 7464: loss = 0.0026198238091493958\n",
      "Iteration 7465: loss = 0.0026198110807622126\n",
      "Iteration 7466: loss = 0.0026197983527997434\n",
      "Iteration 7467: loss = 0.002619785625261718\n",
      "Iteration 7468: loss = 0.0026197728981478693\n",
      "Iteration 7469: loss = 0.002619760171457929\n",
      "Iteration 7470: loss = 0.002619747445191628\n",
      "Iteration 7471: loss = 0.0026197347193486994\n",
      "Iteration 7472: loss = 0.002619721993928875\n",
      "Iteration 7473: loss = 0.002619709268931889\n",
      "Iteration 7474: loss = 0.0026196965443574726\n",
      "Iteration 7475: loss = 0.0026196838202053605\n",
      "Iteration 7476: loss = 0.0026196710964752853\n",
      "Iteration 7477: loss = 0.0026196583731669812\n",
      "Iteration 7478: loss = 0.0026196456502801816\n",
      "Iteration 7479: loss = 0.002619632927814621\n",
      "Iteration 7480: loss = 0.002619620205770034\n",
      "Iteration 7481: loss = 0.002619607484146155\n",
      "Iteration 7482: loss = 0.0026195947629427192\n",
      "Iteration 7483: loss = 0.002619582042159462\n",
      "Iteration 7484: loss = 0.0026195693217961184\n",
      "Iteration 7485: loss = 0.002619556601852424\n",
      "Iteration 7486: loss = 0.0026195438823281153\n",
      "Iteration 7487: loss = 0.002619531163222928\n",
      "Iteration 7488: loss = 0.0026195184445365993\n",
      "Iteration 7489: loss = 0.0026195057262688643\n",
      "Iteration 7490: loss = 0.002619493008419461\n",
      "Iteration 7491: loss = 0.0026194802909881264\n",
      "Iteration 7492: loss = 0.002619467573974598\n",
      "Iteration 7493: loss = 0.0026194548573786136\n",
      "Iteration 7494: loss = 0.0026194421411999107\n",
      "Iteration 7495: loss = 0.0026194294254382267\n",
      "Iteration 7496: loss = 0.002619416710093301\n",
      "Iteration 7497: loss = 0.002619403995164872\n",
      "Iteration 7498: loss = 0.002619391280652678\n",
      "Iteration 7499: loss = 0.0026193785665564596\n",
      "Iteration 7500: loss = 0.0026193658528759546\n",
      "Iteration 7501: loss = 0.0026193531396109027\n",
      "Iteration 7502: loss = 0.002619340426761044\n",
      "Iteration 7503: loss = 0.002619327714326119\n",
      "Iteration 7504: loss = 0.0026193150023058665\n",
      "Iteration 7505: loss = 0.0026193022907000283\n",
      "Iteration 7506: loss = 0.0026192895795083453\n",
      "Iteration 7507: loss = 0.002619276868730558\n",
      "Iteration 7508: loss = 0.0026192641583664077\n",
      "Iteration 7509: loss = 0.0026192514484156353\n",
      "Iteration 7510: loss = 0.002619238738877984\n",
      "Iteration 7511: loss = 0.0026192260297531944\n",
      "Iteration 7512: loss = 0.0026192133210410085\n",
      "Iteration 7513: loss = 0.00261920061274117\n",
      "Iteration 7514: loss = 0.0026191879048534207\n",
      "Iteration 7515: loss = 0.002619175197377504\n",
      "Iteration 7516: loss = 0.0026191624903131624\n",
      "Iteration 7517: loss = 0.0026191497836601406\n",
      "Iteration 7518: loss = 0.0026191370774181796\n",
      "Iteration 7519: loss = 0.002619124371587026\n",
      "Iteration 7520: loss = 0.0026191116661664227\n",
      "Iteration 7521: loss = 0.0026190989611561133\n",
      "Iteration 7522: loss = 0.002619086256555844\n",
      "Iteration 7523: loss = 0.0026190735523653583\n",
      "Iteration 7524: loss = 0.0026190608485844023\n",
      "Iteration 7525: loss = 0.00261904814521272\n",
      "Iteration 7526: loss = 0.0026190354422500578\n",
      "Iteration 7527: loss = 0.0026190227396961606\n",
      "Iteration 7528: loss = 0.0026190100375507756\n",
      "Iteration 7529: loss = 0.0026189973358136487\n",
      "Iteration 7530: loss = 0.0026189846344845252\n",
      "Iteration 7531: loss = 0.0026189719335631525\n",
      "Iteration 7532: loss = 0.002618959233049278\n",
      "Iteration 7533: loss = 0.002618946532942648\n",
      "Iteration 7534: loss = 0.00261893383324301\n",
      "Iteration 7535: loss = 0.0026189211339501125\n",
      "Iteration 7536: loss = 0.002618908435063702\n",
      "Iteration 7537: loss = 0.002618895736583528\n",
      "Iteration 7538: loss = 0.0026188830385093383\n",
      "Iteration 7539: loss = 0.00261887034084088\n",
      "Iteration 7540: loss = 0.002618857643577904\n",
      "Iteration 7541: loss = 0.0026188449467201584\n",
      "Iteration 7542: loss = 0.0026188322502673916\n",
      "Iteration 7543: loss = 0.0026188195542193545\n",
      "Iteration 7544: loss = 0.0026188068585757955\n",
      "Iteration 7545: loss = 0.002618794163336466\n",
      "Iteration 7546: loss = 0.002618781468501115\n",
      "Iteration 7547: loss = 0.0026187687740694934\n",
      "Iteration 7548: loss = 0.002618756080041351\n",
      "Iteration 7549: loss = 0.002618743386416439\n",
      "Iteration 7550: loss = 0.0026187306931945097\n",
      "Iteration 7551: loss = 0.002618718000375313\n",
      "Iteration 7552: loss = 0.0026187053079586007\n",
      "Iteration 7553: loss = 0.002618692615944125\n",
      "Iteration 7554: loss = 0.0026186799243316373\n",
      "Iteration 7555: loss = 0.0026186672331208905\n",
      "Iteration 7556: loss = 0.0026186545423116368\n",
      "Iteration 7557: loss = 0.0026186418519036285\n",
      "Iteration 7558: loss = 0.0026186291618966184\n",
      "Iteration 7559: loss = 0.0026186164722903607\n",
      "Iteration 7560: loss = 0.0026186037830846073\n",
      "Iteration 7561: loss = 0.002618591094279113\n",
      "Iteration 7562: loss = 0.002618578405873631\n",
      "Iteration 7563: loss = 0.002618565717867916\n",
      "Iteration 7564: loss = 0.002618553030261721\n",
      "Iteration 7565: loss = 0.0026185403430548015\n",
      "Iteration 7566: loss = 0.0026185276562469117\n",
      "Iteration 7567: loss = 0.0026185149698378075\n",
      "Iteration 7568: loss = 0.002618502283827243\n",
      "Iteration 7569: loss = 0.002618489598214974\n",
      "Iteration 7570: loss = 0.002618476913000757\n",
      "Iteration 7571: loss = 0.0026184642281843456\n",
      "Iteration 7572: loss = 0.002618451543765498\n",
      "Iteration 7573: loss = 0.00261843885974397\n",
      "Iteration 7574: loss = 0.0026184261761195166\n",
      "Iteration 7575: loss = 0.002618413492891897\n",
      "Iteration 7576: loss = 0.002618400810060867\n",
      "Iteration 7577: loss = 0.002618388127626183\n",
      "Iteration 7578: loss = 0.002618375445587604\n",
      "Iteration 7579: loss = 0.0026183627639448865\n",
      "Iteration 7580: loss = 0.002618350082697789\n",
      "Iteration 7581: loss = 0.0026183374018460687\n",
      "Iteration 7582: loss = 0.002618324721389485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7583: loss = 0.002618312041327796\n",
      "Iteration 7584: loss = 0.0026182993616607605\n",
      "Iteration 7585: loss = 0.002618286682388137\n",
      "Iteration 7586: loss = 0.002618274003509685\n",
      "Iteration 7587: loss = 0.0026182613250251654\n",
      "Iteration 7588: loss = 0.002618248646934335\n",
      "Iteration 7589: loss = 0.0026182359692369563\n",
      "Iteration 7590: loss = 0.0026182232919327872\n",
      "Iteration 7591: loss = 0.0026182106150215902\n",
      "Iteration 7592: loss = 0.0026181979385031242\n",
      "Iteration 7593: loss = 0.0026181852623771503\n",
      "Iteration 7594: loss = 0.0026181725866434294\n",
      "Iteration 7595: loss = 0.002618159911301724\n",
      "Iteration 7596: loss = 0.002618147236351793\n",
      "Iteration 7597: loss = 0.0026181345617934007\n",
      "Iteration 7598: loss = 0.0026181218876263073\n",
      "Iteration 7599: loss = 0.0026181092138502756\n",
      "Iteration 7600: loss = 0.0026180965404650675\n",
      "Iteration 7601: loss = 0.0026180838674704455\n",
      "Iteration 7602: loss = 0.0026180711948661727\n",
      "Iteration 7603: loss = 0.0026180585226520114\n",
      "Iteration 7604: loss = 0.002618045850827726\n",
      "Iteration 7605: loss = 0.002618033179393078\n",
      "Iteration 7606: loss = 0.0026180205083478334\n",
      "Iteration 7607: loss = 0.002618007837691754\n",
      "Iteration 7608: loss = 0.0026179951674246043\n",
      "Iteration 7609: loss = 0.0026179824975461493\n",
      "Iteration 7610: loss = 0.0026179698280561522\n",
      "Iteration 7611: loss = 0.0026179571589543794\n",
      "Iteration 7612: loss = 0.002617944490240595\n",
      "Iteration 7613: loss = 0.0026179318219145637\n",
      "Iteration 7614: loss = 0.002617919153976051\n",
      "Iteration 7615: loss = 0.0026179064864248224\n",
      "Iteration 7616: loss = 0.0026178938192606446\n",
      "Iteration 7617: loss = 0.002617881152483283\n",
      "Iteration 7618: loss = 0.0026178684860925032\n",
      "Iteration 7619: loss = 0.002617855820088072\n",
      "Iteration 7620: loss = 0.002617843154469757\n",
      "Iteration 7621: loss = 0.0026178304892373236\n",
      "Iteration 7622: loss = 0.00261781782439054\n",
      "Iteration 7623: loss = 0.0026178051599291727\n",
      "Iteration 7624: loss = 0.00261779249585299\n",
      "Iteration 7625: loss = 0.0026177798321617585\n",
      "Iteration 7626: loss = 0.0026177671688552475\n",
      "Iteration 7627: loss = 0.002617754505933223\n",
      "Iteration 7628: loss = 0.002617741843395456\n",
      "Iteration 7629: loss = 0.0026177291812417135\n",
      "Iteration 7630: loss = 0.002617716519471764\n",
      "Iteration 7631: loss = 0.0026177038580853782\n",
      "Iteration 7632: loss = 0.0026176911970823237\n",
      "Iteration 7633: loss = 0.00261767853646237\n",
      "Iteration 7634: loss = 0.002617665876225288\n",
      "Iteration 7635: loss = 0.0026176532163708463\n",
      "Iteration 7636: loss = 0.0026176405568988145\n",
      "Iteration 7637: loss = 0.002617627897808965\n",
      "Iteration 7638: loss = 0.0026176152391010664\n",
      "Iteration 7639: loss = 0.00261760258077489\n",
      "Iteration 7640: loss = 0.002617589922830207\n",
      "Iteration 7641: loss = 0.0026175772652667884\n",
      "Iteration 7642: loss = 0.002617564608084405\n",
      "Iteration 7643: loss = 0.0026175519512828286\n",
      "Iteration 7644: loss = 0.0026175392948618315\n",
      "Iteration 7645: loss = 0.0026175266388211845\n",
      "Iteration 7646: loss = 0.002617513983160661\n",
      "Iteration 7647: loss = 0.0026175013278800325\n",
      "Iteration 7648: loss = 0.0026174886729790725\n",
      "Iteration 7649: loss = 0.0026174760184575524\n",
      "Iteration 7650: loss = 0.0026174633643152472\n",
      "Iteration 7651: loss = 0.0026174507105519284\n",
      "Iteration 7652: loss = 0.00261743805716737\n",
      "Iteration 7653: loss = 0.0026174254041613455\n",
      "Iteration 7654: loss = 0.0026174127515336287\n",
      "Iteration 7655: loss = 0.002617400099283994\n",
      "Iteration 7656: loss = 0.002617387447412216\n",
      "Iteration 7657: loss = 0.0026173747959180676\n",
      "Iteration 7658: loss = 0.0026173621448013253\n",
      "Iteration 7659: loss = 0.0026173494940617634\n",
      "Iteration 7660: loss = 0.002617336843699156\n",
      "Iteration 7661: loss = 0.0026173241937132797\n",
      "Iteration 7662: loss = 0.0026173115441039097\n",
      "Iteration 7663: loss = 0.002617298894870821\n",
      "Iteration 7664: loss = 0.0026172862460137904\n",
      "Iteration 7665: loss = 0.0026172735975325935\n",
      "Iteration 7666: loss = 0.0026172609494270072\n",
      "Iteration 7667: loss = 0.002617248301696807\n",
      "Iteration 7668: loss = 0.0026172356543417707\n",
      "Iteration 7669: loss = 0.0026172230073616743\n",
      "Iteration 7670: loss = 0.002617210360756296\n",
      "Iteration 7671: loss = 0.0026171977145254123\n",
      "Iteration 7672: loss = 0.0026171850686688013\n",
      "Iteration 7673: loss = 0.002617172423186241\n",
      "Iteration 7674: loss = 0.0026171597780775087\n",
      "Iteration 7675: loss = 0.0026171471333423825\n",
      "Iteration 7676: loss = 0.002617134488980641\n",
      "Iteration 7677: loss = 0.0026171218449920633\n",
      "Iteration 7678: loss = 0.0026171092013764275\n",
      "Iteration 7679: loss = 0.002617096558133513\n",
      "Iteration 7680: loss = 0.002617083915263099\n",
      "Iteration 7681: loss = 0.002617071272764965\n",
      "Iteration 7682: loss = 0.0026170586306388903\n",
      "Iteration 7683: loss = 0.0026170459888846544\n",
      "Iteration 7684: loss = 0.0026170333475020384\n",
      "Iteration 7685: loss = 0.0026170207064908214\n",
      "Iteration 7686: loss = 0.0026170080658507846\n",
      "Iteration 7687: loss = 0.002616995425581708\n",
      "Iteration 7688: loss = 0.0026169827856833724\n",
      "Iteration 7689: loss = 0.0026169701461555596\n",
      "Iteration 7690: loss = 0.0026169575069980495\n",
      "Iteration 7691: loss = 0.0026169448682106257\n",
      "Iteration 7692: loss = 0.002616932229793068\n",
      "Iteration 7693: loss = 0.0026169195917451586\n",
      "Iteration 7694: loss = 0.00261690695406668\n",
      "Iteration 7695: loss = 0.002616894316757414\n",
      "Iteration 7696: loss = 0.0026168816798171424\n",
      "Iteration 7697: loss = 0.002616869043245649\n",
      "Iteration 7698: loss = 0.0026168564070427167\n",
      "Iteration 7699: loss = 0.002616843771208127\n",
      "Iteration 7700: loss = 0.0026168311357416653\n",
      "Iteration 7701: loss = 0.0026168185006431133\n",
      "Iteration 7702: loss = 0.002616805865912256\n",
      "Iteration 7703: loss = 0.002616793231548875\n",
      "Iteration 7704: loss = 0.002616780597552757\n",
      "Iteration 7705: loss = 0.0026167679639236847\n",
      "Iteration 7706: loss = 0.0026167553306614424\n",
      "Iteration 7707: loss = 0.002616742697765816\n",
      "Iteration 7708: loss = 0.002616730065236589\n",
      "Iteration 7709: loss = 0.0026167174330735473\n",
      "Iteration 7710: loss = 0.0026167048012764754\n",
      "Iteration 7711: loss = 0.002616692169845159\n",
      "Iteration 7712: loss = 0.0026166795387793837\n",
      "Iteration 7713: loss = 0.0026166669080789364\n",
      "Iteration 7714: loss = 0.002616654277743601\n",
      "Iteration 7715: loss = 0.0026166416477731652\n",
      "Iteration 7716: loss = 0.002616629018167415\n",
      "Iteration 7717: loss = 0.002616616388926137\n",
      "Iteration 7718: loss = 0.0026166037600491186\n",
      "Iteration 7719: loss = 0.002616591131536146\n",
      "Iteration 7720: loss = 0.0026165785033870066\n",
      "Iteration 7721: loss = 0.0026165658756014875\n",
      "Iteration 7722: loss = 0.002616553248179376\n",
      "Iteration 7723: loss = 0.0026165406211204616\n",
      "Iteration 7724: loss = 0.0026165279944245306\n",
      "Iteration 7725: loss = 0.002616515368091372\n",
      "Iteration 7726: loss = 0.0026165027421207734\n",
      "Iteration 7727: loss = 0.0026164901165125244\n",
      "Iteration 7728: loss = 0.0026164774912664133\n",
      "Iteration 7729: loss = 0.0026164648663822287\n",
      "Iteration 7730: loss = 0.00261645224185976\n",
      "Iteration 7731: loss = 0.002616439617698796\n",
      "Iteration 7732: loss = 0.0026164269938991278\n",
      "Iteration 7733: loss = 0.0026164143704605434\n",
      "Iteration 7734: loss = 0.0026164017473828335\n",
      "Iteration 7735: loss = 0.0026163891246657888\n",
      "Iteration 7736: loss = 0.002616376502309198\n",
      "Iteration 7737: loss = 0.002616363880312853\n",
      "Iteration 7738: loss = 0.002616351258676544\n",
      "Iteration 7739: loss = 0.0026163386374000613\n",
      "Iteration 7740: loss = 0.0026163260164831976\n",
      "Iteration 7741: loss = 0.002616313395925742\n",
      "Iteration 7742: loss = 0.002616300775727488\n",
      "Iteration 7743: loss = 0.0026162881558882264\n",
      "Iteration 7744: loss = 0.002616275536407749\n",
      "Iteration 7745: loss = 0.0026162629172858474\n",
      "Iteration 7746: loss = 0.002616250298522315\n",
      "Iteration 7747: loss = 0.002616237680116943\n",
      "Iteration 7748: loss = 0.002616225062069525\n",
      "Iteration 7749: loss = 0.0026162124443798525\n",
      "Iteration 7750: loss = 0.00261619982704772\n",
      "Iteration 7751: loss = 0.0026161872100729194\n",
      "Iteration 7752: loss = 0.002616174593455245\n",
      "Iteration 7753: loss = 0.002616161977194491\n",
      "Iteration 7754: loss = 0.0026161493612904484\n",
      "Iteration 7755: loss = 0.0026161367457429143\n",
      "Iteration 7756: loss = 0.002616124130551681\n",
      "Iteration 7757: loss = 0.002616111515716543\n",
      "Iteration 7758: loss = 0.002616098901237296\n",
      "Iteration 7759: loss = 0.0026160862871137335\n",
      "Iteration 7760: loss = 0.0026160736733456505\n",
      "Iteration 7761: loss = 0.0026160610599328424\n",
      "Iteration 7762: loss = 0.002616048446875104\n",
      "Iteration 7763: loss = 0.0026160358341722314\n",
      "Iteration 7764: loss = 0.0026160232218240204\n",
      "Iteration 7765: loss = 0.002616010609830266\n",
      "Iteration 7766: loss = 0.0026159979981907645\n",
      "Iteration 7767: loss = 0.0026159853869053124\n",
      "Iteration 7768: loss = 0.002615972775973706\n",
      "Iteration 7769: loss = 0.002615960165395742\n",
      "Iteration 7770: loss = 0.002615947555171217\n",
      "Iteration 7771: loss = 0.002615934945299928\n",
      "Iteration 7772: loss = 0.0026159223357816713\n",
      "Iteration 7773: loss = 0.002615909726616246\n",
      "Iteration 7774: loss = 0.0026158971178034483\n",
      "Iteration 7775: loss = 0.0026158845093430766\n",
      "Iteration 7776: loss = 0.002615871901234928\n",
      "Iteration 7777: loss = 0.002615859293478801\n",
      "Iteration 7778: loss = 0.0026158466860744935\n",
      "Iteration 7779: loss = 0.002615834079021805\n",
      "Iteration 7780: loss = 0.002615821472320534\n",
      "Iteration 7781: loss = 0.0026158088659704783\n",
      "Iteration 7782: loss = 0.0026157962599714377\n",
      "Iteration 7783: loss = 0.0026157836543232106\n",
      "Iteration 7784: loss = 0.002615771049025597\n",
      "Iteration 7785: loss = 0.0026157584440783966\n",
      "Iteration 7786: loss = 0.002615745839481409\n",
      "Iteration 7787: loss = 0.002615733235234434\n",
      "Iteration 7788: loss = 0.0026157206313372713\n",
      "Iteration 7789: loss = 0.002615708027789722\n",
      "Iteration 7790: loss = 0.0026156954245915858\n",
      "Iteration 7791: loss = 0.0026156828217426646\n",
      "Iteration 7792: loss = 0.0026156702192427573\n",
      "Iteration 7793: loss = 0.0026156576170916672\n",
      "Iteration 7794: loss = 0.002615645015289194\n",
      "Iteration 7795: loss = 0.0026156324138351394\n",
      "Iteration 7796: loss = 0.0026156198127293053\n",
      "Iteration 7797: loss = 0.002615607211971493\n",
      "Iteration 7798: loss = 0.0026155946115615047\n",
      "Iteration 7799: loss = 0.0026155820114991428\n",
      "Iteration 7800: loss = 0.002615569411784209\n",
      "Iteration 7801: loss = 0.0026155568124165062\n",
      "Iteration 7802: loss = 0.002615544213395837\n",
      "Iteration 7803: loss = 0.002615531614722005\n",
      "Iteration 7804: loss = 0.0026155190163948115\n",
      "Iteration 7805: loss = 0.002615506418414061\n",
      "Iteration 7806: loss = 0.0026154938207795563\n",
      "Iteration 7807: loss = 0.002615481223491102\n",
      "Iteration 7808: loss = 0.0026154686265485003\n",
      "Iteration 7809: loss = 0.002615456029951557\n",
      "Iteration 7810: loss = 0.002615443433700075\n",
      "Iteration 7811: loss = 0.0026154308377938583\n",
      "Iteration 7812: loss = 0.0026154182422327117\n",
      "Iteration 7813: loss = 0.0026154056470164403\n",
      "Iteration 7814: loss = 0.002615393052144849\n",
      "Iteration 7815: loss = 0.002615380457617743\n",
      "Iteration 7816: loss = 0.0026153678634349264\n",
      "Iteration 7817: loss = 0.002615355269596205\n",
      "Iteration 7818: loss = 0.002615342676101385\n",
      "Iteration 7819: loss = 0.002615330082950272\n",
      "Iteration 7820: loss = 0.002615317490142671\n",
      "Iteration 7821: loss = 0.0026153048976783892\n",
      "Iteration 7822: loss = 0.0026152923055572323\n",
      "Iteration 7823: loss = 0.0026152797137790067\n",
      "Iteration 7824: loss = 0.0026152671223435195\n",
      "Iteration 7825: loss = 0.0026152545312505774\n",
      "Iteration 7826: loss = 0.0026152419404999864\n",
      "Iteration 7827: loss = 0.0026152293500915553\n",
      "Iteration 7828: loss = 0.0026152167600250902\n",
      "Iteration 7829: loss = 0.0026152041703003987\n",
      "Iteration 7830: loss = 0.0026151915809172897\n",
      "Iteration 7831: loss = 0.0026151789918755696\n",
      "Iteration 7832: loss = 0.0026151664031750467\n",
      "Iteration 7833: loss = 0.0026151538148155304\n",
      "Iteration 7834: loss = 0.0026151412267968284\n",
      "Iteration 7835: loss = 0.002615128639118749\n",
      "Iteration 7836: loss = 0.0026151160517811005\n",
      "Iteration 7837: loss = 0.0026151034647836926\n",
      "Iteration 7838: loss = 0.0026150908781263347\n",
      "Iteration 7839: loss = 0.002615078291808835\n",
      "Iteration 7840: loss = 0.0026150657058310045\n",
      "Iteration 7841: loss = 0.002615053120192652\n",
      "Iteration 7842: loss = 0.0026150405348935864\n",
      "Iteration 7843: loss = 0.0026150279499336185\n",
      "Iteration 7844: loss = 0.0026150153653125593\n",
      "Iteration 7845: loss = 0.002615002781030218\n",
      "Iteration 7846: loss = 0.002614990197086405\n",
      "Iteration 7847: loss = 0.0026149776134809324\n",
      "Iteration 7848: loss = 0.0026149650302136093\n",
      "Iteration 7849: loss = 0.002614952447284248\n",
      "Iteration 7850: loss = 0.002614939864692659\n",
      "Iteration 7851: loss = 0.002614927282438654\n",
      "Iteration 7852: loss = 0.0026149147005220448\n",
      "Iteration 7853: loss = 0.002614902118942642\n",
      "Iteration 7854: loss = 0.0026148895377002585\n",
      "Iteration 7855: loss = 0.002614876956794706\n",
      "Iteration 7856: loss = 0.002614864376225797\n",
      "Iteration 7857: loss = 0.0026148517959933447\n",
      "Iteration 7858: loss = 0.0026148392160971596\n",
      "Iteration 7859: loss = 0.002614826636537056\n",
      "Iteration 7860: loss = 0.0026148140573128476\n",
      "Iteration 7861: loss = 0.002614801478424345\n",
      "Iteration 7862: loss = 0.0026147888998713635\n",
      "Iteration 7863: loss = 0.0026147763216537163\n",
      "Iteration 7864: loss = 0.002614763743771215\n",
      "Iteration 7865: loss = 0.0026147511662236774\n",
      "Iteration 7866: loss = 0.002614738589010914\n",
      "Iteration 7867: loss = 0.0026147260121327396\n",
      "Iteration 7868: loss = 0.0026147134355889697\n",
      "Iteration 7869: loss = 0.002614700859379418\n",
      "Iteration 7870: loss = 0.002614688283503899\n",
      "Iteration 7871: loss = 0.0026146757079622274\n",
      "Iteration 7872: loss = 0.0026146631327542192\n",
      "Iteration 7873: loss = 0.0026146505578796887\n",
      "Iteration 7874: loss = 0.002614637983338451\n",
      "Iteration 7875: loss = 0.0026146254091303223\n",
      "Iteration 7876: loss = 0.0026146128352551178\n",
      "Iteration 7877: loss = 0.0026146002617126536\n",
      "Iteration 7878: loss = 0.002614587688502746\n",
      "Iteration 7879: loss = 0.0026145751156252104\n",
      "Iteration 7880: loss = 0.0026145625430798636\n",
      "Iteration 7881: loss = 0.002614549970866522\n",
      "Iteration 7882: loss = 0.0026145373989850023\n",
      "Iteration 7883: loss = 0.0026145248274351217\n",
      "Iteration 7884: loss = 0.002614512256216697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7885: loss = 0.0026144996853295446\n",
      "Iteration 7886: loss = 0.0026144871147734832\n",
      "Iteration 7887: loss = 0.0026144745445483295\n",
      "Iteration 7888: loss = 0.002614461974653901\n",
      "Iteration 7889: loss = 0.0026144494050900167\n",
      "Iteration 7890: loss = 0.0026144368358564938\n",
      "Iteration 7891: loss = 0.0026144242669531503\n",
      "Iteration 7892: loss = 0.0026144116983798046\n",
      "Iteration 7893: loss = 0.0026143991301362758\n",
      "Iteration 7894: loss = 0.002614386562222382\n",
      "Iteration 7895: loss = 0.002614373994637943\n",
      "Iteration 7896: loss = 0.0026143614273827762\n",
      "Iteration 7897: loss = 0.0026143488604567023\n",
      "Iteration 7898: loss = 0.00261433629385954\n",
      "Iteration 7899: loss = 0.0026143237275911093\n",
      "Iteration 7900: loss = 0.002614311161651229\n",
      "Iteration 7901: loss = 0.0026142985960397203\n",
      "Iteration 7902: loss = 0.002614286030756402\n",
      "Iteration 7903: loss = 0.002614273465801094\n",
      "Iteration 7904: loss = 0.002614260901173619\n",
      "Iteration 7905: loss = 0.0026142483368737945\n",
      "Iteration 7906: loss = 0.0026142357729014433\n",
      "Iteration 7907: loss = 0.0026142232092563854\n",
      "Iteration 7908: loss = 0.0026142106459384417\n",
      "Iteration 7909: loss = 0.002614198082947434\n",
      "Iteration 7910: loss = 0.0026141855202831826\n",
      "Iteration 7911: loss = 0.0026141729579455104\n",
      "Iteration 7912: loss = 0.002614160395934239\n",
      "Iteration 7913: loss = 0.0026141478342491887\n",
      "Iteration 7914: loss = 0.002614135272890183\n",
      "Iteration 7915: loss = 0.0026141227118570425\n",
      "Iteration 7916: loss = 0.0026141101511495918\n",
      "Iteration 7917: loss = 0.002614097590767651\n",
      "Iteration 7918: loss = 0.0026140850307110448\n",
      "Iteration 7919: loss = 0.0026140724709795946\n",
      "Iteration 7920: loss = 0.002614059911573124\n",
      "Iteration 7921: loss = 0.002614047352491456\n",
      "Iteration 7922: loss = 0.002614034793734414\n",
      "Iteration 7923: loss = 0.0026140222353018216\n",
      "Iteration 7924: loss = 0.002614009677193502\n",
      "Iteration 7925: loss = 0.002613997119409279\n",
      "Iteration 7926: loss = 0.002613984561948977\n",
      "Iteration 7927: loss = 0.0026139720048124204\n",
      "Iteration 7928: loss = 0.002613959447999432\n",
      "Iteration 7929: loss = 0.0026139468915098384\n",
      "Iteration 7930: loss = 0.0026139343353434627\n",
      "Iteration 7931: loss = 0.0026139217795001293\n",
      "Iteration 7932: loss = 0.002613909223979665\n",
      "Iteration 7933: loss = 0.0026138966687818924\n",
      "Iteration 7934: loss = 0.002613884113906639\n",
      "Iteration 7935: loss = 0.0026138715593537285\n",
      "Iteration 7936: loss = 0.0026138590051229876\n",
      "Iteration 7937: loss = 0.002613846451214242\n",
      "Iteration 7938: loss = 0.002613833897627317\n",
      "Iteration 7939: loss = 0.002613821344362039\n",
      "Iteration 7940: loss = 0.0026138087914182343\n",
      "Iteration 7941: loss = 0.0026137962387957287\n",
      "Iteration 7942: loss = 0.0026137836864943494\n",
      "Iteration 7943: loss = 0.002613771134513923\n",
      "Iteration 7944: loss = 0.0026137585828542756\n",
      "Iteration 7945: loss = 0.002613746031515235\n",
      "Iteration 7946: loss = 0.0026137334804966278\n",
      "Iteration 7947: loss = 0.0026137209297982826\n",
      "Iteration 7948: loss = 0.002613708379420025\n",
      "Iteration 7949: loss = 0.002613695829361684\n",
      "Iteration 7950: loss = 0.0026136832796230867\n",
      "Iteration 7951: loss = 0.002613670730204062\n",
      "Iteration 7952: loss = 0.002613658181104437\n",
      "Iteration 7953: loss = 0.0026136456323240403\n",
      "Iteration 7954: loss = 0.0026136330838627\n",
      "Iteration 7955: loss = 0.002613620535720245\n",
      "Iteration 7956: loss = 0.0026136079878965046\n",
      "Iteration 7957: loss = 0.0026135954403913074\n",
      "Iteration 7958: loss = 0.002613582893204482\n",
      "Iteration 7959: loss = 0.002613570346335858\n",
      "Iteration 7960: loss = 0.0026135577997852644\n",
      "Iteration 7961: loss = 0.002613545253552531\n",
      "Iteration 7962: loss = 0.0026135327076374872\n",
      "Iteration 7963: loss = 0.0026135201620399637\n",
      "Iteration 7964: loss = 0.0026135076167597897\n",
      "Iteration 7965: loss = 0.0026134950717967955\n",
      "Iteration 7966: loss = 0.002613482527150812\n",
      "Iteration 7967: loss = 0.002613469982821669\n",
      "Iteration 7968: loss = 0.002613457438809197\n",
      "Iteration 7969: loss = 0.002613444895113228\n",
      "Iteration 7970: loss = 0.0026134323517335908\n",
      "Iteration 7971: loss = 0.0026134198086701184\n",
      "Iteration 7972: loss = 0.002613407265922641\n",
      "Iteration 7973: loss = 0.0026133947234909904\n",
      "Iteration 7974: loss = 0.002613382181374998\n",
      "Iteration 7975: loss = 0.0026133696395744966\n",
      "Iteration 7976: loss = 0.0026133570980893164\n",
      "Iteration 7977: loss = 0.0026133445569192903\n",
      "Iteration 7978: loss = 0.0026133320160642505\n",
      "Iteration 7979: loss = 0.0026133194755240284\n",
      "Iteration 7980: loss = 0.002613306935298458\n",
      "Iteration 7981: loss = 0.0026132943953873712\n",
      "Iteration 7982: loss = 0.0026132818557906\n",
      "Iteration 7983: loss = 0.0026132693165079788\n",
      "Iteration 7984: loss = 0.0026132567775393398\n",
      "Iteration 7985: loss = 0.0026132442388845166\n",
      "Iteration 7986: loss = 0.002613231700543343\n",
      "Iteration 7987: loss = 0.002613219162515651\n",
      "Iteration 7988: loss = 0.0026132066248012752\n",
      "Iteration 7989: loss = 0.002613194087400051\n",
      "Iteration 7990: loss = 0.0026131815503118105\n",
      "Iteration 7991: loss = 0.002613169013536388\n",
      "Iteration 7992: loss = 0.002613156477073618\n",
      "Iteration 7993: loss = 0.002613143940923336\n",
      "Iteration 7994: loss = 0.0026131314050853753\n",
      "Iteration 7995: loss = 0.0026131188695595713\n",
      "Iteration 7996: loss = 0.0026131063343457586\n",
      "Iteration 7997: loss = 0.0026130937994437733\n",
      "Iteration 7998: loss = 0.0026130812648534497\n",
      "Iteration 7999: loss = 0.002613068730574623\n",
      "Iteration 8000: loss = 0.0026130561966071285\n",
      "Iteration 8001: loss = 0.0026130436629508034\n",
      "Iteration 8002: loss = 0.002613031129605482\n",
      "Iteration 8003: loss = 0.0026130185965710013\n",
      "Iteration 8004: loss = 0.002613006063847197\n",
      "Iteration 8005: loss = 0.0026129935314339056\n",
      "Iteration 8006: loss = 0.002612980999330963\n",
      "Iteration 8007: loss = 0.0026129684675382067\n",
      "Iteration 8008: loss = 0.002612955936055472\n",
      "Iteration 8009: loss = 0.0026129434048825975\n",
      "Iteration 8010: loss = 0.002612930874019419\n",
      "Iteration 8011: loss = 0.0026129183434657746\n",
      "Iteration 8012: loss = 0.0026129058132215006\n",
      "Iteration 8013: loss = 0.0026128932832864354\n",
      "Iteration 8014: loss = 0.0026128807536604163\n",
      "Iteration 8015: loss = 0.002612868224343281\n",
      "Iteration 8016: loss = 0.0026128556953348677\n",
      "Iteration 8017: loss = 0.002612843166635014\n",
      "Iteration 8018: loss = 0.0026128306382435587\n",
      "Iteration 8019: loss = 0.0026128181101603403\n",
      "Iteration 8020: loss = 0.0026128055823851962\n",
      "Iteration 8021: loss = 0.002612793054917967\n",
      "Iteration 8022: loss = 0.002612780527758489\n",
      "Iteration 8023: loss = 0.002612768000906603\n",
      "Iteration 8024: loss = 0.0026127554743621477\n",
      "Iteration 8025: loss = 0.002612742948124963\n",
      "Iteration 8026: loss = 0.002612730422194887\n",
      "Iteration 8027: loss = 0.00261271789657176\n",
      "Iteration 8028: loss = 0.002612705371255422\n",
      "Iteration 8029: loss = 0.0026126928462457118\n",
      "Iteration 8030: loss = 0.0026126803215424706\n",
      "Iteration 8031: loss = 0.0026126677971455383\n",
      "Iteration 8032: loss = 0.0026126552730547548\n",
      "Iteration 8033: loss = 0.002612642749269961\n",
      "Iteration 8034: loss = 0.002612630225790996\n",
      "Iteration 8035: loss = 0.0026126177026177034\n",
      "Iteration 8036: loss = 0.0026126051797499216\n",
      "Iteration 8037: loss = 0.0026125926571874923\n",
      "Iteration 8038: loss = 0.002612580134930258\n",
      "Iteration 8039: loss = 0.0026125676129780577\n",
      "Iteration 8040: loss = 0.0026125550913307346\n",
      "Iteration 8041: loss = 0.00261254256998813\n",
      "Iteration 8042: loss = 0.002612530048950085\n",
      "Iteration 8043: loss = 0.0026125175282164423\n",
      "Iteration 8044: loss = 0.002612505007787044\n",
      "Iteration 8045: loss = 0.0026124924876617314\n",
      "Iteration 8046: loss = 0.0026124799678403474\n",
      "Iteration 8047: loss = 0.0026124674483227342\n",
      "Iteration 8048: loss = 0.002612454929108735\n",
      "Iteration 8049: loss = 0.0026124424101981922\n",
      "Iteration 8050: loss = 0.0026124298915909494\n",
      "Iteration 8051: loss = 0.002612417373286848\n",
      "Iteration 8052: loss = 0.0026124048552857333\n",
      "Iteration 8053: loss = 0.002612392337587447\n",
      "Iteration 8054: loss = 0.002612379820191833\n",
      "Iteration 8055: loss = 0.0026123673030987353\n",
      "Iteration 8056: loss = 0.0026123547863079978\n",
      "Iteration 8057: loss = 0.002612342269819464\n",
      "Iteration 8058: loss = 0.002612329753632979\n",
      "Iteration 8059: loss = 0.002612317237748385\n",
      "Iteration 8060: loss = 0.0026123047221655285\n",
      "Iteration 8061: loss = 0.002612292206884252\n",
      "Iteration 8062: loss = 0.0026122796919044013\n",
      "Iteration 8063: loss = 0.002612267177225822\n",
      "Iteration 8064: loss = 0.0026122546628483574\n",
      "Iteration 8065: loss = 0.002612242148771854\n",
      "Iteration 8066: loss = 0.002612229634996155\n",
      "Iteration 8067: loss = 0.002612217121521108\n",
      "Iteration 8068: loss = 0.0026122046083465575\n",
      "Iteration 8069: loss = 0.002612192095472349\n",
      "Iteration 8070: loss = 0.002612179582898328\n",
      "Iteration 8071: loss = 0.0026121670706243413\n",
      "Iteration 8072: loss = 0.0026121545586502344\n",
      "Iteration 8073: loss = 0.0026121420469758542\n",
      "Iteration 8074: loss = 0.002612129535601046\n",
      "Iteration 8075: loss = 0.0026121170245256564\n",
      "Iteration 8076: loss = 0.002612104513749533\n",
      "Iteration 8077: loss = 0.002612092003272522\n",
      "Iteration 8078: loss = 0.00261207949309447\n",
      "Iteration 8079: loss = 0.0026120669832152246\n",
      "Iteration 8080: loss = 0.0026120544736346328\n",
      "Iteration 8081: loss = 0.0026120419643525412\n",
      "Iteration 8082: loss = 0.0026120294553687983\n",
      "Iteration 8083: loss = 0.0026120169466832517\n",
      "Iteration 8084: loss = 0.0026120044382957485\n",
      "Iteration 8085: loss = 0.002611991930206137\n",
      "Iteration 8086: loss = 0.0026119794224142654\n",
      "Iteration 8087: loss = 0.0026119669149199808\n",
      "Iteration 8088: loss = 0.002611954407723133\n",
      "Iteration 8089: loss = 0.00261194190082357\n",
      "Iteration 8090: loss = 0.0026119293942211395\n",
      "Iteration 8091: loss = 0.002611916887915691\n",
      "Iteration 8092: loss = 0.002611904381907073\n",
      "Iteration 8093: loss = 0.0026118918761951354\n",
      "Iteration 8094: loss = 0.0026118793707797265\n",
      "Iteration 8095: loss = 0.0026118668656606955\n",
      "Iteration 8096: loss = 0.0026118543608378924\n",
      "Iteration 8097: loss = 0.0026118418563111662\n",
      "Iteration 8098: loss = 0.0026118293520803674\n",
      "Iteration 8099: loss = 0.0026118168481453446\n",
      "Iteration 8100: loss = 0.002611804344505948\n",
      "Iteration 8101: loss = 0.0026117918411620294\n",
      "Iteration 8102: loss = 0.002611779338113437\n",
      "Iteration 8103: loss = 0.002611766835360023\n",
      "Iteration 8104: loss = 0.002611754332901636\n",
      "Iteration 8105: loss = 0.0026117418307381275\n",
      "Iteration 8106: loss = 0.0026117293288693486\n",
      "Iteration 8107: loss = 0.00261171682729515\n",
      "Iteration 8108: loss = 0.0026117043260153825\n",
      "Iteration 8109: loss = 0.0026116918250298988\n",
      "Iteration 8110: loss = 0.0026116793243385478\n",
      "Iteration 8111: loss = 0.0026116668239411825\n",
      "Iteration 8112: loss = 0.0026116543238376543\n",
      "Iteration 8113: loss = 0.0026116418240278148\n",
      "Iteration 8114: loss = 0.0026116293245115165\n",
      "Iteration 8115: loss = 0.00261161682528861\n",
      "Iteration 8116: loss = 0.002611604326358949\n",
      "Iteration 8117: loss = 0.002611591827722385\n",
      "Iteration 8118: loss = 0.002611579329378771\n",
      "Iteration 8119: loss = 0.002611566831327958\n",
      "Iteration 8120: loss = 0.002611554333569801\n",
      "Iteration 8121: loss = 0.0026115418361041514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8122: loss = 0.0026115293389308617\n",
      "Iteration 8123: loss = 0.0026115168420497866\n",
      "Iteration 8124: loss = 0.0026115043454607778\n",
      "Iteration 8125: loss = 0.0026114918491636904\n",
      "Iteration 8126: loss = 0.002611479353158376\n",
      "Iteration 8127: loss = 0.0026114668574446897\n",
      "Iteration 8128: loss = 0.002611454362022485\n",
      "Iteration 8129: loss = 0.0026114418668916144\n",
      "Iteration 8130: loss = 0.0026114293720519337\n",
      "Iteration 8131: loss = 0.002611416877503297\n",
      "Iteration 8132: loss = 0.002611404383245557\n",
      "Iteration 8133: loss = 0.0026113918892785707\n",
      "Iteration 8134: loss = 0.00261137939560219\n",
      "Iteration 8135: loss = 0.002611366902216271\n",
      "Iteration 8136: loss = 0.0026113544091206685\n",
      "Iteration 8137: loss = 0.0026113419163152373\n",
      "Iteration 8138: loss = 0.0026113294237998324\n",
      "Iteration 8139: loss = 0.0026113169315743097\n",
      "Iteration 8140: loss = 0.0026113044396385236\n",
      "Iteration 8141: loss = 0.002611291947992331\n",
      "Iteration 8142: loss = 0.0026112794566355863\n",
      "Iteration 8143: loss = 0.0026112669655681455\n",
      "Iteration 8144: loss = 0.0026112544747898645\n",
      "Iteration 8145: loss = 0.0026112419843005996\n",
      "Iteration 8146: loss = 0.002611229494100207\n",
      "Iteration 8147: loss = 0.0026112170041885426\n",
      "Iteration 8148: loss = 0.0026112045145654634\n",
      "Iteration 8149: loss = 0.0026111920252308257\n",
      "Iteration 8150: loss = 0.0026111795361844865\n",
      "Iteration 8151: loss = 0.0026111670474263018\n",
      "Iteration 8152: loss = 0.0026111545589561293\n",
      "Iteration 8153: loss = 0.0026111420707738255\n",
      "Iteration 8154: loss = 0.002611129582879249\n",
      "Iteration 8155: loss = 0.0026111170952722554\n",
      "Iteration 8156: loss = 0.002611104607952703\n",
      "Iteration 8157: loss = 0.0026110921209204496\n",
      "Iteration 8158: loss = 0.002611079634175353\n",
      "Iteration 8159: loss = 0.00261106714771727\n",
      "Iteration 8160: loss = 0.00261105466154606\n",
      "Iteration 8161: loss = 0.002611042175661581\n",
      "Iteration 8162: loss = 0.0026110296900636897\n",
      "Iteration 8163: loss = 0.002611017204752247\n",
      "Iteration 8164: loss = 0.002611004719727109\n",
      "Iteration 8165: loss = 0.002610992234988136\n",
      "Iteration 8166: loss = 0.0026109797505351863\n",
      "Iteration 8167: loss = 0.002610967266368118\n",
      "Iteration 8168: loss = 0.0026109547824867915\n",
      "Iteration 8169: loss = 0.002610942298891066\n",
      "Iteration 8170: loss = 0.002610929815580799\n",
      "Iteration 8171: loss = 0.0026109173325558517\n",
      "Iteration 8172: loss = 0.0026109048498160832\n",
      "Iteration 8173: loss = 0.002610892367361353\n",
      "Iteration 8174: loss = 0.002610879885191521\n",
      "Iteration 8175: loss = 0.0026108674033064475\n",
      "Iteration 8176: loss = 0.0026108549217059916\n",
      "Iteration 8177: loss = 0.0026108424403900148\n",
      "Iteration 8178: loss = 0.002610829959358376\n",
      "Iteration 8179: loss = 0.002610817478610937\n",
      "Iteration 8180: loss = 0.0026108049981475577\n",
      "Iteration 8181: loss = 0.0026107925179680988\n",
      "Iteration 8182: loss = 0.0026107800380724216\n",
      "Iteration 8183: loss = 0.0026107675584603864\n",
      "Iteration 8184: loss = 0.0026107550791318553\n",
      "Iteration 8185: loss = 0.0026107426000866888\n",
      "Iteration 8186: loss = 0.0026107301213247483\n",
      "Iteration 8187: loss = 0.0026107176428458957\n",
      "Iteration 8188: loss = 0.002610705164649991\n",
      "Iteration 8189: loss = 0.0026106926867368986\n",
      "Iteration 8190: loss = 0.002610680209106479\n",
      "Iteration 8191: loss = 0.002610667731758593\n",
      "Iteration 8192: loss = 0.002610655254693105\n",
      "Iteration 8193: loss = 0.0026106427779098757\n",
      "Iteration 8194: loss = 0.002610630301408768\n",
      "Iteration 8195: loss = 0.0026106178251896445\n",
      "Iteration 8196: loss = 0.0026106053492523677\n",
      "Iteration 8197: loss = 0.0026105928735968005\n",
      "Iteration 8198: loss = 0.002610580398222805\n",
      "Iteration 8199: loss = 0.0026105679231302444\n",
      "Iteration 8200: loss = 0.0026105554483189827\n",
      "Iteration 8201: loss = 0.002610542973788883\n",
      "Iteration 8202: loss = 0.0026105304995398078\n",
      "Iteration 8203: loss = 0.0026105180255716212\n",
      "Iteration 8204: loss = 0.002610505551884187\n",
      "Iteration 8205: loss = 0.0026104930784773693\n",
      "Iteration 8206: loss = 0.0026104806053510307\n",
      "Iteration 8207: loss = 0.0026104681325050355\n",
      "Iteration 8208: loss = 0.002610455659939249\n",
      "Iteration 8209: loss = 0.0026104431876535343\n",
      "Iteration 8210: loss = 0.0026104307156477563\n",
      "Iteration 8211: loss = 0.0026104182439217792\n",
      "Iteration 8212: loss = 0.0026104057724754674\n",
      "Iteration 8213: loss = 0.0026103933013086867\n",
      "Iteration 8214: loss = 0.0026103808304213014\n",
      "Iteration 8215: loss = 0.0026103683598131754\n",
      "Iteration 8216: loss = 0.002610355889484176\n",
      "Iteration 8217: loss = 0.0026103434194341664\n",
      "Iteration 8218: loss = 0.0026103309496630137\n",
      "Iteration 8219: loss = 0.002610318480170582\n",
      "Iteration 8220: loss = 0.0026103060109567366\n",
      "Iteration 8221: loss = 0.0026102935420213454\n",
      "Iteration 8222: loss = 0.0026102810733642724\n",
      "Iteration 8223: loss = 0.0026102686049853845\n",
      "Iteration 8224: loss = 0.002610256136884547\n",
      "Iteration 8225: loss = 0.002610243669061627\n",
      "Iteration 8226: loss = 0.0026102312015164903\n",
      "Iteration 8227: loss = 0.002610218734249003\n",
      "Iteration 8228: loss = 0.0026102062672590325\n",
      "Iteration 8229: loss = 0.002610193800546445\n",
      "Iteration 8230: loss = 0.002610181334111108\n",
      "Iteration 8231: loss = 0.002610168867952888\n",
      "Iteration 8232: loss = 0.0026101564020716513\n",
      "Iteration 8233: loss = 0.0026101439364672667\n",
      "Iteration 8234: loss = 0.0026101314711396003\n",
      "Iteration 8235: loss = 0.0026101190060885205\n",
      "Iteration 8236: loss = 0.0026101065413138935\n",
      "Iteration 8237: loss = 0.002610094076815589\n",
      "Iteration 8238: loss = 0.0026100816125934722\n",
      "Iteration 8239: loss = 0.002610069148647414\n",
      "Iteration 8240: loss = 0.0026100566849772797\n",
      "Iteration 8241: loss = 0.0026100442215829397\n",
      "Iteration 8242: loss = 0.0026100317584642607\n",
      "Iteration 8243: loss = 0.002610019295621112\n",
      "Iteration 8244: loss = 0.0026100068330533618\n",
      "Iteration 8245: loss = 0.0026099943707608786\n",
      "Iteration 8246: loss = 0.0026099819087435317\n",
      "Iteration 8247: loss = 0.0026099694470011897\n",
      "Iteration 8248: loss = 0.0026099569855337216\n",
      "Iteration 8249: loss = 0.0026099445243409963\n",
      "Iteration 8250: loss = 0.002609932063422884\n",
      "Iteration 8251: loss = 0.0026099196027792528\n",
      "Iteration 8252: loss = 0.002609907142409973\n",
      "Iteration 8253: loss = 0.0026098946823149144\n",
      "Iteration 8254: loss = 0.002609882222493946\n",
      "Iteration 8255: loss = 0.002609869762946938\n",
      "Iteration 8256: loss = 0.0026098573036737608\n",
      "Iteration 8257: loss = 0.002609844844674284\n",
      "Iteration 8258: loss = 0.0026098323859483777\n",
      "Iteration 8259: loss = 0.0026098199274959127\n",
      "Iteration 8260: loss = 0.0026098074693167586\n",
      "Iteration 8261: loss = 0.0026097950114107875\n",
      "Iteration 8262: loss = 0.0026097825537778683\n",
      "Iteration 8263: loss = 0.0026097700964178737\n",
      "Iteration 8264: loss = 0.0026097576393306733\n",
      "Iteration 8265: loss = 0.0026097451825161377\n",
      "Iteration 8266: loss = 0.0026097327259741393\n",
      "Iteration 8267: loss = 0.002609720269704549\n",
      "Iteration 8268: loss = 0.002609707813707238\n",
      "Iteration 8269: loss = 0.002609695357982078\n",
      "Iteration 8270: loss = 0.002609682902528941\n",
      "Iteration 8271: loss = 0.002609670447347697\n",
      "Iteration 8272: loss = 0.0026096579924382206\n",
      "Iteration 8273: loss = 0.0026096455378003818\n",
      "Iteration 8274: loss = 0.002609633083434053\n",
      "Iteration 8275: loss = 0.0026096206293391073\n",
      "Iteration 8276: loss = 0.0026096081755154154\n",
      "Iteration 8277: loss = 0.002609595721962851\n",
      "Iteration 8278: loss = 0.0026095832686812877\n",
      "Iteration 8279: loss = 0.0026095708156705965\n",
      "Iteration 8280: loss = 0.00260955836293065\n",
      "Iteration 8281: loss = 0.0026095459104613222\n",
      "Iteration 8282: loss = 0.0026095334582624855\n",
      "Iteration 8283: loss = 0.0026095210063340132\n",
      "Iteration 8284: loss = 0.002609508554675779\n",
      "Iteration 8285: loss = 0.0026094961032876555\n",
      "Iteration 8286: loss = 0.002609483652169517\n",
      "Iteration 8287: loss = 0.0026094712013212365\n",
      "Iteration 8288: loss = 0.0026094587507426883\n",
      "Iteration 8289: loss = 0.0026094463004337457\n",
      "Iteration 8290: loss = 0.0026094338503942827\n",
      "Iteration 8291: loss = 0.002609421400624174\n",
      "Iteration 8292: loss = 0.0026094089511232935\n",
      "Iteration 8293: loss = 0.0026093965018915153\n",
      "Iteration 8294: loss = 0.0026093840529287132\n",
      "Iteration 8295: loss = 0.0026093716042347626\n",
      "Iteration 8296: loss = 0.0026093591558095383\n",
      "Iteration 8297: loss = 0.0026093467076529152\n",
      "Iteration 8298: loss = 0.0026093342597647675\n",
      "Iteration 8299: loss = 0.00260932181214497\n",
      "Iteration 8300: loss = 0.002609309364793399\n",
      "Iteration 8301: loss = 0.002609296917709929\n",
      "Iteration 8302: loss = 0.002609284470894435\n",
      "Iteration 8303: loss = 0.0026092720243467925\n",
      "Iteration 8304: loss = 0.0026092595780668783\n",
      "Iteration 8305: loss = 0.0026092471320545664\n",
      "Iteration 8306: loss = 0.0026092346863097336\n",
      "Iteration 8307: loss = 0.002609222240832255\n",
      "Iteration 8308: loss = 0.0026092097956220083\n",
      "Iteration 8309: loss = 0.002609197350678868\n",
      "Iteration 8310: loss = 0.0026091849060027113\n",
      "Iteration 8311: loss = 0.002609172461593414\n",
      "Iteration 8312: loss = 0.002609160017450853\n",
      "Iteration 8313: loss = 0.002609147573574904\n",
      "Iteration 8314: loss = 0.0026091351299654445\n",
      "Iteration 8315: loss = 0.002609122686622351\n",
      "Iteration 8316: loss = 0.0026091102435455015\n",
      "Iteration 8317: loss = 0.0026090978007347713\n",
      "Iteration 8318: loss = 0.002609085358190039\n",
      "Iteration 8319: loss = 0.002609072915911181\n",
      "Iteration 8320: loss = 0.0026090604738980747\n",
      "Iteration 8321: loss = 0.0026090480321505983\n",
      "Iteration 8322: loss = 0.0026090355906686286\n",
      "Iteration 8323: loss = 0.0026090231494520443\n",
      "Iteration 8324: loss = 0.0026090107085007218\n",
      "Iteration 8325: loss = 0.0026089982678145396\n",
      "Iteration 8326: loss = 0.0026089858273933776\n",
      "Iteration 8327: loss = 0.0026089733872371114\n",
      "Iteration 8328: loss = 0.00260896094734562\n",
      "Iteration 8329: loss = 0.0026089485077187826\n",
      "Iteration 8330: loss = 0.0026089360683564764\n",
      "Iteration 8331: loss = 0.002608923629258581\n",
      "Iteration 8332: loss = 0.002608911190424975\n",
      "Iteration 8333: loss = 0.0026088987518555376\n",
      "Iteration 8334: loss = 0.0026088863135501467\n",
      "Iteration 8335: loss = 0.002608873875508682\n",
      "Iteration 8336: loss = 0.002608861437731022\n",
      "Iteration 8337: loss = 0.0026088490002170476\n",
      "Iteration 8338: loss = 0.002608836562966637\n",
      "Iteration 8339: loss = 0.0026088241259796694\n",
      "Iteration 8340: loss = 0.0026088116892560246\n",
      "Iteration 8341: loss = 0.0026087992527955833\n",
      "Iteration 8342: loss = 0.002608786816598224\n",
      "Iteration 8343: loss = 0.002608774380663827\n",
      "Iteration 8344: loss = 0.002608761944992273\n",
      "Iteration 8345: loss = 0.0026087495095834415\n",
      "Iteration 8346: loss = 0.002608737074437213\n",
      "Iteration 8347: loss = 0.002608724639553468\n",
      "Iteration 8348: loss = 0.0026087122049320874\n",
      "Iteration 8349: loss = 0.0026086997705729504\n",
      "Iteration 8350: loss = 0.0026086873364759393\n",
      "Iteration 8351: loss = 0.0026086749026409334\n",
      "Iteration 8352: loss = 0.0026086624690678144\n",
      "Iteration 8353: loss = 0.002608650035756464\n",
      "Iteration 8354: loss = 0.0026086376027067617\n",
      "Iteration 8355: loss = 0.0026086251699185907\n",
      "Iteration 8356: loss = 0.002608612737391831\n",
      "Iteration 8357: loss = 0.0026086003051263643\n",
      "Iteration 8358: loss = 0.0026085878731220724\n",
      "Iteration 8359: loss = 0.002608575441378837\n",
      "Iteration 8360: loss = 0.0026085630098965397\n",
      "Iteration 8361: loss = 0.002608550578675062\n",
      "Iteration 8362: loss = 0.0026085381477142866\n",
      "Iteration 8363: loss = 0.0026085257170140957\n",
      "Iteration 8364: loss = 0.002608513286574371\n",
      "Iteration 8365: loss = 0.0026085008563949954\n",
      "Iteration 8366: loss = 0.0026084884264758507\n",
      "Iteration 8367: loss = 0.00260847599681682\n",
      "Iteration 8368: loss = 0.0026084635674177856\n",
      "Iteration 8369: loss = 0.00260845113827863\n",
      "Iteration 8370: loss = 0.0026084387093992367\n",
      "Iteration 8371: loss = 0.0026084262807794887\n",
      "Iteration 8372: loss = 0.0026084138524192684\n",
      "Iteration 8373: loss = 0.002608401424318459\n",
      "Iteration 8374: loss = 0.0026083889964769453\n",
      "Iteration 8375: loss = 0.0026083765688946084\n",
      "Iteration 8376: loss = 0.002608364141571334\n",
      "Iteration 8377: loss = 0.0026083517145070035\n",
      "Iteration 8378: loss = 0.0026083392877015027\n",
      "Iteration 8379: loss = 0.002608326861154714\n",
      "Iteration 8380: loss = 0.0026083144348665226\n",
      "Iteration 8381: loss = 0.0026083020088368113\n",
      "Iteration 8382: loss = 0.002608289583065465\n",
      "Iteration 8383: loss = 0.0026082771575523674\n",
      "Iteration 8384: loss = 0.002608264732297403\n",
      "Iteration 8385: loss = 0.002608252307300457\n",
      "Iteration 8386: loss = 0.0026082398825614138\n",
      "Iteration 8387: loss = 0.002608227458080157\n",
      "Iteration 8388: loss = 0.002608215033856572\n",
      "Iteration 8389: loss = 0.002608202609890544\n",
      "Iteration 8390: loss = 0.002608190186181958\n",
      "Iteration 8391: loss = 0.0026081777627306986\n",
      "Iteration 8392: loss = 0.0026081653395366514\n",
      "Iteration 8393: loss = 0.0026081529165997014\n",
      "Iteration 8394: loss = 0.002608140493919734\n",
      "Iteration 8395: loss = 0.0026081280714966354\n",
      "Iteration 8396: loss = 0.0026081156493302908\n",
      "Iteration 8397: loss = 0.002608103227420585\n",
      "Iteration 8398: loss = 0.0026080908057674055\n",
      "Iteration 8399: loss = 0.0026080783843706367\n",
      "Iteration 8400: loss = 0.002608065963230166\n",
      "Iteration 8401: loss = 0.002608053542345878\n",
      "Iteration 8402: loss = 0.002608041121717661\n",
      "Iteration 8403: loss = 0.0026080287013454\n",
      "Iteration 8404: loss = 0.002608016281228981\n",
      "Iteration 8405: loss = 0.0026080038613682916\n",
      "Iteration 8406: loss = 0.002607991441763218\n",
      "Iteration 8407: loss = 0.002607979022413647\n",
      "Iteration 8408: loss = 0.0026079666033194658\n",
      "Iteration 8409: loss = 0.0026079541844805606\n",
      "Iteration 8410: loss = 0.0026079417658968194\n",
      "Iteration 8411: loss = 0.002607929347568129\n",
      "Iteration 8412: loss = 0.0026079169294943756\n",
      "Iteration 8413: loss = 0.002607904511675449\n",
      "Iteration 8414: loss = 0.002607892094111235\n",
      "Iteration 8415: loss = 0.0026078796768016217\n",
      "Iteration 8416: loss = 0.0026078672597464954\n",
      "Iteration 8417: loss = 0.002607854842945746\n",
      "Iteration 8418: loss = 0.0026078424263992605\n",
      "Iteration 8419: loss = 0.002607830010106927\n",
      "Iteration 8420: loss = 0.002607817594068633\n",
      "Iteration 8421: loss = 0.0026078051782842675\n",
      "Iteration 8422: loss = 0.0026077927627537185\n",
      "Iteration 8423: loss = 0.0026077803474768746\n",
      "Iteration 8424: loss = 0.002607767932453624\n",
      "Iteration 8425: loss = 0.002607755517683855\n",
      "Iteration 8426: loss = 0.0026077431031674577\n",
      "Iteration 8427: loss = 0.0026077306889043194\n",
      "Iteration 8428: loss = 0.0026077182748943293\n",
      "Iteration 8429: loss = 0.002607705861137377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8430: loss = 0.002607693447633352\n",
      "Iteration 8431: loss = 0.0026076810343821427\n",
      "Iteration 8432: loss = 0.0026076686213836386\n",
      "Iteration 8433: loss = 0.002607656208637729\n",
      "Iteration 8434: loss = 0.002607643796144304\n",
      "Iteration 8435: loss = 0.0026076313839032516\n",
      "Iteration 8436: loss = 0.0026076189719144638\n",
      "Iteration 8437: loss = 0.0026076065601778288\n",
      "Iteration 8438: loss = 0.0026075941486932374\n",
      "Iteration 8439: loss = 0.002607581737460579\n",
      "Iteration 8440: loss = 0.002607569326479745\n",
      "Iteration 8441: loss = 0.0026075569157506247\n",
      "Iteration 8442: loss = 0.0026075445052731075\n",
      "Iteration 8443: loss = 0.0026075320950470853\n",
      "Iteration 8444: loss = 0.0026075196850724475\n",
      "Iteration 8445: loss = 0.002607507275349086\n",
      "Iteration 8446: loss = 0.0026074948658768907\n",
      "Iteration 8447: loss = 0.0026074824566557527\n",
      "Iteration 8448: loss = 0.0026074700476855623\n",
      "Iteration 8449: loss = 0.002607457638966212\n",
      "Iteration 8450: loss = 0.0026074452304975914\n",
      "Iteration 8451: loss = 0.002607432822279593\n",
      "Iteration 8452: loss = 0.002607420414312107\n",
      "Iteration 8453: loss = 0.002607408006595025\n",
      "Iteration 8454: loss = 0.0026073955991282394\n",
      "Iteration 8455: loss = 0.0026073831919116415\n",
      "Iteration 8456: loss = 0.0026073707849451223\n",
      "Iteration 8457: loss = 0.002607358378228574\n",
      "Iteration 8458: loss = 0.0026073459717618893\n",
      "Iteration 8459: loss = 0.0026073335655449593\n",
      "Iteration 8460: loss = 0.0026073211595776754\n",
      "Iteration 8461: loss = 0.002607308753859932\n",
      "Iteration 8462: loss = 0.00260729634839162\n",
      "Iteration 8463: loss = 0.002607283943172632\n",
      "Iteration 8464: loss = 0.0026072715382028607\n",
      "Iteration 8465: loss = 0.002607259133482198\n",
      "Iteration 8466: loss = 0.0026072467290105383\n",
      "Iteration 8467: loss = 0.002607234324787772\n",
      "Iteration 8468: loss = 0.002607221920813795\n",
      "Iteration 8469: loss = 0.0026072095170884975\n",
      "Iteration 8470: loss = 0.0026071971136117744\n",
      "Iteration 8471: loss = 0.002607184710383518\n",
      "Iteration 8472: loss = 0.0026071723074036213\n",
      "Iteration 8473: loss = 0.002607159904671979\n",
      "Iteration 8474: loss = 0.0026071475021884837\n",
      "Iteration 8475: loss = 0.002607135099953029\n",
      "Iteration 8476: loss = 0.0026071226979655087\n",
      "Iteration 8477: loss = 0.002607110296225817\n",
      "Iteration 8478: loss = 0.002607097894733847\n",
      "Iteration 8479: loss = 0.0026070854934894935\n",
      "Iteration 8480: loss = 0.00260707309249265\n",
      "Iteration 8481: loss = 0.00260706069174321\n",
      "Iteration 8482: loss = 0.0026070482912410697\n",
      "Iteration 8483: loss = 0.002607035890986122\n",
      "Iteration 8484: loss = 0.002607023490978262\n",
      "Iteration 8485: loss = 0.0026070110912173837\n",
      "Iteration 8486: loss = 0.002606998691703382\n",
      "Iteration 8487: loss = 0.0026069862924361516\n",
      "Iteration 8488: loss = 0.0026069738934155883\n",
      "Iteration 8489: loss = 0.002606961494641585\n",
      "Iteration 8490: loss = 0.002606949096114039\n",
      "Iteration 8491: loss = 0.0026069366978328443\n",
      "Iteration 8492: loss = 0.0026069242997978958\n",
      "Iteration 8493: loss = 0.0026069119020090893\n",
      "Iteration 8494: loss = 0.0026068995044663205\n",
      "Iteration 8495: loss = 0.0026068871071694842\n",
      "Iteration 8496: loss = 0.002606874710118477\n",
      "Iteration 8497: loss = 0.0026068623133131933\n",
      "Iteration 8498: loss = 0.0026068499167535304\n",
      "Iteration 8499: loss = 0.002606837520439383\n",
      "Iteration 8500: loss = 0.0026068251243706476\n",
      "Iteration 8501: loss = 0.00260681272854722\n",
      "Iteration 8502: loss = 0.002606800332968997\n",
      "Iteration 8503: loss = 0.002606787937635874\n",
      "Iteration 8504: loss = 0.0026067755425477485\n",
      "Iteration 8505: loss = 0.0026067631477045155\n",
      "Iteration 8506: loss = 0.0026067507531060733\n",
      "Iteration 8507: loss = 0.0026067383587523173\n",
      "Iteration 8508: loss = 0.0026067259646431447\n",
      "Iteration 8509: loss = 0.002606713570778452\n",
      "Iteration 8510: loss = 0.0026067011771581366\n",
      "Iteration 8511: loss = 0.0026066887837820946\n",
      "Iteration 8512: loss = 0.002606676390650225\n",
      "Iteration 8513: loss = 0.002606663997762423\n",
      "Iteration 8514: loss = 0.0026066516051185874\n",
      "Iteration 8515: loss = 0.0026066392127186144\n",
      "Iteration 8516: loss = 0.0026066268205624027\n",
      "Iteration 8517: loss = 0.0026066144286498493\n",
      "Iteration 8518: loss = 0.002606602036980852\n",
      "Iteration 8519: loss = 0.0026065896455553083\n",
      "Iteration 8520: loss = 0.002606577254373116\n",
      "Iteration 8521: loss = 0.002606564863434174\n",
      "Iteration 8522: loss = 0.002606552472738379\n",
      "Iteration 8523: loss = 0.00260654008228563\n",
      "Iteration 8524: loss = 0.002606527692075826\n",
      "Iteration 8525: loss = 0.0026065153021088633\n",
      "Iteration 8526: loss = 0.0026065029123846424\n",
      "Iteration 8527: loss = 0.00260649052290306\n",
      "Iteration 8528: loss = 0.0026064781336640162\n",
      "Iteration 8529: loss = 0.00260646574466741\n",
      "Iteration 8530: loss = 0.0026064533559131383\n",
      "Iteration 8531: loss = 0.0026064409674011013\n",
      "Iteration 8532: loss = 0.002606428579131198\n",
      "Iteration 8533: loss = 0.002606416191103327\n",
      "Iteration 8534: loss = 0.002606403803317387\n",
      "Iteration 8535: loss = 0.002606391415773279\n",
      "Iteration 8536: loss = 0.0026063790284709012\n",
      "Iteration 8537: loss = 0.002606366641410154\n",
      "Iteration 8538: loss = 0.0026063542545909347\n",
      "Iteration 8539: loss = 0.0026063418680131456\n",
      "Iteration 8540: loss = 0.0026063294816766843\n",
      "Iteration 8541: loss = 0.002606317095581452\n",
      "Iteration 8542: loss = 0.002606304709727348\n",
      "Iteration 8543: loss = 0.0026062923241142724\n",
      "Iteration 8544: loss = 0.0026062799387421252\n",
      "Iteration 8545: loss = 0.0026062675536108067\n",
      "Iteration 8546: loss = 0.002606255168720218\n",
      "Iteration 8547: loss = 0.002606242784070258\n",
      "Iteration 8548: loss = 0.002606230399660828\n",
      "Iteration 8549: loss = 0.0026062180154918283\n",
      "Iteration 8550: loss = 0.0026062056315631593\n",
      "Iteration 8551: loss = 0.0026061932478747226\n",
      "Iteration 8552: loss = 0.0026061808644264176\n",
      "Iteration 8553: loss = 0.0026061684812181468\n",
      "Iteration 8554: loss = 0.0026061560982498107\n",
      "Iteration 8555: loss = 0.0026061437155213093\n",
      "Iteration 8556: loss = 0.0026061313330325454\n",
      "Iteration 8557: loss = 0.0026061189507834196\n",
      "Iteration 8558: loss = 0.0026061065687738322\n",
      "Iteration 8559: loss = 0.0026060941870036866\n",
      "Iteration 8560: loss = 0.0026060818054728825\n",
      "Iteration 8561: loss = 0.0026060694241813232\n",
      "Iteration 8562: loss = 0.0026060570431289095\n",
      "Iteration 8563: loss = 0.002606044662315543\n",
      "Iteration 8564: loss = 0.0026060322817411264\n",
      "Iteration 8565: loss = 0.00260601990140556\n",
      "Iteration 8566: loss = 0.0026060075213087484\n",
      "Iteration 8567: loss = 0.0026059951414505915\n",
      "Iteration 8568: loss = 0.0026059827618309934\n",
      "Iteration 8569: loss = 0.0026059703824498548\n",
      "Iteration 8570: loss = 0.0026059580033070797\n",
      "Iteration 8571: loss = 0.002605945624402569\n",
      "Iteration 8572: loss = 0.002605933245736227\n",
      "Iteration 8573: loss = 0.0026059208673079545\n",
      "Iteration 8574: loss = 0.0026059084891176558\n",
      "Iteration 8575: loss = 0.0026058961111652336\n",
      "Iteration 8576: loss = 0.00260588373345059\n",
      "Iteration 8577: loss = 0.002605871355973629\n",
      "Iteration 8578: loss = 0.0026058589787342538\n",
      "Iteration 8579: loss = 0.0026058466017323664\n",
      "Iteration 8580: loss = 0.0026058342249678716\n",
      "Iteration 8581: loss = 0.002605821848440672\n",
      "Iteration 8582: loss = 0.0026058094721506707\n",
      "Iteration 8583: loss = 0.0026057970960977725\n",
      "Iteration 8584: loss = 0.00260578472028188\n",
      "Iteration 8585: loss = 0.0026057723447028974\n",
      "Iteration 8586: loss = 0.0026057599693607284\n",
      "Iteration 8587: loss = 0.002605747594255277\n",
      "Iteration 8588: loss = 0.0026057352193864475\n",
      "Iteration 8589: loss = 0.0026057228447541442\n",
      "Iteration 8590: loss = 0.0026057104703582704\n",
      "Iteration 8591: loss = 0.0026056980961987313\n",
      "Iteration 8592: loss = 0.00260568572227543\n",
      "Iteration 8593: loss = 0.002605673348588272\n",
      "Iteration 8594: loss = 0.0026056609751371625\n",
      "Iteration 8595: loss = 0.0026056486019220043\n",
      "Iteration 8596: loss = 0.002605636228942704\n",
      "Iteration 8597: loss = 0.002605623856199165\n",
      "Iteration 8598: loss = 0.0026056114836912926\n",
      "Iteration 8599: loss = 0.0026055991114189926\n",
      "Iteration 8600: loss = 0.002605586739382169\n",
      "Iteration 8601: loss = 0.0026055743675807265\n",
      "Iteration 8602: loss = 0.0026055619960145718\n",
      "Iteration 8603: loss = 0.00260554962468361\n",
      "Iteration 8604: loss = 0.002605537253587746\n",
      "Iteration 8605: loss = 0.002605524882726885\n",
      "Iteration 8606: loss = 0.002605512512100933\n",
      "Iteration 8607: loss = 0.0026055001417097964\n",
      "Iteration 8608: loss = 0.00260548777155338\n",
      "Iteration 8609: loss = 0.0026054754016315894\n",
      "Iteration 8610: loss = 0.0026054630319443314\n",
      "Iteration 8611: loss = 0.0026054506624915113\n",
      "Iteration 8612: loss = 0.0026054382932730363\n",
      "Iteration 8613: loss = 0.002605425924288811\n",
      "Iteration 8614: loss = 0.0026054135555387426\n",
      "Iteration 8615: loss = 0.0026054011870227374\n",
      "Iteration 8616: loss = 0.002605388818740702\n",
      "Iteration 8617: loss = 0.0026053764506925427\n",
      "Iteration 8618: loss = 0.0026053640828781663\n",
      "Iteration 8619: loss = 0.0026053517152974793\n",
      "Iteration 8620: loss = 0.0026053393479503885\n",
      "Iteration 8621: loss = 0.0026053269808368005\n",
      "Iteration 8622: loss = 0.002605314613956623\n",
      "Iteration 8623: loss = 0.002605302247309763\n",
      "Iteration 8624: loss = 0.0026052898808961262\n",
      "Iteration 8625: loss = 0.0026052775147156213\n",
      "Iteration 8626: loss = 0.0026052651487681548\n",
      "Iteration 8627: loss = 0.002605252783053635\n",
      "Iteration 8628: loss = 0.0026052404175719687\n",
      "Iteration 8629: loss = 0.0026052280523230636\n",
      "Iteration 8630: loss = 0.002605215687306826\n",
      "Iteration 8631: loss = 0.0026052033225231665\n",
      "Iteration 8632: loss = 0.0026051909579719906\n",
      "Iteration 8633: loss = 0.0026051785936532065\n",
      "Iteration 8634: loss = 0.002605166229566723\n",
      "Iteration 8635: loss = 0.0026051538657124477\n",
      "Iteration 8636: loss = 0.0026051415020902882\n",
      "Iteration 8637: loss = 0.0026051291387001537\n",
      "Iteration 8638: loss = 0.0026051167755419525\n",
      "Iteration 8639: loss = 0.0026051044126155923\n",
      "Iteration 8640: loss = 0.0026050920499209812\n",
      "Iteration 8641: loss = 0.0026050796874580286\n",
      "Iteration 8642: loss = 0.002605067325226643\n",
      "Iteration 8643: loss = 0.0026050549632267326\n",
      "Iteration 8644: loss = 0.002605042601458207\n",
      "Iteration 8645: loss = 0.0026050302399209745\n",
      "Iteration 8646: loss = 0.0026050178786149445\n",
      "Iteration 8647: loss = 0.0026050055175400257\n",
      "Iteration 8648: loss = 0.0026049931566961273\n",
      "Iteration 8649: loss = 0.0026049807960831584\n",
      "Iteration 8650: loss = 0.0026049684357010282\n",
      "Iteration 8651: loss = 0.0026049560755496467\n",
      "Iteration 8652: loss = 0.002604943715628923\n",
      "Iteration 8653: loss = 0.002604931355938767\n",
      "Iteration 8654: loss = 0.0026049189964790875\n",
      "Iteration 8655: loss = 0.002604906637249795\n",
      "Iteration 8656: loss = 0.0026048942782507987\n",
      "Iteration 8657: loss = 0.0026048819194820095\n",
      "Iteration 8658: loss = 0.0026048695609433357\n",
      "Iteration 8659: loss = 0.0026048572026346885\n",
      "Iteration 8660: loss = 0.002604844844555978\n",
      "Iteration 8661: loss = 0.0026048324867071143\n",
      "Iteration 8662: loss = 0.0026048201290880075\n",
      "Iteration 8663: loss = 0.002604807771698568\n",
      "Iteration 8664: loss = 0.0026047954145387063\n",
      "Iteration 8665: loss = 0.002604783057608333\n",
      "Iteration 8666: loss = 0.002604770700907359\n",
      "Iteration 8667: loss = 0.0026047583444356935\n",
      "Iteration 8668: loss = 0.0026047459881932498\n",
      "Iteration 8669: loss = 0.0026047336321799366\n",
      "Iteration 8670: loss = 0.002604721276395666\n",
      "Iteration 8671: loss = 0.002604708920840348\n",
      "Iteration 8672: loss = 0.002604696565513895\n",
      "Iteration 8673: loss = 0.002604684210416217\n",
      "Iteration 8674: loss = 0.002604671855547226\n",
      "Iteration 8675: loss = 0.0026046595009068334\n",
      "Iteration 8676: loss = 0.0026046471464949497\n",
      "Iteration 8677: loss = 0.0026046347923114875\n",
      "Iteration 8678: loss = 0.002604622438356358\n",
      "Iteration 8679: loss = 0.0026046100846294725\n",
      "Iteration 8680: loss = 0.002604597731130744\n",
      "Iteration 8681: loss = 0.0026045853778600826\n",
      "Iteration 8682: loss = 0.002604573024817401\n",
      "Iteration 8683: loss = 0.0026045606720026112\n",
      "Iteration 8684: loss = 0.002604548319415625\n",
      "Iteration 8685: loss = 0.002604535967056355\n",
      "Iteration 8686: loss = 0.0026045236149247132\n",
      "Iteration 8687: loss = 0.0026045112630206117\n",
      "Iteration 8688: loss = 0.0026044989113439636\n",
      "Iteration 8689: loss = 0.002604486559894681\n",
      "Iteration 8690: loss = 0.0026044742086726758\n",
      "Iteration 8691: loss = 0.0026044618576778616\n",
      "Iteration 8692: loss = 0.00260444950691015\n",
      "Iteration 8693: loss = 0.0026044371563694545\n",
      "Iteration 8694: loss = 0.0026044248060556885\n",
      "Iteration 8695: loss = 0.002604412455968764\n",
      "Iteration 8696: loss = 0.0026044001061085943\n",
      "Iteration 8697: loss = 0.0026043877564750927\n",
      "Iteration 8698: loss = 0.0026043754070681725\n",
      "Iteration 8699: loss = 0.002604363057887746\n",
      "Iteration 8700: loss = 0.002604350708933728\n",
      "Iteration 8701: loss = 0.002604338360206031\n",
      "Iteration 8702: loss = 0.0026043260117045684\n",
      "Iteration 8703: loss = 0.002604313663429254\n",
      "Iteration 8704: loss = 0.002604301315380002\n",
      "Iteration 8705: loss = 0.002604288967556725\n",
      "Iteration 8706: loss = 0.0026042766199593378\n",
      "Iteration 8707: loss = 0.002604264272587754\n",
      "Iteration 8708: loss = 0.002604251925441887\n",
      "Iteration 8709: loss = 0.002604239578521652\n",
      "Iteration 8710: loss = 0.002604227231826962\n",
      "Iteration 8711: loss = 0.0026042148853577317\n",
      "Iteration 8712: loss = 0.0026042025391138754\n",
      "Iteration 8713: loss = 0.0026041901930953077\n",
      "Iteration 8714: loss = 0.002604177847301942\n",
      "Iteration 8715: loss = 0.002604165501733695\n",
      "Iteration 8716: loss = 0.002604153156390479\n",
      "Iteration 8717: loss = 0.002604140811272209\n",
      "Iteration 8718: loss = 0.0026041284663788013\n",
      "Iteration 8719: loss = 0.002604116121710169\n",
      "Iteration 8720: loss = 0.0026041037772662275\n",
      "Iteration 8721: loss = 0.0026040914330468924\n",
      "Iteration 8722: loss = 0.0026040790890520782\n",
      "Iteration 8723: loss = 0.0026040667452817004\n",
      "Iteration 8724: loss = 0.002604054401735674\n",
      "Iteration 8725: loss = 0.0026040420584139143\n",
      "Iteration 8726: loss = 0.002604029715316336\n",
      "Iteration 8727: loss = 0.0026040173724428558\n",
      "Iteration 8728: loss = 0.0026040050297933886\n",
      "Iteration 8729: loss = 0.00260399268736785\n",
      "Iteration 8730: loss = 0.0026039803451661553\n",
      "Iteration 8731: loss = 0.002603968003188221\n",
      "Iteration 8732: loss = 0.002603955661433962\n",
      "Iteration 8733: loss = 0.0026039433199032957\n",
      "Iteration 8734: loss = 0.0026039309785961364\n",
      "Iteration 8735: loss = 0.002603918637512401\n",
      "Iteration 8736: loss = 0.002603906296652006\n",
      "Iteration 8737: loss = 0.002603893956014867\n",
      "Iteration 8738: loss = 0.0026038816156009\n",
      "Iteration 8739: loss = 0.002603869275410022\n",
      "Iteration 8740: loss = 0.0026038569354421494\n",
      "Iteration 8741: loss = 0.0026038445956971983\n",
      "Iteration 8742: loss = 0.002603832256175086\n",
      "Iteration 8743: loss = 0.002603819916875728\n",
      "Iteration 8744: loss = 0.002603807577799042\n",
      "Iteration 8745: loss = 0.002603795238944945\n",
      "Iteration 8746: loss = 0.0026037829003133527\n",
      "Iteration 8747: loss = 0.0026037705619041834\n",
      "Iteration 8748: loss = 0.0026037582237173535\n",
      "Iteration 8749: loss = 0.0026037458857527795\n",
      "Iteration 8750: loss = 0.00260373354801038\n",
      "Iteration 8751: loss = 0.0026037212104900714\n",
      "Iteration 8752: loss = 0.002603708873191771\n",
      "Iteration 8753: loss = 0.002603696536115397\n",
      "Iteration 8754: loss = 0.002603684199260866\n",
      "Iteration 8755: loss = 0.0026036718626280954\n",
      "Iteration 8756: loss = 0.002603659526217004\n",
      "Iteration 8757: loss = 0.0026036471900275082\n",
      "Iteration 8758: loss = 0.0026036348540595267\n",
      "Iteration 8759: loss = 0.0026036225183129775\n",
      "Iteration 8760: loss = 0.002603610182787778\n",
      "Iteration 8761: loss = 0.002603597847483846\n",
      "Iteration 8762: loss = 0.0026035855124011006\n",
      "Iteration 8763: loss = 0.0026035731775394585\n",
      "Iteration 8764: loss = 0.0026035608428988393\n",
      "Iteration 8765: loss = 0.002603548508479161\n",
      "Iteration 8766: loss = 0.0026035361742803415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8767: loss = 0.0026035238403022995\n",
      "Iteration 8768: loss = 0.0026035115065449537\n",
      "Iteration 8769: loss = 0.0026034991730082223\n",
      "Iteration 8770: loss = 0.002603486839692025\n",
      "Iteration 8771: loss = 0.0026034745065962794\n",
      "Iteration 8772: loss = 0.002603462173720905\n",
      "Iteration 8773: loss = 0.00260344984106582\n",
      "Iteration 8774: loss = 0.002603437508630944\n",
      "Iteration 8775: loss = 0.0026034251764161967\n",
      "Iteration 8776: loss = 0.0026034128444214957\n",
      "Iteration 8777: loss = 0.0026034005126467615\n",
      "Iteration 8778: loss = 0.002603388181091912\n",
      "Iteration 8779: loss = 0.002603375849756868\n",
      "Iteration 8780: loss = 0.002603363518641549\n",
      "Iteration 8781: loss = 0.002603351187745873\n",
      "Iteration 8782: loss = 0.0026033388570697604\n",
      "Iteration 8783: loss = 0.002603326526613131\n",
      "Iteration 8784: loss = 0.002603314196375904\n",
      "Iteration 8785: loss = 0.002603301866358\n",
      "Iteration 8786: loss = 0.0026032895365593386\n",
      "Iteration 8787: loss = 0.0026032772069798393\n",
      "Iteration 8788: loss = 0.002603264877619422\n",
      "Iteration 8789: loss = 0.002603252548478008\n",
      "Iteration 8790: loss = 0.002603240219555516\n",
      "Iteration 8791: loss = 0.0026032278908518663\n",
      "Iteration 8792: loss = 0.0026032155623669805\n",
      "Iteration 8793: loss = 0.002603203234100778\n",
      "Iteration 8794: loss = 0.0026031909060531795\n",
      "Iteration 8795: loss = 0.002603178578224105\n",
      "Iteration 8796: loss = 0.0026031662506134762\n",
      "Iteration 8797: loss = 0.0026031539232212126\n",
      "Iteration 8798: loss = 0.0026031415960472357\n",
      "Iteration 8799: loss = 0.0026031292690914657\n",
      "Iteration 8800: loss = 0.0026031169423538243\n",
      "Iteration 8801: loss = 0.0026031046158342314\n",
      "Iteration 8802: loss = 0.002603092289532609\n",
      "Iteration 8803: loss = 0.002603079963448878\n",
      "Iteration 8804: loss = 0.0026030676375829584\n",
      "Iteration 8805: loss = 0.0026030553119347727\n",
      "Iteration 8806: loss = 0.0026030429865042423\n",
      "Iteration 8807: loss = 0.002603030661291288\n",
      "Iteration 8808: loss = 0.002603018336295831\n",
      "Iteration 8809: loss = 0.0026030060115177934\n",
      "Iteration 8810: loss = 0.0026029936869570967\n",
      "Iteration 8811: loss = 0.0026029813626136625\n",
      "Iteration 8812: loss = 0.0026029690384874126\n",
      "Iteration 8813: loss = 0.002602956714578269\n",
      "Iteration 8814: loss = 0.0026029443908861523\n",
      "Iteration 8815: loss = 0.002602932067410986\n",
      "Iteration 8816: loss = 0.0026029197441526917\n",
      "Iteration 8817: loss = 0.0026029074211111913\n",
      "Iteration 8818: loss = 0.0026028950982864064\n",
      "Iteration 8819: loss = 0.0026028827756782607\n",
      "Iteration 8820: loss = 0.0026028704532866753\n",
      "Iteration 8821: loss = 0.0026028581311115725\n",
      "Iteration 8822: loss = 0.0026028458091528752\n",
      "Iteration 8823: loss = 0.002602833487410506\n",
      "Iteration 8824: loss = 0.002602821165884388\n",
      "Iteration 8825: loss = 0.0026028088445744424\n",
      "Iteration 8826: loss = 0.002602796523480593\n",
      "Iteration 8827: loss = 0.0026027842026027626\n",
      "Iteration 8828: loss = 0.002602771881940873\n",
      "Iteration 8829: loss = 0.0026027595614948485\n",
      "Iteration 8830: loss = 0.0026027472412646115\n",
      "Iteration 8831: loss = 0.002602734921250085\n",
      "Iteration 8832: loss = 0.0026027226014511923\n",
      "Iteration 8833: loss = 0.002602710281867856\n",
      "Iteration 8834: loss = 0.0026026979625000013\n",
      "Iteration 8835: loss = 0.002602685643347549\n",
      "Iteration 8836: loss = 0.0026026733244104237\n",
      "Iteration 8837: loss = 0.002602661005688549\n",
      "Iteration 8838: loss = 0.0026026486871818487\n",
      "Iteration 8839: loss = 0.0026026363688902462\n",
      "Iteration 8840: loss = 0.0026026240508136645\n",
      "Iteration 8841: loss = 0.0026026117329520284\n",
      "Iteration 8842: loss = 0.0026025994153052612\n",
      "Iteration 8843: loss = 0.0026025870978732867\n",
      "Iteration 8844: loss = 0.0026025747806560293\n",
      "Iteration 8845: loss = 0.0026025624636534126\n",
      "Iteration 8846: loss = 0.0026025501468653614\n",
      "Iteration 8847: loss = 0.0026025378302917987\n",
      "Iteration 8848: loss = 0.0026025255139326504\n",
      "Iteration 8849: loss = 0.002602513197787839\n",
      "Iteration 8850: loss = 0.00260250088185729\n",
      "Iteration 8851: loss = 0.002602488566140928\n",
      "Iteration 8852: loss = 0.0026024762506386776\n",
      "Iteration 8853: loss = 0.0026024639353504618\n",
      "Iteration 8854: loss = 0.002602451620276207\n",
      "Iteration 8855: loss = 0.0026024393054158374\n",
      "Iteration 8856: loss = 0.002602426990769278\n",
      "Iteration 8857: loss = 0.0026024146763364534\n",
      "Iteration 8858: loss = 0.002602402362117288\n",
      "Iteration 8859: loss = 0.0026023900481117078\n",
      "Iteration 8860: loss = 0.0026023777343196376\n",
      "Iteration 8861: loss = 0.002602365420741002\n",
      "Iteration 8862: loss = 0.0026023531073757268\n",
      "Iteration 8863: loss = 0.0026023407942237374\n",
      "Iteration 8864: loss = 0.002602328481284958\n",
      "Iteration 8865: loss = 0.0026023161685593157\n",
      "Iteration 8866: loss = 0.0026023038560467346\n",
      "Iteration 8867: loss = 0.0026022915437471416\n",
      "Iteration 8868: loss = 0.0026022792316604602\n",
      "Iteration 8869: loss = 0.002602266919786618\n",
      "Iteration 8870: loss = 0.00260225460812554\n",
      "Iteration 8871: loss = 0.0026022422966771527\n",
      "Iteration 8872: loss = 0.002602229985441381\n",
      "Iteration 8873: loss = 0.002602217674418151\n",
      "Iteration 8874: loss = 0.0026022053636073892\n",
      "Iteration 8875: loss = 0.002602193053009022\n",
      "Iteration 8876: loss = 0.0026021807426229743\n",
      "Iteration 8877: loss = 0.002602168432449174\n",
      "Iteration 8878: loss = 0.0026021561224875453\n",
      "Iteration 8879: loss = 0.0026021438127380165\n",
      "Iteration 8880: loss = 0.0026021315032005127\n",
      "Iteration 8881: loss = 0.002602119193874961\n",
      "Iteration 8882: loss = 0.002602106884761288\n",
      "Iteration 8883: loss = 0.0026020945758594206\n",
      "Iteration 8884: loss = 0.002602082267169285\n",
      "Iteration 8885: loss = 0.002602069958690807\n",
      "Iteration 8886: loss = 0.002602057650423915\n",
      "Iteration 8887: loss = 0.002602045342368536\n",
      "Iteration 8888: loss = 0.0026020330345245955\n",
      "Iteration 8889: loss = 0.002602020726892022\n",
      "Iteration 8890: loss = 0.0026020084194707414\n",
      "Iteration 8891: loss = 0.0026019961122606813\n",
      "Iteration 8892: loss = 0.0026019838052617696\n",
      "Iteration 8893: loss = 0.002601971498473932\n",
      "Iteration 8894: loss = 0.0026019591918970976\n",
      "Iteration 8895: loss = 0.0026019468855311926\n",
      "Iteration 8896: loss = 0.0026019345793761453\n",
      "Iteration 8897: loss = 0.0026019222734318826\n",
      "Iteration 8898: loss = 0.0026019099676983315\n",
      "Iteration 8899: loss = 0.002601897662175422\n",
      "Iteration 8900: loss = 0.00260188535686308\n",
      "Iteration 8901: loss = 0.002601873051761233\n",
      "Iteration 8902: loss = 0.0026018607468698105\n",
      "Iteration 8903: loss = 0.0026018484421887395\n",
      "Iteration 8904: loss = 0.002601836137717947\n",
      "Iteration 8905: loss = 0.002601823833457363\n",
      "Iteration 8906: loss = 0.0026018115294069145\n",
      "Iteration 8907: loss = 0.0026017992255665304\n",
      "Iteration 8908: loss = 0.0026017869219361377\n",
      "Iteration 8909: loss = 0.002601774618515666\n",
      "Iteration 8910: loss = 0.002601762315305043\n",
      "Iteration 8911: loss = 0.0026017500123041975\n",
      "Iteration 8912: loss = 0.002601737709513058\n",
      "Iteration 8913: loss = 0.0026017254069315527\n",
      "Iteration 8914: loss = 0.0026017131045596113\n",
      "Iteration 8915: loss = 0.002601700802397161\n",
      "Iteration 8916: loss = 0.0026016885004441315\n",
      "Iteration 8917: loss = 0.0026016761987004517\n",
      "Iteration 8918: loss = 0.0026016638971660504\n",
      "Iteration 8919: loss = 0.0026016515958408567\n",
      "Iteration 8920: loss = 0.002601639294724799\n",
      "Iteration 8921: loss = 0.0026016269938178077\n",
      "Iteration 8922: loss = 0.00260161469311981\n",
      "Iteration 8923: loss = 0.0026016023926307376\n",
      "Iteration 8924: loss = 0.002601590092350518\n",
      "Iteration 8925: loss = 0.0026015777922790803\n",
      "Iteration 8926: loss = 0.002601565492416355\n",
      "Iteration 8927: loss = 0.002601553192762272\n",
      "Iteration 8928: loss = 0.00260154089331676\n",
      "Iteration 8929: loss = 0.002601528594079748\n",
      "Iteration 8930: loss = 0.002601516295051167\n",
      "Iteration 8931: loss = 0.002601503996230946\n",
      "Iteration 8932: loss = 0.002601491697619015\n",
      "Iteration 8933: loss = 0.002601479399215304\n",
      "Iteration 8934: loss = 0.0026014671010197426\n",
      "Iteration 8935: loss = 0.002601454803032261\n",
      "Iteration 8936: loss = 0.0026014425052527895\n",
      "Iteration 8937: loss = 0.002601430207681258\n",
      "Iteration 8938: loss = 0.002601417910317597\n",
      "Iteration 8939: loss = 0.002601405613161736\n",
      "Iteration 8940: loss = 0.002601393316213606\n",
      "Iteration 8941: loss = 0.0026013810194731373\n",
      "Iteration 8942: loss = 0.00260136872294026\n",
      "Iteration 8943: loss = 0.0026013564266149044\n",
      "Iteration 8944: loss = 0.002601344130497002\n",
      "Iteration 8945: loss = 0.0026013318345864825\n",
      "Iteration 8946: loss = 0.0026013195388832774\n",
      "Iteration 8947: loss = 0.002601307243387317\n",
      "Iteration 8948: loss = 0.0026012949480985325\n",
      "Iteration 8949: loss = 0.0026012826530168545\n",
      "Iteration 8950: loss = 0.002601270358142213\n",
      "Iteration 8951: loss = 0.0026012580634745407\n",
      "Iteration 8952: loss = 0.002601245769013768\n",
      "Iteration 8953: loss = 0.0026012334747598257\n",
      "Iteration 8954: loss = 0.0026012211807126453\n",
      "Iteration 8955: loss = 0.0026012088868721584\n",
      "Iteration 8956: loss = 0.0026011965932382954\n",
      "Iteration 8957: loss = 0.002601184299810988\n",
      "Iteration 8958: loss = 0.0026011720065901686\n",
      "Iteration 8959: loss = 0.0026011597135757675\n",
      "Iteration 8960: loss = 0.0026011474207677174\n",
      "Iteration 8961: loss = 0.0026011351281659484\n",
      "Iteration 8962: loss = 0.0026011228357703934\n",
      "Iteration 8963: loss = 0.0026011105435809834\n",
      "Iteration 8964: loss = 0.0026010982515976516\n",
      "Iteration 8965: loss = 0.002601085959820328\n",
      "Iteration 8966: loss = 0.002601073668248946\n",
      "Iteration 8967: loss = 0.002601061376883437\n",
      "Iteration 8968: loss = 0.0026010490857237335\n",
      "Iteration 8969: loss = 0.002601036794769767\n",
      "Iteration 8970: loss = 0.00260102450402147\n",
      "Iteration 8971: loss = 0.002601012213478774\n",
      "Iteration 8972: loss = 0.0026009999231416123\n",
      "Iteration 8973: loss = 0.0026009876330099174\n",
      "Iteration 8974: loss = 0.002600975343083621\n",
      "Iteration 8975: loss = 0.002600963053362656\n",
      "Iteration 8976: loss = 0.002600950763846955\n",
      "Iteration 8977: loss = 0.002600938474536451\n",
      "Iteration 8978: loss = 0.0026009261854310754\n",
      "Iteration 8979: loss = 0.0026009138965307614\n",
      "Iteration 8980: loss = 0.002600901607835443\n",
      "Iteration 8981: loss = 0.0026008893193450516\n",
      "Iteration 8982: loss = 0.002600877031059521\n",
      "Iteration 8983: loss = 0.0026008647429787836\n",
      "Iteration 8984: loss = 0.002600852455102772\n",
      "Iteration 8985: loss = 0.0026008401674314217\n",
      "Iteration 8986: loss = 0.002600827879964663\n",
      "Iteration 8987: loss = 0.0026008155927024303\n",
      "Iteration 8988: loss = 0.002600803305644657\n",
      "Iteration 8989: loss = 0.002600791018791277\n",
      "Iteration 8990: loss = 0.0026007787321422225\n",
      "Iteration 8991: loss = 0.0026007664456974278\n",
      "Iteration 8992: loss = 0.0026007541594568256\n",
      "Iteration 8993: loss = 0.0026007418734203505\n",
      "Iteration 8994: loss = 0.0026007295875879353\n",
      "Iteration 8995: loss = 0.0026007173019595145\n",
      "Iteration 8996: loss = 0.0026007050165350208\n",
      "Iteration 8997: loss = 0.002600692731314389\n",
      "Iteration 8998: loss = 0.0026006804462975525\n",
      "Iteration 8999: loss = 0.0026006681614844456\n",
      "Iteration 9000: loss = 0.002600655876875002\n",
      "Iteration 9001: loss = 0.002600643592469156\n",
      "Iteration 9002: loss = 0.0026006313082668413\n",
      "Iteration 9003: loss = 0.0026006190242679928\n",
      "Iteration 9004: loss = 0.0026006067404725436\n",
      "Iteration 9005: loss = 0.002600594456880429\n",
      "Iteration 9006: loss = 0.0026005821734915832\n",
      "Iteration 9007: loss = 0.0026005698903059406\n",
      "Iteration 9008: loss = 0.0026005576073234358\n",
      "Iteration 9009: loss = 0.0026005453245440025\n",
      "Iteration 9010: loss = 0.002600533041967576\n",
      "Iteration 9011: loss = 0.002600520759594092\n",
      "Iteration 9012: loss = 0.002600508477423483\n",
      "Iteration 9013: loss = 0.002600496195455685\n",
      "Iteration 9014: loss = 0.0026004839136906325\n",
      "Iteration 9015: loss = 0.0026004716321282612\n",
      "Iteration 9016: loss = 0.0026004593507685052\n",
      "Iteration 9017: loss = 0.0026004470696112995\n",
      "Iteration 9018: loss = 0.00260043478865658\n",
      "Iteration 9019: loss = 0.0026004225079042814\n",
      "Iteration 9020: loss = 0.0026004102273543385\n",
      "Iteration 9021: loss = 0.0026003979470066866\n",
      "Iteration 9022: loss = 0.0026003856668612613\n",
      "Iteration 9023: loss = 0.0026003733869179976\n",
      "Iteration 9024: loss = 0.0026003611071768315\n",
      "Iteration 9025: loss = 0.002600348827637698\n",
      "Iteration 9026: loss = 0.002600336548300533\n",
      "Iteration 9027: loss = 0.002600324269165272\n",
      "Iteration 9028: loss = 0.0026003119902318515\n",
      "Iteration 9029: loss = 0.002600299711500205\n",
      "Iteration 9030: loss = 0.0026002874329702696\n",
      "Iteration 9031: loss = 0.0026002751546419824\n",
      "Iteration 9032: loss = 0.0026002628765152765\n",
      "Iteration 9033: loss = 0.0026002505985900904\n",
      "Iteration 9034: loss = 0.002600238320866359\n",
      "Iteration 9035: loss = 0.002600226043344018\n",
      "Iteration 9036: loss = 0.002600213766023004\n",
      "Iteration 9037: loss = 0.002600201488903253\n",
      "Iteration 9038: loss = 0.0026001892119847016\n",
      "Iteration 9039: loss = 0.0026001769352672855\n",
      "Iteration 9040: loss = 0.0026001646587509417\n",
      "Iteration 9041: loss = 0.0026001523824356067\n",
      "Iteration 9042: loss = 0.0026001401063212156\n",
      "Iteration 9043: loss = 0.002600127830407707\n",
      "Iteration 9044: loss = 0.0026001155546950157\n",
      "Iteration 9045: loss = 0.0026001032791830793\n",
      "Iteration 9046: loss = 0.002600091003871834\n",
      "Iteration 9047: loss = 0.002600078728761217\n",
      "Iteration 9048: loss = 0.0026000664538511644\n",
      "Iteration 9049: loss = 0.0026000541791416144\n",
      "Iteration 9050: loss = 0.0026000419046325026\n",
      "Iteration 9051: loss = 0.002600029630323767\n",
      "Iteration 9052: loss = 0.002600017356215343\n",
      "Iteration 9053: loss = 0.002600005082307169\n",
      "Iteration 9054: loss = 0.0025999928085991826\n",
      "Iteration 9055: loss = 0.00259998053509132\n",
      "Iteration 9056: loss = 0.002599968261783519\n",
      "Iteration 9057: loss = 0.0025999559886757165\n",
      "Iteration 9058: loss = 0.0025999437157678503\n",
      "Iteration 9059: loss = 0.0025999314430598577\n",
      "Iteration 9060: loss = 0.0025999191705516757\n",
      "Iteration 9061: loss = 0.0025999068982432427\n",
      "Iteration 9062: loss = 0.0025998946261344964\n",
      "Iteration 9063: loss = 0.0025998823542253734\n",
      "Iteration 9064: loss = 0.0025998700825158117\n",
      "Iteration 9065: loss = 0.00259985781100575\n",
      "Iteration 9066: loss = 0.002599845539695125\n",
      "Iteration 9067: loss = 0.0025998332685838754\n",
      "Iteration 9068: loss = 0.0025998209976719386\n",
      "Iteration 9069: loss = 0.0025998087269592527\n",
      "Iteration 9070: loss = 0.0025997964564457563\n",
      "Iteration 9071: loss = 0.002599784186131387\n",
      "Iteration 9072: loss = 0.002599771916016083\n",
      "Iteration 9073: loss = 0.002599759646099782\n",
      "Iteration 9074: loss = 0.0025997473763824243\n",
      "Iteration 9075: loss = 0.0025997351068639454\n",
      "Iteration 9076: loss = 0.0025997228375442857\n",
      "Iteration 9077: loss = 0.002599710568423383\n",
      "Iteration 9078: loss = 0.0025996982995011757\n",
      "Iteration 9079: loss = 0.002599686030777602\n",
      "Iteration 9080: loss = 0.0025996737622526024\n",
      "Iteration 9081: loss = 0.0025996614939261128\n",
      "Iteration 9082: loss = 0.0025996492257980737\n",
      "Iteration 9083: loss = 0.0025996369578684237\n",
      "Iteration 9084: loss = 0.002599624690137101\n",
      "Iteration 9085: loss = 0.0025996124226040454\n",
      "Iteration 9086: loss = 0.002599600155269195\n",
      "Iteration 9087: loss = 0.002599587888132489\n",
      "Iteration 9088: loss = 0.002599575621193867\n",
      "Iteration 9089: loss = 0.0025995633544532676\n",
      "Iteration 9090: loss = 0.0025995510879106296\n",
      "Iteration 9091: loss = 0.0025995388215658936\n",
      "Iteration 9092: loss = 0.0025995265554189977\n",
      "Iteration 9093: loss = 0.002599514289469881\n",
      "Iteration 9094: loss = 0.002599502023718484\n",
      "Iteration 9095: loss = 0.002599489758164745\n",
      "Iteration 9096: loss = 0.002599477492808604\n",
      "Iteration 9097: loss = 0.002599465227650001\n",
      "Iteration 9098: loss = 0.002599452962688874\n",
      "Iteration 9099: loss = 0.002599440697925165\n",
      "Iteration 9100: loss = 0.0025994284333588124\n",
      "Iteration 9101: loss = 0.0025994161689897556\n",
      "Iteration 9102: loss = 0.0025994039048179356\n",
      "Iteration 9103: loss = 0.0025993916408432907\n",
      "Iteration 9104: loss = 0.002599379377065762\n",
      "Iteration 9105: loss = 0.00259936711348529\n",
      "Iteration 9106: loss = 0.0025993548501018135\n",
      "Iteration 9107: loss = 0.002599342586915272\n",
      "Iteration 9108: loss = 0.002599330323925608\n",
      "Iteration 9109: loss = 0.00259931806113276\n",
      "Iteration 9110: loss = 0.0025993057985366688\n",
      "Iteration 9111: loss = 0.002599293536137274\n",
      "Iteration 9112: loss = 0.002599281273934517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9113: loss = 0.0025992690119283368\n",
      "Iteration 9114: loss = 0.0025992567501186763\n",
      "Iteration 9115: loss = 0.0025992444885054736\n",
      "Iteration 9116: loss = 0.0025992322270886703\n",
      "Iteration 9117: loss = 0.002599219965868207\n",
      "Iteration 9118: loss = 0.002599207704844024\n",
      "Iteration 9119: loss = 0.0025991954440160625\n",
      "Iteration 9120: loss = 0.0025991831833842628\n",
      "Iteration 9121: loss = 0.002599170922948567\n",
      "Iteration 9122: loss = 0.002599158662708914\n",
      "Iteration 9123: loss = 0.0025991464026652467\n",
      "Iteration 9124: loss = 0.0025991341428175047\n",
      "Iteration 9125: loss = 0.00259912188316563\n",
      "Iteration 9126: loss = 0.002599109623709563\n",
      "Iteration 9127: loss = 0.002599097364449245\n",
      "Iteration 9128: loss = 0.002599085105384618\n",
      "Iteration 9129: loss = 0.0025990728465156215\n",
      "Iteration 9130: loss = 0.0025990605878421992\n",
      "Iteration 9131: loss = 0.0025990483293642903\n",
      "Iteration 9132: loss = 0.0025990360710818378\n",
      "Iteration 9133: loss = 0.0025990238129947824\n",
      "Iteration 9134: loss = 0.0025990115551030655\n",
      "Iteration 9135: loss = 0.002598999297406629\n",
      "Iteration 9136: loss = 0.0025989870399054154\n",
      "Iteration 9137: loss = 0.002598974782599365\n",
      "Iteration 9138: loss = 0.0025989625254884195\n",
      "Iteration 9139: loss = 0.0025989502685725215\n",
      "Iteration 9140: loss = 0.002598938011851613\n",
      "Iteration 9141: loss = 0.0025989257553256356\n",
      "Iteration 9142: loss = 0.00259891349899453\n",
      "Iteration 9143: loss = 0.0025989012428582407\n",
      "Iteration 9144: loss = 0.0025988889869167082\n",
      "Iteration 9145: loss = 0.002598876731169874\n",
      "Iteration 9146: loss = 0.0025988644756176817\n",
      "Iteration 9147: loss = 0.002598852220260073\n",
      "Iteration 9148: loss = 0.00259883996509699\n",
      "Iteration 9149: loss = 0.002598827710128375\n",
      "Iteration 9150: loss = 0.0025988154553541706\n",
      "Iteration 9151: loss = 0.002598803200774319\n",
      "Iteration 9152: loss = 0.0025987909463887626\n",
      "Iteration 9153: loss = 0.0025987786921974447\n",
      "Iteration 9154: loss = 0.002598766438200307\n",
      "Iteration 9155: loss = 0.002598754184397292\n",
      "Iteration 9156: loss = 0.002598741930788343\n",
      "Iteration 9157: loss = 0.002598729677373403\n",
      "Iteration 9158: loss = 0.0025987174241524137\n",
      "Iteration 9159: loss = 0.0025987051711253194\n",
      "Iteration 9160: loss = 0.0025986929182920614\n",
      "Iteration 9161: loss = 0.0025986806656525833\n",
      "Iteration 9162: loss = 0.0025986684132068287\n",
      "Iteration 9163: loss = 0.0025986561609547404\n",
      "Iteration 9164: loss = 0.0025986439088962606\n",
      "Iteration 9165: loss = 0.002598631657031333\n",
      "Iteration 9166: loss = 0.002598619405359901\n",
      "Iteration 9167: loss = 0.0025986071538819083\n",
      "Iteration 9168: loss = 0.002598594902597297\n",
      "Iteration 9169: loss = 0.002598582651506012\n",
      "Iteration 9170: loss = 0.002598570400607995\n",
      "Iteration 9171: loss = 0.00259855814990319\n",
      "Iteration 9172: loss = 0.0025985458993915407\n",
      "Iteration 9173: loss = 0.002598533649072991\n",
      "Iteration 9174: loss = 0.0025985213989474843\n",
      "Iteration 9175: loss = 0.002598509149014964\n",
      "Iteration 9176: loss = 0.0025984968992753743\n",
      "Iteration 9177: loss = 0.002598484649728658\n",
      "Iteration 9178: loss = 0.0025984724003747604\n",
      "Iteration 9179: loss = 0.002598460151213624\n",
      "Iteration 9180: loss = 0.002598447902245193\n",
      "Iteration 9181: loss = 0.0025984356534694123\n",
      "Iteration 9182: loss = 0.0025984234048862243\n",
      "Iteration 9183: loss = 0.0025984111564955744\n",
      "Iteration 9184: loss = 0.002598398908297406\n",
      "Iteration 9185: loss = 0.002598386660291664\n",
      "Iteration 9186: loss = 0.002598374412478292\n",
      "Iteration 9187: loss = 0.0025983621648572337\n",
      "Iteration 9188: loss = 0.0025983499174284347\n",
      "Iteration 9189: loss = 0.0025983376701918387\n",
      "Iteration 9190: loss = 0.0025983254231473907\n",
      "Iteration 9191: loss = 0.002598313176295034\n",
      "Iteration 9192: loss = 0.0025983009296347134\n",
      "Iteration 9193: loss = 0.0025982886831663744\n",
      "Iteration 9194: loss = 0.0025982764368899613\n",
      "Iteration 9195: loss = 0.0025982641908054178\n",
      "Iteration 9196: loss = 0.0025982519449126895\n",
      "Iteration 9197: loss = 0.0025982396992117215\n",
      "Iteration 9198: loss = 0.002598227453702458\n",
      "Iteration 9199: loss = 0.0025982152083848437\n",
      "Iteration 9200: loss = 0.002598202963258824\n",
      "Iteration 9201: loss = 0.002598190718324343\n",
      "Iteration 9202: loss = 0.002598178473581347\n",
      "Iteration 9203: loss = 0.0025981662290297802\n",
      "Iteration 9204: loss = 0.002598153984669588\n",
      "Iteration 9205: loss = 0.002598141740500715\n",
      "Iteration 9206: loss = 0.002598129496523108\n",
      "Iteration 9207: loss = 0.0025981172527367105\n",
      "Iteration 9208: loss = 0.002598105009141468\n",
      "Iteration 9209: loss = 0.002598092765737327\n",
      "Iteration 9210: loss = 0.002598080522524232\n",
      "Iteration 9211: loss = 0.0025980682795021283\n",
      "Iteration 9212: loss = 0.0025980560366709628\n",
      "Iteration 9213: loss = 0.002598043794030679\n",
      "Iteration 9214: loss = 0.002598031551581223\n",
      "Iteration 9215: loss = 0.002598019309322542\n",
      "Iteration 9216: loss = 0.002598007067254581\n",
      "Iteration 9217: loss = 0.002597994825377285\n",
      "Iteration 9218: loss = 0.0025979825836906\n",
      "Iteration 9219: loss = 0.0025979703421944726\n",
      "Iteration 9220: loss = 0.0025979581008888477\n",
      "Iteration 9221: loss = 0.0025979458597736723\n",
      "Iteration 9222: loss = 0.002597933618848892\n",
      "Iteration 9223: loss = 0.0025979213781144516\n",
      "Iteration 9224: loss = 0.002597909137570299\n",
      "Iteration 9225: loss = 0.0025978968972163795\n",
      "Iteration 9226: loss = 0.002597884657052639\n",
      "Iteration 9227: loss = 0.0025978724170790243\n",
      "Iteration 9228: loss = 0.0025978601772954817\n",
      "Iteration 9229: loss = 0.0025978479377019577\n",
      "Iteration 9230: loss = 0.0025978356982983976\n",
      "Iteration 9231: loss = 0.0025978234590847494\n",
      "Iteration 9232: loss = 0.002597811220060958\n",
      "Iteration 9233: loss = 0.0025977989812269705\n",
      "Iteration 9234: loss = 0.0025977867425827344\n",
      "Iteration 9235: loss = 0.002597774504128195\n",
      "Iteration 9236: loss = 0.0025977622658632997\n",
      "Iteration 9237: loss = 0.0025977500277879952\n",
      "Iteration 9238: loss = 0.0025977377899022275\n",
      "Iteration 9239: loss = 0.0025977255522059445\n",
      "Iteration 9240: loss = 0.0025977133146990925\n",
      "Iteration 9241: loss = 0.0025977010773816185\n",
      "Iteration 9242: loss = 0.002597688840253469\n",
      "Iteration 9243: loss = 0.0025976766033145924\n",
      "Iteration 9244: loss = 0.0025976643665649346\n",
      "Iteration 9245: loss = 0.002597652130004442\n",
      "Iteration 9246: loss = 0.002597639893633063\n",
      "Iteration 9247: loss = 0.0025976276574507452\n",
      "Iteration 9248: loss = 0.0025976154214574347\n",
      "Iteration 9249: loss = 0.0025976031856530793\n",
      "Iteration 9250: loss = 0.0025975909500376257\n",
      "Iteration 9251: loss = 0.0025975787146110215\n",
      "Iteration 9252: loss = 0.0025975664793732155\n",
      "Iteration 9253: loss = 0.002597554244324154\n",
      "Iteration 9254: loss = 0.0025975420094637844\n",
      "Iteration 9255: loss = 0.002597529774792054\n",
      "Iteration 9256: loss = 0.0025975175403089115\n",
      "Iteration 9257: loss = 0.0025975053060143037\n",
      "Iteration 9258: loss = 0.0025974930719081793\n",
      "Iteration 9259: loss = 0.002597480837990485\n",
      "Iteration 9260: loss = 0.002597468604261168\n",
      "Iteration 9261: loss = 0.0025974563707201782\n",
      "Iteration 9262: loss = 0.0025974441373674624\n",
      "Iteration 9263: loss = 0.0025974319042029684\n",
      "Iteration 9264: loss = 0.0025974196712266447\n",
      "Iteration 9265: loss = 0.0025974074384384387\n",
      "Iteration 9266: loss = 0.0025973952058382993\n",
      "Iteration 9267: loss = 0.0025973829734261735\n",
      "Iteration 9268: loss = 0.002597370741202011\n",
      "Iteration 9269: loss = 0.0025973585091657587\n",
      "Iteration 9270: loss = 0.002597346277317365\n",
      "Iteration 9271: loss = 0.0025973340456567793\n",
      "Iteration 9272: loss = 0.002597321814183949\n",
      "Iteration 9273: loss = 0.002597309582898823\n",
      "Iteration 9274: loss = 0.002597297351801349\n",
      "Iteration 9275: loss = 0.002597285120891476\n",
      "Iteration 9276: loss = 0.002597272890169153\n",
      "Iteration 9277: loss = 0.0025972606596343286\n",
      "Iteration 9278: loss = 0.002597248429286951\n",
      "Iteration 9279: loss = 0.0025972361991269677\n",
      "Iteration 9280: loss = 0.00259722396915433\n",
      "Iteration 9281: loss = 0.002597211739368985\n",
      "Iteration 9282: loss = 0.0025971995097708814\n",
      "Iteration 9283: loss = 0.002597187280359969\n",
      "Iteration 9284: loss = 0.002597175051136197\n",
      "Iteration 9285: loss = 0.002597162822099513\n",
      "Iteration 9286: loss = 0.0025971505932498667\n",
      "Iteration 9287: loss = 0.002597138364587207\n",
      "Iteration 9288: loss = 0.002597126136111483\n",
      "Iteration 9289: loss = 0.0025971139078226446\n",
      "Iteration 9290: loss = 0.00259710167972064\n",
      "Iteration 9291: loss = 0.002597089451805419\n",
      "Iteration 9292: loss = 0.002597077224076931\n",
      "Iteration 9293: loss = 0.0025970649965351244\n",
      "Iteration 9294: loss = 0.0025970527691799496\n",
      "Iteration 9295: loss = 0.002597040542011356\n",
      "Iteration 9296: loss = 0.0025970283150292925\n",
      "Iteration 9297: loss = 0.002597016088233709\n",
      "Iteration 9298: loss = 0.002597003861624555\n",
      "Iteration 9299: loss = 0.00259699163520178\n",
      "Iteration 9300: loss = 0.0025969794089653335\n",
      "Iteration 9301: loss = 0.0025969671829151656\n",
      "Iteration 9302: loss = 0.0025969549570512263\n",
      "Iteration 9303: loss = 0.002596942731373464\n",
      "Iteration 9304: loss = 0.00259693050588183\n",
      "Iteration 9305: loss = 0.0025969182805762733\n",
      "Iteration 9306: loss = 0.0025969060554567446\n",
      "Iteration 9307: loss = 0.0025968938305231935\n",
      "Iteration 9308: loss = 0.0025968816057755696\n",
      "Iteration 9309: loss = 0.0025968693812138234\n",
      "Iteration 9310: loss = 0.002596857156837905\n",
      "Iteration 9311: loss = 0.002596844932647764\n",
      "Iteration 9312: loss = 0.0025968327086433518\n",
      "Iteration 9313: loss = 0.0025968204848246176\n",
      "Iteration 9314: loss = 0.0025968082611915124\n",
      "Iteration 9315: loss = 0.002596796037743985\n",
      "Iteration 9316: loss = 0.002596783814481988\n",
      "Iteration 9317: loss = 0.0025967715914054705\n",
      "Iteration 9318: loss = 0.0025967593685143828\n",
      "Iteration 9319: loss = 0.0025967471458086754\n",
      "Iteration 9320: loss = 0.0025967349232883\n",
      "Iteration 9321: loss = 0.002596722700953206\n",
      "Iteration 9322: loss = 0.0025967104788033445\n",
      "Iteration 9323: loss = 0.002596698256838666\n",
      "Iteration 9324: loss = 0.002596686035059122\n",
      "Iteration 9325: loss = 0.002596673813464662\n",
      "Iteration 9326: loss = 0.002596661592055238\n",
      "Iteration 9327: loss = 0.0025966493708307996\n",
      "Iteration 9328: loss = 0.002596637149791298\n",
      "Iteration 9329: loss = 0.002596624928936686\n",
      "Iteration 9330: loss = 0.002596612708266912\n",
      "Iteration 9331: loss = 0.0025966004877819283\n",
      "Iteration 9332: loss = 0.0025965882674816865\n",
      "Iteration 9333: loss = 0.002596576047366137\n",
      "Iteration 9334: loss = 0.0025965638274352302\n",
      "Iteration 9335: loss = 0.002596551607688919\n",
      "Iteration 9336: loss = 0.0025965393881271534\n",
      "Iteration 9337: loss = 0.0025965271687498855\n",
      "Iteration 9338: loss = 0.002596514949557066\n",
      "Iteration 9339: loss = 0.0025965027305486464\n",
      "Iteration 9340: loss = 0.0025964905117245784\n",
      "Iteration 9341: loss = 0.0025964782930848135\n",
      "Iteration 9342: loss = 0.0025964660746293027\n",
      "Iteration 9343: loss = 0.0025964538563579987\n",
      "Iteration 9344: loss = 0.002596441638270851\n",
      "Iteration 9345: loss = 0.002596429420367814\n",
      "Iteration 9346: loss = 0.002596417202648837\n",
      "Iteration 9347: loss = 0.002596404985113873\n",
      "Iteration 9348: loss = 0.0025963927677628736\n",
      "Iteration 9349: loss = 0.002596380550595791\n",
      "Iteration 9350: loss = 0.0025963683336125754\n",
      "Iteration 9351: loss = 0.002596356116813181\n",
      "Iteration 9352: loss = 0.0025963439001975576\n",
      "Iteration 9353: loss = 0.0025963316837656586\n",
      "Iteration 9354: loss = 0.002596319467517436\n",
      "Iteration 9355: loss = 0.002596307251452841\n",
      "Iteration 9356: loss = 0.002596295035571827\n",
      "Iteration 9357: loss = 0.0025962828198743447\n",
      "Iteration 9358: loss = 0.0025962706043603473\n",
      "Iteration 9359: loss = 0.002596258389029787\n",
      "Iteration 9360: loss = 0.0025962461738826153\n",
      "Iteration 9361: loss = 0.002596233958918786\n",
      "Iteration 9362: loss = 0.0025962217441382494\n",
      "Iteration 9363: loss = 0.0025962095295409596\n",
      "Iteration 9364: loss = 0.002596197315126869\n",
      "Iteration 9365: loss = 0.002596185100895929\n",
      "Iteration 9366: loss = 0.002596172886848093\n",
      "Iteration 9367: loss = 0.002596160672983314\n",
      "Iteration 9368: loss = 0.0025961484593015434\n",
      "Iteration 9369: loss = 0.0025961362458027347\n",
      "Iteration 9370: loss = 0.0025961240324868405\n",
      "Iteration 9371: loss = 0.0025961118193538134\n",
      "Iteration 9372: loss = 0.0025960996064036067\n",
      "Iteration 9373: loss = 0.0025960873936361727\n",
      "Iteration 9374: loss = 0.0025960751810514644\n",
      "Iteration 9375: loss = 0.0025960629686494346\n",
      "Iteration 9376: loss = 0.0025960507564300366\n",
      "Iteration 9377: loss = 0.0025960385443932234\n",
      "Iteration 9378: loss = 0.0025960263325389477\n",
      "Iteration 9379: loss = 0.0025960141208671633\n",
      "Iteration 9380: loss = 0.0025960019093778223\n",
      "Iteration 9381: loss = 0.0025959896980708784\n",
      "Iteration 9382: loss = 0.002595977486946285\n",
      "Iteration 9383: loss = 0.0025959652760039957\n",
      "Iteration 9384: loss = 0.002595953065243963\n",
      "Iteration 9385: loss = 0.0025959408546661405\n",
      "Iteration 9386: loss = 0.002595928644270482\n",
      "Iteration 9387: loss = 0.0025959164340569403\n",
      "Iteration 9388: loss = 0.00259590422402547\n",
      "Iteration 9389: loss = 0.002595892014176023\n",
      "Iteration 9390: loss = 0.002595879804508554\n",
      "Iteration 9391: loss = 0.002595867595023016\n",
      "Iteration 9392: loss = 0.0025958553857193633\n",
      "Iteration 9393: loss = 0.002595843176597549\n",
      "Iteration 9394: loss = 0.002595830967657527\n",
      "Iteration 9395: loss = 0.0025958187588992518\n",
      "Iteration 9396: loss = 0.002595806550322676\n",
      "Iteration 9397: loss = 0.0025957943419277537\n",
      "Iteration 9398: loss = 0.002595782133714439\n",
      "Iteration 9399: loss = 0.0025957699256826863\n",
      "Iteration 9400: loss = 0.0025957577178324495\n",
      "Iteration 9401: loss = 0.0025957455101636817\n",
      "Iteration 9402: loss = 0.002595733302676338\n",
      "Iteration 9403: loss = 0.0025957210953703714\n",
      "Iteration 9404: loss = 0.002595708888245737\n",
      "Iteration 9405: loss = 0.0025956966813023893\n",
      "Iteration 9406: loss = 0.0025956844745402808\n",
      "Iteration 9407: loss = 0.002595672267959367\n",
      "Iteration 9408: loss = 0.0025956600615596025\n",
      "Iteration 9409: loss = 0.0025956478553409417\n",
      "Iteration 9410: loss = 0.0025956356493033374\n",
      "Iteration 9411: loss = 0.0025956234434467455\n",
      "Iteration 9412: loss = 0.00259561123777112\n",
      "Iteration 9413: loss = 0.0025955990322764156\n",
      "Iteration 9414: loss = 0.002595586826962587\n",
      "Iteration 9415: loss = 0.002595574621829588\n",
      "Iteration 9416: loss = 0.002595562416877374\n",
      "Iteration 9417: loss = 0.002595550212105899\n",
      "Iteration 9418: loss = 0.002595538007515119\n",
      "Iteration 9419: loss = 0.002595525803104987\n",
      "Iteration 9420: loss = 0.0025955135988754595\n",
      "Iteration 9421: loss = 0.0025955013948264897\n",
      "Iteration 9422: loss = 0.0025954891909580335\n",
      "Iteration 9423: loss = 0.0025954769872700454\n",
      "Iteration 9424: loss = 0.002595464783762481\n",
      "Iteration 9425: loss = 0.0025954525804352947\n",
      "Iteration 9426: loss = 0.002595440377288442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9427: loss = 0.0025954281743218767\n",
      "Iteration 9428: loss = 0.002595415971535555\n",
      "Iteration 9429: loss = 0.0025954037689294327\n",
      "Iteration 9430: loss = 0.0025953915665034633\n",
      "Iteration 9431: loss = 0.0025953793642576038\n",
      "Iteration 9432: loss = 0.0025953671621918076\n",
      "Iteration 9433: loss = 0.002595354960306032\n",
      "Iteration 9434: loss = 0.002595342758600231\n",
      "Iteration 9435: loss = 0.00259533055707436\n",
      "Iteration 9436: loss = 0.0025953183557283752\n",
      "Iteration 9437: loss = 0.0025953061545622314\n",
      "Iteration 9438: loss = 0.0025952939535758847\n",
      "Iteration 9439: loss = 0.00259528175276929\n",
      "Iteration 9440: loss = 0.0025952695521424037\n",
      "Iteration 9441: loss = 0.0025952573516951805\n",
      "Iteration 9442: loss = 0.002595245151427577\n",
      "Iteration 9443: loss = 0.002595232951339548\n",
      "Iteration 9444: loss = 0.00259522075143105\n",
      "Iteration 9445: loss = 0.0025952085517020385\n",
      "Iteration 9446: loss = 0.0025951963521524683\n",
      "Iteration 9447: loss = 0.002595184152782298\n",
      "Iteration 9448: loss = 0.002595171953591481\n",
      "Iteration 9449: loss = 0.0025951597545799737\n",
      "Iteration 9450: loss = 0.002595147555747733\n",
      "Iteration 9451: loss = 0.0025951353570947138\n",
      "Iteration 9452: loss = 0.0025951231586208738\n",
      "Iteration 9453: loss = 0.002595110960326167\n",
      "Iteration 9454: loss = 0.0025950987622105513\n",
      "Iteration 9455: loss = 0.0025950865642739816\n",
      "Iteration 9456: loss = 0.002595074366516415\n",
      "Iteration 9457: loss = 0.0025950621689378073\n",
      "Iteration 9458: loss = 0.0025950499715381155\n",
      "Iteration 9459: loss = 0.002595037774317295\n",
      "Iteration 9460: loss = 0.0025950255772753026\n",
      "Iteration 9461: loss = 0.002595013380412095\n",
      "Iteration 9462: loss = 0.0025950011837276277\n",
      "Iteration 9463: loss = 0.0025949889872218584\n",
      "Iteration 9464: loss = 0.002594976790894743\n",
      "Iteration 9465: loss = 0.002594964594746238\n",
      "Iteration 9466: loss = 0.0025949523987763004\n",
      "Iteration 9467: loss = 0.0025949402029848867\n",
      "Iteration 9468: loss = 0.0025949280073719536\n",
      "Iteration 9469: loss = 0.0025949158119374575\n",
      "Iteration 9470: loss = 0.002594903616681356\n",
      "Iteration 9471: loss = 0.0025948914216036047\n",
      "Iteration 9472: loss = 0.002594879226704161\n",
      "Iteration 9473: loss = 0.0025948670319829825\n",
      "Iteration 9474: loss = 0.002594854837440025\n",
      "Iteration 9475: loss = 0.0025948426430752462\n",
      "Iteration 9476: loss = 0.0025948304488886023\n",
      "Iteration 9477: loss = 0.002594818254880052\n",
      "Iteration 9478: loss = 0.002594806061049551\n",
      "Iteration 9479: loss = 0.0025947938673970563\n",
      "Iteration 9480: loss = 0.0025947816739225254\n",
      "Iteration 9481: loss = 0.002594769480625916\n",
      "Iteration 9482: loss = 0.0025947572875071843\n",
      "Iteration 9483: loss = 0.002594745094566289\n",
      "Iteration 9484: loss = 0.0025947329018031857\n",
      "Iteration 9485: loss = 0.002594720709217833\n",
      "Iteration 9486: loss = 0.002594708516810188\n",
      "Iteration 9487: loss = 0.0025946963245802074\n",
      "Iteration 9488: loss = 0.0025946841325278497\n",
      "Iteration 9489: loss = 0.0025946719406530717\n",
      "Iteration 9490: loss = 0.0025946597489558314\n",
      "Iteration 9491: loss = 0.0025946475574360863\n",
      "Iteration 9492: loss = 0.002594635366093794\n",
      "Iteration 9493: loss = 0.002594623174928911\n",
      "Iteration 9494: loss = 0.0025946109839413973\n",
      "Iteration 9495: loss = 0.0025945987931312088\n",
      "Iteration 9496: loss = 0.0025945866024983037\n",
      "Iteration 9497: loss = 0.0025945744120426397\n",
      "Iteration 9498: loss = 0.0025945622217641755\n",
      "Iteration 9499: loss = 0.002594550031662868\n",
      "Iteration 9500: loss = 0.002594537841738675\n",
      "Iteration 9501: loss = 0.0025945256519915552\n",
      "Iteration 9502: loss = 0.002594513462421466\n",
      "Iteration 9503: loss = 0.002594501273028366\n",
      "Iteration 9504: loss = 0.0025944890838122134\n",
      "Iteration 9505: loss = 0.002594476894772965\n",
      "Iteration 9506: loss = 0.00259446470591058\n",
      "Iteration 9507: loss = 0.0025944525172250164\n",
      "Iteration 9508: loss = 0.0025944403287162326\n",
      "Iteration 9509: loss = 0.0025944281403841862\n",
      "Iteration 9510: loss = 0.0025944159522288362\n",
      "Iteration 9511: loss = 0.0025944037642501405\n",
      "Iteration 9512: loss = 0.002594391576448058\n",
      "Iteration 9513: loss = 0.0025943793888225466\n",
      "Iteration 9514: loss = 0.0025943672013735647\n",
      "Iteration 9515: loss = 0.0025943550141010706\n",
      "Iteration 9516: loss = 0.002594342827005023\n",
      "Iteration 9517: loss = 0.0025943306400853807\n",
      "Iteration 9518: loss = 0.002594318453342102\n",
      "Iteration 9519: loss = 0.0025943062667751465\n",
      "Iteration 9520: loss = 0.0025942940803844712\n",
      "Iteration 9521: loss = 0.002594281894170036\n",
      "Iteration 9522: loss = 0.002594269708131799\n",
      "Iteration 9523: loss = 0.0025942575222697196\n",
      "Iteration 9524: loss = 0.0025942453365837563\n",
      "Iteration 9525: loss = 0.002594233151073867\n",
      "Iteration 9526: loss = 0.002594220965740012\n",
      "Iteration 9527: loss = 0.0025942087805821495\n",
      "Iteration 9528: loss = 0.002594196595600239\n",
      "Iteration 9529: loss = 0.0025941844107942383\n",
      "Iteration 9530: loss = 0.0025941722261641083\n",
      "Iteration 9531: loss = 0.0025941600417098065\n",
      "Iteration 9532: loss = 0.002594147857431292\n",
      "Iteration 9533: loss = 0.002594135673328525\n",
      "Iteration 9534: loss = 0.002594123489401463\n",
      "Iteration 9535: loss = 0.002594111305650068\n",
      "Iteration 9536: loss = 0.0025940991220742963\n",
      "Iteration 9537: loss = 0.0025940869386741087\n",
      "Iteration 9538: loss = 0.0025940747554494643\n",
      "Iteration 9539: loss = 0.0025940625724003223\n",
      "Iteration 9540: loss = 0.0025940503895266424\n",
      "Iteration 9541: loss = 0.0025940382068283837\n",
      "Iteration 9542: loss = 0.0025940260243055057\n",
      "Iteration 9543: loss = 0.002594013841957968\n",
      "Iteration 9544: loss = 0.00259400165978573\n",
      "Iteration 9545: loss = 0.002593989477788751\n",
      "Iteration 9546: loss = 0.0025939772959669917\n",
      "Iteration 9547: loss = 0.0025939651143204105\n",
      "Iteration 9548: loss = 0.0025939529328489674\n",
      "Iteration 9549: loss = 0.002593940751552622\n",
      "Iteration 9550: loss = 0.0025939285704313355\n",
      "Iteration 9551: loss = 0.002593916389485066\n",
      "Iteration 9552: loss = 0.002593904208713774\n",
      "Iteration 9553: loss = 0.0025938920281174185\n",
      "Iteration 9554: loss = 0.0025938798476959606\n",
      "Iteration 9555: loss = 0.00259386766744936\n",
      "Iteration 9556: loss = 0.0025938554873775757\n",
      "Iteration 9557: loss = 0.002593843307480569\n",
      "Iteration 9558: loss = 0.0025938311277582987\n",
      "Iteration 9559: loss = 0.002593818948210726\n",
      "Iteration 9560: loss = 0.00259380676883781\n",
      "Iteration 9561: loss = 0.002593794589639512\n",
      "Iteration 9562: loss = 0.002593782410615791\n",
      "Iteration 9563: loss = 0.002593770231766608\n",
      "Iteration 9564: loss = 0.002593758053091923\n",
      "Iteration 9565: loss = 0.0025937458745916965\n",
      "Iteration 9566: loss = 0.002593733696265888\n",
      "Iteration 9567: loss = 0.0025937215181144587\n",
      "Iteration 9568: loss = 0.0025937093401373684\n",
      "Iteration 9569: loss = 0.0025936971623345783\n",
      "Iteration 9570: loss = 0.0025936849847060482\n",
      "Iteration 9571: loss = 0.002593672807251739\n",
      "Iteration 9572: loss = 0.002593660629971611\n",
      "Iteration 9573: loss = 0.002593648452865625\n",
      "Iteration 9574: loss = 0.0025936362759337406\n",
      "Iteration 9575: loss = 0.0025936240991759197\n",
      "Iteration 9576: loss = 0.0025936119225921225\n",
      "Iteration 9577: loss = 0.00259359974618231\n",
      "Iteration 9578: loss = 0.002593587569946442\n",
      "Iteration 9579: loss = 0.0025935753938844805\n",
      "Iteration 9580: loss = 0.0025935632179963854\n",
      "Iteration 9581: loss = 0.0025935510422821183\n",
      "Iteration 9582: loss = 0.0025935388667416392\n",
      "Iteration 9583: loss = 0.0025935266913749097\n",
      "Iteration 9584: loss = 0.0025935145161818905\n",
      "Iteration 9585: loss = 0.0025935023411625432\n",
      "Iteration 9586: loss = 0.002593490166316828\n",
      "Iteration 9587: loss = 0.002593477991644705\n",
      "Iteration 9588: loss = 0.002593465817146138\n",
      "Iteration 9589: loss = 0.002593453642821086\n",
      "Iteration 9590: loss = 0.0025934414686695105\n",
      "Iteration 9591: loss = 0.0025934292946913733\n",
      "Iteration 9592: loss = 0.0025934171208866355\n",
      "Iteration 9593: loss = 0.0025934049472552574\n",
      "Iteration 9594: loss = 0.002593392773797202\n",
      "Iteration 9595: loss = 0.002593380600512429\n",
      "Iteration 9596: loss = 0.0025933684274009007\n",
      "Iteration 9597: loss = 0.0025933562544625784\n",
      "Iteration 9598: loss = 0.0025933440816974223\n",
      "Iteration 9599: loss = 0.002593331909105396\n",
      "Iteration 9600: loss = 0.0025933197366864603\n",
      "Iteration 9601: loss = 0.0025933075644405754\n",
      "Iteration 9602: loss = 0.0025932953923677045\n",
      "Iteration 9603: loss = 0.002593283220467808\n",
      "Iteration 9604: loss = 0.0025932710487408485\n",
      "Iteration 9605: loss = 0.0025932588771867874\n",
      "Iteration 9606: loss = 0.0025932467058055863\n",
      "Iteration 9607: loss = 0.0025932345345972066\n",
      "Iteration 9608: loss = 0.0025932223635616106\n",
      "Iteration 9609: loss = 0.00259321019269876\n",
      "Iteration 9610: loss = 0.002593198022008616\n",
      "Iteration 9611: loss = 0.0025931858514911415\n",
      "Iteration 9612: loss = 0.0025931736811462976\n",
      "Iteration 9613: loss = 0.0025931615109740466\n",
      "Iteration 9614: loss = 0.002593149340974351\n",
      "Iteration 9615: loss = 0.0025931371711471717\n",
      "Iteration 9616: loss = 0.002593125001492472\n",
      "Iteration 9617: loss = 0.0025931128320102127\n",
      "Iteration 9618: loss = 0.0025931006627003565\n",
      "Iteration 9619: loss = 0.002593088493562866\n",
      "Iteration 9620: loss = 0.002593076324597703\n",
      "Iteration 9621: loss = 0.002593064155804829\n",
      "Iteration 9622: loss = 0.0025930519871842075\n",
      "Iteration 9623: loss = 0.0025930398187357998\n",
      "Iteration 9624: loss = 0.002593027650459569\n",
      "Iteration 9625: loss = 0.0025930154823554765\n",
      "Iteration 9626: loss = 0.0025930033144234855\n",
      "Iteration 9627: loss = 0.0025929911466635583\n",
      "Iteration 9628: loss = 0.0025929789790756575\n",
      "Iteration 9629: loss = 0.002592966811659744\n",
      "Iteration 9630: loss = 0.0025929546444157835\n",
      "Iteration 9631: loss = 0.002592942477343735\n",
      "Iteration 9632: loss = 0.002592930310443564\n",
      "Iteration 9633: loss = 0.002592918143715231\n",
      "Iteration 9634: loss = 0.0025929059771586994\n",
      "Iteration 9635: loss = 0.002592893810773933\n",
      "Iteration 9636: loss = 0.0025928816445608925\n",
      "Iteration 9637: loss = 0.002592869478519542\n",
      "Iteration 9638: loss = 0.002592857312649844\n",
      "Iteration 9639: loss = 0.0025928451469517614\n",
      "Iteration 9640: loss = 0.002592832981425257\n",
      "Iteration 9641: loss = 0.002592820816070293\n",
      "Iteration 9642: loss = 0.002592808650886833\n",
      "Iteration 9643: loss = 0.0025927964858748404\n",
      "Iteration 9644: loss = 0.0025927843210342767\n",
      "Iteration 9645: loss = 0.0025927721563651068\n",
      "Iteration 9646: loss = 0.0025927599918672923\n",
      "Iteration 9647: loss = 0.0025927478275407966\n",
      "Iteration 9648: loss = 0.002592735663385583\n",
      "Iteration 9649: loss = 0.002592723499401615\n",
      "Iteration 9650: loss = 0.002592711335588855\n",
      "Iteration 9651: loss = 0.0025926991719472667\n",
      "Iteration 9652: loss = 0.0025926870084768136\n",
      "Iteration 9653: loss = 0.0025926748451774585\n",
      "Iteration 9654: loss = 0.0025926626820491645\n",
      "Iteration 9655: loss = 0.0025926505190918946\n",
      "Iteration 9656: loss = 0.0025926383563056148\n",
      "Iteration 9657: loss = 0.0025926261936902845\n",
      "Iteration 9658: loss = 0.00259261403124587\n",
      "Iteration 9659: loss = 0.0025926018689723346\n",
      "Iteration 9660: loss = 0.0025925897068696403\n",
      "Iteration 9661: loss = 0.0025925775449377516\n",
      "Iteration 9662: loss = 0.0025925653831766317\n",
      "Iteration 9663: loss = 0.002592553221586245\n",
      "Iteration 9664: loss = 0.0025925410601665543\n",
      "Iteration 9665: loss = 0.0025925288989175235\n",
      "Iteration 9666: loss = 0.0025925167378391166\n",
      "Iteration 9667: loss = 0.002592504576931297\n",
      "Iteration 9668: loss = 0.0025924924161940285\n",
      "Iteration 9669: loss = 0.0025924802556272744\n",
      "Iteration 9670: loss = 0.0025924680952309994\n",
      "Iteration 9671: loss = 0.0025924559350051675\n",
      "Iteration 9672: loss = 0.0025924437749497414\n",
      "Iteration 9673: loss = 0.002592431615064686\n",
      "Iteration 9674: loss = 0.002592419455349965\n",
      "Iteration 9675: loss = 0.0025924072958055425\n",
      "Iteration 9676: loss = 0.002592395136431382\n",
      "Iteration 9677: loss = 0.002592382977227448\n",
      "Iteration 9678: loss = 0.0025923708181937047\n",
      "Iteration 9679: loss = 0.002592358659330116\n",
      "Iteration 9680: loss = 0.002592346500636646\n",
      "Iteration 9681: loss = 0.0025923343421132587\n",
      "Iteration 9682: loss = 0.0025923221837599185\n",
      "Iteration 9683: loss = 0.00259231002557659\n",
      "Iteration 9684: loss = 0.002592297867563237\n",
      "Iteration 9685: loss = 0.002592285709719824\n",
      "Iteration 9686: loss = 0.0025922735520463153\n",
      "Iteration 9687: loss = 0.0025922613945426757\n",
      "Iteration 9688: loss = 0.0025922492372088685\n",
      "Iteration 9689: loss = 0.0025922370800448586\n",
      "Iteration 9690: loss = 0.002592224923050611\n",
      "Iteration 9691: loss = 0.00259221276622609\n",
      "Iteration 9692: loss = 0.00259220060957126\n",
      "Iteration 9693: loss = 0.0025921884530860848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9694: loss = 0.0025921762967705307\n",
      "Iteration 9695: loss = 0.0025921641406245603\n",
      "Iteration 9696: loss = 0.0025921519846481404\n",
      "Iteration 9697: loss = 0.0025921398288412337\n",
      "Iteration 9698: loss = 0.002592127673203806\n",
      "Iteration 9699: loss = 0.0025921155177358215\n",
      "Iteration 9700: loss = 0.0025921033624372457\n",
      "Iteration 9701: loss = 0.002592091207308043\n",
      "Iteration 9702: loss = 0.0025920790523481785\n",
      "Iteration 9703: loss = 0.002592066897557616\n",
      "Iteration 9704: loss = 0.0025920547429363217\n",
      "Iteration 9705: loss = 0.00259204258848426\n",
      "Iteration 9706: loss = 0.0025920304342013957\n",
      "Iteration 9707: loss = 0.0025920182800876945\n",
      "Iteration 9708: loss = 0.0025920061261431203\n",
      "Iteration 9709: loss = 0.002591993972367639\n",
      "Iteration 9710: loss = 0.0025919818187612152\n",
      "Iteration 9711: loss = 0.0025919696653238147\n",
      "Iteration 9712: loss = 0.0025919575120554018\n",
      "Iteration 9713: loss = 0.0025919453589559422\n",
      "Iteration 9714: loss = 0.002591933206025401\n",
      "Iteration 9715: loss = 0.0025919210532637433\n",
      "Iteration 9716: loss = 0.002591908900670935\n",
      "Iteration 9717: loss = 0.0025918967482469408\n",
      "Iteration 9718: loss = 0.0025918845959917256\n",
      "Iteration 9719: loss = 0.0025918724439052557\n",
      "Iteration 9720: loss = 0.002591860291987496\n",
      "Iteration 9721: loss = 0.002591848140238412\n",
      "Iteration 9722: loss = 0.0025918359886579695\n",
      "Iteration 9723: loss = 0.002591823837246133\n",
      "Iteration 9724: loss = 0.0025918116860028694\n",
      "Iteration 9725: loss = 0.002591799534928143\n",
      "Iteration 9726: loss = 0.00259178738402192\n",
      "Iteration 9727: loss = 0.002591775233284166\n",
      "Iteration 9728: loss = 0.0025917630827148462\n",
      "Iteration 9729: loss = 0.0025917509323139273\n",
      "Iteration 9730: loss = 0.0025917387820813737\n",
      "Iteration 9731: loss = 0.0025917266320171525\n",
      "Iteration 9732: loss = 0.002591714482121228\n",
      "Iteration 9733: loss = 0.002591702332393567\n",
      "Iteration 9734: loss = 0.0025916901828341348\n",
      "Iteration 9735: loss = 0.002591678033442898\n",
      "Iteration 9736: loss = 0.002591665884219822\n",
      "Iteration 9737: loss = 0.0025916537351648723\n",
      "Iteration 9738: loss = 0.002591641586278015\n",
      "Iteration 9739: loss = 0.0025916294375592157\n",
      "Iteration 9740: loss = 0.002591617289008442\n",
      "Iteration 9741: loss = 0.002591605140625658\n",
      "Iteration 9742: loss = 0.0025915929924108317\n",
      "Iteration 9743: loss = 0.0025915808443639273\n",
      "Iteration 9744: loss = 0.002591568696484912\n",
      "Iteration 9745: loss = 0.002591556548773752\n",
      "Iteration 9746: loss = 0.002591544401230413\n",
      "Iteration 9747: loss = 0.0025915322538548613\n",
      "Iteration 9748: loss = 0.002591520106647064\n",
      "Iteration 9749: loss = 0.0025915079596069866\n",
      "Iteration 9750: loss = 0.002591495812734595\n",
      "Iteration 9751: loss = 0.0025914836660298553\n",
      "Iteration 9752: loss = 0.002591471519492735\n",
      "Iteration 9753: loss = 0.0025914593731232007\n",
      "Iteration 9754: loss = 0.0025914472269212173\n",
      "Iteration 9755: loss = 0.002591435080886752\n",
      "Iteration 9756: loss = 0.002591422935019772\n",
      "Iteration 9757: loss = 0.002591410789320243\n",
      "Iteration 9758: loss = 0.0025913986437881317\n",
      "Iteration 9759: loss = 0.0025913864984234043\n",
      "Iteration 9760: loss = 0.002591374353226028\n",
      "Iteration 9761: loss = 0.0025913622081959694\n",
      "Iteration 9762: loss = 0.0025913500633331947\n",
      "Iteration 9763: loss = 0.002591337918637671\n",
      "Iteration 9764: loss = 0.002591325774109365\n",
      "Iteration 9765: loss = 0.002591313629748243\n",
      "Iteration 9766: loss = 0.002591301485554272\n",
      "Iteration 9767: loss = 0.0025912893415274193\n",
      "Iteration 9768: loss = 0.0025912771976676512\n",
      "Iteration 9769: loss = 0.0025912650539749345\n",
      "Iteration 9770: loss = 0.0025912529104492365\n",
      "Iteration 9771: loss = 0.002591240767090524\n",
      "Iteration 9772: loss = 0.002591228623898763\n",
      "Iteration 9773: loss = 0.002591216480873922\n",
      "Iteration 9774: loss = 0.002591204338015968\n",
      "Iteration 9775: loss = 0.002591192195324866\n",
      "Iteration 9776: loss = 0.0025911800528005853\n",
      "Iteration 9777: loss = 0.0025911679104430917\n",
      "Iteration 9778: loss = 0.002591155768252353\n",
      "Iteration 9779: loss = 0.0025911436262283363\n",
      "Iteration 9780: loss = 0.002591131484371008\n",
      "Iteration 9781: loss = 0.0025911193426803366\n",
      "Iteration 9782: loss = 0.0025911072011562883\n",
      "Iteration 9783: loss = 0.0025910950597988307\n",
      "Iteration 9784: loss = 0.0025910829186079312\n",
      "Iteration 9785: loss = 0.0025910707775835566\n",
      "Iteration 9786: loss = 0.002591058636725676\n",
      "Iteration 9787: loss = 0.002591046496034254\n",
      "Iteration 9788: loss = 0.0025910343555092604\n",
      "Iteration 9789: loss = 0.0025910222151506613\n",
      "Iteration 9790: loss = 0.002591010074958425\n",
      "Iteration 9791: loss = 0.002590997934932518\n",
      "Iteration 9792: loss = 0.002590985795072909\n",
      "Iteration 9793: loss = 0.0025909736553795647\n",
      "Iteration 9794: loss = 0.0025909615158524533\n",
      "Iteration 9795: loss = 0.0025909493764915414\n",
      "Iteration 9796: loss = 0.0025909372372967983\n",
      "Iteration 9797: loss = 0.0025909250982681905\n",
      "Iteration 9798: loss = 0.0025909129594056854\n",
      "Iteration 9799: loss = 0.0025909008207092516\n",
      "Iteration 9800: loss = 0.0025908886821788563\n",
      "Iteration 9801: loss = 0.0025908765438144676\n",
      "Iteration 9802: loss = 0.0025908644056160533\n",
      "Iteration 9803: loss = 0.0025908522675835814\n",
      "Iteration 9804: loss = 0.002590840129717019\n",
      "Iteration 9805: loss = 0.002590827992016335\n",
      "Iteration 9806: loss = 0.0025908158544814966\n",
      "Iteration 9807: loss = 0.002590803717112472\n",
      "Iteration 9808: loss = 0.0025907915799092294\n",
      "Iteration 9809: loss = 0.0025907794428717363\n",
      "Iteration 9810: loss = 0.0025907673059999614\n",
      "Iteration 9811: loss = 0.002590755169293872\n",
      "Iteration 9812: loss = 0.0025907430327534366\n",
      "Iteration 9813: loss = 0.0025907308963786235\n",
      "Iteration 9814: loss = 0.0025907187601694006\n",
      "Iteration 9815: loss = 0.0025907066241257367\n",
      "Iteration 9816: loss = 0.0025906944882475988\n",
      "Iteration 9817: loss = 0.002590682352534956\n",
      "Iteration 9818: loss = 0.0025906702169877765\n",
      "Iteration 9819: loss = 0.002590658081606029\n",
      "Iteration 9820: loss = 0.0025906459463896805\n",
      "Iteration 9821: loss = 0.0025906338113387004\n",
      "Iteration 9822: loss = 0.002590621676453057\n",
      "Iteration 9823: loss = 0.0025906095417327184\n",
      "Iteration 9824: loss = 0.002590597407177653\n",
      "Iteration 9825: loss = 0.00259058527278783\n",
      "Iteration 9826: loss = 0.002590573138563217\n",
      "Iteration 9827: loss = 0.0025905610045037827\n",
      "Iteration 9828: loss = 0.0025905488706094958\n",
      "Iteration 9829: loss = 0.002590536736880325\n",
      "Iteration 9830: loss = 0.0025905246033162384\n",
      "Iteration 9831: loss = 0.002590512469917205\n",
      "Iteration 9832: loss = 0.002590500336683194\n",
      "Iteration 9833: loss = 0.002590488203614173\n",
      "Iteration 9834: loss = 0.002590476070710112\n",
      "Iteration 9835: loss = 0.002590463937970978\n",
      "Iteration 9836: loss = 0.002590451805396741\n",
      "Iteration 9837: loss = 0.0025904396729873698\n",
      "Iteration 9838: loss = 0.0025904275407428326\n",
      "Iteration 9839: loss = 0.0025904154086630994\n",
      "Iteration 9840: loss = 0.0025904032767481372\n",
      "Iteration 9841: loss = 0.0025903911449979165\n",
      "Iteration 9842: loss = 0.0025903790134124065\n",
      "Iteration 9843: loss = 0.0025903668819915737\n",
      "Iteration 9844: loss = 0.0025903547507353897\n",
      "Iteration 9845: loss = 0.002590342619643822\n",
      "Iteration 9846: loss = 0.0025903304887168405\n",
      "Iteration 9847: loss = 0.0025903183579544147\n",
      "Iteration 9848: loss = 0.002590306227356512\n",
      "Iteration 9849: loss = 0.002590294096923103\n",
      "Iteration 9850: loss = 0.0025902819666541567\n",
      "Iteration 9851: loss = 0.0025902698365496413\n",
      "Iteration 9852: loss = 0.0025902577066095264\n",
      "Iteration 9853: loss = 0.002590245576833782\n",
      "Iteration 9854: loss = 0.0025902334472223767\n",
      "Iteration 9855: loss = 0.00259022131777528\n",
      "Iteration 9856: loss = 0.0025902091884924604\n",
      "Iteration 9857: loss = 0.0025901970593738884\n",
      "Iteration 9858: loss = 0.0025901849304195332\n",
      "Iteration 9859: loss = 0.0025901728016293635\n",
      "Iteration 9860: loss = 0.002590160673003349\n",
      "Iteration 9861: loss = 0.0025901485445414597\n",
      "Iteration 9862: loss = 0.0025901364162436644\n",
      "Iteration 9863: loss = 0.002590124288109933\n",
      "Iteration 9864: loss = 0.0025901121601402343\n",
      "Iteration 9865: loss = 0.002590100032334539\n",
      "Iteration 9866: loss = 0.002590087904692816\n",
      "Iteration 9867: loss = 0.0025900757772150344\n",
      "Iteration 9868: loss = 0.002590063649901165\n",
      "Iteration 9869: loss = 0.002590051522751177\n",
      "Iteration 9870: loss = 0.0025900393957650394\n",
      "Iteration 9871: loss = 0.0025900272689427235\n",
      "Iteration 9872: loss = 0.002590015142284197\n",
      "Iteration 9873: loss = 0.0025900030157894314\n",
      "Iteration 9874: loss = 0.0025899908894583953\n",
      "Iteration 9875: loss = 0.0025899787632910594\n",
      "Iteration 9876: loss = 0.002589966637287393\n",
      "Iteration 9877: loss = 0.0025899545114473656\n",
      "Iteration 9878: loss = 0.0025899423857709486\n",
      "Iteration 9879: loss = 0.00258993026025811\n",
      "Iteration 9880: loss = 0.0025899181349088216\n",
      "Iteration 9881: loss = 0.002589906009723052\n",
      "Iteration 9882: loss = 0.0025898938847007725\n",
      "Iteration 9883: loss = 0.0025898817598419516\n",
      "Iteration 9884: loss = 0.00258986963514656\n",
      "Iteration 9885: loss = 0.002589857510614568\n",
      "Iteration 9886: loss = 0.0025898453862459457\n",
      "Iteration 9887: loss = 0.002589833262040663\n",
      "Iteration 9888: loss = 0.00258982113799869\n",
      "Iteration 9889: loss = 0.0025898090141199975\n",
      "Iteration 9890: loss = 0.0025897968904045553\n",
      "Iteration 9891: loss = 0.0025897847668523337\n",
      "Iteration 9892: loss = 0.002589772643463302\n",
      "Iteration 9893: loss = 0.0025897605202374325\n",
      "Iteration 9894: loss = 0.002589748397174694\n",
      "Iteration 9895: loss = 0.002589736274275057\n",
      "Iteration 9896: loss = 0.0025897241515384925\n",
      "Iteration 9897: loss = 0.0025897120289649707\n",
      "Iteration 9898: loss = 0.0025896999065544613\n",
      "Iteration 9899: loss = 0.002589687784306935\n",
      "Iteration 9900: loss = 0.002589675662222364\n",
      "Iteration 9901: loss = 0.002589663540300717\n",
      "Iteration 9902: loss = 0.0025896514185419638\n",
      "Iteration 9903: loss = 0.002589639296946077\n",
      "Iteration 9904: loss = 0.002589627175513026\n",
      "Iteration 9905: loss = 0.002589615054242782\n",
      "Iteration 9906: loss = 0.002589602933135315\n",
      "Iteration 9907: loss = 0.0025895908121905957\n",
      "Iteration 9908: loss = 0.0025895786914085952\n",
      "Iteration 9909: loss = 0.0025895665707892845\n",
      "Iteration 9910: loss = 0.0025895544503326336\n",
      "Iteration 9911: loss = 0.0025895423300386134\n",
      "Iteration 9912: loss = 0.0025895302099071945\n",
      "Iteration 9913: loss = 0.0025895180899383487\n",
      "Iteration 9914: loss = 0.002589505970132046\n",
      "Iteration 9915: loss = 0.0025894938504882575\n",
      "Iteration 9916: loss = 0.002589481731006954\n",
      "Iteration 9917: loss = 0.002589469611688106\n",
      "Iteration 9918: loss = 0.0025894574925316854\n",
      "Iteration 9919: loss = 0.0025894453735376627\n",
      "Iteration 9920: loss = 0.0025894332547060084\n",
      "Iteration 9921: loss = 0.0025894211360366942\n",
      "Iteration 9922: loss = 0.0025894090175296912\n",
      "Iteration 9923: loss = 0.00258939689918497\n",
      "Iteration 9924: loss = 0.0025893847810025015\n",
      "Iteration 9925: loss = 0.002589372662982257\n",
      "Iteration 9926: loss = 0.0025893605451242085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9927: loss = 0.0025893484274283262\n",
      "Iteration 9928: loss = 0.0025893363098945816\n",
      "Iteration 9929: loss = 0.002589324192522946\n",
      "Iteration 9930: loss = 0.002589312075313391\n",
      "Iteration 9931: loss = 0.0025892999582658865\n",
      "Iteration 9932: loss = 0.002589287841380405\n",
      "Iteration 9933: loss = 0.0025892757246569176\n",
      "Iteration 9934: loss = 0.0025892636080953957\n",
      "Iteration 9935: loss = 0.0025892514916958102\n",
      "Iteration 9936: loss = 0.002589239375458133\n",
      "Iteration 9937: loss = 0.0025892272593823354\n",
      "Iteration 9938: loss = 0.002589215143468389\n",
      "Iteration 9939: loss = 0.0025892030277162646\n",
      "Iteration 9940: loss = 0.002589190912125935\n",
      "Iteration 9941: loss = 0.0025891787966973705\n",
      "Iteration 9942: loss = 0.0025891666814305426\n",
      "Iteration 9943: loss = 0.002589154566325424\n",
      "Iteration 9944: loss = 0.002589142451381985\n",
      "Iteration 9945: loss = 0.0025891303366001985\n",
      "Iteration 9946: loss = 0.002589118221980035\n",
      "Iteration 9947: loss = 0.002589106107521467\n",
      "Iteration 9948: loss = 0.002589093993224465\n",
      "Iteration 9949: loss = 0.0025890818790890026\n",
      "Iteration 9950: loss = 0.00258906976511505\n",
      "Iteration 9951: loss = 0.0025890576513025792\n",
      "Iteration 9952: loss = 0.002589045537651563\n",
      "Iteration 9953: loss = 0.002589033424161972\n",
      "Iteration 9954: loss = 0.0025890213108337784\n",
      "Iteration 9955: loss = 0.0025890091976669546\n",
      "Iteration 9956: loss = 0.0025889970846614723\n",
      "Iteration 9957: loss = 0.0025889849718173027\n",
      "Iteration 9958: loss = 0.0025889728591344183\n",
      "Iteration 9959: loss = 0.002588960746612791\n",
      "Iteration 9960: loss = 0.002588948634252393\n",
      "Iteration 9961: loss = 0.0025889365220531963\n",
      "Iteration 9962: loss = 0.0025889244100151722\n",
      "Iteration 9963: loss = 0.002588912298138294\n",
      "Iteration 9964: loss = 0.0025889001864225327\n",
      "Iteration 9965: loss = 0.002588888074867861\n",
      "Iteration 9966: loss = 0.0025888759634742505\n",
      "Iteration 9967: loss = 0.002588863852241674\n",
      "Iteration 9968: loss = 0.0025888517411701037\n",
      "Iteration 9969: loss = 0.002588839630259511\n",
      "Iteration 9970: loss = 0.002588827519509869\n",
      "Iteration 9971: loss = 0.0025888154089211495\n",
      "Iteration 9972: loss = 0.0025888032984933253\n",
      "Iteration 9973: loss = 0.0025887911882263677\n",
      "Iteration 9974: loss = 0.0025887790781202505\n",
      "Iteration 9975: loss = 0.0025887669681749445\n",
      "Iteration 9976: loss = 0.0025887548583904234\n",
      "Iteration 9977: loss = 0.0025887427487666583\n",
      "Iteration 9978: loss = 0.0025887306393036222\n",
      "Iteration 9979: loss = 0.0025887185300012884\n",
      "Iteration 9980: loss = 0.0025887064208596285\n",
      "Iteration 9981: loss = 0.0025886943118786153\n",
      "Iteration 9982: loss = 0.0025886822030582205\n",
      "Iteration 9983: loss = 0.002588670094398418\n",
      "Iteration 9984: loss = 0.0025886579858991794\n",
      "Iteration 9985: loss = 0.0025886458775604775\n",
      "Iteration 9986: loss = 0.0025886337693822857\n",
      "Iteration 9987: loss = 0.0025886216613645752\n",
      "Iteration 9988: loss = 0.0025886095535073207\n",
      "Iteration 9989: loss = 0.002588597445810492\n",
      "Iteration 9990: loss = 0.002588585338274064\n",
      "Iteration 9991: loss = 0.0025885732308980093\n",
      "Iteration 9992: loss = 0.0025885611236822997\n",
      "Iteration 9993: loss = 0.0025885490166269094\n",
      "Iteration 9994: loss = 0.0025885369097318096\n",
      "Iteration 9995: loss = 0.002588524802996974\n",
      "Iteration 9996: loss = 0.0025885126964223758\n",
      "Iteration 9997: loss = 0.002588500590007987\n",
      "Iteration 9998: loss = 0.0025884884837537813\n",
      "Iteration 9999: loss = 0.002588476377659731\n",
      "Iteration 10000: loss = 0.0025884642717258093\n",
      "Iteration 10001: loss = 0.0025884521659519893\n",
      "Iteration 10002: loss = 0.0025884400603382444\n",
      "Iteration 10003: loss = 0.0025884279548845468\n",
      "Iteration 10004: loss = 0.0025884158495908696\n",
      "Iteration 10005: loss = 0.002588403744457187\n",
      "Iteration 10006: loss = 0.0025883916394834707\n",
      "Iteration 10007: loss = 0.0025883795346696944\n",
      "Iteration 10008: loss = 0.002588367430015831\n",
      "Iteration 10009: loss = 0.0025883553255218543\n",
      "Iteration 10010: loss = 0.002588343221187737\n",
      "Iteration 10011: loss = 0.0025883311170134525\n",
      "Iteration 10012: loss = 0.0025883190129989734\n",
      "Iteration 10013: loss = 0.0025883069091442745\n",
      "Iteration 10014: loss = 0.0025882948054493276\n",
      "Iteration 10015: loss = 0.002588282701914106\n",
      "Iteration 10016: loss = 0.002588270598538584\n",
      "Iteration 10017: loss = 0.0025882584953227346\n",
      "Iteration 10018: loss = 0.0025882463922665307\n",
      "Iteration 10019: loss = 0.002588234289369946\n",
      "Iteration 10020: loss = 0.0025882221866329546\n",
      "Iteration 10021: loss = 0.002588210084055529\n",
      "Iteration 10022: loss = 0.0025881979816376435\n",
      "Iteration 10023: loss = 0.0025881858793792706\n",
      "Iteration 10024: loss = 0.0025881737772803838\n",
      "Iteration 10025: loss = 0.002588161675340958\n",
      "Iteration 10026: loss = 0.0025881495735609657\n",
      "Iteration 10027: loss = 0.0025881374719403808\n",
      "Iteration 10028: loss = 0.0025881253704791766\n",
      "Iteration 10029: loss = 0.0025881132691773262\n",
      "Iteration 10030: loss = 0.0025881011680348055\n",
      "Iteration 10031: loss = 0.0025880890670515857\n",
      "Iteration 10032: loss = 0.0025880769662276417\n",
      "Iteration 10033: loss = 0.0025880648655629474\n",
      "Iteration 10034: loss = 0.0025880527650574752\n",
      "Iteration 10035: loss = 0.002588040664711201\n",
      "Iteration 10036: loss = 0.0025880285645240968\n",
      "Iteration 10037: loss = 0.002588016464496137\n",
      "Iteration 10038: loss = 0.002588004364627295\n",
      "Iteration 10039: loss = 0.0025879922649175457\n",
      "Iteration 10040: loss = 0.0025879801653668623\n",
      "Iteration 10041: loss = 0.002587968065975219\n",
      "Iteration 10042: loss = 0.0025879559667425895\n",
      "Iteration 10043: loss = 0.002587943867668947\n",
      "Iteration 10044: loss = 0.0025879317687542676\n",
      "Iteration 10045: loss = 0.0025879196699985234\n",
      "Iteration 10046: loss = 0.0025879075714016894\n",
      "Iteration 10047: loss = 0.0025878954729637387\n",
      "Iteration 10048: loss = 0.002587883374684646\n",
      "Iteration 10049: loss = 0.0025878712765643854\n",
      "Iteration 10050: loss = 0.002587859178602931\n",
      "Iteration 10051: loss = 0.002587847080800257\n",
      "Iteration 10052: loss = 0.002587834983156337\n",
      "Iteration 10053: loss = 0.002587822885671146\n",
      "Iteration 10054: loss = 0.0025878107883446568\n",
      "Iteration 10055: loss = 0.0025877986911768455\n",
      "Iteration 10056: loss = 0.0025877865941676854\n",
      "Iteration 10057: loss = 0.0025877744973171503\n",
      "Iteration 10058: loss = 0.0025877624006252157\n",
      "Iteration 10059: loss = 0.0025877503040918548\n",
      "Iteration 10060: loss = 0.0025877382077170424\n",
      "Iteration 10061: loss = 0.0025877261115007525\n",
      "Iteration 10062: loss = 0.00258771401544296\n",
      "Iteration 10063: loss = 0.002587701919543639\n",
      "Iteration 10064: loss = 0.002587689823802765\n",
      "Iteration 10065: loss = 0.0025876777282203105\n",
      "Iteration 10066: loss = 0.002587665632796251\n",
      "Iteration 10067: loss = 0.002587653537530561\n",
      "Iteration 10068: loss = 0.0025876414424232147\n",
      "Iteration 10069: loss = 0.002587629347474188\n",
      "Iteration 10070: loss = 0.002587617252683453\n",
      "Iteration 10071: loss = 0.002587605158050986\n",
      "Iteration 10072: loss = 0.002587593063576762\n",
      "Iteration 10073: loss = 0.002587580969260754\n",
      "Iteration 10074: loss = 0.002587568875102938\n",
      "Iteration 10075: loss = 0.002587556781103288\n",
      "Iteration 10076: loss = 0.002587544687261779\n",
      "Iteration 10077: loss = 0.002587532593578385\n",
      "Iteration 10078: loss = 0.0025875205000530812\n",
      "Iteration 10079: loss = 0.0025875084066858435\n",
      "Iteration 10080: loss = 0.002587496313476645\n",
      "Iteration 10081: loss = 0.002587484220425461\n",
      "Iteration 10082: loss = 0.002587472127532267\n",
      "Iteration 10083: loss = 0.0025874600347970365\n",
      "Iteration 10084: loss = 0.0025874479422197455\n",
      "Iteration 10085: loss = 0.002587435849800369\n",
      "Iteration 10086: loss = 0.0025874237575388816\n",
      "Iteration 10087: loss = 0.0025874116654352576\n",
      "Iteration 10088: loss = 0.0025873995734894727\n",
      "Iteration 10089: loss = 0.002587387481701502\n",
      "Iteration 10090: loss = 0.0025873753900713193\n",
      "Iteration 10091: loss = 0.002587363298598901\n",
      "Iteration 10092: loss = 0.0025873512072842214\n",
      "Iteration 10093: loss = 0.0025873391161272555\n",
      "Iteration 10094: loss = 0.002587327025127979\n",
      "Iteration 10095: loss = 0.002587314934286367\n",
      "Iteration 10096: loss = 0.002587302843602394\n",
      "Iteration 10097: loss = 0.002587290753076035\n",
      "Iteration 10098: loss = 0.002587278662707266\n",
      "Iteration 10099: loss = 0.002587266572496062\n",
      "Iteration 10100: loss = 0.0025872544824423974\n",
      "Iteration 10101: loss = 0.0025872423925462486\n",
      "Iteration 10102: loss = 0.0025872303028075905\n",
      "Iteration 10103: loss = 0.002587218213226398\n",
      "Iteration 10104: loss = 0.0025872061238026464\n",
      "Iteration 10105: loss = 0.002587194034536311\n",
      "Iteration 10106: loss = 0.002587181945427368\n",
      "Iteration 10107: loss = 0.002587169856475791\n",
      "Iteration 10108: loss = 0.0025871577676815573\n",
      "Iteration 10109: loss = 0.002587145679044641\n",
      "Iteration 10110: loss = 0.0025871335905650198\n",
      "Iteration 10111: loss = 0.002587121502242666\n",
      "Iteration 10112: loss = 0.002587109414077556\n",
      "Iteration 10113: loss = 0.0025870973260696665\n",
      "Iteration 10114: loss = 0.002587085238218972\n",
      "Iteration 10115: loss = 0.0025870731505254485\n",
      "Iteration 10116: loss = 0.0025870610629890713\n",
      "Iteration 10117: loss = 0.002587048975609816\n",
      "Iteration 10118: loss = 0.0025870368883876577\n",
      "Iteration 10119: loss = 0.0025870248013225737\n",
      "Iteration 10120: loss = 0.002587012714414538\n",
      "Iteration 10121: loss = 0.002587000627663527\n",
      "Iteration 10122: loss = 0.0025869885410695157\n",
      "Iteration 10123: loss = 0.00258697645463248\n",
      "Iteration 10124: loss = 0.0025869643683523965\n",
      "Iteration 10125: loss = 0.0025869522822292404\n",
      "Iteration 10126: loss = 0.0025869401962629866\n",
      "Iteration 10127: loss = 0.0025869281104536125\n",
      "Iteration 10128: loss = 0.0025869160248010933\n",
      "Iteration 10129: loss = 0.0025869039393054036\n",
      "Iteration 10130: loss = 0.002586891853966521\n",
      "Iteration 10131: loss = 0.0025868797687844205\n",
      "Iteration 10132: loss = 0.0025868676837590785\n",
      "Iteration 10133: loss = 0.00258685559889047\n",
      "Iteration 10134: loss = 0.0025868435141785722\n",
      "Iteration 10135: loss = 0.00258683142962336\n",
      "Iteration 10136: loss = 0.00258681934522481\n",
      "Iteration 10137: loss = 0.002586807260982897\n",
      "Iteration 10138: loss = 0.0025867951768975984\n",
      "Iteration 10139: loss = 0.002586783092968891\n",
      "Iteration 10140: loss = 0.0025867710091967483\n",
      "Iteration 10141: loss = 0.002586758925581148\n",
      "Iteration 10142: loss = 0.0025867468421220657\n",
      "Iteration 10143: loss = 0.002586734758819479\n",
      "Iteration 10144: loss = 0.0025867226756733615\n",
      "Iteration 10145: loss = 0.002586710592683691\n",
      "Iteration 10146: loss = 0.0025866985098504437\n",
      "Iteration 10147: loss = 0.002586686427173595\n",
      "Iteration 10148: loss = 0.0025866743446531218\n",
      "Iteration 10149: loss = 0.002586662262289001\n",
      "Iteration 10150: loss = 0.0025866501800812067\n",
      "Iteration 10151: loss = 0.002586638098029717\n",
      "Iteration 10152: loss = 0.0025866260161345076\n",
      "Iteration 10153: loss = 0.002586613934395556\n",
      "Iteration 10154: loss = 0.0025866018528128364\n",
      "Iteration 10155: loss = 0.002586589771386326\n",
      "Iteration 10156: loss = 0.0025865776901160026\n",
      "Iteration 10157: loss = 0.0025865656090018406\n",
      "Iteration 10158: loss = 0.0025865535280438176\n",
      "Iteration 10159: loss = 0.0025865414472419097\n",
      "Iteration 10160: loss = 0.0025865293665960935\n",
      "Iteration 10161: loss = 0.002586517286106345\n",
      "Iteration 10162: loss = 0.0025865052057726418\n",
      "Iteration 10163: loss = 0.0025864931255949594\n",
      "Iteration 10164: loss = 0.0025864810455732746\n",
      "Iteration 10165: loss = 0.0025864689657075644\n",
      "Iteration 10166: loss = 0.002586456885997805\n",
      "Iteration 10167: loss = 0.002586444806443973\n",
      "Iteration 10168: loss = 0.002586432727046046\n",
      "Iteration 10169: loss = 0.0025864206478039982\n",
      "Iteration 10170: loss = 0.002586408568717809\n",
      "Iteration 10171: loss = 0.002586396489787454\n",
      "Iteration 10172: loss = 0.002586384411012909\n",
      "Iteration 10173: loss = 0.0025863723323941525\n",
      "Iteration 10174: loss = 0.0025863602539311597\n",
      "Iteration 10175: loss = 0.002586348175623909\n",
      "Iteration 10176: loss = 0.0025863360974723764\n",
      "Iteration 10177: loss = 0.0025863240194765377\n",
      "Iteration 10178: loss = 0.002586311941636371\n",
      "Iteration 10179: loss = 0.002586299863951852\n",
      "Iteration 10180: loss = 0.0025862877864229594\n",
      "Iteration 10181: loss = 0.0025862757090496682\n",
      "Iteration 10182: loss = 0.002586263631831957\n",
      "Iteration 10183: loss = 0.0025862515547698016\n",
      "Iteration 10184: loss = 0.002586239477863179\n",
      "Iteration 10185: loss = 0.0025862274011120667\n",
      "Iteration 10186: loss = 0.0025862153245164413\n",
      "Iteration 10187: loss = 0.0025862032480762796\n",
      "Iteration 10188: loss = 0.0025861911717915595\n",
      "Iteration 10189: loss = 0.0025861790956622577\n",
      "Iteration 10190: loss = 0.002586167019688351\n",
      "Iteration 10191: loss = 0.0025861549438698167\n",
      "Iteration 10192: loss = 0.002586142868206631\n",
      "Iteration 10193: loss = 0.0025861307926987732\n",
      "Iteration 10194: loss = 0.002586118717346218\n",
      "Iteration 10195: loss = 0.002586106642148944\n",
      "Iteration 10196: loss = 0.0025860945671069284\n",
      "Iteration 10197: loss = 0.002586082492220148\n",
      "Iteration 10198: loss = 0.00258607041748858\n",
      "Iteration 10199: loss = 0.0025860583429122017\n",
      "Iteration 10200: loss = 0.002586046268490991\n",
      "Iteration 10201: loss = 0.002586034194224924\n",
      "Iteration 10202: loss = 0.002586022120113979\n",
      "Iteration 10203: loss = 0.002586010046158133\n",
      "Iteration 10204: loss = 0.0025859979723573635\n",
      "Iteration 10205: loss = 0.002585985898711648\n",
      "Iteration 10206: loss = 0.002585973825220963\n",
      "Iteration 10207: loss = 0.002585961751885287\n",
      "Iteration 10208: loss = 0.0025859496787045966\n",
      "Iteration 10209: loss = 0.0025859376056788703\n",
      "Iteration 10210: loss = 0.002585925532808084\n",
      "Iteration 10211: loss = 0.002585913460092217\n",
      "Iteration 10212: loss = 0.002585901387531245\n",
      "Iteration 10213: loss = 0.002585889315125147\n",
      "Iteration 10214: loss = 0.0025858772428738997\n",
      "Iteration 10215: loss = 0.0025858651707774817\n",
      "Iteration 10216: loss = 0.0025858530988358687\n",
      "Iteration 10217: loss = 0.0025858410270490403\n",
      "Iteration 10218: loss = 0.002585828955416973\n",
      "Iteration 10219: loss = 0.002585816883939645\n",
      "Iteration 10220: loss = 0.0025858048126170324\n",
      "Iteration 10221: loss = 0.002585792741449115\n",
      "Iteration 10222: loss = 0.0025857806704358698\n",
      "Iteration 10223: loss = 0.002585768599577274\n",
      "Iteration 10224: loss = 0.0025857565288733065\n",
      "Iteration 10225: loss = 0.0025857444583239435\n",
      "Iteration 10226: loss = 0.002585732387929164\n",
      "Iteration 10227: loss = 0.002585720317688945\n",
      "Iteration 10228: loss = 0.0025857082476032646\n",
      "Iteration 10229: loss = 0.0025856961776721007\n",
      "Iteration 10230: loss = 0.0025856841078954317\n",
      "Iteration 10231: loss = 0.0025856720382732346\n",
      "Iteration 10232: loss = 0.002585659968805488\n",
      "Iteration 10233: loss = 0.0025856478994921686\n",
      "Iteration 10234: loss = 0.002585635830333256\n",
      "Iteration 10235: loss = 0.0025856237613287266\n",
      "Iteration 10236: loss = 0.00258561169247856\n",
      "Iteration 10237: loss = 0.002585599623782732\n",
      "Iteration 10238: loss = 0.0025855875552412235\n",
      "Iteration 10239: loss = 0.00258557548685401\n",
      "Iteration 10240: loss = 0.0025855634186210705\n",
      "Iteration 10241: loss = 0.002585551350542383\n",
      "Iteration 10242: loss = 0.0025855392826179256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10243: loss = 0.0025855272148476768\n",
      "Iteration 10244: loss = 0.0025855151472316137\n",
      "Iteration 10245: loss = 0.002585503079769716\n",
      "Iteration 10246: loss = 0.00258549101246196\n",
      "Iteration 10247: loss = 0.002585478945308325\n",
      "Iteration 10248: loss = 0.0025854668783087897\n",
      "Iteration 10249: loss = 0.0025854548114633315\n",
      "Iteration 10250: loss = 0.0025854427447719276\n",
      "Iteration 10251: loss = 0.0025854306782345585\n",
      "Iteration 10252: loss = 0.002585418611851201\n",
      "Iteration 10253: loss = 0.002585406545621834\n",
      "Iteration 10254: loss = 0.0025853944795464355\n",
      "Iteration 10255: loss = 0.002585382413624984\n",
      "Iteration 10256: loss = 0.0025853703478574573\n",
      "Iteration 10257: loss = 0.0025853582822438353\n",
      "Iteration 10258: loss = 0.0025853462167840942\n",
      "Iteration 10259: loss = 0.0025853341514782144\n",
      "Iteration 10260: loss = 0.002585322086326173\n",
      "Iteration 10261: loss = 0.002585310021327949\n",
      "Iteration 10262: loss = 0.0025852979564835203\n",
      "Iteration 10263: loss = 0.002585285891792866\n",
      "Iteration 10264: loss = 0.002585273827255965\n",
      "Iteration 10265: loss = 0.0025852617628727946\n",
      "Iteration 10266: loss = 0.0025852496986433346\n",
      "Iteration 10267: loss = 0.0025852376345675628\n",
      "Iteration 10268: loss = 0.002585225570645457\n",
      "Iteration 10269: loss = 0.0025852135068769977\n",
      "Iteration 10270: loss = 0.0025852014432621614\n",
      "Iteration 10271: loss = 0.0025851893798009284\n",
      "Iteration 10272: loss = 0.002585177316493277\n",
      "Iteration 10273: loss = 0.002585165253339186\n",
      "Iteration 10274: loss = 0.0025851531903386326\n",
      "Iteration 10275: loss = 0.002585141127491597\n",
      "Iteration 10276: loss = 0.0025851290647980582\n",
      "Iteration 10277: loss = 0.002585117002257993\n",
      "Iteration 10278: loss = 0.002585104939871382\n",
      "Iteration 10279: loss = 0.002585092877638203\n",
      "Iteration 10280: loss = 0.002585080815558436\n",
      "Iteration 10281: loss = 0.0025850687536320585\n",
      "Iteration 10282: loss = 0.00258505669185905\n",
      "Iteration 10283: loss = 0.002585044630239389\n",
      "Iteration 10284: loss = 0.0025850325687730544\n",
      "Iteration 10285: loss = 0.0025850205074600253\n",
      "Iteration 10286: loss = 0.0025850084463002803\n",
      "Iteration 10287: loss = 0.0025849963852937986\n",
      "Iteration 10288: loss = 0.002584984324440559\n",
      "Iteration 10289: loss = 0.0025849722637405407\n",
      "Iteration 10290: loss = 0.002584960203193722\n",
      "Iteration 10291: loss = 0.0025849481428000826\n",
      "Iteration 10292: loss = 0.0025849360825596014\n",
      "Iteration 10293: loss = 0.002584924022472257\n",
      "Iteration 10294: loss = 0.002584911962538029\n",
      "Iteration 10295: loss = 0.002584899902756896\n",
      "Iteration 10296: loss = 0.0025848878431288373\n",
      "Iteration 10297: loss = 0.0025848757836538323\n",
      "Iteration 10298: loss = 0.0025848637243318594\n",
      "Iteration 10299: loss = 0.0025848516651628974\n",
      "Iteration 10300: loss = 0.0025848396061469276\n",
      "Iteration 10301: loss = 0.002584827547283927\n",
      "Iteration 10302: loss = 0.0025848154885738757\n",
      "Iteration 10303: loss = 0.0025848034300167523\n",
      "Iteration 10304: loss = 0.002584791371612537\n",
      "Iteration 10305: loss = 0.0025847793133612082\n",
      "Iteration 10306: loss = 0.0025847672552627454\n",
      "Iteration 10307: loss = 0.002584755197317128\n",
      "Iteration 10308: loss = 0.0025847431395243353\n",
      "Iteration 10309: loss = 0.0025847310818843464\n",
      "Iteration 10310: loss = 0.002584719024397141\n",
      "Iteration 10311: loss = 0.002584706967062698\n",
      "Iteration 10312: loss = 0.002584694909880997\n",
      "Iteration 10313: loss = 0.0025846828528520173\n",
      "Iteration 10314: loss = 0.0025846707959757387\n",
      "Iteration 10315: loss = 0.00258465873925214\n",
      "Iteration 10316: loss = 0.002584646682681201\n",
      "Iteration 10317: loss = 0.002584634626262901\n",
      "Iteration 10318: loss = 0.0025846225699972195\n",
      "Iteration 10319: loss = 0.0025846105138841364\n",
      "Iteration 10320: loss = 0.0025845984579236305\n",
      "Iteration 10321: loss = 0.0025845864021156813\n",
      "Iteration 10322: loss = 0.0025845743464602694\n",
      "Iteration 10323: loss = 0.0025845622909573735\n",
      "Iteration 10324: loss = 0.002584550235606973\n",
      "Iteration 10325: loss = 0.0025845381804090488\n",
      "Iteration 10326: loss = 0.0025845261253635786\n",
      "Iteration 10327: loss = 0.0025845140704705426\n",
      "Iteration 10328: loss = 0.002584502015729922\n",
      "Iteration 10329: loss = 0.0025844899611416946\n",
      "Iteration 10330: loss = 0.0025844779067058406\n",
      "Iteration 10331: loss = 0.002584465852422341\n",
      "Iteration 10332: loss = 0.002584453798291173\n",
      "Iteration 10333: loss = 0.002584441744312318\n",
      "Iteration 10334: loss = 0.002584429690485756\n",
      "Iteration 10335: loss = 0.0025844176368114663\n",
      "Iteration 10336: loss = 0.0025844055832894277\n",
      "Iteration 10337: loss = 0.002584393529919622\n",
      "Iteration 10338: loss = 0.0025843814767020274\n",
      "Iteration 10339: loss = 0.0025843694236366245\n",
      "Iteration 10340: loss = 0.002584357370723393\n",
      "Iteration 10341: loss = 0.0025843453179623123\n",
      "Iteration 10342: loss = 0.002584333265353363\n",
      "Iteration 10343: loss = 0.0025843212128965247\n",
      "Iteration 10344: loss = 0.002584309160591777\n",
      "Iteration 10345: loss = 0.0025842971084391007\n",
      "Iteration 10346: loss = 0.0025842850564384754\n",
      "Iteration 10347: loss = 0.0025842730045898803\n",
      "Iteration 10348: loss = 0.0025842609528932965\n",
      "Iteration 10349: loss = 0.002584248901348703\n",
      "Iteration 10350: loss = 0.0025842368499560805\n",
      "Iteration 10351: loss = 0.0025842247987154093\n",
      "Iteration 10352: loss = 0.0025842127476266683\n",
      "Iteration 10353: loss = 0.002584200696689839\n",
      "Iteration 10354: loss = 0.002584188645904901\n",
      "Iteration 10355: loss = 0.002584176595271834\n",
      "Iteration 10356: loss = 0.002584164544790618\n",
      "Iteration 10357: loss = 0.002584152494461234\n",
      "Iteration 10358: loss = 0.002584140444283661\n",
      "Iteration 10359: loss = 0.0025841283942578805\n",
      "Iteration 10360: loss = 0.002584116344383872\n",
      "Iteration 10361: loss = 0.0025841042946616152\n",
      "Iteration 10362: loss = 0.0025840922450910916\n",
      "Iteration 10363: loss = 0.0025840801956722808\n",
      "Iteration 10364: loss = 0.0025840681464051624\n",
      "Iteration 10365: loss = 0.002584056097289718\n",
      "Iteration 10366: loss = 0.0025840440483259263\n",
      "Iteration 10367: loss = 0.0025840319995137696\n",
      "Iteration 10368: loss = 0.002584019950853227\n",
      "Iteration 10369: loss = 0.0025840079023442784\n",
      "Iteration 10370: loss = 0.0025839958539869046\n",
      "Iteration 10371: loss = 0.0025839838057810866\n",
      "Iteration 10372: loss = 0.002583971757726804\n",
      "Iteration 10373: loss = 0.0025839597098240383\n",
      "Iteration 10374: loss = 0.0025839476620727686\n",
      "Iteration 10375: loss = 0.002583935614472976\n",
      "Iteration 10376: loss = 0.002583923567024641\n",
      "Iteration 10377: loss = 0.002583911519727744\n",
      "Iteration 10378: loss = 0.0025838994725822656\n",
      "Iteration 10379: loss = 0.0025838874255881858\n",
      "Iteration 10380: loss = 0.002583875378745486\n",
      "Iteration 10381: loss = 0.002583863332054146\n",
      "Iteration 10382: loss = 0.002583851285514147\n",
      "Iteration 10383: loss = 0.0025838392391254687\n",
      "Iteration 10384: loss = 0.0025838271928880926\n",
      "Iteration 10385: loss = 0.002583815146801999\n",
      "Iteration 10386: loss = 0.0025838031008671688\n",
      "Iteration 10387: loss = 0.0025837910550835816\n",
      "Iteration 10388: loss = 0.0025837790094512198\n",
      "Iteration 10389: loss = 0.0025837669639700626\n",
      "Iteration 10390: loss = 0.002583754918640091\n",
      "Iteration 10391: loss = 0.002583742873461286\n",
      "Iteration 10392: loss = 0.002583730828433628\n",
      "Iteration 10393: loss = 0.002583718783557098\n",
      "Iteration 10394: loss = 0.002583706738831677\n",
      "Iteration 10395: loss = 0.002583694694257346\n",
      "Iteration 10396: loss = 0.002583682649834085\n",
      "Iteration 10397: loss = 0.002583670605561875\n",
      "Iteration 10398: loss = 0.002583658561440697\n",
      "Iteration 10399: loss = 0.002583646517470532\n",
      "Iteration 10400: loss = 0.0025836344736513603\n",
      "Iteration 10401: loss = 0.002583622429983163\n",
      "Iteration 10402: loss = 0.002583610386465922\n",
      "Iteration 10403: loss = 0.002583598343099617\n",
      "Iteration 10404: loss = 0.002583586299884229\n",
      "Iteration 10405: loss = 0.00258357425681974\n",
      "Iteration 10406: loss = 0.0025835622139061295\n",
      "Iteration 10407: loss = 0.0025835501711433794\n",
      "Iteration 10408: loss = 0.002583538128531471\n",
      "Iteration 10409: loss = 0.002583526086070384\n",
      "Iteration 10410: loss = 0.0025835140437601004\n",
      "Iteration 10411: loss = 0.0025835020016006013\n",
      "Iteration 10412: loss = 0.0025834899595918673\n",
      "Iteration 10413: loss = 0.00258347791773388\n",
      "Iteration 10414: loss = 0.0025834658760266202\n",
      "Iteration 10415: loss = 0.0025834538344700684\n",
      "Iteration 10416: loss = 0.0025834417930642064\n",
      "Iteration 10417: loss = 0.0025834297518090156\n",
      "Iteration 10418: loss = 0.0025834177107044772\n",
      "Iteration 10419: loss = 0.0025834056697505714\n",
      "Iteration 10420: loss = 0.0025833936289472803\n",
      "Iteration 10421: loss = 0.0025833815882945845\n",
      "Iteration 10422: loss = 0.0025833695477924658\n",
      "Iteration 10423: loss = 0.0025833575074409045\n",
      "Iteration 10424: loss = 0.002583345467239883\n",
      "Iteration 10425: loss = 0.0025833334271893817\n",
      "Iteration 10426: loss = 0.0025833213872893824\n",
      "Iteration 10427: loss = 0.002583309347539867\n",
      "Iteration 10428: loss = 0.002583297307940815\n",
      "Iteration 10429: loss = 0.0025832852684922088\n",
      "Iteration 10430: loss = 0.00258327322919403\n",
      "Iteration 10431: loss = 0.00258326119004626\n",
      "Iteration 10432: loss = 0.0025832491510488793\n",
      "Iteration 10433: loss = 0.0025832371122018697\n",
      "Iteration 10434: loss = 0.002583225073505214\n",
      "Iteration 10435: loss = 0.0025832130349588913\n",
      "Iteration 10436: loss = 0.002583200996562884\n",
      "Iteration 10437: loss = 0.002583188958317174\n",
      "Iteration 10438: loss = 0.0025831769202217425\n",
      "Iteration 10439: loss = 0.002583164882276571\n",
      "Iteration 10440: loss = 0.0025831528444816404\n",
      "Iteration 10441: loss = 0.002583140806836934\n",
      "Iteration 10442: loss = 0.002583128769342431\n",
      "Iteration 10443: loss = 0.0025831167319981144\n",
      "Iteration 10444: loss = 0.002583104694803966\n",
      "Iteration 10445: loss = 0.0025830926577599662\n",
      "Iteration 10446: loss = 0.002583080620866097\n",
      "Iteration 10447: loss = 0.0025830685841223408\n",
      "Iteration 10448: loss = 0.002583056547528678\n",
      "Iteration 10449: loss = 0.0025830445110850914\n",
      "Iteration 10450: loss = 0.0025830324747915623\n",
      "Iteration 10451: loss = 0.0025830204386480715\n",
      "Iteration 10452: loss = 0.002583008402654602\n",
      "Iteration 10453: loss = 0.0025829963668111354\n",
      "Iteration 10454: loss = 0.0025829843311176515\n",
      "Iteration 10455: loss = 0.002582972295574135\n",
      "Iteration 10456: loss = 0.002582960260180565\n",
      "Iteration 10457: loss = 0.0025829482249369257\n",
      "Iteration 10458: loss = 0.002582936189843197\n",
      "Iteration 10459: loss = 0.002582924154899361\n",
      "Iteration 10460: loss = 0.0025829121201053997\n",
      "Iteration 10461: loss = 0.0025829000854612955\n",
      "Iteration 10462: loss = 0.00258288805096703\n",
      "Iteration 10463: loss = 0.0025828760166225844\n",
      "Iteration 10464: loss = 0.002582863982427941\n",
      "Iteration 10465: loss = 0.0025828519483830823\n",
      "Iteration 10466: loss = 0.002582839914487989\n",
      "Iteration 10467: loss = 0.0025828278807426444\n",
      "Iteration 10468: loss = 0.0025828158471470296\n",
      "Iteration 10469: loss = 0.0025828038137011263\n",
      "Iteration 10470: loss = 0.002582791780404917\n",
      "Iteration 10471: loss = 0.0025827797472583835\n",
      "Iteration 10472: loss = 0.002582767714261508\n",
      "Iteration 10473: loss = 0.0025827556814142717\n",
      "Iteration 10474: loss = 0.0025827436487166583\n",
      "Iteration 10475: loss = 0.0025827316161686487\n",
      "Iteration 10476: loss = 0.0025827195837702247\n",
      "Iteration 10477: loss = 0.0025827075515213686\n",
      "Iteration 10478: loss = 0.0025826955194220625\n",
      "Iteration 10479: loss = 0.002582683487472289\n",
      "Iteration 10480: loss = 0.00258267145567203\n",
      "Iteration 10481: loss = 0.002582659424021267\n",
      "Iteration 10482: loss = 0.002582647392519983\n",
      "Iteration 10483: loss = 0.0025826353611681603\n",
      "Iteration 10484: loss = 0.00258262332996578\n",
      "Iteration 10485: loss = 0.0025826112989128258\n",
      "Iteration 10486: loss = 0.0025825992680092785\n",
      "Iteration 10487: loss = 0.002582587237255121\n",
      "Iteration 10488: loss = 0.002582575206650335\n",
      "Iteration 10489: loss = 0.0025825631761949032\n",
      "Iteration 10490: loss = 0.002582551145888808\n",
      "Iteration 10491: loss = 0.0025825391157320316\n",
      "Iteration 10492: loss = 0.0025825270857245566\n",
      "Iteration 10493: loss = 0.0025825150558663646\n",
      "Iteration 10494: loss = 0.002582503026157439\n",
      "Iteration 10495: loss = 0.0025824909965977603\n",
      "Iteration 10496: loss = 0.002582478967187313\n",
      "Iteration 10497: loss = 0.0025824669379260774\n",
      "Iteration 10498: loss = 0.0025824549088140376\n",
      "Iteration 10499: loss = 0.0025824428798511758\n",
      "Iteration 10500: loss = 0.0025824308510374737\n",
      "Iteration 10501: loss = 0.002582418822372914\n",
      "Iteration 10502: loss = 0.0025824067938574786\n",
      "Iteration 10503: loss = 0.002582394765491152\n",
      "Iteration 10504: loss = 0.002582382737273914\n",
      "Iteration 10505: loss = 0.0025823707092057484\n",
      "Iteration 10506: loss = 0.0025823586812866385\n",
      "Iteration 10507: loss = 0.002582346653516565\n",
      "Iteration 10508: loss = 0.0025823346258955124\n",
      "Iteration 10509: loss = 0.0025823225984234616\n",
      "Iteration 10510: loss = 0.002582310571100396\n",
      "Iteration 10511: loss = 0.002582298543926298\n",
      "Iteration 10512: loss = 0.002582286516901151\n",
      "Iteration 10513: loss = 0.002582274490024936\n",
      "Iteration 10514: loss = 0.002582262463297637\n",
      "Iteration 10515: loss = 0.0025822504367192358\n",
      "Iteration 10516: loss = 0.0025822384102897156\n",
      "Iteration 10517: loss = 0.0025822263840090593\n",
      "Iteration 10518: loss = 0.0025822143578772484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10519: loss = 0.0025822023318942664\n",
      "Iteration 10520: loss = 0.0025821903060600964\n",
      "Iteration 10521: loss = 0.0025821782803747205\n",
      "Iteration 10522: loss = 0.0025821662548381214\n",
      "Iteration 10523: loss = 0.0025821542294502827\n",
      "Iteration 10524: loss = 0.0025821422042111865\n",
      "Iteration 10525: loss = 0.0025821301791208156\n",
      "Iteration 10526: loss = 0.002582118154179153\n",
      "Iteration 10527: loss = 0.0025821061293861817\n",
      "Iteration 10528: loss = 0.002582094104741884\n",
      "Iteration 10529: loss = 0.002582082080246243\n",
      "Iteration 10530: loss = 0.002582070055899241\n",
      "Iteration 10531: loss = 0.0025820580317008623\n",
      "Iteration 10532: loss = 0.002582046007651089\n",
      "Iteration 10533: loss = 0.0025820339837499037\n",
      "Iteration 10534: loss = 0.002582021959997289\n",
      "Iteration 10535: loss = 0.0025820099363932296\n",
      "Iteration 10536: loss = 0.0025819979129377063\n",
      "Iteration 10537: loss = 0.0025819858896307035\n",
      "Iteration 10538: loss = 0.0025819738664722034\n",
      "Iteration 10539: loss = 0.0025819618434621892\n",
      "Iteration 10540: loss = 0.0025819498206006448\n",
      "Iteration 10541: loss = 0.0025819377978875515\n",
      "Iteration 10542: loss = 0.0025819257753228937\n",
      "Iteration 10543: loss = 0.0025819137529066545\n",
      "Iteration 10544: loss = 0.0025819017306388157\n",
      "Iteration 10545: loss = 0.0025818897085193616\n",
      "Iteration 10546: loss = 0.002581877686548275\n",
      "Iteration 10547: loss = 0.0025818656647255385\n",
      "Iteration 10548: loss = 0.0025818536430511356\n",
      "Iteration 10549: loss = 0.00258184162152505\n",
      "Iteration 10550: loss = 0.002581829600147264\n",
      "Iteration 10551: loss = 0.0025818175789177602\n",
      "Iteration 10552: loss = 0.0025818055578365235\n",
      "Iteration 10553: loss = 0.002581793536903536\n",
      "Iteration 10554: loss = 0.0025817815161187814\n",
      "Iteration 10555: loss = 0.0025817694954822423\n",
      "Iteration 10556: loss = 0.0025817574749939028\n",
      "Iteration 10557: loss = 0.002581745454653745\n",
      "Iteration 10558: loss = 0.002581733434461753\n",
      "Iteration 10559: loss = 0.00258172141441791\n",
      "Iteration 10560: loss = 0.0025817093945221994\n",
      "Iteration 10561: loss = 0.0025816973747746046\n",
      "Iteration 10562: loss = 0.0025816853551751074\n",
      "Iteration 10563: loss = 0.0025816733357236937\n",
      "Iteration 10564: loss = 0.0025816613164203446\n",
      "Iteration 10565: loss = 0.0025816492972650447\n",
      "Iteration 10566: loss = 0.0025816372782577765\n",
      "Iteration 10567: loss = 0.0025816252593985245\n",
      "Iteration 10568: loss = 0.002581613240687271\n",
      "Iteration 10569: loss = 0.002581601222124001\n",
      "Iteration 10570: loss = 0.0025815892037086953\n",
      "Iteration 10571: loss = 0.00258157718544134\n",
      "Iteration 10572: loss = 0.0025815651673219172\n",
      "Iteration 10573: loss = 0.0025815531493504113\n",
      "Iteration 10574: loss = 0.0025815411315268043\n",
      "Iteration 10575: loss = 0.002581529113851081\n",
      "Iteration 10576: loss = 0.0025815170963232243\n",
      "Iteration 10577: loss = 0.002581505078943218\n",
      "Iteration 10578: loss = 0.002581493061711045\n",
      "Iteration 10579: loss = 0.00258148104462669\n",
      "Iteration 10580: loss = 0.002581469027690136\n",
      "Iteration 10581: loss = 0.0025814570109013663\n",
      "Iteration 10582: loss = 0.0025814449942603646\n",
      "Iteration 10583: loss = 0.0025814329777671145\n",
      "Iteration 10584: loss = 0.002581420961421601\n",
      "Iteration 10585: loss = 0.002581408945223806\n",
      "Iteration 10586: loss = 0.002581396929173713\n",
      "Iteration 10587: loss = 0.0025813849132713066\n",
      "Iteration 10588: loss = 0.002581372897516571\n",
      "Iteration 10589: loss = 0.002581360881909488\n",
      "Iteration 10590: loss = 0.0025813488664500435\n",
      "Iteration 10591: loss = 0.00258133685113822\n",
      "Iteration 10592: loss = 0.0025813248359740013\n",
      "Iteration 10593: loss = 0.0025813128209573703\n",
      "Iteration 10594: loss = 0.0025813008060883123\n",
      "Iteration 10595: loss = 0.002581288791366811\n",
      "Iteration 10596: loss = 0.00258127677679285\n",
      "Iteration 10597: loss = 0.002581264762366412\n",
      "Iteration 10598: loss = 0.0025812527480874813\n",
      "Iteration 10599: loss = 0.002581240733956043\n",
      "Iteration 10600: loss = 0.002581228719972079\n",
      "Iteration 10601: loss = 0.0025812167061355753\n",
      "Iteration 10602: loss = 0.0025812046924465143\n",
      "Iteration 10603: loss = 0.00258119267890488\n",
      "Iteration 10604: loss = 0.0025811806655106563\n",
      "Iteration 10605: loss = 0.002581168652263827\n",
      "Iteration 10606: loss = 0.0025811566391643775\n",
      "Iteration 10607: loss = 0.0025811446262122895\n",
      "Iteration 10608: loss = 0.002581132613407549\n",
      "Iteration 10609: loss = 0.0025811206007501377\n",
      "Iteration 10610: loss = 0.0025811085882400417\n",
      "Iteration 10611: loss = 0.002581096575877245\n",
      "Iteration 10612: loss = 0.0025810845636617297\n",
      "Iteration 10613: loss = 0.002581072551593481\n",
      "Iteration 10614: loss = 0.0025810605396724834\n",
      "Iteration 10615: loss = 0.00258104852789872\n",
      "Iteration 10616: loss = 0.002581036516272176\n",
      "Iteration 10617: loss = 0.0025810245047928337\n",
      "Iteration 10618: loss = 0.002581012493460679\n",
      "Iteration 10619: loss = 0.0025810004822756946\n",
      "Iteration 10620: loss = 0.0025809884712378653\n",
      "Iteration 10621: loss = 0.0025809764603471758\n",
      "Iteration 10622: loss = 0.0025809644496036094\n",
      "Iteration 10623: loss = 0.00258095243900715\n",
      "Iteration 10624: loss = 0.0025809404285577825\n",
      "Iteration 10625: loss = 0.002580928418255491\n",
      "Iteration 10626: loss = 0.002580916408100259\n",
      "Iteration 10627: loss = 0.0025809043980920716\n",
      "Iteration 10628: loss = 0.002580892388230913\n",
      "Iteration 10629: loss = 0.0025808803785167663\n",
      "Iteration 10630: loss = 0.0025808683689496174\n",
      "Iteration 10631: loss = 0.002580856359529449\n",
      "Iteration 10632: loss = 0.0025808443502562464\n",
      "Iteration 10633: loss = 0.0025808323411299935\n",
      "Iteration 10634: loss = 0.0025808203321506743\n",
      "Iteration 10635: loss = 0.002580808323318274\n",
      "Iteration 10636: loss = 0.002580796314632776\n",
      "Iteration 10637: loss = 0.0025807843060941657\n",
      "Iteration 10638: loss = 0.002580772297702426\n",
      "Iteration 10639: loss = 0.0025807602894575422\n",
      "Iteration 10640: loss = 0.002580748281359499\n",
      "Iteration 10641: loss = 0.00258073627340828\n",
      "Iteration 10642: loss = 0.002580724265603869\n",
      "Iteration 10643: loss = 0.0025807122579462527\n",
      "Iteration 10644: loss = 0.0025807002504354137\n",
      "Iteration 10645: loss = 0.002580688243071337\n",
      "Iteration 10646: loss = 0.002580676235854007\n",
      "Iteration 10647: loss = 0.002580664228783408\n",
      "Iteration 10648: loss = 0.002580652221859525\n",
      "Iteration 10649: loss = 0.0025806402150823416\n",
      "Iteration 10650: loss = 0.0025806282084518428\n",
      "Iteration 10651: loss = 0.0025806162019680135\n",
      "Iteration 10652: loss = 0.0025806041956308377\n",
      "Iteration 10653: loss = 0.0025805921894403003\n",
      "Iteration 10654: loss = 0.0025805801833963856\n",
      "Iteration 10655: loss = 0.0025805681774990785\n",
      "Iteration 10656: loss = 0.0025805561717483633\n",
      "Iteration 10657: loss = 0.0025805441661442244\n",
      "Iteration 10658: loss = 0.0025805321606866467\n",
      "Iteration 10659: loss = 0.002580520155375615\n",
      "Iteration 10660: loss = 0.0025805081502111135\n",
      "Iteration 10661: loss = 0.0025804961451931278\n",
      "Iteration 10662: loss = 0.0025804841403216407\n",
      "Iteration 10663: loss = 0.002580472135596639\n",
      "Iteration 10664: loss = 0.002580460131018106\n",
      "Iteration 10665: loss = 0.0025804481265860268\n",
      "Iteration 10666: loss = 0.0025804361223003857\n",
      "Iteration 10667: loss = 0.0025804241181611688\n",
      "Iteration 10668: loss = 0.002580412114168359\n",
      "Iteration 10669: loss = 0.0025804001103219426\n",
      "Iteration 10670: loss = 0.0025803881066219034\n",
      "Iteration 10671: loss = 0.002580376103068227\n",
      "Iteration 10672: loss = 0.0025803640996608975\n",
      "Iteration 10673: loss = 0.0025803520963998995\n",
      "Iteration 10674: loss = 0.002580340093285218\n",
      "Iteration 10675: loss = 0.0025803280903168384\n",
      "Iteration 10676: loss = 0.0025803160874947454\n",
      "Iteration 10677: loss = 0.002580304084818924\n",
      "Iteration 10678: loss = 0.002580292082289358\n",
      "Iteration 10679: loss = 0.002580280079906033\n",
      "Iteration 10680: loss = 0.002580268077668934\n",
      "Iteration 10681: loss = 0.0025802560755780464\n",
      "Iteration 10682: loss = 0.0025802440736333535\n",
      "Iteration 10683: loss = 0.002580232071834842\n",
      "Iteration 10684: loss = 0.002580220070182496\n",
      "Iteration 10685: loss = 0.0025802080686762995\n",
      "Iteration 10686: loss = 0.0025801960673162397\n",
      "Iteration 10687: loss = 0.0025801840661023\n",
      "Iteration 10688: loss = 0.002580172065034466\n",
      "Iteration 10689: loss = 0.0025801600641127227\n",
      "Iteration 10690: loss = 0.002580148063337054\n",
      "Iteration 10691: loss = 0.0025801360627074467\n",
      "Iteration 10692: loss = 0.002580124062223884\n",
      "Iteration 10693: loss = 0.0025801120618863527\n",
      "Iteration 10694: loss = 0.002580100061694837\n",
      "Iteration 10695: loss = 0.0025800880616493223\n",
      "Iteration 10696: loss = 0.002580076061749793\n",
      "Iteration 10697: loss = 0.002580064061996235\n",
      "Iteration 10698: loss = 0.0025800520623886327\n",
      "Iteration 10699: loss = 0.002580040062926972\n",
      "Iteration 10700: loss = 0.0025800280636112376\n",
      "Iteration 10701: loss = 0.0025800160644414145\n",
      "Iteration 10702: loss = 0.0025800040654174886\n",
      "Iteration 10703: loss = 0.002579992066539444\n",
      "Iteration 10704: loss = 0.0025799800678072667\n",
      "Iteration 10705: loss = 0.0025799680692209417\n",
      "Iteration 10706: loss = 0.002579956070780454\n",
      "Iteration 10707: loss = 0.0025799440724857893\n",
      "Iteration 10708: loss = 0.0025799320743369325\n",
      "Iteration 10709: loss = 0.0025799200763338688\n",
      "Iteration 10710: loss = 0.002579908078476584\n",
      "Iteration 10711: loss = 0.0025798960807650625\n",
      "Iteration 10712: loss = 0.0025798840831992896\n",
      "Iteration 10713: loss = 0.0025798720857792513\n",
      "Iteration 10714: loss = 0.002579860088504933\n",
      "Iteration 10715: loss = 0.0025798480913763197\n",
      "Iteration 10716: loss = 0.0025798360943933966\n",
      "Iteration 10717: loss = 0.0025798240975561494\n",
      "Iteration 10718: loss = 0.0025798121008645627\n",
      "Iteration 10719: loss = 0.002579800104318623\n",
      "Iteration 10720: loss = 0.002579788107918315\n",
      "Iteration 10721: loss = 0.002579776111663624\n",
      "Iteration 10722: loss = 0.0025797641155545356\n",
      "Iteration 10723: loss = 0.002579752119591035\n",
      "Iteration 10724: loss = 0.0025797401237731086\n",
      "Iteration 10725: loss = 0.0025797281281007404\n",
      "Iteration 10726: loss = 0.002579716132573917\n",
      "Iteration 10727: loss = 0.0025797041371926235\n",
      "Iteration 10728: loss = 0.0025796921419568453\n",
      "Iteration 10729: loss = 0.0025796801468665675\n",
      "Iteration 10730: loss = 0.0025796681519217766\n",
      "Iteration 10731: loss = 0.0025796561571224575\n",
      "Iteration 10732: loss = 0.0025796441624685957\n",
      "Iteration 10733: loss = 0.0025796321679601763\n",
      "Iteration 10734: loss = 0.0025796201735971865\n",
      "Iteration 10735: loss = 0.00257960817937961\n",
      "Iteration 10736: loss = 0.0025795961853074334\n",
      "Iteration 10737: loss = 0.0025795841913806416\n",
      "Iteration 10738: loss = 0.002579572197599221\n",
      "Iteration 10739: loss = 0.0025795602039631565\n",
      "Iteration 10740: loss = 0.0025795482104724347\n",
      "Iteration 10741: loss = 0.0025795362171270406\n",
      "Iteration 10742: loss = 0.00257952422392696\n",
      "Iteration 10743: loss = 0.0025795122308721777\n",
      "Iteration 10744: loss = 0.002579500237962681\n",
      "Iteration 10745: loss = 0.0025794882451984544\n",
      "Iteration 10746: loss = 0.0025794762525794836\n",
      "Iteration 10747: loss = 0.0025794642601057547\n",
      "Iteration 10748: loss = 0.002579452267777254\n",
      "Iteration 10749: loss = 0.0025794402755939657\n",
      "Iteration 10750: loss = 0.002579428283555877\n",
      "Iteration 10751: loss = 0.002579416291662973\n",
      "Iteration 10752: loss = 0.0025794042999152393\n",
      "Iteration 10753: loss = 0.0025793923083126617\n",
      "Iteration 10754: loss = 0.002579380316855227\n",
      "Iteration 10755: loss = 0.00257936832554292\n",
      "Iteration 10756: loss = 0.002579356334375727\n",
      "Iteration 10757: loss = 0.002579344343353633\n",
      "Iteration 10758: loss = 0.0025793323524766244\n",
      "Iteration 10759: loss = 0.0025793203617446876\n",
      "Iteration 10760: loss = 0.0025793083711578083\n",
      "Iteration 10761: loss = 0.0025792963807159714\n",
      "Iteration 10762: loss = 0.002579284390419163\n",
      "Iteration 10763: loss = 0.00257927240026737\n",
      "Iteration 10764: loss = 0.002579260410260578\n",
      "Iteration 10765: loss = 0.002579248420398772\n",
      "Iteration 10766: loss = 0.002579236430681939\n",
      "Iteration 10767: loss = 0.0025792244411100645\n",
      "Iteration 10768: loss = 0.0025792124516831345\n",
      "Iteration 10769: loss = 0.0025792004624011346\n",
      "Iteration 10770: loss = 0.0025791884732640517\n",
      "Iteration 10771: loss = 0.002579176484271871\n",
      "Iteration 10772: loss = 0.0025791644954245786\n",
      "Iteration 10773: loss = 0.002579152506722161\n",
      "Iteration 10774: loss = 0.002579140518164604\n",
      "Iteration 10775: loss = 0.002579128529751893\n",
      "Iteration 10776: loss = 0.002579116541484015\n",
      "Iteration 10777: loss = 0.002579104553360955\n",
      "Iteration 10778: loss = 0.0025790925653827006\n",
      "Iteration 10779: loss = 0.0025790805775492364\n",
      "Iteration 10780: loss = 0.0025790685898605496\n",
      "Iteration 10781: loss = 0.0025790566023166255\n",
      "Iteration 10782: loss = 0.0025790446149174502\n",
      "Iteration 10783: loss = 0.0025790326276630103\n",
      "Iteration 10784: loss = 0.0025790206405532923\n",
      "Iteration 10785: loss = 0.002579008653588282\n",
      "Iteration 10786: loss = 0.0025789966667679643\n",
      "Iteration 10787: loss = 0.0025789846800923274\n",
      "Iteration 10788: loss = 0.002578972693561356\n",
      "Iteration 10789: loss = 0.002578960707175037\n",
      "Iteration 10790: loss = 0.002578948720933357\n",
      "Iteration 10791: loss = 0.002578936734836301\n",
      "Iteration 10792: loss = 0.0025789247488838566\n",
      "Iteration 10793: loss = 0.002578912763076009\n",
      "Iteration 10794: loss = 0.0025789007774127456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10795: loss = 0.0025788887918940513\n",
      "Iteration 10796: loss = 0.0025788768065199133\n",
      "Iteration 10797: loss = 0.002578864821290317\n",
      "Iteration 10798: loss = 0.00257885283620525\n",
      "Iteration 10799: loss = 0.0025788408512646974\n",
      "Iteration 10800: loss = 0.0025788288664686464\n",
      "Iteration 10801: loss = 0.0025788168818170827\n",
      "Iteration 10802: loss = 0.0025788048973099924\n",
      "Iteration 10803: loss = 0.0025787929129473634\n",
      "Iteration 10804: loss = 0.00257878092872918\n",
      "Iteration 10805: loss = 0.0025787689446554306\n",
      "Iteration 10806: loss = 0.0025787569607261004\n",
      "Iteration 10807: loss = 0.0025787449769411764\n",
      "Iteration 10808: loss = 0.0025787329933006434\n",
      "Iteration 10809: loss = 0.0025787210098044905\n",
      "Iteration 10810: loss = 0.0025787090264527022\n",
      "Iteration 10811: loss = 0.002578697043245265\n",
      "Iteration 10812: loss = 0.002578685060182166\n",
      "Iteration 10813: loss = 0.002578673077263391\n",
      "Iteration 10814: loss = 0.002578661094488928\n",
      "Iteration 10815: loss = 0.0025786491118587615\n",
      "Iteration 10816: loss = 0.0025786371293728795\n",
      "Iteration 10817: loss = 0.002578625147031268\n",
      "Iteration 10818: loss = 0.002578613164833914\n",
      "Iteration 10819: loss = 0.002578601182780802\n",
      "Iteration 10820: loss = 0.002578589200871922\n",
      "Iteration 10821: loss = 0.002578577219107257\n",
      "Iteration 10822: loss = 0.002578565237486796\n",
      "Iteration 10823: loss = 0.0025785532560105243\n",
      "Iteration 10824: loss = 0.0025785412746784295\n",
      "Iteration 10825: loss = 0.002578529293490498\n",
      "Iteration 10826: loss = 0.002578517312446715\n",
      "Iteration 10827: loss = 0.0025785053315470693\n",
      "Iteration 10828: loss = 0.002578493350791546\n",
      "Iteration 10829: loss = 0.002578481370180132\n",
      "Iteration 10830: loss = 0.0025784693897128144\n",
      "Iteration 10831: loss = 0.00257845740938958\n",
      "Iteration 10832: loss = 0.0025784454292104146\n",
      "Iteration 10833: loss = 0.0025784334491753057\n",
      "Iteration 10834: loss = 0.002578421469284239\n",
      "Iteration 10835: loss = 0.002578409489537203\n",
      "Iteration 10836: loss = 0.002578397509934183\n",
      "Iteration 10837: loss = 0.002578385530475166\n",
      "Iteration 10838: loss = 0.002578373551160139\n",
      "Iteration 10839: loss = 0.0025783615719890884\n",
      "Iteration 10840: loss = 0.002578349592962001\n",
      "Iteration 10841: loss = 0.002578337614078864\n",
      "Iteration 10842: loss = 0.002578325635339664\n",
      "Iteration 10843: loss = 0.0025783136567443872\n",
      "Iteration 10844: loss = 0.0025783016782930214\n",
      "Iteration 10845: loss = 0.002578289699985553\n",
      "Iteration 10846: loss = 0.0025782777218219693\n",
      "Iteration 10847: loss = 0.002578265743802255\n",
      "Iteration 10848: loss = 0.0025782537659264004\n",
      "Iteration 10849: loss = 0.0025782417881943893\n",
      "Iteration 10850: loss = 0.0025782298106062107\n",
      "Iteration 10851: loss = 0.00257821783316185\n",
      "Iteration 10852: loss = 0.0025782058558612955\n",
      "Iteration 10853: loss = 0.002578193878704532\n",
      "Iteration 10854: loss = 0.0025781819016915493\n",
      "Iteration 10855: loss = 0.002578169924822332\n",
      "Iteration 10856: loss = 0.0025781579480968673\n",
      "Iteration 10857: loss = 0.0025781459715151435\n",
      "Iteration 10858: loss = 0.002578133995077146\n",
      "Iteration 10859: loss = 0.0025781220187828633\n",
      "Iteration 10860: loss = 0.002578110042632281\n",
      "Iteration 10861: loss = 0.0025780980666253867\n",
      "Iteration 10862: loss = 0.0025780860907621684\n",
      "Iteration 10863: loss = 0.002578074115042611\n",
      "Iteration 10864: loss = 0.0025780621394667024\n",
      "Iteration 10865: loss = 0.0025780501640344303\n",
      "Iteration 10866: loss = 0.002578038188745782\n",
      "Iteration 10867: loss = 0.0025780262136007428\n",
      "Iteration 10868: loss = 0.0025780142385993013\n",
      "Iteration 10869: loss = 0.0025780022637414443\n",
      "Iteration 10870: loss = 0.0025779902890271586\n",
      "Iteration 10871: loss = 0.002577978314456431\n",
      "Iteration 10872: loss = 0.0025779663400292492\n",
      "Iteration 10873: loss = 0.0025779543657456004\n",
      "Iteration 10874: loss = 0.0025779423916054713\n",
      "Iteration 10875: loss = 0.002577930417608849\n",
      "Iteration 10876: loss = 0.002577918443755721\n",
      "Iteration 10877: loss = 0.0025779064700460746\n",
      "Iteration 10878: loss = 0.002577894496479897\n",
      "Iteration 10879: loss = 0.002577882523057174\n",
      "Iteration 10880: loss = 0.0025778705497778946\n",
      "Iteration 10881: loss = 0.002577858576642045\n",
      "Iteration 10882: loss = 0.002577846603649613\n",
      "Iteration 10883: loss = 0.002577834630800585\n",
      "Iteration 10884: loss = 0.002577822658094949\n",
      "Iteration 10885: loss = 0.002577810685532693\n",
      "Iteration 10886: loss = 0.002577798713113802\n",
      "Iteration 10887: loss = 0.002577786740838265\n",
      "Iteration 10888: loss = 0.002577774768706069\n",
      "Iteration 10889: loss = 0.0025777627967172004\n",
      "Iteration 10890: loss = 0.0025777508248716473\n",
      "Iteration 10891: loss = 0.002577738853169397\n",
      "Iteration 10892: loss = 0.0025777268816104372\n",
      "Iteration 10893: loss = 0.002577714910194755\n",
      "Iteration 10894: loss = 0.002577702938922337\n",
      "Iteration 10895: loss = 0.002577690967793171\n",
      "Iteration 10896: loss = 0.0025776789968072447\n",
      "Iteration 10897: loss = 0.0025776670259645444\n",
      "Iteration 10898: loss = 0.002577655055265059\n",
      "Iteration 10899: loss = 0.002577643084708775\n",
      "Iteration 10900: loss = 0.00257763111429568\n",
      "Iteration 10901: loss = 0.002577619144025761\n",
      "Iteration 10902: loss = 0.0025776071738990063\n",
      "Iteration 10903: loss = 0.0025775952039154026\n",
      "Iteration 10904: loss = 0.002577583234074938\n",
      "Iteration 10905: loss = 0.002577571264377599\n",
      "Iteration 10906: loss = 0.002577559294823374\n",
      "Iteration 10907: loss = 0.0025775473254122496\n",
      "Iteration 10908: loss = 0.002577535356144214\n",
      "Iteration 10909: loss = 0.0025775233870192547\n",
      "Iteration 10910: loss = 0.002577511418037358\n",
      "Iteration 10911: loss = 0.002577499449198513\n",
      "Iteration 10912: loss = 0.002577487480502707\n",
      "Iteration 10913: loss = 0.0025774755119499265\n",
      "Iteration 10914: loss = 0.0025774635435401603\n",
      "Iteration 10915: loss = 0.0025774515752733946\n",
      "Iteration 10916: loss = 0.0025774396071496174\n",
      "Iteration 10917: loss = 0.002577427639168818\n",
      "Iteration 10918: loss = 0.002577415671330981\n",
      "Iteration 10919: loss = 0.0025774037036360967\n",
      "Iteration 10920: loss = 0.0025773917360841507\n",
      "Iteration 10921: loss = 0.0025773797686751317\n",
      "Iteration 10922: loss = 0.002577367801409028\n",
      "Iteration 10923: loss = 0.0025773558342858252\n",
      "Iteration 10924: loss = 0.0025773438673055123\n",
      "Iteration 10925: loss = 0.0025773319004680773\n",
      "Iteration 10926: loss = 0.002577319933773507\n",
      "Iteration 10927: loss = 0.0025773079672217892\n",
      "Iteration 10928: loss = 0.0025772960008129116\n",
      "Iteration 10929: loss = 0.002577284034546862\n",
      "Iteration 10930: loss = 0.0025772720684236286\n",
      "Iteration 10931: loss = 0.002577260102443199\n",
      "Iteration 10932: loss = 0.00257724813660556\n",
      "Iteration 10933: loss = 0.0025772361709106998\n",
      "Iteration 10934: loss = 0.0025772242053586068\n",
      "Iteration 10935: loss = 0.002577212239949268\n",
      "Iteration 10936: loss = 0.0025772002746826716\n",
      "Iteration 10937: loss = 0.002577188309558805\n",
      "Iteration 10938: loss = 0.002577176344577656\n",
      "Iteration 10939: loss = 0.002577164379739213\n",
      "Iteration 10940: loss = 0.0025771524150434637\n",
      "Iteration 10941: loss = 0.002577140450490395\n",
      "Iteration 10942: loss = 0.002577128486079995\n",
      "Iteration 10943: loss = 0.0025771165218122527\n",
      "Iteration 10944: loss = 0.0025771045576871545\n",
      "Iteration 10945: loss = 0.002577092593704689\n",
      "Iteration 10946: loss = 0.0025770806298648437\n",
      "Iteration 10947: loss = 0.0025770686661676073\n",
      "Iteration 10948: loss = 0.002577056702612967\n",
      "Iteration 10949: loss = 0.0025770447392009107\n",
      "Iteration 10950: loss = 0.002577032775931426\n",
      "Iteration 10951: loss = 0.0025770208128045014\n",
      "Iteration 10952: loss = 0.0025770088498201244\n",
      "Iteration 10953: loss = 0.0025769968869782836\n",
      "Iteration 10954: loss = 0.0025769849242789663\n",
      "Iteration 10955: loss = 0.002576972961722162\n",
      "Iteration 10956: loss = 0.0025769609993078553\n",
      "Iteration 10957: loss = 0.0025769490370360373\n",
      "Iteration 10958: loss = 0.002576937074906694\n",
      "Iteration 10959: loss = 0.0025769251129198153\n",
      "Iteration 10960: loss = 0.0025769131510753876\n",
      "Iteration 10961: loss = 0.0025769011893733997\n",
      "Iteration 10962: loss = 0.002576889227813839\n",
      "Iteration 10963: loss = 0.002576877266396694\n",
      "Iteration 10964: loss = 0.0025768653051219535\n",
      "Iteration 10965: loss = 0.002576853343989604\n",
      "Iteration 10966: loss = 0.002576841382999634\n",
      "Iteration 10967: loss = 0.0025768294221520326\n",
      "Iteration 10968: loss = 0.002576817461446787\n",
      "Iteration 10969: loss = 0.002576805500883886\n",
      "Iteration 10970: loss = 0.002576793540463316\n",
      "Iteration 10971: loss = 0.002576781580185067\n",
      "Iteration 10972: loss = 0.0025767696200491258\n",
      "Iteration 10973: loss = 0.002576757660055481\n",
      "Iteration 10974: loss = 0.002576745700204122\n",
      "Iteration 10975: loss = 0.0025767337404950345\n",
      "Iteration 10976: loss = 0.002576721780928209\n",
      "Iteration 10977: loss = 0.002576709821503632\n",
      "Iteration 10978: loss = 0.002576697862221292\n",
      "Iteration 10979: loss = 0.002576685903081178\n",
      "Iteration 10980: loss = 0.0025766739440832775\n",
      "Iteration 10981: loss = 0.002576661985227579\n",
      "Iteration 10982: loss = 0.0025766500265140702\n",
      "Iteration 10983: loss = 0.0025766380679427404\n",
      "Iteration 10984: loss = 0.002576626109513576\n",
      "Iteration 10985: loss = 0.0025766141512265674\n",
      "Iteration 10986: loss = 0.0025766021930817016\n",
      "Iteration 10987: loss = 0.002576590235078967\n",
      "Iteration 10988: loss = 0.002576578277218352\n",
      "Iteration 10989: loss = 0.0025765663194998447\n",
      "Iteration 10990: loss = 0.002576554361923434\n",
      "Iteration 10991: loss = 0.0025765424044891074\n",
      "Iteration 10992: loss = 0.002576530447196853\n",
      "Iteration 10993: loss = 0.0025765184900466607\n",
      "Iteration 10994: loss = 0.002576506533038517\n",
      "Iteration 10995: loss = 0.0025764945761724114\n",
      "Iteration 10996: loss = 0.0025764826194483318\n",
      "Iteration 10997: loss = 0.0025764706628662666\n",
      "Iteration 10998: loss = 0.0025764587064262046\n",
      "Iteration 10999: loss = 0.0025764467501281333\n",
      "Iteration 11000: loss = 0.002576434793972042\n",
      "Iteration 11001: loss = 0.0025764228379579184\n",
      "Iteration 11002: loss = 0.002576410882085751\n",
      "Iteration 11003: loss = 0.0025763989263555285\n",
      "Iteration 11004: loss = 0.0025763869707672396\n",
      "Iteration 11005: loss = 0.0025763750153208714\n",
      "Iteration 11006: loss = 0.002576363060016414\n",
      "Iteration 11007: loss = 0.002576351104853855\n",
      "Iteration 11008: loss = 0.002576339149833183\n",
      "Iteration 11009: loss = 0.002576327194954387\n",
      "Iteration 11010: loss = 0.002576315240217454\n",
      "Iteration 11011: loss = 0.002576303285622374\n",
      "Iteration 11012: loss = 0.0025762913311691347\n",
      "Iteration 11013: loss = 0.002576279376857724\n",
      "Iteration 11014: loss = 0.002576267422688133\n",
      "Iteration 11015: loss = 0.002576255468660347\n",
      "Iteration 11016: loss = 0.002576243514774357\n",
      "Iteration 11017: loss = 0.00257623156103015\n",
      "Iteration 11018: loss = 0.002576219607427715\n",
      "Iteration 11019: loss = 0.0025762076539670407\n",
      "Iteration 11020: loss = 0.002576195700648116\n",
      "Iteration 11021: loss = 0.0025761837474709286\n",
      "Iteration 11022: loss = 0.002576171794435468\n",
      "Iteration 11023: loss = 0.002576159841541722\n",
      "Iteration 11024: loss = 0.00257614788878968\n",
      "Iteration 11025: loss = 0.00257613593617933\n",
      "Iteration 11026: loss = 0.0025761239837106607\n",
      "Iteration 11027: loss = 0.002576112031383661\n",
      "Iteration 11028: loss = 0.0025761000791983193\n",
      "Iteration 11029: loss = 0.0025760881271546248\n",
      "Iteration 11030: loss = 0.0025760761752525656\n",
      "Iteration 11031: loss = 0.0025760642234921305\n",
      "Iteration 11032: loss = 0.002576052271873308\n",
      "Iteration 11033: loss = 0.0025760403203960863\n",
      "Iteration 11034: loss = 0.002576028369060456\n",
      "Iteration 11035: loss = 0.0025760164178664042\n",
      "Iteration 11036: loss = 0.0025760044668139198\n",
      "Iteration 11037: loss = 0.0025759925159029914\n",
      "Iteration 11038: loss = 0.002575980565133609\n",
      "Iteration 11039: loss = 0.0025759686145057597\n",
      "Iteration 11040: loss = 0.0025759566640194335\n",
      "Iteration 11041: loss = 0.002575944713674618\n",
      "Iteration 11042: loss = 0.002575932763471303\n",
      "Iteration 11043: loss = 0.0025759208134094767\n",
      "Iteration 11044: loss = 0.0025759088634891283\n",
      "Iteration 11045: loss = 0.002575896913710246\n",
      "Iteration 11046: loss = 0.002575884964072819\n",
      "Iteration 11047: loss = 0.002575873014576836\n",
      "Iteration 11048: loss = 0.002575861065222286\n",
      "Iteration 11049: loss = 0.0025758491160091583\n",
      "Iteration 11050: loss = 0.00257583716693744\n",
      "Iteration 11051: loss = 0.0025758252180071222\n",
      "Iteration 11052: loss = 0.002575813269218192\n",
      "Iteration 11053: loss = 0.00257580132057064\n",
      "Iteration 11054: loss = 0.002575789372064453\n",
      "Iteration 11055: loss = 0.0025757774236996214\n",
      "Iteration 11056: loss = 0.002575765475476133\n",
      "Iteration 11057: loss = 0.002575753527393978\n",
      "Iteration 11058: loss = 0.0025757415794531446\n",
      "Iteration 11059: loss = 0.002575729631653622\n",
      "Iteration 11060: loss = 0.002575717683995399\n",
      "Iteration 11061: loss = 0.002575705736478463\n",
      "Iteration 11062: loss = 0.0025756937891028057\n",
      "Iteration 11063: loss = 0.0025756818418684146\n",
      "Iteration 11064: loss = 0.0025756698947752787\n",
      "Iteration 11065: loss = 0.0025756579478233873\n",
      "Iteration 11066: loss = 0.0025756460010127285\n",
      "Iteration 11067: loss = 0.002575634054343293\n",
      "Iteration 11068: loss = 0.0025756221078150677\n",
      "Iteration 11069: loss = 0.0025756101614280436\n",
      "Iteration 11070: loss = 0.0025755982151822085\n",
      "Iteration 11071: loss = 0.0025755862690775514\n",
      "Iteration 11072: loss = 0.002575574323114062\n",
      "Iteration 11073: loss = 0.002575562377291729\n",
      "Iteration 11074: loss = 0.002575550431610542\n",
      "Iteration 11075: loss = 0.002575538486070489\n",
      "Iteration 11076: loss = 0.002575526540671559\n",
      "Iteration 11077: loss = 0.0025755145954137427\n",
      "Iteration 11078: loss = 0.0025755026502970284\n",
      "Iteration 11079: loss = 0.002575490705321404\n",
      "Iteration 11080: loss = 0.00257547876048686\n",
      "Iteration 11081: loss = 0.002575466815793385\n",
      "Iteration 11082: loss = 0.0025754548712409683\n",
      "Iteration 11083: loss = 0.0025754429268295994\n",
      "Iteration 11084: loss = 0.0025754309825592667\n",
      "Iteration 11085: loss = 0.00257541903842996\n",
      "Iteration 11086: loss = 0.002575407094441667\n",
      "Iteration 11087: loss = 0.002575395150594379\n",
      "Iteration 11088: loss = 0.0025753832068880845\n",
      "Iteration 11089: loss = 0.0025753712633227715\n",
      "Iteration 11090: loss = 0.0025753593198984304\n",
      "Iteration 11091: loss = 0.0025753473766150503\n",
      "Iteration 11092: loss = 0.0025753354334726196\n",
      "Iteration 11093: loss = 0.002575323490471128\n",
      "Iteration 11094: loss = 0.0025753115476105652\n",
      "Iteration 11095: loss = 0.0025752996048909204\n",
      "Iteration 11096: loss = 0.0025752876623121815\n",
      "Iteration 11097: loss = 0.0025752757198743404\n",
      "Iteration 11098: loss = 0.0025752637775773835\n",
      "Iteration 11099: loss = 0.0025752518354213018\n",
      "Iteration 11100: loss = 0.002575239893406084\n",
      "Iteration 11101: loss = 0.002575227951531719\n",
      "Iteration 11102: loss = 0.002575216009798197\n",
      "Iteration 11103: loss = 0.002575204068205507\n",
      "Iteration 11104: loss = 0.002575192126753638\n",
      "Iteration 11105: loss = 0.002575180185442579\n",
      "Iteration 11106: loss = 0.0025751682442723208\n",
      "Iteration 11107: loss = 0.0025751563032428504\n",
      "Iteration 11108: loss = 0.00257514436235416\n",
      "Iteration 11109: loss = 0.0025751324216062373\n",
      "Iteration 11110: loss = 0.002575120480999071\n",
      "Iteration 11111: loss = 0.0025751085405326516\n",
      "Iteration 11112: loss = 0.0025750966002069686\n",
      "Iteration 11113: loss = 0.002575084660022011\n",
      "Iteration 11114: loss = 0.002575072719977768\n",
      "Iteration 11115: loss = 0.002575060780074229\n",
      "Iteration 11116: loss = 0.0025750488403113838\n",
      "Iteration 11117: loss = 0.0025750369006892217\n",
      "Iteration 11118: loss = 0.0025750249612077323\n",
      "Iteration 11119: loss = 0.0025750130218669047\n",
      "Iteration 11120: loss = 0.002575001082666728\n",
      "Iteration 11121: loss = 0.0025749891436071927\n",
      "Iteration 11122: loss = 0.002574977204688287\n",
      "Iteration 11123: loss = 0.0025749652659100017\n",
      "Iteration 11124: loss = 0.0025749533272723256\n",
      "Iteration 11125: loss = 0.002574941388775248\n",
      "Iteration 11126: loss = 0.0025749294504187584\n",
      "Iteration 11127: loss = 0.0025749175122028473\n",
      "Iteration 11128: loss = 0.002574905574127503\n",
      "Iteration 11129: loss = 0.0025748936361927157\n",
      "Iteration 11130: loss = 0.0025748816983984747\n",
      "Iteration 11131: loss = 0.0025748697607447697\n",
      "Iteration 11132: loss = 0.00257485782323159\n",
      "Iteration 11133: loss = 0.0025748458858589254\n",
      "Iteration 11134: loss = 0.002574833948626765\n",
      "Iteration 11135: loss = 0.002574822011535099\n",
      "Iteration 11136: loss = 0.0025748100745839166\n",
      "Iteration 11137: loss = 0.0025747981377732074\n",
      "Iteration 11138: loss = 0.002574786201102961\n",
      "Iteration 11139: loss = 0.0025747742645731674\n",
      "Iteration 11140: loss = 0.002574762328183816\n",
      "Iteration 11141: loss = 0.002574750391934896\n",
      "Iteration 11142: loss = 0.002574738455826398\n",
      "Iteration 11143: loss = 0.0025747265198583103\n",
      "Iteration 11144: loss = 0.0025747145840306234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11145: loss = 0.0025747026483433273\n",
      "Iteration 11146: loss = 0.0025746907127964106\n",
      "Iteration 11147: loss = 0.0025746787773898642\n",
      "Iteration 11148: loss = 0.0025746668421236765\n",
      "Iteration 11149: loss = 0.002574654906997838\n",
      "Iteration 11150: loss = 0.0025746429720123385\n",
      "Iteration 11151: loss = 0.0025746310371671673\n",
      "Iteration 11152: loss = 0.0025746191024623145\n",
      "Iteration 11153: loss = 0.002574607167897769\n",
      "Iteration 11154: loss = 0.0025745952334735216\n",
      "Iteration 11155: loss = 0.0025745832991895617\n",
      "Iteration 11156: loss = 0.002574571365045879\n",
      "Iteration 11157: loss = 0.0025745594310424623\n",
      "Iteration 11158: loss = 0.0025745474971793024\n",
      "Iteration 11159: loss = 0.0025745355634563894\n",
      "Iteration 11160: loss = 0.0025745236298737122\n",
      "Iteration 11161: loss = 0.0025745116964312614\n",
      "Iteration 11162: loss = 0.002574499763129026\n",
      "Iteration 11163: loss = 0.0025744878299669965\n",
      "Iteration 11164: loss = 0.002574475896945162\n",
      "Iteration 11165: loss = 0.002574463964063513\n",
      "Iteration 11166: loss = 0.0025744520313220384\n",
      "Iteration 11167: loss = 0.002574440098720729\n",
      "Iteration 11168: loss = 0.0025744281662595744\n",
      "Iteration 11169: loss = 0.002574416233938564\n",
      "Iteration 11170: loss = 0.0025744043017576885\n",
      "Iteration 11171: loss = 0.002574392369716937\n",
      "Iteration 11172: loss = 0.0025743804378163\n",
      "Iteration 11173: loss = 0.0025743685060557664\n",
      "Iteration 11174: loss = 0.002574356574435327\n",
      "Iteration 11175: loss = 0.002574344642954972\n",
      "Iteration 11176: loss = 0.0025743327116146903\n",
      "Iteration 11177: loss = 0.002574320780414472\n",
      "Iteration 11178: loss = 0.0025743088493543074\n",
      "Iteration 11179: loss = 0.0025742969184341866\n",
      "Iteration 11180: loss = 0.0025742849876540987\n",
      "Iteration 11181: loss = 0.0025742730570140347\n",
      "Iteration 11182: loss = 0.0025742611265139837\n",
      "Iteration 11183: loss = 0.0025742491961539362\n",
      "Iteration 11184: loss = 0.002574237265933882\n",
      "Iteration 11185: loss = 0.002574225335853811\n",
      "Iteration 11186: loss = 0.0025742134059137133\n",
      "Iteration 11187: loss = 0.002574201476113579\n",
      "Iteration 11188: loss = 0.002574189546453398\n",
      "Iteration 11189: loss = 0.00257417761693316\n",
      "Iteration 11190: loss = 0.002574165687552855\n",
      "Iteration 11191: loss = 0.0025741537583124745\n",
      "Iteration 11192: loss = 0.0025741418292120058\n",
      "Iteration 11193: loss = 0.002574129900251441\n",
      "Iteration 11194: loss = 0.00257411797143077\n",
      "Iteration 11195: loss = 0.0025741060427499824\n",
      "Iteration 11196: loss = 0.002574094114209068\n",
      "Iteration 11197: loss = 0.0025740821858080175\n",
      "Iteration 11198: loss = 0.0025740702575468206\n",
      "Iteration 11199: loss = 0.002574058329425468\n",
      "Iteration 11200: loss = 0.002574046401443949\n",
      "Iteration 11201: loss = 0.002574034473602254\n",
      "Iteration 11202: loss = 0.002574022545900373\n",
      "Iteration 11203: loss = 0.002574010618338296\n",
      "Iteration 11204: loss = 0.0025739986909160137\n",
      "Iteration 11205: loss = 0.002573986763633516\n",
      "Iteration 11206: loss = 0.0025739748364907926\n",
      "Iteration 11207: loss = 0.002573962909487834\n",
      "Iteration 11208: loss = 0.0025739509826246307\n",
      "Iteration 11209: loss = 0.0025739390559011725\n",
      "Iteration 11210: loss = 0.0025739271293174493\n",
      "Iteration 11211: loss = 0.0025739152028734515\n",
      "Iteration 11212: loss = 0.00257390327656917\n",
      "Iteration 11213: loss = 0.0025738913504045943\n",
      "Iteration 11214: loss = 0.0025738794243797144\n",
      "Iteration 11215: loss = 0.00257386749849452\n",
      "Iteration 11216: loss = 0.0025738555727490026\n",
      "Iteration 11217: loss = 0.0025738436471431534\n",
      "Iteration 11218: loss = 0.0025738317216769595\n",
      "Iteration 11219: loss = 0.0025738197963504133\n",
      "Iteration 11220: loss = 0.0025738078711635046\n",
      "Iteration 11221: loss = 0.0025737959461162234\n",
      "Iteration 11222: loss = 0.0025737840212085605\n",
      "Iteration 11223: loss = 0.0025737720964405055\n",
      "Iteration 11224: loss = 0.00257376017181205\n",
      "Iteration 11225: loss = 0.0025737482473231826\n",
      "Iteration 11226: loss = 0.002573736322973894\n",
      "Iteration 11227: loss = 0.0025737243987641756\n",
      "Iteration 11228: loss = 0.0025737124746940164\n",
      "Iteration 11229: loss = 0.002573700550763408\n",
      "Iteration 11230: loss = 0.0025736886269723393\n",
      "Iteration 11231: loss = 0.0025736767033208013\n",
      "Iteration 11232: loss = 0.0025736647798087846\n",
      "Iteration 11233: loss = 0.002573652856436279\n",
      "Iteration 11234: loss = 0.002573640933203276\n",
      "Iteration 11235: loss = 0.0025736290101097646\n",
      "Iteration 11236: loss = 0.002573617087155736\n",
      "Iteration 11237: loss = 0.00257360516434118\n",
      "Iteration 11238: loss = 0.002573593241666087\n",
      "Iteration 11239: loss = 0.0025735813191304483\n",
      "Iteration 11240: loss = 0.0025735693967342535\n",
      "Iteration 11241: loss = 0.0025735574744774935\n",
      "Iteration 11242: loss = 0.0025735455523601585\n",
      "Iteration 11243: loss = 0.0025735336303822384\n",
      "Iteration 11244: loss = 0.002573521708543724\n",
      "Iteration 11245: loss = 0.0025735097868446063\n",
      "Iteration 11246: loss = 0.0025734978652848747\n",
      "Iteration 11247: loss = 0.002573485943864521\n",
      "Iteration 11248: loss = 0.0025734740225835345\n",
      "Iteration 11249: loss = 0.002573462101441906\n",
      "Iteration 11250: loss = 0.002573450180439625\n",
      "Iteration 11251: loss = 0.002573438259576685\n",
      "Iteration 11252: loss = 0.0025734263388530734\n",
      "Iteration 11253: loss = 0.002573414418268782\n",
      "Iteration 11254: loss = 0.002573402497823801\n",
      "Iteration 11255: loss = 0.0025733905775181215\n",
      "Iteration 11256: loss = 0.0025733786573517337\n",
      "Iteration 11257: loss = 0.0025733667373246272\n",
      "Iteration 11258: loss = 0.0025733548174367944\n",
      "Iteration 11259: loss = 0.0025733428976882234\n",
      "Iteration 11260: loss = 0.0025733309780789065\n",
      "Iteration 11261: loss = 0.0025733190586088345\n",
      "Iteration 11262: loss = 0.002573307139277997\n",
      "Iteration 11263: loss = 0.0025732952200863855\n",
      "Iteration 11264: loss = 0.0025732833010339894\n",
      "Iteration 11265: loss = 0.0025732713821207997\n",
      "Iteration 11266: loss = 0.0025732594633468076\n",
      "Iteration 11267: loss = 0.0025732475447120037\n",
      "Iteration 11268: loss = 0.002573235626216377\n",
      "Iteration 11269: loss = 0.0025732237078599208\n",
      "Iteration 11270: loss = 0.002573211789642623\n",
      "Iteration 11271: loss = 0.0025731998715644764\n",
      "Iteration 11272: loss = 0.0025731879536254705\n",
      "Iteration 11273: loss = 0.0025731760358255955\n",
      "Iteration 11274: loss = 0.0025731641181648435\n",
      "Iteration 11275: loss = 0.0025731522006432043\n",
      "Iteration 11276: loss = 0.002573140283260668\n",
      "Iteration 11277: loss = 0.0025731283660172266\n",
      "Iteration 11278: loss = 0.00257311644891287\n",
      "Iteration 11279: loss = 0.002573104531947589\n",
      "Iteration 11280: loss = 0.0025730926151213743\n",
      "Iteration 11281: loss = 0.0025730806984342164\n",
      "Iteration 11282: loss = 0.002573068781886107\n",
      "Iteration 11283: loss = 0.0025730568654770353\n",
      "Iteration 11284: loss = 0.002573044949206993\n",
      "Iteration 11285: loss = 0.0025730330330759705\n",
      "Iteration 11286: loss = 0.002573021117083959\n",
      "Iteration 11287: loss = 0.0025730092012309484\n",
      "Iteration 11288: loss = 0.00257299728551693\n",
      "Iteration 11289: loss = 0.002572985369941895\n",
      "Iteration 11290: loss = 0.0025729734545058332\n",
      "Iteration 11291: loss = 0.002572961539208736\n",
      "Iteration 11292: loss = 0.0025729496240505948\n",
      "Iteration 11293: loss = 0.0025729377090313987\n",
      "Iteration 11294: loss = 0.00257292579415114\n",
      "Iteration 11295: loss = 0.002572913879409808\n",
      "Iteration 11296: loss = 0.0025729019648073953\n",
      "Iteration 11297: loss = 0.002572890050343892\n",
      "Iteration 11298: loss = 0.0025728781360192885\n",
      "Iteration 11299: loss = 0.002572866221833576\n",
      "Iteration 11300: loss = 0.0025728543077867456\n",
      "Iteration 11301: loss = 0.0025728423938787873\n",
      "Iteration 11302: loss = 0.002572830480109693\n",
      "Iteration 11303: loss = 0.0025728185664794525\n",
      "Iteration 11304: loss = 0.0025728066529880573\n",
      "Iteration 11305: loss = 0.002572794739635498\n",
      "Iteration 11306: loss = 0.002572782826421766\n",
      "Iteration 11307: loss = 0.002572770913346852\n",
      "Iteration 11308: loss = 0.002572759000410747\n",
      "Iteration 11309: loss = 0.0025727470876134415\n",
      "Iteration 11310: loss = 0.0025727351749549265\n",
      "Iteration 11311: loss = 0.002572723262435193\n",
      "Iteration 11312: loss = 0.002572711350054232\n",
      "Iteration 11313: loss = 0.002572699437812034\n",
      "Iteration 11314: loss = 0.0025726875257085905\n",
      "Iteration 11315: loss = 0.002572675613743893\n",
      "Iteration 11316: loss = 0.0025726637019179307\n",
      "Iteration 11317: loss = 0.002572651790230696\n",
      "Iteration 11318: loss = 0.0025726398786821794\n",
      "Iteration 11319: loss = 0.002572627967272372\n",
      "Iteration 11320: loss = 0.0025726160560012646\n",
      "Iteration 11321: loss = 0.0025726041448688483\n",
      "Iteration 11322: loss = 0.002572592233875114\n",
      "Iteration 11323: loss = 0.002572580323020053\n",
      "Iteration 11324: loss = 0.002572568412303656\n",
      "Iteration 11325: loss = 0.0025725565017259144\n",
      "Iteration 11326: loss = 0.0025725445912868185\n",
      "Iteration 11327: loss = 0.00257253268098636\n",
      "Iteration 11328: loss = 0.00257252077082453\n",
      "Iteration 11329: loss = 0.002572508860801319\n",
      "Iteration 11330: loss = 0.002572496950916718\n",
      "Iteration 11331: loss = 0.0025724850411707185\n",
      "Iteration 11332: loss = 0.0025724731315633115\n",
      "Iteration 11333: loss = 0.002572461222094488\n",
      "Iteration 11334: loss = 0.0025724493127642395\n",
      "Iteration 11335: loss = 0.002572437403572556\n",
      "Iteration 11336: loss = 0.00257242549451943\n",
      "Iteration 11337: loss = 0.002572413585604851\n",
      "Iteration 11338: loss = 0.0025724016768288115\n",
      "Iteration 11339: loss = 0.0025723897681913022\n",
      "Iteration 11340: loss = 0.002572377859692314\n",
      "Iteration 11341: loss = 0.002572365951331838\n",
      "Iteration 11342: loss = 0.0025723540431098653\n",
      "Iteration 11343: loss = 0.0025723421350263877\n",
      "Iteration 11344: loss = 0.0025723302270813956\n",
      "Iteration 11345: loss = 0.00257231831927488\n",
      "Iteration 11346: loss = 0.002572306411606833\n",
      "Iteration 11347: loss = 0.002572294504077245\n",
      "Iteration 11348: loss = 0.0025722825966861076\n",
      "Iteration 11349: loss = 0.002572270689433412\n",
      "Iteration 11350: loss = 0.0025722587823191487\n",
      "Iteration 11351: loss = 0.00257224687534331\n",
      "Iteration 11352: loss = 0.0025722349685058862\n",
      "Iteration 11353: loss = 0.0025722230618068684\n",
      "Iteration 11354: loss = 0.002572211155246248\n",
      "Iteration 11355: loss = 0.0025721992488240177\n",
      "Iteration 11356: loss = 0.0025721873425401665\n",
      "Iteration 11357: loss = 0.002572175436394687\n",
      "Iteration 11358: loss = 0.00257216353038757\n",
      "Iteration 11359: loss = 0.0025721516245188064\n",
      "Iteration 11360: loss = 0.0025721397187883884\n",
      "Iteration 11361: loss = 0.0025721278131963063\n",
      "Iteration 11362: loss = 0.0025721159077425515\n",
      "Iteration 11363: loss = 0.002572104002427116\n",
      "Iteration 11364: loss = 0.002572092097249991\n",
      "Iteration 11365: loss = 0.0025720801922111667\n",
      "Iteration 11366: loss = 0.0025720682873106356\n",
      "Iteration 11367: loss = 0.002572056382548388\n",
      "Iteration 11368: loss = 0.0025720444779244165\n",
      "Iteration 11369: loss = 0.002572032573438712\n",
      "Iteration 11370: loss = 0.0025720206690912645\n",
      "Iteration 11371: loss = 0.0025720087648820667\n",
      "Iteration 11372: loss = 0.0025719968608111095\n",
      "Iteration 11373: loss = 0.002571984956878384\n",
      "Iteration 11374: loss = 0.002571973053083882\n",
      "Iteration 11375: loss = 0.0025719611494275955\n",
      "Iteration 11376: loss = 0.002571949245909514\n",
      "Iteration 11377: loss = 0.0025719373425296304\n",
      "Iteration 11378: loss = 0.0025719254392879355\n",
      "Iteration 11379: loss = 0.0025719135361844205\n",
      "Iteration 11380: loss = 0.0025719016332190776\n",
      "Iteration 11381: loss = 0.0025718897303918975\n",
      "Iteration 11382: loss = 0.0025718778277028717\n",
      "Iteration 11383: loss = 0.002571865925151992\n",
      "Iteration 11384: loss = 0.002571854022739249\n",
      "Iteration 11385: loss = 0.0025718421204646348\n",
      "Iteration 11386: loss = 0.002571830218328141\n",
      "Iteration 11387: loss = 0.0025718183163297584\n",
      "Iteration 11388: loss = 0.002571806414469479\n",
      "Iteration 11389: loss = 0.0025717945127472935\n",
      "Iteration 11390: loss = 0.002571782611163194\n",
      "Iteration 11391: loss = 0.0025717707097171723\n",
      "Iteration 11392: loss = 0.002571758808409219\n",
      "Iteration 11393: loss = 0.0025717469072393255\n",
      "Iteration 11394: loss = 0.002571735006207484\n",
      "Iteration 11395: loss = 0.0025717231053136863\n",
      "Iteration 11396: loss = 0.002571711204557923\n",
      "Iteration 11397: loss = 0.0025716993039401853\n",
      "Iteration 11398: loss = 0.002571687403460466\n",
      "Iteration 11399: loss = 0.002571675503118756\n",
      "Iteration 11400: loss = 0.0025716636029150465\n",
      "Iteration 11401: loss = 0.002571651702849329\n",
      "Iteration 11402: loss = 0.0025716398029215957\n",
      "Iteration 11403: loss = 0.0025716279031318375\n",
      "Iteration 11404: loss = 0.002571616003480046\n",
      "Iteration 11405: loss = 0.0025716041039662136\n",
      "Iteration 11406: loss = 0.002571592204590331\n",
      "Iteration 11407: loss = 0.00257158030535239\n",
      "Iteration 11408: loss = 0.002571568406252382\n",
      "Iteration 11409: loss = 0.0025715565072902988\n",
      "Iteration 11410: loss = 0.002571544608466132\n",
      "Iteration 11411: loss = 0.0025715327097798725\n",
      "Iteration 11412: loss = 0.0025715208112315133\n",
      "Iteration 11413: loss = 0.002571508912821045\n",
      "Iteration 11414: loss = 0.002571497014548459\n",
      "Iteration 11415: loss = 0.0025714851164137477\n",
      "Iteration 11416: loss = 0.0025714732184169025\n",
      "Iteration 11417: loss = 0.0025714613205579146\n",
      "Iteration 11418: loss = 0.002571449422836776\n",
      "Iteration 11419: loss = 0.002571437525253478\n",
      "Iteration 11420: loss = 0.002571425627808013\n",
      "Iteration 11421: loss = 0.0025714137305003723\n",
      "Iteration 11422: loss = 0.0025714018333305466\n",
      "Iteration 11423: loss = 0.0025713899362985286\n",
      "Iteration 11424: loss = 0.00257137803940431\n",
      "Iteration 11425: loss = 0.0025713661426478825\n",
      "Iteration 11426: loss = 0.0025713542460292374\n",
      "Iteration 11427: loss = 0.0025713423495483667\n",
      "Iteration 11428: loss = 0.0025713304532052617\n",
      "Iteration 11429: loss = 0.002571318556999914\n",
      "Iteration 11430: loss = 0.002571306660932316\n",
      "Iteration 11431: loss = 0.002571294765002459\n",
      "Iteration 11432: loss = 0.002571282869210335\n",
      "Iteration 11433: loss = 0.002571270973555935\n",
      "Iteration 11434: loss = 0.002571259078039252\n",
      "Iteration 11435: loss = 0.0025712471826602767\n",
      "Iteration 11436: loss = 0.0025712352874190008\n",
      "Iteration 11437: loss = 0.002571223392315417\n",
      "Iteration 11438: loss = 0.002571211497349516\n",
      "Iteration 11439: loss = 0.00257119960252129\n",
      "Iteration 11440: loss = 0.002571187707830731\n",
      "Iteration 11441: loss = 0.002571175813277831\n",
      "Iteration 11442: loss = 0.002571163918862581\n",
      "Iteration 11443: loss = 0.0025711520245849727\n",
      "Iteration 11444: loss = 0.002571140130444999\n",
      "Iteration 11445: loss = 0.002571128236442651\n",
      "Iteration 11446: loss = 0.002571116342577921\n",
      "Iteration 11447: loss = 0.0025711044488507992\n",
      "Iteration 11448: loss = 0.0025710925552612797\n",
      "Iteration 11449: loss = 0.002571080661809353\n",
      "Iteration 11450: loss = 0.0025710687684950117\n",
      "Iteration 11451: loss = 0.0025710568753182462\n",
      "Iteration 11452: loss = 0.0025710449822790495\n",
      "Iteration 11453: loss = 0.002571033089377414\n",
      "Iteration 11454: loss = 0.00257102119661333\n",
      "Iteration 11455: loss = 0.00257100930398679\n",
      "Iteration 11456: loss = 0.002570997411497787\n",
      "Iteration 11457: loss = 0.002570985519146312\n",
      "Iteration 11458: loss = 0.0025709736269323557\n",
      "Iteration 11459: loss = 0.002570961734855912\n",
      "Iteration 11460: loss = 0.0025709498429169714\n",
      "Iteration 11461: loss = 0.0025709379511155263\n",
      "Iteration 11462: loss = 0.002570926059451569\n",
      "Iteration 11463: loss = 0.002570914167925091\n",
      "Iteration 11464: loss = 0.0025709022765360847\n",
      "Iteration 11465: loss = 0.0025708903852845405\n",
      "Iteration 11466: loss = 0.002570878494170452\n",
      "Iteration 11467: loss = 0.002570866603193811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11468: loss = 0.0025708547123546085\n",
      "Iteration 11469: loss = 0.0025708428216528367\n",
      "Iteration 11470: loss = 0.0025708309310884885\n",
      "Iteration 11471: loss = 0.002570819040661555\n",
      "Iteration 11472: loss = 0.0025708071503720287\n",
      "Iteration 11473: loss = 0.0025707952602199\n",
      "Iteration 11474: loss = 0.0025707833702051635\n",
      "Iteration 11475: loss = 0.00257077148032781\n",
      "Iteration 11476: loss = 0.00257075959058783\n",
      "Iteration 11477: loss = 0.0025707477009852178\n",
      "Iteration 11478: loss = 0.002570735811519964\n",
      "Iteration 11479: loss = 0.0025707239221920606\n",
      "Iteration 11480: loss = 0.002570712033001501\n",
      "Iteration 11481: loss = 0.002570700143948276\n",
      "Iteration 11482: loss = 0.0025706882550323774\n",
      "Iteration 11483: loss = 0.0025706763662537986\n",
      "Iteration 11484: loss = 0.002570664477612531\n",
      "Iteration 11485: loss = 0.002570652589108565\n",
      "Iteration 11486: loss = 0.002570640700741895\n",
      "Iteration 11487: loss = 0.0025706288125125127\n",
      "Iteration 11488: loss = 0.002570616924420409\n",
      "Iteration 11489: loss = 0.0025706050364655763\n",
      "Iteration 11490: loss = 0.002570593148648008\n",
      "Iteration 11491: loss = 0.0025705812609676942\n",
      "Iteration 11492: loss = 0.0025705693734246284\n",
      "Iteration 11493: loss = 0.0025705574860188027\n",
      "Iteration 11494: loss = 0.0025705455987502083\n",
      "Iteration 11495: loss = 0.0025705337116188375\n",
      "Iteration 11496: loss = 0.0025705218246246833\n",
      "Iteration 11497: loss = 0.0025705099377677366\n",
      "Iteration 11498: loss = 0.0025704980510479905\n",
      "Iteration 11499: loss = 0.002570486164465437\n",
      "Iteration 11500: loss = 0.002570474278020068\n",
      "Iteration 11501: loss = 0.002570462391711875\n",
      "Iteration 11502: loss = 0.0025704505055408515\n",
      "Iteration 11503: loss = 0.0025704386195069887\n",
      "Iteration 11504: loss = 0.002570426733610279\n",
      "Iteration 11505: loss = 0.0025704148478507146\n",
      "Iteration 11506: loss = 0.0025704029622282878\n",
      "Iteration 11507: loss = 0.0025703910767429904\n",
      "Iteration 11508: loss = 0.002570379191394815\n",
      "Iteration 11509: loss = 0.0025703673061837537\n",
      "Iteration 11510: loss = 0.002570355421109799\n",
      "Iteration 11511: loss = 0.0025703435361729425\n",
      "Iteration 11512: loss = 0.0025703316513731762\n",
      "Iteration 11513: loss = 0.0025703197667104934\n",
      "Iteration 11514: loss = 0.002570307882184885\n",
      "Iteration 11515: loss = 0.002570295997796345\n",
      "Iteration 11516: loss = 0.0025702841135448636\n",
      "Iteration 11517: loss = 0.0025702722294304336\n",
      "Iteration 11518: loss = 0.002570260345453049\n",
      "Iteration 11519: loss = 0.0025702484616126997\n",
      "Iteration 11520: loss = 0.0025702365779093793\n",
      "Iteration 11521: loss = 0.0025702246943430794\n",
      "Iteration 11522: loss = 0.0025702128109137932\n",
      "Iteration 11523: loss = 0.002570200927621512\n",
      "Iteration 11524: loss = 0.0025701890444662276\n",
      "Iteration 11525: loss = 0.0025701771614479343\n",
      "Iteration 11526: loss = 0.0025701652785666226\n",
      "Iteration 11527: loss = 0.0025701533958222863\n",
      "Iteration 11528: loss = 0.002570141513214916\n",
      "Iteration 11529: loss = 0.002570129630744505\n",
      "Iteration 11530: loss = 0.0025701177484110447\n",
      "Iteration 11531: loss = 0.0025701058662145285\n",
      "Iteration 11532: loss = 0.002570093984154949\n",
      "Iteration 11533: loss = 0.0025700821022322977\n",
      "Iteration 11534: loss = 0.002570070220446567\n",
      "Iteration 11535: loss = 0.0025700583387977486\n",
      "Iteration 11536: loss = 0.0025700464572858367\n",
      "Iteration 11537: loss = 0.0025700345759108226\n",
      "Iteration 11538: loss = 0.0025700226946726978\n",
      "Iteration 11539: loss = 0.002570010813571456\n",
      "Iteration 11540: loss = 0.0025699989326070888\n",
      "Iteration 11541: loss = 0.0025699870517795894\n",
      "Iteration 11542: loss = 0.0025699751710889494\n",
      "Iteration 11543: loss = 0.002569963290535161\n",
      "Iteration 11544: loss = 0.002569951410118217\n",
      "Iteration 11545: loss = 0.002569939529838111\n",
      "Iteration 11546: loss = 0.002569927649694833\n",
      "Iteration 11547: loss = 0.0025699157696883767\n",
      "Iteration 11548: loss = 0.0025699038898187353\n",
      "Iteration 11549: loss = 0.0025698920100859003\n",
      "Iteration 11550: loss = 0.0025698801304898634\n",
      "Iteration 11551: loss = 0.002569868251030618\n",
      "Iteration 11552: loss = 0.0025698563717081566\n",
      "Iteration 11553: loss = 0.0025698444925224715\n",
      "Iteration 11554: loss = 0.0025698326134735547\n",
      "Iteration 11555: loss = 0.0025698207345613996\n",
      "Iteration 11556: loss = 0.002569808855785998\n",
      "Iteration 11557: loss = 0.0025697969771473423\n",
      "Iteration 11558: loss = 0.002569785098645424\n",
      "Iteration 11559: loss = 0.0025697732202802385\n",
      "Iteration 11560: loss = 0.002569761342051775\n",
      "Iteration 11561: loss = 0.002569749463960029\n",
      "Iteration 11562: loss = 0.00256973758600499\n",
      "Iteration 11563: loss = 0.002569725708186653\n",
      "Iteration 11564: loss = 0.0025697138305050093\n",
      "Iteration 11565: loss = 0.0025697019529600518\n",
      "Iteration 11566: loss = 0.002569690075551772\n",
      "Iteration 11567: loss = 0.0025696781982801633\n",
      "Iteration 11568: loss = 0.0025696663211452193\n",
      "Iteration 11569: loss = 0.00256965444414693\n",
      "Iteration 11570: loss = 0.0025696425672852904\n",
      "Iteration 11571: loss = 0.002569630690560292\n",
      "Iteration 11572: loss = 0.0025696188139719264\n",
      "Iteration 11573: loss = 0.0025696069375201877\n",
      "Iteration 11574: loss = 0.0025695950612050675\n",
      "Iteration 11575: loss = 0.002569583185026559\n",
      "Iteration 11576: loss = 0.0025695713089846544\n",
      "Iteration 11577: loss = 0.0025695594330793466\n",
      "Iteration 11578: loss = 0.0025695475573106275\n",
      "Iteration 11579: loss = 0.00256953568167849\n",
      "Iteration 11580: loss = 0.0025695238061829275\n",
      "Iteration 11581: loss = 0.002569511930823932\n",
      "Iteration 11582: loss = 0.002569500055601496\n",
      "Iteration 11583: loss = 0.002569488180515611\n",
      "Iteration 11584: loss = 0.0025694763055662717\n",
      "Iteration 11585: loss = 0.0025694644307534703\n",
      "Iteration 11586: loss = 0.0025694525560771976\n",
      "Iteration 11587: loss = 0.0025694406815374485\n",
      "Iteration 11588: loss = 0.0025694288071342144\n",
      "Iteration 11589: loss = 0.0025694169328674887\n",
      "Iteration 11590: loss = 0.002569405058737263\n",
      "Iteration 11591: loss = 0.002569393184743531\n",
      "Iteration 11592: loss = 0.0025693813108862845\n",
      "Iteration 11593: loss = 0.002569369437165516\n",
      "Iteration 11594: loss = 0.00256935756358122\n",
      "Iteration 11595: loss = 0.0025693456901333877\n",
      "Iteration 11596: loss = 0.002569333816822011\n",
      "Iteration 11597: loss = 0.002569321943647084\n",
      "Iteration 11598: loss = 0.0025693100706085997\n",
      "Iteration 11599: loss = 0.0025692981977065493\n",
      "Iteration 11600: loss = 0.0025692863249409266\n",
      "Iteration 11601: loss = 0.0025692744523117244\n",
      "Iteration 11602: loss = 0.002569262579818934\n",
      "Iteration 11603: loss = 0.0025692507074625503\n",
      "Iteration 11604: loss = 0.002569238835242563\n",
      "Iteration 11605: loss = 0.002569226963158969\n",
      "Iteration 11606: loss = 0.002569215091211757\n",
      "Iteration 11607: loss = 0.0025692032194009223\n",
      "Iteration 11608: loss = 0.0025691913477264568\n",
      "Iteration 11609: loss = 0.002569179476188352\n",
      "Iteration 11610: loss = 0.0025691676047866034\n",
      "Iteration 11611: loss = 0.0025691557335212017\n",
      "Iteration 11612: loss = 0.00256914386239214\n",
      "Iteration 11613: loss = 0.002569131991399412\n",
      "Iteration 11614: loss = 0.002569120120543009\n",
      "Iteration 11615: loss = 0.002569108249822925\n",
      "Iteration 11616: loss = 0.002569096379239152\n",
      "Iteration 11617: loss = 0.0025690845087916834\n",
      "Iteration 11618: loss = 0.0025690726384805123\n",
      "Iteration 11619: loss = 0.00256906076830563\n",
      "Iteration 11620: loss = 0.002569048898267031\n",
      "Iteration 11621: loss = 0.002569037028364707\n",
      "Iteration 11622: loss = 0.002569025158598651\n",
      "Iteration 11623: loss = 0.002569013288968857\n",
      "Iteration 11624: loss = 0.002569001419475316\n",
      "Iteration 11625: loss = 0.0025689895501180217\n",
      "Iteration 11626: loss = 0.0025689776808969673\n",
      "Iteration 11627: loss = 0.002568965811812145\n",
      "Iteration 11628: loss = 0.0025689539428635478\n",
      "Iteration 11629: loss = 0.002568942074051169\n",
      "Iteration 11630: loss = 0.002568930205375002\n",
      "Iteration 11631: loss = 0.002568918336835037\n",
      "Iteration 11632: loss = 0.00256890646843127\n",
      "Iteration 11633: loss = 0.0025688946001636917\n",
      "Iteration 11634: loss = 0.002568882732032297\n",
      "Iteration 11635: loss = 0.002568870864037077\n",
      "Iteration 11636: loss = 0.002568858996178025\n",
      "Iteration 11637: loss = 0.0025688471284551345\n",
      "Iteration 11638: loss = 0.0025688352608683983\n",
      "Iteration 11639: loss = 0.0025688233934178088\n",
      "Iteration 11640: loss = 0.002568811526103359\n",
      "Iteration 11641: loss = 0.002568799658925042\n",
      "Iteration 11642: loss = 0.002568787791882851\n",
      "Iteration 11643: loss = 0.002568775924976779\n",
      "Iteration 11644: loss = 0.0025687640582068177\n",
      "Iteration 11645: loss = 0.0025687521915729607\n",
      "Iteration 11646: loss = 0.0025687403250752023\n",
      "Iteration 11647: loss = 0.002568728458713534\n",
      "Iteration 11648: loss = 0.0025687165924879484\n",
      "Iteration 11649: loss = 0.00256870472639844\n",
      "Iteration 11650: loss = 0.0025686928604450004\n",
      "Iteration 11651: loss = 0.002568680994627623\n",
      "Iteration 11652: loss = 0.002568669128946302\n",
      "Iteration 11653: loss = 0.0025686572634010273\n",
      "Iteration 11654: loss = 0.002568645397991795\n",
      "Iteration 11655: loss = 0.002568633532718597\n",
      "Iteration 11656: loss = 0.002568621667581426\n",
      "Iteration 11657: loss = 0.0025686098025802747\n",
      "Iteration 11658: loss = 0.0025685979377151375\n",
      "Iteration 11659: loss = 0.0025685860729860064\n",
      "Iteration 11660: loss = 0.0025685742083928747\n",
      "Iteration 11661: loss = 0.002568562343935735\n",
      "Iteration 11662: loss = 0.0025685504796145796\n",
      "Iteration 11663: loss = 0.002568538615429404\n",
      "Iteration 11664: loss = 0.0025685267513801996\n",
      "Iteration 11665: loss = 0.002568514887466959\n",
      "Iteration 11666: loss = 0.0025685030236896763\n",
      "Iteration 11667: loss = 0.0025684911600483435\n",
      "Iteration 11668: loss = 0.002568479296542955\n",
      "Iteration 11669: loss = 0.0025684674331735028\n",
      "Iteration 11670: loss = 0.002568455569939981\n",
      "Iteration 11671: loss = 0.002568443706842381\n",
      "Iteration 11672: loss = 0.0025684318438806975\n",
      "Iteration 11673: loss = 0.002568419981054923\n",
      "Iteration 11674: loss = 0.00256840811836505\n",
      "Iteration 11675: loss = 0.002568396255811072\n",
      "Iteration 11676: loss = 0.0025683843933929826\n",
      "Iteration 11677: loss = 0.002568372531110775\n",
      "Iteration 11678: loss = 0.0025683606689644416\n",
      "Iteration 11679: loss = 0.002568348806953975\n",
      "Iteration 11680: loss = 0.0025683369450793694\n",
      "Iteration 11681: loss = 0.0025683250833406185\n",
      "Iteration 11682: loss = 0.0025683132217377133\n",
      "Iteration 11683: loss = 0.002568301360270649\n",
      "Iteration 11684: loss = 0.002568289498939418\n",
      "Iteration 11685: loss = 0.002568277637744013\n",
      "Iteration 11686: loss = 0.002568265776684427\n",
      "Iteration 11687: loss = 0.0025682539157606543\n",
      "Iteration 11688: loss = 0.0025682420549726875\n",
      "Iteration 11689: loss = 0.002568230194320519\n",
      "Iteration 11690: loss = 0.0025682183338041438\n",
      "Iteration 11691: loss = 0.002568206473423553\n",
      "Iteration 11692: loss = 0.0025681946131787406\n",
      "Iteration 11693: loss = 0.0025681827530697004\n",
      "Iteration 11694: loss = 0.0025681708930964248\n",
      "Iteration 11695: loss = 0.002568159033258907\n",
      "Iteration 11696: loss = 0.0025681471735571407\n",
      "Iteration 11697: loss = 0.002568135313991119\n",
      "Iteration 11698: loss = 0.002568123454560835\n",
      "Iteration 11699: loss = 0.0025681115952662817\n",
      "Iteration 11700: loss = 0.0025680997361074527\n",
      "Iteration 11701: loss = 0.0025680878770843406\n",
      "Iteration 11702: loss = 0.002568076018196939\n",
      "Iteration 11703: loss = 0.002568064159445242\n",
      "Iteration 11704: loss = 0.002568052300829241\n",
      "Iteration 11705: loss = 0.0025680404423489315\n",
      "Iteration 11706: loss = 0.002568028584004304\n",
      "Iteration 11707: loss = 0.0025680167257953546\n",
      "Iteration 11708: loss = 0.0025680048677220748\n",
      "Iteration 11709: loss = 0.002567993009784458\n",
      "Iteration 11710: loss = 0.0025679811519824978\n",
      "Iteration 11711: loss = 0.0025679692943161875\n",
      "Iteration 11712: loss = 0.00256795743678552\n",
      "Iteration 11713: loss = 0.0025679455793904894\n",
      "Iteration 11714: loss = 0.002567933722131088\n",
      "Iteration 11715: loss = 0.0025679218650073105\n",
      "Iteration 11716: loss = 0.002567910008019148\n",
      "Iteration 11717: loss = 0.0025678981511665956\n",
      "Iteration 11718: loss = 0.0025678862944496458\n",
      "Iteration 11719: loss = 0.0025678744378682923\n",
      "Iteration 11720: loss = 0.0025678625814225284\n",
      "Iteration 11721: loss = 0.0025678507251123473\n",
      "Iteration 11722: loss = 0.0025678388689377423\n",
      "Iteration 11723: loss = 0.0025678270128987072\n",
      "Iteration 11724: loss = 0.0025678151569952343\n",
      "Iteration 11725: loss = 0.002567803301227317\n",
      "Iteration 11726: loss = 0.00256779144559495\n",
      "Iteration 11727: loss = 0.002567779590098126\n",
      "Iteration 11728: loss = 0.0025677677347368377\n",
      "Iteration 11729: loss = 0.002567755879511079\n",
      "Iteration 11730: loss = 0.002567744024420843\n",
      "Iteration 11731: loss = 0.0025677321694661244\n",
      "Iteration 11732: loss = 0.002567720314646914\n",
      "Iteration 11733: loss = 0.002567708459963207\n",
      "Iteration 11734: loss = 0.0025676966054149967\n",
      "Iteration 11735: loss = 0.002567684751002276\n",
      "Iteration 11736: loss = 0.0025676728967250384\n",
      "Iteration 11737: loss = 0.0025676610425832773\n",
      "Iteration 11738: loss = 0.002567649188576987\n",
      "Iteration 11739: loss = 0.0025676373347061593\n",
      "Iteration 11740: loss = 0.0025676254809707884\n",
      "Iteration 11741: loss = 0.0025676136273708678\n",
      "Iteration 11742: loss = 0.002567601773906391\n",
      "Iteration 11743: loss = 0.002567589920577351\n",
      "Iteration 11744: loss = 0.0025675780673837414\n",
      "Iteration 11745: loss = 0.0025675662143255563\n",
      "Iteration 11746: loss = 0.002567554361402788\n",
      "Iteration 11747: loss = 0.002567542508615431\n",
      "Iteration 11748: loss = 0.0025675306559634777\n",
      "Iteration 11749: loss = 0.0025675188034469222\n",
      "Iteration 11750: loss = 0.0025675069510657585\n",
      "Iteration 11751: loss = 0.0025674950988199786\n",
      "Iteration 11752: loss = 0.002567483246709577\n",
      "Iteration 11753: loss = 0.002567471394734547\n",
      "Iteration 11754: loss = 0.002567459542894882\n",
      "Iteration 11755: loss = 0.0025674476911905757\n",
      "Iteration 11756: loss = 0.0025674358396216212\n",
      "Iteration 11757: loss = 0.0025674239881880124\n",
      "Iteration 11758: loss = 0.002567412136889742\n",
      "Iteration 11759: loss = 0.0025674002857268047\n",
      "Iteration 11760: loss = 0.0025673884346991936\n",
      "Iteration 11761: loss = 0.002567376583806901\n",
      "Iteration 11762: loss = 0.0025673647330499223\n",
      "Iteration 11763: loss = 0.00256735288242825\n",
      "Iteration 11764: loss = 0.002567341031941878\n",
      "Iteration 11765: loss = 0.0025673291815907994\n",
      "Iteration 11766: loss = 0.0025673173313750072\n",
      "Iteration 11767: loss = 0.002567305481294496\n",
      "Iteration 11768: loss = 0.0025672936313492598\n",
      "Iteration 11769: loss = 0.0025672817815392906\n",
      "Iteration 11770: loss = 0.0025672699318645834\n",
      "Iteration 11771: loss = 0.00256725808232513\n",
      "Iteration 11772: loss = 0.002567246232920925\n",
      "Iteration 11773: loss = 0.0025672343836519625\n",
      "Iteration 11774: loss = 0.0025672225345182366\n",
      "Iteration 11775: loss = 0.002567210685519738\n",
      "Iteration 11776: loss = 0.002567198836656463\n",
      "Iteration 11777: loss = 0.0025671869879284035\n",
      "Iteration 11778: loss = 0.002567175139335555\n",
      "Iteration 11779: loss = 0.0025671632908779093\n",
      "Iteration 11780: loss = 0.0025671514425554607\n",
      "Iteration 11781: loss = 0.0025671395943682026\n",
      "Iteration 11782: loss = 0.0025671277463161293\n",
      "Iteration 11783: loss = 0.0025671158983992335\n",
      "Iteration 11784: loss = 0.002567104050617509\n",
      "Iteration 11785: loss = 0.00256709220297095\n",
      "Iteration 11786: loss = 0.0025670803554595496\n",
      "Iteration 11787: loss = 0.0025670685080833016\n",
      "Iteration 11788: loss = 0.0025670566608421994\n",
      "Iteration 11789: loss = 0.0025670448137362365\n",
      "Iteration 11790: loss = 0.0025670329667654077\n",
      "Iteration 11791: loss = 0.0025670211199297055\n",
      "Iteration 11792: loss = 0.0025670092732291237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11793: loss = 0.002566997426663656\n",
      "Iteration 11794: loss = 0.0025669855802332962\n",
      "Iteration 11795: loss = 0.0025669737339380386\n",
      "Iteration 11796: loss = 0.0025669618877778755\n",
      "Iteration 11797: loss = 0.002566950041752802\n",
      "Iteration 11798: loss = 0.00256693819586281\n",
      "Iteration 11799: loss = 0.002566926350107895\n",
      "Iteration 11800: loss = 0.0025669145044880494\n",
      "Iteration 11801: loss = 0.0025669026590032684\n",
      "Iteration 11802: loss = 0.002566890813653544\n",
      "Iteration 11803: loss = 0.0025668789684388706\n",
      "Iteration 11804: loss = 0.002566867123359242\n",
      "Iteration 11805: loss = 0.0025668552784146523\n",
      "Iteration 11806: loss = 0.002566843433605094\n",
      "Iteration 11807: loss = 0.0025668315889305618\n",
      "Iteration 11808: loss = 0.00256681974439105\n",
      "Iteration 11809: loss = 0.0025668078999865505\n",
      "Iteration 11810: loss = 0.002566796055717058\n",
      "Iteration 11811: loss = 0.0025667842115825665\n",
      "Iteration 11812: loss = 0.0025667723675830696\n",
      "Iteration 11813: loss = 0.002566760523718561\n",
      "Iteration 11814: loss = 0.002566748679989035\n",
      "Iteration 11815: loss = 0.002566736836394484\n",
      "Iteration 11816: loss = 0.002566724992934903\n",
      "Iteration 11817: loss = 0.0025667131496102846\n",
      "Iteration 11818: loss = 0.002566701306420624\n",
      "Iteration 11819: loss = 0.002566689463365914\n",
      "Iteration 11820: loss = 0.0025666776204461487\n",
      "Iteration 11821: loss = 0.0025666657776613213\n",
      "Iteration 11822: loss = 0.0025666539350114263\n",
      "Iteration 11823: loss = 0.0025666420924964576\n",
      "Iteration 11824: loss = 0.002566630250116409\n",
      "Iteration 11825: loss = 0.002566618407871273\n",
      "Iteration 11826: loss = 0.0025666065657610447\n",
      "Iteration 11827: loss = 0.0025665947237857177\n",
      "Iteration 11828: loss = 0.0025665828819452854\n",
      "Iteration 11829: loss = 0.0025665710402397422\n",
      "Iteration 11830: loss = 0.002566559198669081\n",
      "Iteration 11831: loss = 0.0025665473572332974\n",
      "Iteration 11832: loss = 0.002566535515932383\n",
      "Iteration 11833: loss = 0.0025665236747663333\n",
      "Iteration 11834: loss = 0.002566511833735141\n",
      "Iteration 11835: loss = 0.0025664999928388006\n",
      "Iteration 11836: loss = 0.0025664881520773065\n",
      "Iteration 11837: loss = 0.002566476311450651\n",
      "Iteration 11838: loss = 0.002566464470958829\n",
      "Iteration 11839: loss = 0.0025664526306018345\n",
      "Iteration 11840: loss = 0.0025664407903796607\n",
      "Iteration 11841: loss = 0.002566428950292302\n",
      "Iteration 11842: loss = 0.002566417110339752\n",
      "Iteration 11843: loss = 0.0025664052705220044\n",
      "Iteration 11844: loss = 0.002566393430839054\n",
      "Iteration 11845: loss = 0.002566381591290894\n",
      "Iteration 11846: loss = 0.0025663697518775175\n",
      "Iteration 11847: loss = 0.0025663579125989196\n",
      "Iteration 11848: loss = 0.0025663460734550937\n",
      "Iteration 11849: loss = 0.002566334234446034\n",
      "Iteration 11850: loss = 0.0025663223955717344\n",
      "Iteration 11851: loss = 0.0025663105568321885\n",
      "Iteration 11852: loss = 0.0025662987182273897\n",
      "Iteration 11853: loss = 0.002566286879757333\n",
      "Iteration 11854: loss = 0.002566275041422012\n",
      "Iteration 11855: loss = 0.00256626320322142\n",
      "Iteration 11856: loss = 0.0025662513651555524\n",
      "Iteration 11857: loss = 0.0025662395272244014\n",
      "Iteration 11858: loss = 0.0025662276894279617\n",
      "Iteration 11859: loss = 0.0025662158517662276\n",
      "Iteration 11860: loss = 0.002566204014239192\n",
      "Iteration 11861: loss = 0.0025661921768468507\n",
      "Iteration 11862: loss = 0.0025661803395891952\n",
      "Iteration 11863: loss = 0.002566168502466221\n",
      "Iteration 11864: loss = 0.002566156665477923\n",
      "Iteration 11865: loss = 0.002566144828624293\n",
      "Iteration 11866: loss = 0.002566132991905327\n",
      "Iteration 11867: loss = 0.0025661211553210162\n",
      "Iteration 11868: loss = 0.0025661093188713577\n",
      "Iteration 11869: loss = 0.0025660974825563437\n",
      "Iteration 11870: loss = 0.0025660856463759684\n",
      "Iteration 11871: loss = 0.002566073810330226\n",
      "Iteration 11872: loss = 0.002566061974419111\n",
      "Iteration 11873: loss = 0.0025660501386426168\n",
      "Iteration 11874: loss = 0.002566038303000737\n",
      "Iteration 11875: loss = 0.0025660264674934666\n",
      "Iteration 11876: loss = 0.002566014632120799\n",
      "Iteration 11877: loss = 0.0025660027968827286\n",
      "Iteration 11878: loss = 0.002565990961779249\n",
      "Iteration 11879: loss = 0.002565979126810354\n",
      "Iteration 11880: loss = 0.0025659672919760386\n",
      "Iteration 11881: loss = 0.002565955457276296\n",
      "Iteration 11882: loss = 0.0025659436227111207\n",
      "Iteration 11883: loss = 0.002565931788280506\n",
      "Iteration 11884: loss = 0.002565919953984447\n",
      "Iteration 11885: loss = 0.0025659081198229376\n",
      "Iteration 11886: loss = 0.002565896285795971\n",
      "Iteration 11887: loss = 0.0025658844519035415\n",
      "Iteration 11888: loss = 0.0025658726181456443\n",
      "Iteration 11889: loss = 0.0025658607845222723\n",
      "Iteration 11890: loss = 0.0025658489510334195\n",
      "Iteration 11891: loss = 0.002565837117679081\n",
      "Iteration 11892: loss = 0.00256582528445925\n",
      "Iteration 11893: loss = 0.00256581345137392\n",
      "Iteration 11894: loss = 0.002565801618423087\n",
      "Iteration 11895: loss = 0.002565789785606744\n",
      "Iteration 11896: loss = 0.0025657779529248844\n",
      "Iteration 11897: loss = 0.0025657661203775037\n",
      "Iteration 11898: loss = 0.0025657542879645953\n",
      "Iteration 11899: loss = 0.002565742455686153\n",
      "Iteration 11900: loss = 0.002565730623542171\n",
      "Iteration 11901: loss = 0.002565718791532644\n",
      "Iteration 11902: loss = 0.002565706959657566\n",
      "Iteration 11903: loss = 0.0025656951279169307\n",
      "Iteration 11904: loss = 0.002565683296310733\n",
      "Iteration 11905: loss = 0.0025656714648389653\n",
      "Iteration 11906: loss = 0.0025656596335016236\n",
      "Iteration 11907: loss = 0.0025656478022987018\n",
      "Iteration 11908: loss = 0.002565635971230193\n",
      "Iteration 11909: loss = 0.0025656241402960923\n",
      "Iteration 11910: loss = 0.0025656123094963935\n",
      "Iteration 11911: loss = 0.0025656004788310906\n",
      "Iteration 11912: loss = 0.002565588648300178\n",
      "Iteration 11913: loss = 0.0025655768179036497\n",
      "Iteration 11914: loss = 0.0025655649876414996\n",
      "Iteration 11915: loss = 0.0025655531575137233\n",
      "Iteration 11916: loss = 0.002565541327520313\n",
      "Iteration 11917: loss = 0.002565529497661264\n",
      "Iteration 11918: loss = 0.0025655176679365713\n",
      "Iteration 11919: loss = 0.0025655058383462276\n",
      "Iteration 11920: loss = 0.0025654940088902273\n",
      "Iteration 11921: loss = 0.002565482179568565\n",
      "Iteration 11922: loss = 0.0025654703503812347\n",
      "Iteration 11923: loss = 0.00256545852132823\n",
      "Iteration 11924: loss = 0.002565446692409547\n",
      "Iteration 11925: loss = 0.0025654348636251784\n",
      "Iteration 11926: loss = 0.002565423034975118\n",
      "Iteration 11927: loss = 0.002565411206459362\n",
      "Iteration 11928: loss = 0.0025653993780779026\n",
      "Iteration 11929: loss = 0.0025653875498307353\n",
      "Iteration 11930: loss = 0.002565375721717854\n",
      "Iteration 11931: loss = 0.002565363893739252\n",
      "Iteration 11932: loss = 0.0025653520658949253\n",
      "Iteration 11933: loss = 0.0025653402381848665\n",
      "Iteration 11934: loss = 0.002565328410609071\n",
      "Iteration 11935: loss = 0.002565316583167532\n",
      "Iteration 11936: loss = 0.0025653047558602446\n",
      "Iteration 11937: loss = 0.0025652929286872034\n",
      "Iteration 11938: loss = 0.0025652811016484015\n",
      "Iteration 11939: loss = 0.0025652692747438342\n",
      "Iteration 11940: loss = 0.0025652574479734956\n",
      "Iteration 11941: loss = 0.0025652456213373785\n",
      "Iteration 11942: loss = 0.002565233794835479\n",
      "Iteration 11943: loss = 0.002565221968467791\n",
      "Iteration 11944: loss = 0.0025652101422343086\n",
      "Iteration 11945: loss = 0.0025651983161350256\n",
      "Iteration 11946: loss = 0.0025651864901699373\n",
      "Iteration 11947: loss = 0.0025651746643390373\n",
      "Iteration 11948: loss = 0.0025651628386423207\n",
      "Iteration 11949: loss = 0.00256515101307978\n",
      "Iteration 11950: loss = 0.0025651391876514115\n",
      "Iteration 11951: loss = 0.002565127362357208\n",
      "Iteration 11952: loss = 0.0025651155371971654\n",
      "Iteration 11953: loss = 0.002565103712171277\n",
      "Iteration 11954: loss = 0.002565091887279537\n",
      "Iteration 11955: loss = 0.0025650800625219403\n",
      "Iteration 11956: loss = 0.0025650682378984806\n",
      "Iteration 11957: loss = 0.0025650564134091533\n",
      "Iteration 11958: loss = 0.0025650445890539513\n",
      "Iteration 11959: loss = 0.00256503276483287\n",
      "Iteration 11960: loss = 0.002565020940745904\n",
      "Iteration 11961: loss = 0.0025650091167930466\n",
      "Iteration 11962: loss = 0.0025649972929742922\n",
      "Iteration 11963: loss = 0.0025649854692896368\n",
      "Iteration 11964: loss = 0.0025649736457390724\n",
      "Iteration 11965: loss = 0.0025649618223225956\n",
      "Iteration 11966: loss = 0.002564949999040199\n",
      "Iteration 11967: loss = 0.0025649381758918784\n",
      "Iteration 11968: loss = 0.0025649263528776272\n",
      "Iteration 11969: loss = 0.00256491452999744\n",
      "Iteration 11970: loss = 0.0025649027072513113\n",
      "Iteration 11971: loss = 0.0025648908846392354\n",
      "Iteration 11972: loss = 0.002564879062161207\n",
      "Iteration 11973: loss = 0.00256486723981722\n",
      "Iteration 11974: loss = 0.0025648554176072695\n",
      "Iteration 11975: loss = 0.0025648435955313497\n",
      "Iteration 11976: loss = 0.0025648317735894543\n",
      "Iteration 11977: loss = 0.0025648199517815783\n",
      "Iteration 11978: loss = 0.0025648081301077167\n",
      "Iteration 11979: loss = 0.002564796308567863\n",
      "Iteration 11980: loss = 0.0025647844871620118\n",
      "Iteration 11981: loss = 0.002564772665890157\n",
      "Iteration 11982: loss = 0.0025647608447522947\n",
      "Iteration 11983: loss = 0.002564749023748418\n",
      "Iteration 11984: loss = 0.0025647372028785214\n",
      "Iteration 11985: loss = 0.0025647253821426005\n",
      "Iteration 11986: loss = 0.002564713561540648\n",
      "Iteration 11987: loss = 0.002564701741072659\n",
      "Iteration 11988: loss = 0.002564689920738629\n",
      "Iteration 11989: loss = 0.0025646781005385514\n",
      "Iteration 11990: loss = 0.002564666280472421\n",
      "Iteration 11991: loss = 0.0025646544605402322\n",
      "Iteration 11992: loss = 0.0025646426407419792\n",
      "Iteration 11993: loss = 0.002564630821077657\n",
      "Iteration 11994: loss = 0.002564619001547259\n",
      "Iteration 11995: loss = 0.002564607182150782\n",
      "Iteration 11996: loss = 0.002564595362888218\n",
      "Iteration 11997: loss = 0.0025645835437595626\n",
      "Iteration 11998: loss = 0.0025645717247648105\n",
      "Iteration 11999: loss = 0.0025645599059039557\n",
      "Iteration 12000: loss = 0.0025645480871769933\n",
      "Iteration 12001: loss = 0.0025645362685839165\n",
      "Iteration 12002: loss = 0.0025645244501247217\n",
      "Iteration 12003: loss = 0.002564512631799402\n",
      "Iteration 12004: loss = 0.0025645008136079527\n",
      "Iteration 12005: loss = 0.0025644889955503676\n",
      "Iteration 12006: loss = 0.0025644771776266407\n",
      "Iteration 12007: loss = 0.002564465359836769\n",
      "Iteration 12008: loss = 0.0025644535421807447\n",
      "Iteration 12009: loss = 0.0025644417246585634\n",
      "Iteration 12010: loss = 0.0025644299072702195\n",
      "Iteration 12011: loss = 0.0025644180900157073\n",
      "Iteration 12012: loss = 0.002564406272895021\n",
      "Iteration 12013: loss = 0.002564394455908156\n",
      "Iteration 12014: loss = 0.0025643826390551057\n",
      "Iteration 12015: loss = 0.0025643708223358668\n",
      "Iteration 12016: loss = 0.002564359005750432\n",
      "Iteration 12017: loss = 0.0025643471892987956\n",
      "Iteration 12018: loss = 0.0025643353729809535\n",
      "Iteration 12019: loss = 0.0025643235567968997\n",
      "Iteration 12020: loss = 0.002564311740746629\n",
      "Iteration 12021: loss = 0.002564299924830135\n",
      "Iteration 12022: loss = 0.0025642881090474135\n",
      "Iteration 12023: loss = 0.0025642762933984586\n",
      "Iteration 12024: loss = 0.0025642644778832656\n",
      "Iteration 12025: loss = 0.0025642526625018268\n",
      "Iteration 12026: loss = 0.00256424084725414\n",
      "Iteration 12027: loss = 0.0025642290321401975\n",
      "Iteration 12028: loss = 0.0025642172171599946\n",
      "Iteration 12029: loss = 0.002564205402313526\n",
      "Iteration 12030: loss = 0.0025641935876007864\n",
      "Iteration 12031: loss = 0.0025641817730217703\n",
      "Iteration 12032: loss = 0.0025641699585764724\n",
      "Iteration 12033: loss = 0.0025641581442648873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12034: loss = 0.0025641463300870087\n",
      "Iteration 12035: loss = 0.0025641345160428326\n",
      "Iteration 12036: loss = 0.002564122702132353\n",
      "Iteration 12037: loss = 0.0025641108883555654\n",
      "Iteration 12038: loss = 0.0025640990747124626\n",
      "Iteration 12039: loss = 0.002564087261203041\n",
      "Iteration 12040: loss = 0.0025640754478272946\n",
      "Iteration 12041: loss = 0.0025640636345852174\n",
      "Iteration 12042: loss = 0.0025640518214768053\n",
      "Iteration 12043: loss = 0.0025640400085020524\n",
      "Iteration 12044: loss = 0.0025640281956609535\n",
      "Iteration 12045: loss = 0.002564016382953503\n",
      "Iteration 12046: loss = 0.0025640045703796952\n",
      "Iteration 12047: loss = 0.0025639927579395256\n",
      "Iteration 12048: loss = 0.002563980945632988\n",
      "Iteration 12049: loss = 0.0025639691334600786\n",
      "Iteration 12050: loss = 0.0025639573214207905\n",
      "Iteration 12051: loss = 0.002563945509515119\n",
      "Iteration 12052: loss = 0.002563933697743059\n",
      "Iteration 12053: loss = 0.0025639218861046053\n",
      "Iteration 12054: loss = 0.0025639100745997517\n",
      "Iteration 12055: loss = 0.0025638982632284935\n",
      "Iteration 12056: loss = 0.002563886451990826\n",
      "Iteration 12057: loss = 0.0025638746408867424\n",
      "Iteration 12058: loss = 0.0025638628299162387\n",
      "Iteration 12059: loss = 0.002563851019079309\n",
      "Iteration 12060: loss = 0.002563839208375949\n",
      "Iteration 12061: loss = 0.0025638273978061525\n",
      "Iteration 12062: loss = 0.0025638155873699137\n",
      "Iteration 12063: loss = 0.0025638037770672286\n",
      "Iteration 12064: loss = 0.0025637919668980913\n",
      "Iteration 12065: loss = 0.002563780156862497\n",
      "Iteration 12066: loss = 0.0025637683469604397\n",
      "Iteration 12067: loss = 0.002563756537191914\n",
      "Iteration 12068: loss = 0.0025637447275569164\n",
      "Iteration 12069: loss = 0.0025637329180554394\n",
      "Iteration 12070: loss = 0.002563721108687479\n",
      "Iteration 12071: loss = 0.0025637092994530296\n",
      "Iteration 12072: loss = 0.002563697490352086\n",
      "Iteration 12073: loss = 0.002563685681384644\n",
      "Iteration 12074: loss = 0.002563673872550697\n",
      "Iteration 12075: loss = 0.0025636620638502397\n",
      "Iteration 12076: loss = 0.002563650255283268\n",
      "Iteration 12077: loss = 0.0025636384468497755\n",
      "Iteration 12078: loss = 0.0025636266385497585\n",
      "Iteration 12079: loss = 0.0025636148303832098\n",
      "Iteration 12080: loss = 0.0025636030223501255\n",
      "Iteration 12081: loss = 0.0025635912144505\n",
      "Iteration 12082: loss = 0.0025635794066843284\n",
      "Iteration 12083: loss = 0.0025635675990516053\n",
      "Iteration 12084: loss = 0.0025635557915523253\n",
      "Iteration 12085: loss = 0.002563543984186484\n",
      "Iteration 12086: loss = 0.002563532176954076\n",
      "Iteration 12087: loss = 0.002563520369855094\n",
      "Iteration 12088: loss = 0.002563508562889536\n",
      "Iteration 12089: loss = 0.0025634967560573957\n",
      "Iteration 12090: loss = 0.002563484949358667\n",
      "Iteration 12091: loss = 0.002563473142793345\n",
      "Iteration 12092: loss = 0.002563461336361426\n",
      "Iteration 12093: loss = 0.0025634495300629026\n",
      "Iteration 12094: loss = 0.0025634377238977716\n",
      "Iteration 12095: loss = 0.0025634259178660265\n",
      "Iteration 12096: loss = 0.002563414111967663\n",
      "Iteration 12097: loss = 0.0025634023062026754\n",
      "Iteration 12098: loss = 0.002563390500571059\n",
      "Iteration 12099: loss = 0.0025633786950728086\n",
      "Iteration 12100: loss = 0.002563366889707918\n",
      "Iteration 12101: loss = 0.0025633550844763836\n",
      "Iteration 12102: loss = 0.0025633432793781994\n",
      "Iteration 12103: loss = 0.0025633314744133604\n",
      "Iteration 12104: loss = 0.002563319669581862\n",
      "Iteration 12105: loss = 0.0025633078648836984\n",
      "Iteration 12106: loss = 0.002563296060318865\n",
      "Iteration 12107: loss = 0.002563284255887356\n",
      "Iteration 12108: loss = 0.002563272451589167\n",
      "Iteration 12109: loss = 0.002563260647424292\n",
      "Iteration 12110: loss = 0.002563248843392727\n",
      "Iteration 12111: loss = 0.002563237039494466\n",
      "Iteration 12112: loss = 0.0025632252357295046\n",
      "Iteration 12113: loss = 0.002563213432097837\n",
      "Iteration 12114: loss = 0.0025632016285994596\n",
      "Iteration 12115: loss = 0.002563189825234365\n",
      "Iteration 12116: loss = 0.0025631780220025497\n",
      "Iteration 12117: loss = 0.0025631662189040077\n",
      "Iteration 12118: loss = 0.0025631544159387346\n",
      "Iteration 12119: loss = 0.002563142613106726\n",
      "Iteration 12120: loss = 0.0025631308104079754\n",
      "Iteration 12121: loss = 0.0025631190078424784\n",
      "Iteration 12122: loss = 0.00256310720541023\n",
      "Iteration 12123: loss = 0.0025630954031112246\n",
      "Iteration 12124: loss = 0.002563083600945458\n",
      "Iteration 12125: loss = 0.002563071798912924\n",
      "Iteration 12126: loss = 0.002563059997013619\n",
      "Iteration 12127: loss = 0.0025630481952475365\n",
      "Iteration 12128: loss = 0.002563036393614673\n",
      "Iteration 12129: loss = 0.002563024592115022\n",
      "Iteration 12130: loss = 0.0025630127907485794\n",
      "Iteration 12131: loss = 0.002563000989515339\n",
      "Iteration 12132: loss = 0.002562989188415297\n",
      "Iteration 12133: loss = 0.002562977387448448\n",
      "Iteration 12134: loss = 0.0025629655866147875\n",
      "Iteration 12135: loss = 0.002562953785914309\n",
      "Iteration 12136: loss = 0.0025629419853470093\n",
      "Iteration 12137: loss = 0.0025629301849128816\n",
      "Iteration 12138: loss = 0.0025629183846119226\n",
      "Iteration 12139: loss = 0.0025629065844441257\n",
      "Iteration 12140: loss = 0.002562894784409487\n",
      "Iteration 12141: loss = 0.002562882984508001\n",
      "Iteration 12142: loss = 0.002562871184739663\n",
      "Iteration 12143: loss = 0.002562859385104467\n",
      "Iteration 12144: loss = 0.0025628475856024095\n",
      "Iteration 12145: loss = 0.0025628357862334853\n",
      "Iteration 12146: loss = 0.0025628239869976885\n",
      "Iteration 12147: loss = 0.0025628121878950145\n",
      "Iteration 12148: loss = 0.0025628003889254587\n",
      "Iteration 12149: loss = 0.0025627885900890153\n",
      "Iteration 12150: loss = 0.00256277679138568\n",
      "Iteration 12151: loss = 0.0025627649928154483\n",
      "Iteration 12152: loss = 0.0025627531943783143\n",
      "Iteration 12153: loss = 0.002562741396074273\n",
      "Iteration 12154: loss = 0.0025627295979033203\n",
      "Iteration 12155: loss = 0.00256271779986545\n",
      "Iteration 12156: loss = 0.0025627060019606587\n",
      "Iteration 12157: loss = 0.0025626942041889395\n",
      "Iteration 12158: loss = 0.00256268240655029\n",
      "Iteration 12159: loss = 0.0025626706090447035\n",
      "Iteration 12160: loss = 0.002562658811672175\n",
      "Iteration 12161: loss = 0.0025626470144326993\n",
      "Iteration 12162: loss = 0.002562635217326273\n",
      "Iteration 12163: loss = 0.00256262342035289\n",
      "Iteration 12164: loss = 0.0025626116235125456\n",
      "Iteration 12165: loss = 0.0025625998268052354\n",
      "Iteration 12166: loss = 0.0025625880302309533\n",
      "Iteration 12167: loss = 0.0025625762337896955\n",
      "Iteration 12168: loss = 0.0025625644374814567\n",
      "Iteration 12169: loss = 0.0025625526413062317\n",
      "Iteration 12170: loss = 0.0025625408452640158\n",
      "Iteration 12171: loss = 0.0025625290493548046\n",
      "Iteration 12172: loss = 0.0025625172535785924\n",
      "Iteration 12173: loss = 0.002562505457935375\n",
      "Iteration 12174: loss = 0.0025624936624251463\n",
      "Iteration 12175: loss = 0.0025624818670479024\n",
      "Iteration 12176: loss = 0.0025624700718036393\n",
      "Iteration 12177: loss = 0.0025624582766923497\n",
      "Iteration 12178: loss = 0.0025624464817140314\n",
      "Iteration 12179: loss = 0.002562434686868677\n",
      "Iteration 12180: loss = 0.0025624228921562837\n",
      "Iteration 12181: loss = 0.002562411097576845\n",
      "Iteration 12182: loss = 0.0025623993031303575\n",
      "Iteration 12183: loss = 0.0025623875088168157\n",
      "Iteration 12184: loss = 0.002562375714636214\n",
      "Iteration 12185: loss = 0.002562363920588548\n",
      "Iteration 12186: loss = 0.0025623521266738133\n",
      "Iteration 12187: loss = 0.002562340332892005\n",
      "Iteration 12188: loss = 0.0025623285392431176\n",
      "Iteration 12189: loss = 0.0025623167457271474\n",
      "Iteration 12190: loss = 0.0025623049523440878\n",
      "Iteration 12191: loss = 0.002562293159093936\n",
      "Iteration 12192: loss = 0.0025622813659766856\n",
      "Iteration 12193: loss = 0.0025622695729923322\n",
      "Iteration 12194: loss = 0.0025622577801408704\n",
      "Iteration 12195: loss = 0.0025622459874222975\n",
      "Iteration 12196: loss = 0.002562234194836606\n",
      "Iteration 12197: loss = 0.0025622224023837932\n",
      "Iteration 12198: loss = 0.002562210610063853\n",
      "Iteration 12199: loss = 0.0025621988178767805\n",
      "Iteration 12200: loss = 0.002562187025822572\n",
      "Iteration 12201: loss = 0.0025621752339012216\n",
      "Iteration 12202: loss = 0.002562163442112725\n",
      "Iteration 12203: loss = 0.002562151650457077\n",
      "Iteration 12204: loss = 0.0025621398589342733\n",
      "Iteration 12205: loss = 0.0025621280675443086\n",
      "Iteration 12206: loss = 0.002562116276287179\n",
      "Iteration 12207: loss = 0.0025621044851628786\n",
      "Iteration 12208: loss = 0.002562092694171403\n",
      "Iteration 12209: loss = 0.0025620809033127484\n",
      "Iteration 12210: loss = 0.0025620691125869083\n",
      "Iteration 12211: loss = 0.0025620573219938786\n",
      "Iteration 12212: loss = 0.002562045531533655\n",
      "Iteration 12213: loss = 0.0025620337412062323\n",
      "Iteration 12214: loss = 0.0025620219510116057\n",
      "Iteration 12215: loss = 0.002562010160949771\n",
      "Iteration 12216: loss = 0.0025619983710207226\n",
      "Iteration 12217: loss = 0.0025619865812244566\n",
      "Iteration 12218: loss = 0.002561974791560967\n",
      "Iteration 12219: loss = 0.00256196300203025\n",
      "Iteration 12220: loss = 0.0025619512126323013\n",
      "Iteration 12221: loss = 0.002561939423367115\n",
      "Iteration 12222: loss = 0.0025619276342346868\n",
      "Iteration 12223: loss = 0.002561915845235012\n",
      "Iteration 12224: loss = 0.0025619040563680862\n",
      "Iteration 12225: loss = 0.0025618922676339034\n",
      "Iteration 12226: loss = 0.0025618804790324603\n",
      "Iteration 12227: loss = 0.0025618686905637517\n",
      "Iteration 12228: loss = 0.002561856902227773\n",
      "Iteration 12229: loss = 0.0025618451140245195\n",
      "Iteration 12230: loss = 0.0025618333259539855\n",
      "Iteration 12231: loss = 0.002561821538016168\n",
      "Iteration 12232: loss = 0.00256180975021106\n",
      "Iteration 12233: loss = 0.0025617979625386592\n",
      "Iteration 12234: loss = 0.0025617861749989593\n",
      "Iteration 12235: loss = 0.002561774387591956\n",
      "Iteration 12236: loss = 0.002561762600317646\n",
      "Iteration 12237: loss = 0.0025617508131760215\n",
      "Iteration 12238: loss = 0.00256173902616708\n",
      "Iteration 12239: loss = 0.0025617272392908174\n",
      "Iteration 12240: loss = 0.0025617154525472267\n",
      "Iteration 12241: loss = 0.0025617036659363055\n",
      "Iteration 12242: loss = 0.0025616918794580477\n",
      "Iteration 12243: loss = 0.002561680093112449\n",
      "Iteration 12244: loss = 0.0025616683068995045\n",
      "Iteration 12245: loss = 0.00256165652081921\n",
      "Iteration 12246: loss = 0.002561644734871561\n",
      "Iteration 12247: loss = 0.0025616329490565524\n",
      "Iteration 12248: loss = 0.0025616211633741784\n",
      "Iteration 12249: loss = 0.002561609377824437\n",
      "Iteration 12250: loss = 0.002561597592407321\n",
      "Iteration 12251: loss = 0.002561585807122827\n",
      "Iteration 12252: loss = 0.00256157402197095\n",
      "Iteration 12253: loss = 0.0025615622369516856\n",
      "Iteration 12254: loss = 0.0025615504520650288\n",
      "Iteration 12255: loss = 0.002561538667310975\n",
      "Iteration 12256: loss = 0.0025615268826895197\n",
      "Iteration 12257: loss = 0.002561515098200659\n",
      "Iteration 12258: loss = 0.0025615033138443872\n",
      "Iteration 12259: loss = 0.0025614915296206997\n",
      "Iteration 12260: loss = 0.002561479745529592\n",
      "Iteration 12261: loss = 0.0025614679615710593\n",
      "Iteration 12262: loss = 0.0025614561777450974\n",
      "Iteration 12263: loss = 0.0025614443940517023\n",
      "Iteration 12264: loss = 0.0025614326104908686\n",
      "Iteration 12265: loss = 0.002561420827062591\n",
      "Iteration 12266: loss = 0.0025614090437668657\n",
      "Iteration 12267: loss = 0.0025613972606036883\n",
      "Iteration 12268: loss = 0.002561385477573054\n",
      "Iteration 12269: loss = 0.0025613736946749573\n",
      "Iteration 12270: loss = 0.0025613619119093946\n",
      "Iteration 12271: loss = 0.0025613501292763613\n",
      "Iteration 12272: loss = 0.0025613383467758526\n",
      "Iteration 12273: loss = 0.0025613265644078634\n",
      "Iteration 12274: loss = 0.00256131478217239\n",
      "Iteration 12275: loss = 0.002561303000069427\n",
      "Iteration 12276: loss = 0.00256129121809897\n",
      "Iteration 12277: loss = 0.0025612794362610152\n",
      "Iteration 12278: loss = 0.002561267654555557\n",
      "Iteration 12279: loss = 0.0025612558729825913\n",
      "Iteration 12280: loss = 0.0025612440915421136\n",
      "Iteration 12281: loss = 0.0025612323102341184\n",
      "Iteration 12282: loss = 0.002561220529058603\n",
      "Iteration 12283: loss = 0.002561208748015561\n",
      "Iteration 12284: loss = 0.002561196967104989\n",
      "Iteration 12285: loss = 0.0025611851863268816\n",
      "Iteration 12286: loss = 0.0025611734056812346\n",
      "Iteration 12287: loss = 0.002561161625168044\n",
      "Iteration 12288: loss = 0.002561149844787304\n",
      "Iteration 12289: loss = 0.0025611380645390114\n",
      "Iteration 12290: loss = 0.0025611262844231603\n",
      "Iteration 12291: loss = 0.0025611145044397475\n",
      "Iteration 12292: loss = 0.002561102724588768\n",
      "Iteration 12293: loss = 0.0025610909448702166\n",
      "Iteration 12294: loss = 0.0025610791652840898\n",
      "Iteration 12295: loss = 0.0025610673858303813\n",
      "Iteration 12296: loss = 0.002561055606509089\n",
      "Iteration 12297: loss = 0.002561043827320207\n",
      "Iteration 12298: loss = 0.0025610320482637307\n",
      "Iteration 12299: loss = 0.002561020269339656\n",
      "Iteration 12300: loss = 0.0025610084905479777\n",
      "Iteration 12301: loss = 0.002560996711888692\n",
      "Iteration 12302: loss = 0.002560984933361794\n",
      "Iteration 12303: loss = 0.0025609731549672797\n",
      "Iteration 12304: loss = 0.002560961376705145\n",
      "Iteration 12305: loss = 0.0025609495985753836\n",
      "Iteration 12306: loss = 0.0025609378205779924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12307: loss = 0.0025609260427129662\n",
      "Iteration 12308: loss = 0.0025609142649803012\n",
      "Iteration 12309: loss = 0.0025609024873799917\n",
      "Iteration 12310: loss = 0.002560890709912035\n",
      "Iteration 12311: loss = 0.0025608789325764253\n",
      "Iteration 12312: loss = 0.002560867155373159\n",
      "Iteration 12313: loss = 0.00256085537830223\n",
      "Iteration 12314: loss = 0.0025608436013636357\n",
      "Iteration 12315: loss = 0.0025608318245573707\n",
      "Iteration 12316: loss = 0.002560820047883431\n",
      "Iteration 12317: loss = 0.0025608082713418114\n",
      "Iteration 12318: loss = 0.0025607964949325075\n",
      "Iteration 12319: loss = 0.002560784718655515\n",
      "Iteration 12320: loss = 0.0025607729425108305\n",
      "Iteration 12321: loss = 0.0025607611664984478\n",
      "Iteration 12322: loss = 0.002560749390618364\n",
      "Iteration 12323: loss = 0.002560737614870573\n",
      "Iteration 12324: loss = 0.002560725839255072\n",
      "Iteration 12325: loss = 0.002560714063771856\n",
      "Iteration 12326: loss = 0.0025607022884209193\n",
      "Iteration 12327: loss = 0.0025606905132022593\n",
      "Iteration 12328: loss = 0.0025606787381158703\n",
      "Iteration 12329: loss = 0.0025606669631617488\n",
      "Iteration 12330: loss = 0.00256065518833989\n",
      "Iteration 12331: loss = 0.0025606434136502894\n",
      "Iteration 12332: loss = 0.0025606316390929416\n",
      "Iteration 12333: loss = 0.0025606198646678435\n",
      "Iteration 12334: loss = 0.0025606080903749903\n",
      "Iteration 12335: loss = 0.002560596316214378\n",
      "Iteration 12336: loss = 0.0025605845421860017\n",
      "Iteration 12337: loss = 0.0025605727682898563\n",
      "Iteration 12338: loss = 0.002560560994525939\n",
      "Iteration 12339: loss = 0.002560549220894244\n",
      "Iteration 12340: loss = 0.0025605374473947674\n",
      "Iteration 12341: loss = 0.002560525674027505\n",
      "Iteration 12342: loss = 0.002560513900792452\n",
      "Iteration 12343: loss = 0.0025605021276896038\n",
      "Iteration 12344: loss = 0.0025604903547189564\n",
      "Iteration 12345: loss = 0.0025604785818805053\n",
      "Iteration 12346: loss = 0.0025604668091742467\n",
      "Iteration 12347: loss = 0.0025604550366001757\n",
      "Iteration 12348: loss = 0.0025604432641582876\n",
      "Iteration 12349: loss = 0.002560431491848578\n",
      "Iteration 12350: loss = 0.0025604197196710437\n",
      "Iteration 12351: loss = 0.0025604079476256787\n",
      "Iteration 12352: loss = 0.00256039617571248\n",
      "Iteration 12353: loss = 0.0025603844039314423\n",
      "Iteration 12354: loss = 0.0025603726322825614\n",
      "Iteration 12355: loss = 0.002560360860765833\n",
      "Iteration 12356: loss = 0.002560349089381253\n",
      "Iteration 12357: loss = 0.002560337318128817\n",
      "Iteration 12358: loss = 0.0025603255470085197\n",
      "Iteration 12359: loss = 0.0025603137760203576\n",
      "Iteration 12360: loss = 0.0025603020051643273\n",
      "Iteration 12361: loss = 0.0025602902344404226\n",
      "Iteration 12362: loss = 0.0025602784638486405\n",
      "Iteration 12363: loss = 0.0025602666933889754\n",
      "Iteration 12364: loss = 0.0025602549230614243\n",
      "Iteration 12365: loss = 0.002560243152865982\n",
      "Iteration 12366: loss = 0.0025602313828026442\n",
      "Iteration 12367: loss = 0.0025602196128714067\n",
      "Iteration 12368: loss = 0.0025602078430722653\n",
      "Iteration 12369: loss = 0.0025601960734052153\n",
      "Iteration 12370: loss = 0.002560184303870254\n",
      "Iteration 12371: loss = 0.002560172534467374\n",
      "Iteration 12372: loss = 0.0025601607651965738\n",
      "Iteration 12373: loss = 0.0025601489960578477\n",
      "Iteration 12374: loss = 0.002560137227051191\n",
      "Iteration 12375: loss = 0.0025601254581766007\n",
      "Iteration 12376: loss = 0.002560113689434072\n",
      "Iteration 12377: loss = 0.0025601019208235996\n",
      "Iteration 12378: loss = 0.0025600901523451804\n",
      "Iteration 12379: loss = 0.0025600783839988105\n",
      "Iteration 12380: loss = 0.002560066615784484\n",
      "Iteration 12381: loss = 0.0025600548477021975\n",
      "Iteration 12382: loss = 0.002560043079751947\n",
      "Iteration 12383: loss = 0.002560031311933727\n",
      "Iteration 12384: loss = 0.0025600195442475344\n",
      "Iteration 12385: loss = 0.002560007776693365\n",
      "Iteration 12386: loss = 0.0025599960092712137\n",
      "Iteration 12387: loss = 0.0025599842419810766\n",
      "Iteration 12388: loss = 0.002559972474822949\n",
      "Iteration 12389: loss = 0.0025599607077968272\n",
      "Iteration 12390: loss = 0.002559948940902707\n",
      "Iteration 12391: loss = 0.002559937174140584\n",
      "Iteration 12392: loss = 0.0025599254075104536\n",
      "Iteration 12393: loss = 0.0025599136410123114\n",
      "Iteration 12394: loss = 0.0025599018746461535\n",
      "Iteration 12395: loss = 0.0025598901084119756\n",
      "Iteration 12396: loss = 0.0025598783423097736\n",
      "Iteration 12397: loss = 0.0025598665763395434\n",
      "Iteration 12398: loss = 0.0025598548105012797\n",
      "Iteration 12399: loss = 0.0025598430447949794\n",
      "Iteration 12400: loss = 0.0025598312792206373\n",
      "Iteration 12401: loss = 0.00255981951377825\n",
      "Iteration 12402: loss = 0.002559807748467813\n",
      "Iteration 12403: loss = 0.002559795983289322\n",
      "Iteration 12404: loss = 0.0025597842182427727\n",
      "Iteration 12405: loss = 0.0025597724533281607\n",
      "Iteration 12406: loss = 0.0025597606885454814\n",
      "Iteration 12407: loss = 0.0025597489238947326\n",
      "Iteration 12408: loss = 0.0025597371593759073\n",
      "Iteration 12409: loss = 0.0025597253949890034\n",
      "Iteration 12410: loss = 0.0025597136307340153\n",
      "Iteration 12411: loss = 0.0025597018666109394\n",
      "Iteration 12412: loss = 0.0025596901026197715\n",
      "Iteration 12413: loss = 0.002559678338760507\n",
      "Iteration 12414: loss = 0.0025596665750331426\n",
      "Iteration 12415: loss = 0.0025596548114376733\n",
      "Iteration 12416: loss = 0.0025596430479740946\n",
      "Iteration 12417: loss = 0.0025596312846424022\n",
      "Iteration 12418: loss = 0.0025596195214425935\n",
      "Iteration 12419: loss = 0.002559607758374663\n",
      "Iteration 12420: loss = 0.002559595995438607\n",
      "Iteration 12421: loss = 0.0025595842326344206\n",
      "Iteration 12422: loss = 0.0025595724699621\n",
      "Iteration 12423: loss = 0.002559560707421641\n",
      "Iteration 12424: loss = 0.0025595489450130396\n",
      "Iteration 12425: loss = 0.002559537182736291\n",
      "Iteration 12426: loss = 0.0025595254205913924\n",
      "Iteration 12427: loss = 0.002559513658578338\n",
      "Iteration 12428: loss = 0.0025595018966971246\n",
      "Iteration 12429: loss = 0.0025594901349477476\n",
      "Iteration 12430: loss = 0.002559478373330203\n",
      "Iteration 12431: loss = 0.002559466611844487\n",
      "Iteration 12432: loss = 0.0025594548504905943\n",
      "Iteration 12433: loss = 0.002559443089268522\n",
      "Iteration 12434: loss = 0.0025594313281782654\n",
      "Iteration 12435: loss = 0.00255941956721982\n",
      "Iteration 12436: loss = 0.0025594078063931824\n",
      "Iteration 12437: loss = 0.0025593960456983477\n",
      "Iteration 12438: loss = 0.0025593842851353117\n",
      "Iteration 12439: loss = 0.0025593725247040713\n",
      "Iteration 12440: loss = 0.0025593607644046217\n",
      "Iteration 12441: loss = 0.0025593490042369587\n",
      "Iteration 12442: loss = 0.0025593372442010774\n",
      "Iteration 12443: loss = 0.0025593254842969752\n",
      "Iteration 12444: loss = 0.0025593137245246466\n",
      "Iteration 12445: loss = 0.002559301964884089\n",
      "Iteration 12446: loss = 0.002559290205375296\n",
      "Iteration 12447: loss = 0.0025592784459982656\n",
      "Iteration 12448: loss = 0.002559266686752993\n",
      "Iteration 12449: loss = 0.0025592549276394734\n",
      "Iteration 12450: loss = 0.0025592431686577042\n",
      "Iteration 12451: loss = 0.002559231409807679\n",
      "Iteration 12452: loss = 0.0025592196510893952\n",
      "Iteration 12453: loss = 0.002559207892502849\n",
      "Iteration 12454: loss = 0.002559196134048036\n",
      "Iteration 12455: loss = 0.0025591843757249507\n",
      "Iteration 12456: loss = 0.002559172617533591\n",
      "Iteration 12457: loss = 0.002559160859473951\n",
      "Iteration 12458: loss = 0.0025591491015460282\n",
      "Iteration 12459: loss = 0.002559137343749818\n",
      "Iteration 12460: loss = 0.002559125586085316\n",
      "Iteration 12461: loss = 0.002559113828552518\n",
      "Iteration 12462: loss = 0.00255910207115142\n",
      "Iteration 12463: loss = 0.002559090313882018\n",
      "Iteration 12464: loss = 0.0025590785567443076\n",
      "Iteration 12465: loss = 0.002559066799738286\n",
      "Iteration 12466: loss = 0.0025590550428639474\n",
      "Iteration 12467: loss = 0.0025590432861212885\n",
      "Iteration 12468: loss = 0.0025590315295103056\n",
      "Iteration 12469: loss = 0.002559019773030994\n",
      "Iteration 12470: loss = 0.002559008016683349\n",
      "Iteration 12471: loss = 0.0025589962604673686\n",
      "Iteration 12472: loss = 0.002558984504383047\n",
      "Iteration 12473: loss = 0.0025589727484303807\n",
      "Iteration 12474: loss = 0.002558960992609365\n",
      "Iteration 12475: loss = 0.0025589492369199967\n",
      "Iteration 12476: loss = 0.002558937481362272\n",
      "Iteration 12477: loss = 0.0025589257259361854\n",
      "Iteration 12478: loss = 0.002558913970641734\n",
      "Iteration 12479: loss = 0.0025589022154789134\n",
      "Iteration 12480: loss = 0.0025588904604477197\n",
      "Iteration 12481: loss = 0.0025588787055481487\n",
      "Iteration 12482: loss = 0.0025588669507801962\n",
      "Iteration 12483: loss = 0.0025588551961438586\n",
      "Iteration 12484: loss = 0.002558843441639131\n",
      "Iteration 12485: loss = 0.002558831687266011\n",
      "Iteration 12486: loss = 0.002558819933024493\n",
      "Iteration 12487: loss = 0.002558808178914574\n",
      "Iteration 12488: loss = 0.0025587964249362484\n",
      "Iteration 12489: loss = 0.0025587846710895144\n",
      "Iteration 12490: loss = 0.002558772917374366\n",
      "Iteration 12491: loss = 0.0025587611637907996\n",
      "Iteration 12492: loss = 0.0025587494103388128\n",
      "Iteration 12493: loss = 0.0025587376570183995\n",
      "Iteration 12494: loss = 0.0025587259038295563\n",
      "Iteration 12495: loss = 0.00255871415077228\n",
      "Iteration 12496: loss = 0.0025587023978465655\n",
      "Iteration 12497: loss = 0.0025586906450524097\n",
      "Iteration 12498: loss = 0.0025586788923898083\n",
      "Iteration 12499: loss = 0.0025586671398587567\n",
      "Iteration 12500: loss = 0.0025586553874592517\n",
      "Iteration 12501: loss = 0.0025586436351912887\n",
      "Iteration 12502: loss = 0.002558631883054864\n",
      "Iteration 12503: loss = 0.0025586201310499736\n",
      "Iteration 12504: loss = 0.0025586083791766133\n",
      "Iteration 12505: loss = 0.0025585966274347793\n",
      "Iteration 12506: loss = 0.002558584875824468\n",
      "Iteration 12507: loss = 0.0025585731243456746\n",
      "Iteration 12508: loss = 0.0025585613729983956\n",
      "Iteration 12509: loss = 0.002558549621782627\n",
      "Iteration 12510: loss = 0.002558537870698364\n",
      "Iteration 12511: loss = 0.0025585261197456043\n",
      "Iteration 12512: loss = 0.002558514368924342\n",
      "Iteration 12513: loss = 0.0025585026182345753\n",
      "Iteration 12514: loss = 0.002558490867676298\n",
      "Iteration 12515: loss = 0.0025584791172495076\n",
      "Iteration 12516: loss = 0.0025584673669541997\n",
      "Iteration 12517: loss = 0.0025584556167903704\n",
      "Iteration 12518: loss = 0.0025584438667580153\n",
      "Iteration 12519: loss = 0.0025584321168571305\n",
      "Iteration 12520: loss = 0.0025584203670877136\n",
      "Iteration 12521: loss = 0.0025584086174497587\n",
      "Iteration 12522: loss = 0.0025583968679432625\n",
      "Iteration 12523: loss = 0.0025583851185682215\n",
      "Iteration 12524: loss = 0.0025583733693246304\n",
      "Iteration 12525: loss = 0.0025583616202124863\n",
      "Iteration 12526: loss = 0.0025583498712317856\n",
      "Iteration 12527: loss = 0.002558338122382524\n",
      "Iteration 12528: loss = 0.0025583263736646974\n",
      "Iteration 12529: loss = 0.0025583146250783015\n",
      "Iteration 12530: loss = 0.002558302876623333\n",
      "Iteration 12531: loss = 0.002558291128299788\n",
      "Iteration 12532: loss = 0.0025582793801076625\n",
      "Iteration 12533: loss = 0.002558267632046952\n",
      "Iteration 12534: loss = 0.002558255884117653\n",
      "Iteration 12535: loss = 0.0025582441363197614\n",
      "Iteration 12536: loss = 0.002558232388653274\n",
      "Iteration 12537: loss = 0.002558220641118186\n",
      "Iteration 12538: loss = 0.0025582088937144932\n",
      "Iteration 12539: loss = 0.0025581971464421925\n",
      "Iteration 12540: loss = 0.0025581853993012797\n",
      "Iteration 12541: loss = 0.002558173652291752\n",
      "Iteration 12542: loss = 0.0025581619054136032\n",
      "Iteration 12543: loss = 0.0025581501586668318\n",
      "Iteration 12544: loss = 0.0025581384120514318\n",
      "Iteration 12545: loss = 0.0025581266655674006\n",
      "Iteration 12546: loss = 0.002558114919214733\n",
      "Iteration 12547: loss = 0.0025581031729934277\n",
      "Iteration 12548: loss = 0.0025580914269034785\n",
      "Iteration 12549: loss = 0.0025580796809448816\n",
      "Iteration 12550: loss = 0.002558067935117634\n",
      "Iteration 12551: loss = 0.0025580561894217313\n",
      "Iteration 12552: loss = 0.002558044443857171\n",
      "Iteration 12553: loss = 0.002558032698423946\n",
      "Iteration 12554: loss = 0.002558020953122056\n",
      "Iteration 12555: loss = 0.002558009207951494\n",
      "Iteration 12556: loss = 0.002557997462912259\n",
      "Iteration 12557: loss = 0.0025579857180043457\n",
      "Iteration 12558: loss = 0.00255797397322775\n",
      "Iteration 12559: loss = 0.002557962228582468\n",
      "Iteration 12560: loss = 0.0025579504840684965\n",
      "Iteration 12561: loss = 0.0025579387396858317\n",
      "Iteration 12562: loss = 0.0025579269954344695\n",
      "Iteration 12563: loss = 0.002557915251314405\n",
      "Iteration 12564: loss = 0.0025579035073256353\n",
      "Iteration 12565: loss = 0.002557891763468157\n",
      "Iteration 12566: loss = 0.0025578800197419652\n",
      "Iteration 12567: loss = 0.002557868276147057\n",
      "Iteration 12568: loss = 0.002557856532683428\n",
      "Iteration 12569: loss = 0.0025578447893510746\n",
      "Iteration 12570: loss = 0.0025578330461499925\n",
      "Iteration 12571: loss = 0.002557821303080178\n",
      "Iteration 12572: loss = 0.0025578095601416283\n",
      "Iteration 12573: loss = 0.002557797817334338\n",
      "Iteration 12574: loss = 0.002557786074658304\n",
      "Iteration 12575: loss = 0.0025577743321135225\n",
      "Iteration 12576: loss = 0.00255776258969999\n",
      "Iteration 12577: loss = 0.0025577508474177012\n",
      "Iteration 12578: loss = 0.002557739105266655\n",
      "Iteration 12579: loss = 0.0025577273632468446\n",
      "Iteration 12580: loss = 0.002557715621358268\n",
      "Iteration 12581: loss = 0.0025577038796009205\n",
      "Iteration 12582: loss = 0.002557692137974799\n",
      "Iteration 12583: loss = 0.0025576803964798994\n",
      "Iteration 12584: loss = 0.0025576686551162173\n",
      "Iteration 12585: loss = 0.0025576569138837497\n",
      "Iteration 12586: loss = 0.002557645172782493\n",
      "Iteration 12587: loss = 0.0025576334318124423\n",
      "Iteration 12588: loss = 0.002557621690973594\n",
      "Iteration 12589: loss = 0.0025576099502659455\n",
      "Iteration 12590: loss = 0.0025575982096894914\n",
      "Iteration 12591: loss = 0.002557586469244229\n",
      "Iteration 12592: loss = 0.0025575747289301545\n",
      "Iteration 12593: loss = 0.0025575629887472635\n",
      "Iteration 12594: loss = 0.0025575512486955524\n",
      "Iteration 12595: loss = 0.002557539508775018\n",
      "Iteration 12596: loss = 0.002557527768985655\n",
      "Iteration 12597: loss = 0.002557516029327461\n",
      "Iteration 12598: loss = 0.0025575042898004327\n",
      "Iteration 12599: loss = 0.0025574925504045646\n",
      "Iteration 12600: loss = 0.0025574808111398545\n",
      "Iteration 12601: loss = 0.0025574690720062965\n",
      "Iteration 12602: loss = 0.0025574573330038896\n",
      "Iteration 12603: loss = 0.0025574455941326283\n",
      "Iteration 12604: loss = 0.002557433855392508\n",
      "Iteration 12605: loss = 0.0025574221167835275\n",
      "Iteration 12606: loss = 0.0025574103783056815\n",
      "Iteration 12607: loss = 0.002557398639958966\n",
      "Iteration 12608: loss = 0.0025573869017433775\n",
      "Iteration 12609: loss = 0.002557375163658912\n",
      "Iteration 12610: loss = 0.0025573634257055667\n",
      "Iteration 12611: loss = 0.0025573516878833373\n",
      "Iteration 12612: loss = 0.0025573399501922197\n",
      "Iteration 12613: loss = 0.0025573282126322102\n",
      "Iteration 12614: loss = 0.0025573164752033055\n",
      "Iteration 12615: loss = 0.0025573047379055017\n",
      "Iteration 12616: loss = 0.002557293000738795\n",
      "Iteration 12617: loss = 0.0025572812637031815\n",
      "Iteration 12618: loss = 0.0025572695267986572\n",
      "Iteration 12619: loss = 0.002557257790025219\n",
      "Iteration 12620: loss = 0.0025572460533828637\n",
      "Iteration 12621: loss = 0.002557234316871586\n",
      "Iteration 12622: loss = 0.002557222580491383\n",
      "Iteration 12623: loss = 0.0025572108442422506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12624: loss = 0.002557199108124186\n",
      "Iteration 12625: loss = 0.002557187372137184\n",
      "Iteration 12626: loss = 0.002557175636281243\n",
      "Iteration 12627: loss = 0.002557163900556357\n",
      "Iteration 12628: loss = 0.002557152164962523\n",
      "Iteration 12629: loss = 0.0025571404294997386\n",
      "Iteration 12630: loss = 0.0025571286941679988\n",
      "Iteration 12631: loss = 0.002557116958967299\n",
      "Iteration 12632: loss = 0.002557105223897638\n",
      "Iteration 12633: loss = 0.0025570934889590103\n",
      "Iteration 12634: loss = 0.0025570817541514116\n",
      "Iteration 12635: loss = 0.0025570700194748404\n",
      "Iteration 12636: loss = 0.002557058284929291\n",
      "Iteration 12637: loss = 0.002557046550514761\n",
      "Iteration 12638: loss = 0.0025570348162312464\n",
      "Iteration 12639: loss = 0.0025570230820787425\n",
      "Iteration 12640: loss = 0.0025570113480572467\n",
      "Iteration 12641: loss = 0.002556999614166755\n",
      "Iteration 12642: loss = 0.0025569878804072633\n",
      "Iteration 12643: loss = 0.0025569761467787696\n",
      "Iteration 12644: loss = 0.0025569644132812674\n",
      "Iteration 12645: loss = 0.002556952679914755\n",
      "Iteration 12646: loss = 0.0025569409466792287\n",
      "Iteration 12647: loss = 0.0025569292135746843\n",
      "Iteration 12648: loss = 0.0025569174806011176\n",
      "Iteration 12649: loss = 0.002556905747758526\n",
      "Iteration 12650: loss = 0.0025568940150469052\n",
      "Iteration 12651: loss = 0.0025568822824662512\n",
      "Iteration 12652: loss = 0.002556870550016561\n",
      "Iteration 12653: loss = 0.0025568588176978313\n",
      "Iteration 12654: loss = 0.002556847085510057\n",
      "Iteration 12655: loss = 0.002556835353453236\n",
      "Iteration 12656: loss = 0.002556823621527364\n",
      "Iteration 12657: loss = 0.0025568118897324366\n",
      "Iteration 12658: loss = 0.002556800158068451\n",
      "Iteration 12659: loss = 0.0025567884265354033\n",
      "Iteration 12660: loss = 0.00255677669513329\n",
      "Iteration 12661: loss = 0.002556764963862107\n",
      "Iteration 12662: loss = 0.0025567532327218515\n",
      "Iteration 12663: loss = 0.0025567415017125183\n",
      "Iteration 12664: loss = 0.002556729770834106\n",
      "Iteration 12665: loss = 0.002556718040086609\n",
      "Iteration 12666: loss = 0.002556706309470025\n",
      "Iteration 12667: loss = 0.002556694578984349\n",
      "Iteration 12668: loss = 0.002556682848629579\n",
      "Iteration 12669: loss = 0.00255667111840571\n",
      "Iteration 12670: loss = 0.0025566593883127385\n",
      "Iteration 12671: loss = 0.0025566476583506617\n",
      "Iteration 12672: loss = 0.002556635928519475\n",
      "Iteration 12673: loss = 0.0025566241988191755\n",
      "Iteration 12674: loss = 0.002556612469249759\n",
      "Iteration 12675: loss = 0.0025566007398112227\n",
      "Iteration 12676: loss = 0.002556589010503562\n",
      "Iteration 12677: loss = 0.002556577281326774\n",
      "Iteration 12678: loss = 0.002556565552280855\n",
      "Iteration 12679: loss = 0.002556553823365801\n",
      "Iteration 12680: loss = 0.0025565420945816084\n",
      "Iteration 12681: loss = 0.002556530365928274\n",
      "Iteration 12682: loss = 0.002556518637405794\n",
      "Iteration 12683: loss = 0.0025565069090141643\n",
      "Iteration 12684: loss = 0.002556495180753382\n",
      "Iteration 12685: loss = 0.002556483452623443\n",
      "Iteration 12686: loss = 0.0025564717246243447\n",
      "Iteration 12687: loss = 0.0025564599967560823\n",
      "Iteration 12688: loss = 0.0025564482690186526\n",
      "Iteration 12689: loss = 0.002556436541412052\n",
      "Iteration 12690: loss = 0.0025564248139362767\n",
      "Iteration 12691: loss = 0.0025564130865913235\n",
      "Iteration 12692: loss = 0.002556401359377189\n",
      "Iteration 12693: loss = 0.0025563896322938692\n",
      "Iteration 12694: loss = 0.0025563779053413608\n",
      "Iteration 12695: loss = 0.002556366178519659\n",
      "Iteration 12696: loss = 0.002556354451828762\n",
      "Iteration 12697: loss = 0.002556342725268665\n",
      "Iteration 12698: loss = 0.0025563309988393653\n",
      "Iteration 12699: loss = 0.0025563192725408584\n",
      "Iteration 12700: loss = 0.002556307546373142\n",
      "Iteration 12701: loss = 0.0025562958203362105\n",
      "Iteration 12702: loss = 0.0025562840944300625\n",
      "Iteration 12703: loss = 0.002556272368654693\n",
      "Iteration 12704: loss = 0.0025562606430100986\n",
      "Iteration 12705: loss = 0.002556248917496277\n",
      "Iteration 12706: loss = 0.002556237192113223\n",
      "Iteration 12707: loss = 0.002556225466860934\n",
      "Iteration 12708: loss = 0.002556213741739406\n",
      "Iteration 12709: loss = 0.0025562020167486355\n",
      "Iteration 12710: loss = 0.002556190291888619\n",
      "Iteration 12711: loss = 0.0025561785671593534\n",
      "Iteration 12712: loss = 0.0025561668425608344\n",
      "Iteration 12713: loss = 0.002556155118093059\n",
      "Iteration 12714: loss = 0.002556143393756023\n",
      "Iteration 12715: loss = 0.002556131669549724\n",
      "Iteration 12716: loss = 0.0025561199454741564\n",
      "Iteration 12717: loss = 0.0025561082215293193\n",
      "Iteration 12718: loss = 0.002556096497715207\n",
      "Iteration 12719: loss = 0.002556084774031817\n",
      "Iteration 12720: loss = 0.0025560730504791463\n",
      "Iteration 12721: loss = 0.0025560613270571896\n",
      "Iteration 12722: loss = 0.0025560496037659456\n",
      "Iteration 12723: loss = 0.0025560378806054086\n",
      "Iteration 12724: loss = 0.002556026157575576\n",
      "Iteration 12725: loss = 0.002556014434676445\n",
      "Iteration 12726: loss = 0.0025560027119080108\n",
      "Iteration 12727: loss = 0.0025559909892702705\n",
      "Iteration 12728: loss = 0.002555979266763221\n",
      "Iteration 12729: loss = 0.002555967544386858\n",
      "Iteration 12730: loss = 0.0025559558221411784\n",
      "Iteration 12731: loss = 0.0025559441000261784\n",
      "Iteration 12732: loss = 0.0025559323780418547\n",
      "Iteration 12733: loss = 0.002555920656188204\n",
      "Iteration 12734: loss = 0.002555908934465222\n",
      "Iteration 12735: loss = 0.0025558972128729065\n",
      "Iteration 12736: loss = 0.0025558854914112528\n",
      "Iteration 12737: loss = 0.002555873770080258\n",
      "Iteration 12738: loss = 0.0025558620488799182\n",
      "Iteration 12739: loss = 0.0025558503278102305\n",
      "Iteration 12740: loss = 0.002555838606871191\n",
      "Iteration 12741: loss = 0.002555826886062796\n",
      "Iteration 12742: loss = 0.0025558151653850415\n",
      "Iteration 12743: loss = 0.0025558034448379258\n",
      "Iteration 12744: loss = 0.002555791724421444\n",
      "Iteration 12745: loss = 0.0025557800041355926\n",
      "Iteration 12746: loss = 0.0025557682839803695\n",
      "Iteration 12747: loss = 0.0025557565639557694\n",
      "Iteration 12748: loss = 0.002555744844061789\n",
      "Iteration 12749: loss = 0.0025557331242984263\n",
      "Iteration 12750: loss = 0.0025557214046656772\n",
      "Iteration 12751: loss = 0.002555709685163537\n",
      "Iteration 12752: loss = 0.0025556979657920037\n",
      "Iteration 12753: loss = 0.0025556862465510735\n",
      "Iteration 12754: loss = 0.002555674527440742\n",
      "Iteration 12755: loss = 0.002555662808461007\n",
      "Iteration 12756: loss = 0.002555651089611865\n",
      "Iteration 12757: loss = 0.002555639370893311\n",
      "Iteration 12758: loss = 0.002555627652305343\n",
      "Iteration 12759: loss = 0.0025556159338479575\n",
      "Iteration 12760: loss = 0.00255560421552115\n",
      "Iteration 12761: loss = 0.0025555924973249176\n",
      "Iteration 12762: loss = 0.0025555807792592574\n",
      "Iteration 12763: loss = 0.002555569061324165\n",
      "Iteration 12764: loss = 0.0025555573435196376\n",
      "Iteration 12765: loss = 0.002555545625845672\n",
      "Iteration 12766: loss = 0.0025555339083022637\n",
      "Iteration 12767: loss = 0.0025555221908894095\n",
      "Iteration 12768: loss = 0.002555510473607107\n",
      "Iteration 12769: loss = 0.002555498756455352\n",
      "Iteration 12770: loss = 0.0025554870394341404\n",
      "Iteration 12771: loss = 0.0025554753225434704\n",
      "Iteration 12772: loss = 0.0025554636057833376\n",
      "Iteration 12773: loss = 0.002555451889153738\n",
      "Iteration 12774: loss = 0.002555440172654669\n",
      "Iteration 12775: loss = 0.0025554284562861267\n",
      "Iteration 12776: loss = 0.0025554167400481075\n",
      "Iteration 12777: loss = 0.0025554050239406092\n",
      "Iteration 12778: loss = 0.0025553933079636272\n",
      "Iteration 12779: loss = 0.0025553815921171585\n",
      "Iteration 12780: loss = 0.002555369876401199\n",
      "Iteration 12781: loss = 0.0025553581608157468\n",
      "Iteration 12782: loss = 0.002555346445360797\n",
      "Iteration 12783: loss = 0.0025553347300363463\n",
      "Iteration 12784: loss = 0.0025553230148423925\n",
      "Iteration 12785: loss = 0.0025553112997789306\n",
      "Iteration 12786: loss = 0.0025552995848459586\n",
      "Iteration 12787: loss = 0.0025552878700434716\n",
      "Iteration 12788: loss = 0.0025552761553714683\n",
      "Iteration 12789: loss = 0.002555264440829943\n",
      "Iteration 12790: loss = 0.002555252726418894\n",
      "Iteration 12791: loss = 0.0025552410121383157\n",
      "Iteration 12792: loss = 0.002555229297988208\n",
      "Iteration 12793: loss = 0.0025552175839685643\n",
      "Iteration 12794: loss = 0.002555205870079384\n",
      "Iteration 12795: loss = 0.0025551941563206615\n",
      "Iteration 12796: loss = 0.002555182442692394\n",
      "Iteration 12797: loss = 0.0025551707291945783\n",
      "Iteration 12798: loss = 0.0025551590158272115\n",
      "Iteration 12799: loss = 0.002555147302590289\n",
      "Iteration 12800: loss = 0.002555135589483809\n",
      "Iteration 12801: loss = 0.0025551238765077664\n",
      "Iteration 12802: loss = 0.00255511216366216\n",
      "Iteration 12803: loss = 0.002555100450946984\n",
      "Iteration 12804: loss = 0.0025550887383622364\n",
      "Iteration 12805: loss = 0.002555077025907913\n",
      "Iteration 12806: loss = 0.002555065313584011\n",
      "Iteration 12807: loss = 0.002555053601390528\n",
      "Iteration 12808: loss = 0.0025550418893274585\n",
      "Iteration 12809: loss = 0.0025550301773948\n",
      "Iteration 12810: loss = 0.0025550184655925502\n",
      "Iteration 12811: loss = 0.002555006753920705\n",
      "Iteration 12812: loss = 0.0025549950423792603\n",
      "Iteration 12813: loss = 0.0025549833309682136\n",
      "Iteration 12814: loss = 0.002554971619687561\n",
      "Iteration 12815: loss = 0.002554959908537299\n",
      "Iteration 12816: loss = 0.002554948197517425\n",
      "Iteration 12817: loss = 0.002554936486627936\n",
      "Iteration 12818: loss = 0.002554924775868827\n",
      "Iteration 12819: loss = 0.002554913065240096\n",
      "Iteration 12820: loss = 0.0025549013547417395\n",
      "Iteration 12821: loss = 0.0025548896443737534\n",
      "Iteration 12822: loss = 0.002554877934136135\n",
      "Iteration 12823: loss = 0.0025548662240288803\n",
      "Iteration 12824: loss = 0.002554854514051987\n",
      "Iteration 12825: loss = 0.0025548428042054504\n",
      "Iteration 12826: loss = 0.0025548310944892684\n",
      "Iteration 12827: loss = 0.002554819384903437\n",
      "Iteration 12828: loss = 0.0025548076754479533\n",
      "Iteration 12829: loss = 0.0025547959661228136\n",
      "Iteration 12830: loss = 0.002554784256928015\n",
      "Iteration 12831: loss = 0.0025547725478635534\n",
      "Iteration 12832: loss = 0.0025547608389294256\n",
      "Iteration 12833: loss = 0.002554749130125629\n",
      "Iteration 12834: loss = 0.00255473742145216\n",
      "Iteration 12835: loss = 0.002554725712909015\n",
      "Iteration 12836: loss = 0.00255471400449619\n",
      "Iteration 12837: loss = 0.002554702296213683\n",
      "Iteration 12838: loss = 0.0025546905880614906\n",
      "Iteration 12839: loss = 0.002554678880039609\n",
      "Iteration 12840: loss = 0.002554667172148034\n",
      "Iteration 12841: loss = 0.0025546554643867633\n",
      "Iteration 12842: loss = 0.0025546437567557944\n",
      "Iteration 12843: loss = 0.0025546320492551224\n",
      "Iteration 12844: loss = 0.0025546203418847445\n",
      "Iteration 12845: loss = 0.0025546086346446575\n",
      "Iteration 12846: loss = 0.0025545969275348585\n",
      "Iteration 12847: loss = 0.0025545852205553434\n",
      "Iteration 12848: loss = 0.0025545735137061094\n",
      "Iteration 12849: loss = 0.002554561806987153\n",
      "Iteration 12850: loss = 0.0025545501003984713\n",
      "Iteration 12851: loss = 0.0025545383939400606\n",
      "Iteration 12852: loss = 0.0025545266876119176\n",
      "Iteration 12853: loss = 0.002554514981414039\n",
      "Iteration 12854: loss = 0.002554503275346421\n",
      "Iteration 12855: loss = 0.002554491569409062\n",
      "Iteration 12856: loss = 0.002554479863601957\n",
      "Iteration 12857: loss = 0.0025544681579251034\n",
      "Iteration 12858: loss = 0.0025544564523784976\n",
      "Iteration 12859: loss = 0.002554444746962137\n",
      "Iteration 12860: loss = 0.0025544330416760177\n",
      "Iteration 12861: loss = 0.0025544213365201363\n",
      "Iteration 12862: loss = 0.0025544096314944896\n",
      "Iteration 12863: loss = 0.002554397926599075\n",
      "Iteration 12864: loss = 0.002554386221833888\n",
      "Iteration 12865: loss = 0.002554374517198927\n",
      "Iteration 12866: loss = 0.0025543628126941872\n",
      "Iteration 12867: loss = 0.0025543511083196664\n",
      "Iteration 12868: loss = 0.0025543394040753605\n",
      "Iteration 12869: loss = 0.0025543276999612664\n",
      "Iteration 12870: loss = 0.0025543159959773806\n",
      "Iteration 12871: loss = 0.0025543042921237014\n",
      "Iteration 12872: loss = 0.0025542925884002236\n",
      "Iteration 12873: loss = 0.002554280884806944\n",
      "Iteration 12874: loss = 0.002554269181343861\n",
      "Iteration 12875: loss = 0.00255425747801097\n",
      "Iteration 12876: loss = 0.002554245774808268\n",
      "Iteration 12877: loss = 0.002554234071735752\n",
      "Iteration 12878: loss = 0.0025542223687934186\n",
      "Iteration 12879: loss = 0.002554210665981265\n",
      "Iteration 12880: loss = 0.002554198963299287\n",
      "Iteration 12881: loss = 0.002554187260747481\n",
      "Iteration 12882: loss = 0.002554175558325846\n",
      "Iteration 12883: loss = 0.002554163856034377\n",
      "Iteration 12884: loss = 0.002554152153873071\n",
      "Iteration 12885: loss = 0.002554140451841924\n",
      "Iteration 12886: loss = 0.002554128749940935\n",
      "Iteration 12887: loss = 0.002554117048170098\n",
      "Iteration 12888: loss = 0.002554105346529412\n",
      "Iteration 12889: loss = 0.0025540936450188724\n",
      "Iteration 12890: loss = 0.0025540819436384765\n",
      "Iteration 12891: loss = 0.0025540702423882213\n",
      "Iteration 12892: loss = 0.0025540585412681034\n",
      "Iteration 12893: loss = 0.002554046840278119\n",
      "Iteration 12894: loss = 0.0025540351394182656\n",
      "Iteration 12895: loss = 0.00255402343868854\n",
      "Iteration 12896: loss = 0.002554011738088938\n",
      "Iteration 12897: loss = 0.0025540000376194573\n",
      "Iteration 12898: loss = 0.002553988337280094\n",
      "Iteration 12899: loss = 0.002553976637070846\n",
      "Iteration 12900: loss = 0.0025539649369917096\n",
      "Iteration 12901: loss = 0.00255395323704268\n",
      "Iteration 12902: loss = 0.0025539415372237565\n",
      "Iteration 12903: loss = 0.0025539298375349346\n",
      "Iteration 12904: loss = 0.0025539181379762113\n",
      "Iteration 12905: loss = 0.0025539064385475827\n",
      "Iteration 12906: loss = 0.0025538947392490462\n",
      "Iteration 12907: loss = 0.0025538830400805993\n",
      "Iteration 12908: loss = 0.002553871341042237\n",
      "Iteration 12909: loss = 0.002553859642133958\n",
      "Iteration 12910: loss = 0.0025538479433557587\n",
      "Iteration 12911: loss = 0.0025538362447076347\n",
      "Iteration 12912: loss = 0.0025538245461895833\n",
      "Iteration 12913: loss = 0.002553812847801602\n",
      "Iteration 12914: loss = 0.0025538011495436874\n",
      "Iteration 12915: loss = 0.0025537894514158356\n",
      "Iteration 12916: loss = 0.0025537777534180442\n",
      "Iteration 12917: loss = 0.00255376605555031\n",
      "Iteration 12918: loss = 0.002553754357812629\n",
      "Iteration 12919: loss = 0.002553742660204998\n",
      "Iteration 12920: loss = 0.002553730962727415\n",
      "Iteration 12921: loss = 0.002553719265379877\n",
      "Iteration 12922: loss = 0.0025537075681623784\n",
      "Iteration 12923: loss = 0.002553695871074918\n",
      "Iteration 12924: loss = 0.002553684174117492\n",
      "Iteration 12925: loss = 0.002553672477290098\n",
      "Iteration 12926: loss = 0.002553660780592732\n",
      "Iteration 12927: loss = 0.002553649084025391\n",
      "Iteration 12928: loss = 0.002553637387588072\n",
      "Iteration 12929: loss = 0.0025536256912807717\n",
      "Iteration 12930: loss = 0.0025536139951034867\n",
      "Iteration 12931: loss = 0.002553602299056214\n",
      "Iteration 12932: loss = 0.002553590603138951\n",
      "Iteration 12933: loss = 0.0025535789073516933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12934: loss = 0.0025535672116944394\n",
      "Iteration 12935: loss = 0.0025535555161671844\n",
      "Iteration 12936: loss = 0.0025535438207699265\n",
      "Iteration 12937: loss = 0.002553532125502661\n",
      "Iteration 12938: loss = 0.002553520430365387\n",
      "Iteration 12939: loss = 0.002553508735358099\n",
      "Iteration 12940: loss = 0.0025534970404807954\n",
      "Iteration 12941: loss = 0.0025534853457334725\n",
      "Iteration 12942: loss = 0.0025534736511161273\n",
      "Iteration 12943: loss = 0.002553461956628756\n",
      "Iteration 12944: loss = 0.0025534502622713566\n",
      "Iteration 12945: loss = 0.002553438568043925\n",
      "Iteration 12946: loss = 0.002553426873946459\n",
      "Iteration 12947: loss = 0.0025534151799789543\n",
      "Iteration 12948: loss = 0.002553403486141409\n",
      "Iteration 12949: loss = 0.002553391792433818\n",
      "Iteration 12950: loss = 0.0025533800988561807\n",
      "Iteration 12951: loss = 0.0025533684054084914\n",
      "Iteration 12952: loss = 0.0025533567120907493\n",
      "Iteration 12953: loss = 0.00255334501890295\n",
      "Iteration 12954: loss = 0.0025533333258450906\n",
      "Iteration 12955: loss = 0.0025533216329171673\n",
      "Iteration 12956: loss = 0.0025533099401191784\n",
      "Iteration 12957: loss = 0.0025532982474511198\n",
      "Iteration 12958: loss = 0.002553286554912988\n",
      "Iteration 12959: loss = 0.0025532748625047806\n",
      "Iteration 12960: loss = 0.0025532631702264957\n",
      "Iteration 12961: loss = 0.002553251478078127\n",
      "Iteration 12962: loss = 0.002553239786059674\n",
      "Iteration 12963: loss = 0.002553228094171133\n",
      "Iteration 12964: loss = 0.0025532164024125\n",
      "Iteration 12965: loss = 0.0025532047107837724\n",
      "Iteration 12966: loss = 0.0025531930192849474\n",
      "Iteration 12967: loss = 0.0025531813279160223\n",
      "Iteration 12968: loss = 0.002553169636676993\n",
      "Iteration 12969: loss = 0.002553157945567856\n",
      "Iteration 12970: loss = 0.0025531462545886094\n",
      "Iteration 12971: loss = 0.00255313456373925\n",
      "Iteration 12972: loss = 0.002553122873019774\n",
      "Iteration 12973: loss = 0.002553111182430179\n",
      "Iteration 12974: loss = 0.002553099491970461\n",
      "Iteration 12975: loss = 0.0025530878016406175\n",
      "Iteration 12976: loss = 0.002553076111440646\n",
      "Iteration 12977: loss = 0.002553064421370542\n",
      "Iteration 12978: loss = 0.002553052731430303\n",
      "Iteration 12979: loss = 0.0025530410416199265\n",
      "Iteration 12980: loss = 0.002553029351939409\n",
      "Iteration 12981: loss = 0.002553017662388747\n",
      "Iteration 12982: loss = 0.0025530059729679377\n",
      "Iteration 12983: loss = 0.0025529942836769784\n",
      "Iteration 12984: loss = 0.0025529825945158654\n",
      "Iteration 12985: loss = 0.0025529709054845956\n",
      "Iteration 12986: loss = 0.0025529592165831672\n",
      "Iteration 12987: loss = 0.002552947527811575\n",
      "Iteration 12988: loss = 0.0025529358391698175\n",
      "Iteration 12989: loss = 0.0025529241506578914\n",
      "Iteration 12990: loss = 0.0025529124622757932\n",
      "Iteration 12991: loss = 0.0025529007740235196\n",
      "Iteration 12992: loss = 0.0025528890859010684\n",
      "Iteration 12993: loss = 0.0025528773979084356\n",
      "Iteration 12994: loss = 0.002552865710045619\n",
      "Iteration 12995: loss = 0.0025528540223126147\n",
      "Iteration 12996: loss = 0.0025528423347094204\n",
      "Iteration 12997: loss = 0.0025528306472360324\n",
      "Iteration 12998: loss = 0.0025528189598924477\n",
      "Iteration 12999: loss = 0.002552807272678664\n",
      "Iteration 13000: loss = 0.002552795585594677\n",
      "Iteration 13001: loss = 0.0025527838986404844\n",
      "Iteration 13002: loss = 0.002552772211816084\n",
      "Iteration 13003: loss = 0.0025527605251214705\n",
      "Iteration 13004: loss = 0.002552748838556643\n",
      "Iteration 13005: loss = 0.0025527371521215972\n",
      "Iteration 13006: loss = 0.0025527254658163304\n",
      "Iteration 13007: loss = 0.002552713779640839\n",
      "Iteration 13008: loss = 0.0025527020935951217\n",
      "Iteration 13009: loss = 0.0025526904076791732\n",
      "Iteration 13010: loss = 0.002552678721892992\n",
      "Iteration 13011: loss = 0.002552667036236574\n",
      "Iteration 13012: loss = 0.002552655350709917\n",
      "Iteration 13013: loss = 0.002552643665313018\n",
      "Iteration 13014: loss = 0.002552631980045873\n",
      "Iteration 13015: loss = 0.00255262029490848\n",
      "Iteration 13016: loss = 0.0025526086099008356\n",
      "Iteration 13017: loss = 0.0025525969250229366\n",
      "Iteration 13018: loss = 0.0025525852402747797\n",
      "Iteration 13019: loss = 0.0025525735556563623\n",
      "Iteration 13020: loss = 0.0025525618711676815\n",
      "Iteration 13021: loss = 0.002552550186808734\n",
      "Iteration 13022: loss = 0.0025525385025795167\n",
      "Iteration 13023: loss = 0.0025525268184800266\n",
      "Iteration 13024: loss = 0.002552515134510261\n",
      "Iteration 13025: loss = 0.0025525034506702167\n",
      "Iteration 13026: loss = 0.00255249176695989\n",
      "Iteration 13027: loss = 0.002552480083379279\n",
      "Iteration 13028: loss = 0.00255246839992838\n",
      "Iteration 13029: loss = 0.0025524567166071896\n",
      "Iteration 13030: loss = 0.0025524450334157064\n",
      "Iteration 13031: loss = 0.0025524333503539258\n",
      "Iteration 13032: loss = 0.0025524216674218444\n",
      "Iteration 13033: loss = 0.002552409984619461\n",
      "Iteration 13034: loss = 0.002552398301946772\n",
      "Iteration 13035: loss = 0.0025523866194037733\n",
      "Iteration 13036: loss = 0.002552374936990462\n",
      "Iteration 13037: loss = 0.0025523632547068366\n",
      "Iteration 13038: loss = 0.0025523515725528927\n",
      "Iteration 13039: loss = 0.002552339890528628\n",
      "Iteration 13040: loss = 0.0025523282086340397\n",
      "Iteration 13041: loss = 0.0025523165268691236\n",
      "Iteration 13042: loss = 0.0025523048452338777\n",
      "Iteration 13043: loss = 0.0025522931637282993\n",
      "Iteration 13044: loss = 0.0025522814823523846\n",
      "Iteration 13045: loss = 0.0025522698011061305\n",
      "Iteration 13046: loss = 0.0025522581199895348\n",
      "Iteration 13047: loss = 0.0025522464390025936\n",
      "Iteration 13048: loss = 0.002552234758145305\n",
      "Iteration 13049: loss = 0.002552223077417665\n",
      "Iteration 13050: loss = 0.002552211396819671\n",
      "Iteration 13051: loss = 0.0025521997163513196\n",
      "Iteration 13052: loss = 0.0025521880360126086\n",
      "Iteration 13053: loss = 0.0025521763558035344\n",
      "Iteration 13054: loss = 0.0025521646757240947\n",
      "Iteration 13055: loss = 0.0025521529957742858\n",
      "Iteration 13056: loss = 0.0025521413159541044\n",
      "Iteration 13057: loss = 0.002552129636263549\n",
      "Iteration 13058: loss = 0.0025521179567026147\n",
      "Iteration 13059: loss = 0.0025521062772713003\n",
      "Iteration 13060: loss = 0.0025520945979696018\n",
      "Iteration 13061: loss = 0.002552082918797516\n",
      "Iteration 13062: loss = 0.0025520712397550413\n",
      "Iteration 13063: loss = 0.0025520595608421737\n",
      "Iteration 13064: loss = 0.00255204788205891\n",
      "Iteration 13065: loss = 0.0025520362034052472\n",
      "Iteration 13066: loss = 0.002552024524881183\n",
      "Iteration 13067: loss = 0.0025520128464867143\n",
      "Iteration 13068: loss = 0.002552001168221838\n",
      "Iteration 13069: loss = 0.0025519894900865514\n",
      "Iteration 13070: loss = 0.00255197781208085\n",
      "Iteration 13071: loss = 0.002551966134204733\n",
      "Iteration 13072: loss = 0.0025519544564581963\n",
      "Iteration 13073: loss = 0.0025519427788412373\n",
      "Iteration 13074: loss = 0.0025519311013538523\n",
      "Iteration 13075: loss = 0.0025519194239960394\n",
      "Iteration 13076: loss = 0.0025519077467677956\n",
      "Iteration 13077: loss = 0.002551896069669117\n",
      "Iteration 13078: loss = 0.002551884392700001\n",
      "Iteration 13079: loss = 0.002551872715860445\n",
      "Iteration 13080: loss = 0.0025518610391504462\n",
      "Iteration 13081: loss = 0.002551849362570001\n",
      "Iteration 13082: loss = 0.002551837686119107\n",
      "Iteration 13083: loss = 0.002551826009797761\n",
      "Iteration 13084: loss = 0.0025518143336059596\n",
      "Iteration 13085: loss = 0.002551802657543701\n",
      "Iteration 13086: loss = 0.002551790981610981\n",
      "Iteration 13087: loss = 0.0025517793058077976\n",
      "Iteration 13088: loss = 0.002551767630134147\n",
      "Iteration 13089: loss = 0.0025517559545900274\n",
      "Iteration 13090: loss = 0.002551744279175435\n",
      "Iteration 13091: loss = 0.0025517326038903675\n",
      "Iteration 13092: loss = 0.0025517209287348208\n",
      "Iteration 13093: loss = 0.0025517092537087932\n",
      "Iteration 13094: loss = 0.002551697578812281\n",
      "Iteration 13095: loss = 0.002551685904045282\n",
      "Iteration 13096: loss = 0.002551674229407792\n",
      "Iteration 13097: loss = 0.0025516625548998102\n",
      "Iteration 13098: loss = 0.0025516508805213316\n",
      "Iteration 13099: loss = 0.0025516392062723543\n",
      "Iteration 13100: loss = 0.0025516275321528755\n",
      "Iteration 13101: loss = 0.0025516158581628907\n",
      "Iteration 13102: loss = 0.0025516041843023996\n",
      "Iteration 13103: loss = 0.002551592510571397\n",
      "Iteration 13104: loss = 0.0025515808369698813\n",
      "Iteration 13105: loss = 0.0025515691634978486\n",
      "Iteration 13106: loss = 0.002551557490155297\n",
      "Iteration 13107: loss = 0.0025515458169422228\n",
      "Iteration 13108: loss = 0.0025515341438586236\n",
      "Iteration 13109: loss = 0.002551522470904496\n",
      "Iteration 13110: loss = 0.002551510798079838\n",
      "Iteration 13111: loss = 0.0025514991253846454\n",
      "Iteration 13112: loss = 0.0025514874528189168\n",
      "Iteration 13113: loss = 0.002551475780382648\n",
      "Iteration 13114: loss = 0.0025514641080758363\n",
      "Iteration 13115: loss = 0.0025514524358984793\n",
      "Iteration 13116: loss = 0.0025514407638505742\n",
      "Iteration 13117: loss = 0.002551429091932117\n",
      "Iteration 13118: loss = 0.0025514174201431063\n",
      "Iteration 13119: loss = 0.0025514057484835382\n",
      "Iteration 13120: loss = 0.00255139407695341\n",
      "Iteration 13121: loss = 0.0025513824055527186\n",
      "Iteration 13122: loss = 0.002551370734281462\n",
      "Iteration 13123: loss = 0.0025513590631396363\n",
      "Iteration 13124: loss = 0.002551347392127239\n",
      "Iteration 13125: loss = 0.002551335721244267\n",
      "Iteration 13126: loss = 0.0025513240504907177\n",
      "Iteration 13127: loss = 0.0025513123798665883\n",
      "Iteration 13128: loss = 0.0025513007093718757\n",
      "Iteration 13129: loss = 0.0025512890390065766\n",
      "Iteration 13130: loss = 0.0025512773687706896\n",
      "Iteration 13131: loss = 0.00255126569866421\n",
      "Iteration 13132: loss = 0.0025512540286871358\n",
      "Iteration 13133: loss = 0.002551242358839464\n",
      "Iteration 13134: loss = 0.0025512306891211922\n",
      "Iteration 13135: loss = 0.0025512190195323163\n",
      "Iteration 13136: loss = 0.002551207350072834\n",
      "Iteration 13137: loss = 0.0025511956807427435\n",
      "Iteration 13138: loss = 0.002551184011542041\n",
      "Iteration 13139: loss = 0.0025511723424707236\n",
      "Iteration 13140: loss = 0.0025511606735287885\n",
      "Iteration 13141: loss = 0.0025511490047162326\n",
      "Iteration 13142: loss = 0.0025511373360330536\n",
      "Iteration 13143: loss = 0.002551125667479248\n",
      "Iteration 13144: loss = 0.0025511139990548135\n",
      "Iteration 13145: loss = 0.0025511023307597464\n",
      "Iteration 13146: loss = 0.0025510906625940445\n",
      "Iteration 13147: loss = 0.0025510789945577053\n",
      "Iteration 13148: loss = 0.0025510673266507257\n",
      "Iteration 13149: loss = 0.0025510556588731023\n",
      "Iteration 13150: loss = 0.0025510439912248325\n",
      "Iteration 13151: loss = 0.0025510323237059136\n",
      "Iteration 13152: loss = 0.002551020656316343\n",
      "Iteration 13153: loss = 0.002551008989056117\n",
      "Iteration 13154: loss = 0.0025509973219252336\n",
      "Iteration 13155: loss = 0.0025509856549236895\n",
      "Iteration 13156: loss = 0.0025509739880514816\n",
      "Iteration 13157: loss = 0.002550962321308608\n",
      "Iteration 13158: loss = 0.0025509506546950653\n",
      "Iteration 13159: loss = 0.00255093898821085\n",
      "Iteration 13160: loss = 0.00255092732185596\n",
      "Iteration 13161: loss = 0.002550915655630393\n",
      "Iteration 13162: loss = 0.002550903989534145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13163: loss = 0.0025508923235672133\n",
      "Iteration 13164: loss = 0.002550880657729596\n",
      "Iteration 13165: loss = 0.002550868992021289\n",
      "Iteration 13166: loss = 0.002550857326442291\n",
      "Iteration 13167: loss = 0.002550845660992598\n",
      "Iteration 13168: loss = 0.002550833995672207\n",
      "Iteration 13169: loss = 0.0025508223304811163\n",
      "Iteration 13170: loss = 0.002550810665419322\n",
      "Iteration 13171: loss = 0.002550799000486822\n",
      "Iteration 13172: loss = 0.0025507873356836125\n",
      "Iteration 13173: loss = 0.002550775671009692\n",
      "Iteration 13174: loss = 0.0025507640064650564\n",
      "Iteration 13175: loss = 0.0025507523420497044\n",
      "Iteration 13176: loss = 0.0025507406777636314\n",
      "Iteration 13177: loss = 0.0025507290136068356\n",
      "Iteration 13178: loss = 0.0025507173495793144\n",
      "Iteration 13179: loss = 0.002550705685681064\n",
      "Iteration 13180: loss = 0.0025506940219120824\n",
      "Iteration 13181: loss = 0.002550682358272366\n",
      "Iteration 13182: loss = 0.0025506706947619135\n",
      "Iteration 13183: loss = 0.0025506590313807203\n",
      "Iteration 13184: loss = 0.002550647368128785\n",
      "Iteration 13185: loss = 0.002550635705006104\n",
      "Iteration 13186: loss = 0.002550624042012675\n",
      "Iteration 13187: loss = 0.002550612379148494\n",
      "Iteration 13188: loss = 0.002550600716413559\n",
      "Iteration 13189: loss = 0.002550589053807868\n",
      "Iteration 13190: loss = 0.0025505773913314174\n",
      "Iteration 13191: loss = 0.0025505657289842045\n",
      "Iteration 13192: loss = 0.0025505540667662262\n",
      "Iteration 13193: loss = 0.0025505424046774797\n",
      "Iteration 13194: loss = 0.0025505307427179627\n",
      "Iteration 13195: loss = 0.002550519080887672\n",
      "Iteration 13196: loss = 0.0025505074191866043\n",
      "Iteration 13197: loss = 0.0025504957576147585\n",
      "Iteration 13198: loss = 0.00255048409617213\n",
      "Iteration 13199: loss = 0.002550472434858717\n",
      "Iteration 13200: loss = 0.002550460773674517\n",
      "Iteration 13201: loss = 0.0025504491126195255\n",
      "Iteration 13202: loss = 0.0025504374516937415\n",
      "Iteration 13203: loss = 0.002550425790897162\n",
      "Iteration 13204: loss = 0.0025504141302297827\n",
      "Iteration 13205: loss = 0.0025504024696916027\n",
      "Iteration 13206: loss = 0.002550390809282618\n",
      "Iteration 13207: loss = 0.0025503791490028268\n",
      "Iteration 13208: loss = 0.0025503674888522248\n",
      "Iteration 13209: loss = 0.002550355828830811\n",
      "Iteration 13210: loss = 0.0025503441689385814\n",
      "Iteration 13211: loss = 0.0025503325091755335\n",
      "Iteration 13212: loss = 0.002550320849541665\n",
      "Iteration 13213: loss = 0.002550309190036972\n",
      "Iteration 13214: loss = 0.002550297530661453\n",
      "Iteration 13215: loss = 0.002550285871415105\n",
      "Iteration 13216: loss = 0.0025502742122979244\n",
      "Iteration 13217: loss = 0.002550262553309909\n",
      "Iteration 13218: loss = 0.0025502508944510558\n",
      "Iteration 13219: loss = 0.002550239235721362\n",
      "Iteration 13220: loss = 0.0025502275771208255\n",
      "Iteration 13221: loss = 0.0025502159186494423\n",
      "Iteration 13222: loss = 0.0025502042603072115\n",
      "Iteration 13223: loss = 0.0025501926020941286\n",
      "Iteration 13224: loss = 0.002550180944010191\n",
      "Iteration 13225: loss = 0.0025501692860553974\n",
      "Iteration 13226: loss = 0.0025501576282297433\n",
      "Iteration 13227: loss = 0.0025501459705332265\n",
      "Iteration 13228: loss = 0.002550134312965845\n",
      "Iteration 13229: loss = 0.002550122655527595\n",
      "Iteration 13230: loss = 0.0025501109982184744\n",
      "Iteration 13231: loss = 0.00255009934103848\n",
      "Iteration 13232: loss = 0.00255008768398761\n",
      "Iteration 13233: loss = 0.00255007602706586\n",
      "Iteration 13234: loss = 0.002550064370273228\n",
      "Iteration 13235: loss = 0.0025500527136097127\n",
      "Iteration 13236: loss = 0.0025500410570753087\n",
      "Iteration 13237: loss = 0.0025500294006700155\n",
      "Iteration 13238: loss = 0.0025500177443938295\n",
      "Iteration 13239: loss = 0.002550006088246747\n",
      "Iteration 13240: loss = 0.0025499944322287674\n",
      "Iteration 13241: loss = 0.0025499827763398857\n",
      "Iteration 13242: loss = 0.0025499711205801004\n",
      "Iteration 13243: loss = 0.0025499594649494085\n",
      "Iteration 13244: loss = 0.0025499478094478077\n",
      "Iteration 13245: loss = 0.0025499361540752947\n",
      "Iteration 13246: loss = 0.0025499244988318667\n",
      "Iteration 13247: loss = 0.0025499128437175213\n",
      "Iteration 13248: loss = 0.002549901188732256\n",
      "Iteration 13249: loss = 0.002549889533876067\n",
      "Iteration 13250: loss = 0.0025498778791489527\n",
      "Iteration 13251: loss = 0.0025498662245509096\n",
      "Iteration 13252: loss = 0.002549854570081936\n",
      "Iteration 13253: loss = 0.0025498429157420283\n",
      "Iteration 13254: loss = 0.0025498312615311832\n",
      "Iteration 13255: loss = 0.0025498196074494\n",
      "Iteration 13256: loss = 0.0025498079534966734\n",
      "Iteration 13257: loss = 0.0025497962996730026\n",
      "Iteration 13258: loss = 0.0025497846459783843\n",
      "Iteration 13259: loss = 0.0025497729924128156\n",
      "Iteration 13260: loss = 0.002549761338976294\n",
      "Iteration 13261: loss = 0.0025497496856688165\n",
      "Iteration 13262: loss = 0.002549738032490381\n",
      "Iteration 13263: loss = 0.002549726379440984\n",
      "Iteration 13264: loss = 0.0025497147265206225\n",
      "Iteration 13265: loss = 0.002549703073729295\n",
      "Iteration 13266: loss = 0.0025496914210669983\n",
      "Iteration 13267: loss = 0.0025496797685337293\n",
      "Iteration 13268: loss = 0.002549668116129486\n",
      "Iteration 13269: loss = 0.002549656463854265\n",
      "Iteration 13270: loss = 0.002549644811708064\n",
      "Iteration 13271: loss = 0.0025496331596908794\n",
      "Iteration 13272: loss = 0.0025496215078027098\n",
      "Iteration 13273: loss = 0.002549609856043552\n",
      "Iteration 13274: loss = 0.0025495982044134034\n",
      "Iteration 13275: loss = 0.0025495865529122606\n",
      "Iteration 13276: loss = 0.0025495749015401214\n",
      "Iteration 13277: loss = 0.0025495632502969836\n",
      "Iteration 13278: loss = 0.0025495515991828437\n",
      "Iteration 13279: loss = 0.0025495399481976996\n",
      "Iteration 13280: loss = 0.0025495282973415473\n",
      "Iteration 13281: loss = 0.002549516646614386\n",
      "Iteration 13282: loss = 0.002549504996016212\n",
      "Iteration 13283: loss = 0.0025494933455470228\n",
      "Iteration 13284: loss = 0.0025494816952068154\n",
      "Iteration 13285: loss = 0.002549470044995588\n",
      "Iteration 13286: loss = 0.0025494583949133356\n",
      "Iteration 13287: loss = 0.0025494467449600584\n",
      "Iteration 13288: loss = 0.002549435095135752\n",
      "Iteration 13289: loss = 0.002549423445440415\n",
      "Iteration 13290: loss = 0.0025494117958740426\n",
      "Iteration 13291: loss = 0.002549400146436634\n",
      "Iteration 13292: loss = 0.002549388497128186\n",
      "Iteration 13293: loss = 0.002549376847948696\n",
      "Iteration 13294: loss = 0.0025493651988981606\n",
      "Iteration 13295: loss = 0.002549353549976578\n",
      "Iteration 13296: loss = 0.002549341901183945\n",
      "Iteration 13297: loss = 0.002549330252520259\n",
      "Iteration 13298: loss = 0.0025493186039855175\n",
      "Iteration 13299: loss = 0.0025493069555797177\n",
      "Iteration 13300: loss = 0.002549295307302857\n",
      "Iteration 13301: loss = 0.0025492836591549327\n",
      "Iteration 13302: loss = 0.002549272011135942\n",
      "Iteration 13303: loss = 0.0025492603632458824\n",
      "Iteration 13304: loss = 0.002549248715484751\n",
      "Iteration 13305: loss = 0.0025492370678525454\n",
      "Iteration 13306: loss = 0.002549225420349263\n",
      "Iteration 13307: loss = 0.002549213772974901\n",
      "Iteration 13308: loss = 0.0025492021257294563\n",
      "Iteration 13309: loss = 0.002549190478612927\n",
      "Iteration 13310: loss = 0.0025491788316253094\n",
      "Iteration 13311: loss = 0.0025491671847666024\n",
      "Iteration 13312: loss = 0.0025491555380368017\n",
      "Iteration 13313: loss = 0.0025491438914359062\n",
      "Iteration 13314: loss = 0.0025491322449639115\n",
      "Iteration 13315: loss = 0.002549120598620816\n",
      "Iteration 13316: loss = 0.002549108952406617\n",
      "Iteration 13317: loss = 0.002549097306321312\n",
      "Iteration 13318: loss = 0.0025490856603648977\n",
      "Iteration 13319: loss = 0.0025490740145373723\n",
      "Iteration 13320: loss = 0.002549062368838732\n",
      "Iteration 13321: loss = 0.0025490507232689753\n",
      "Iteration 13322: loss = 0.002549039077828099\n",
      "Iteration 13323: loss = 0.0025490274325161\n",
      "Iteration 13324: loss = 0.0025490157873329766\n",
      "Iteration 13325: loss = 0.002549004142278726\n",
      "Iteration 13326: loss = 0.0025489924973533446\n",
      "Iteration 13327: loss = 0.002548980852556831\n",
      "Iteration 13328: loss = 0.0025489692078891815\n",
      "Iteration 13329: loss = 0.002548957563350394\n",
      "Iteration 13330: loss = 0.0025489459189404653\n",
      "Iteration 13331: loss = 0.002548934274659394\n",
      "Iteration 13332: loss = 0.0025489226305071768\n",
      "Iteration 13333: loss = 0.0025489109864838106\n",
      "Iteration 13334: loss = 0.002548899342589293\n",
      "Iteration 13335: loss = 0.002548887698823622\n",
      "Iteration 13336: loss = 0.0025488760551867938\n",
      "Iteration 13337: loss = 0.0025488644116788066\n",
      "Iteration 13338: loss = 0.0025488527682996577\n",
      "Iteration 13339: loss = 0.0025488411250493446\n",
      "Iteration 13340: loss = 0.0025488294819278634\n",
      "Iteration 13341: loss = 0.002548817838935214\n",
      "Iteration 13342: loss = 0.002548806196071391\n",
      "Iteration 13343: loss = 0.0025487945533363933\n",
      "Iteration 13344: loss = 0.002548782910730218\n",
      "Iteration 13345: loss = 0.002548771268252863\n",
      "Iteration 13346: loss = 0.0025487596259043243\n",
      "Iteration 13347: loss = 0.002548747983684601\n",
      "Iteration 13348: loss = 0.0025487363415936885\n",
      "Iteration 13349: loss = 0.002548724699631586\n",
      "Iteration 13350: loss = 0.0025487130577982902\n",
      "Iteration 13351: loss = 0.002548701416093798\n",
      "Iteration 13352: loss = 0.0025486897745181075\n",
      "Iteration 13353: loss = 0.0025486781330712155\n",
      "Iteration 13354: loss = 0.00254866649175312\n",
      "Iteration 13355: loss = 0.002548654850563818\n",
      "Iteration 13356: loss = 0.0025486432095033064\n",
      "Iteration 13357: loss = 0.0025486315685715843\n",
      "Iteration 13358: loss = 0.002548619927768647\n",
      "Iteration 13359: loss = 0.002548608287094493\n",
      "Iteration 13360: loss = 0.0025485966465491196\n",
      "Iteration 13361: loss = 0.0025485850061325237\n",
      "Iteration 13362: loss = 0.0025485733658447028\n",
      "Iteration 13363: loss = 0.002548561725685655\n",
      "Iteration 13364: loss = 0.0025485500856553774\n",
      "Iteration 13365: loss = 0.0025485384457538673\n",
      "Iteration 13366: loss = 0.002548526805981122\n",
      "Iteration 13367: loss = 0.002548515166337139\n",
      "Iteration 13368: loss = 0.002548503526821915\n",
      "Iteration 13369: loss = 0.0025484918874354484\n",
      "Iteration 13370: loss = 0.0025484802481777363\n",
      "Iteration 13371: loss = 0.002548468609048776\n",
      "Iteration 13372: loss = 0.0025484569700485646\n",
      "Iteration 13373: loss = 0.002548445331177101\n",
      "Iteration 13374: loss = 0.00254843369243438\n",
      "Iteration 13375: loss = 0.002548422053820401\n",
      "Iteration 13376: loss = 0.0025484104153351613\n",
      "Iteration 13377: loss = 0.002548398776978657\n",
      "Iteration 13378: loss = 0.002548387138750887\n",
      "Iteration 13379: loss = 0.002548375500651848\n",
      "Iteration 13380: loss = 0.002548363862681538\n",
      "Iteration 13381: loss = 0.0025483522248399525\n",
      "Iteration 13382: loss = 0.002548340587127091\n",
      "Iteration 13383: loss = 0.0025483289495429502\n",
      "Iteration 13384: loss = 0.002548317312087528\n",
      "Iteration 13385: loss = 0.0025483056747608213\n",
      "Iteration 13386: loss = 0.0025482940375628274\n",
      "Iteration 13387: loss = 0.002548282400493544\n",
      "Iteration 13388: loss = 0.002548270763552968\n",
      "Iteration 13389: loss = 0.0025482591267410974\n",
      "Iteration 13390: loss = 0.002548247490057929\n",
      "Iteration 13391: loss = 0.0025482358535034613\n",
      "Iteration 13392: loss = 0.0025482242170776913\n",
      "Iteration 13393: loss = 0.0025482125807806157\n",
      "Iteration 13394: loss = 0.002548200944612232\n",
      "Iteration 13395: loss = 0.0025481893085725392\n",
      "Iteration 13396: loss = 0.002548177672661533\n",
      "Iteration 13397: loss = 0.0025481660368792116\n",
      "Iteration 13398: loss = 0.002548154401225572\n",
      "Iteration 13399: loss = 0.002548142765700612\n",
      "Iteration 13400: loss = 0.002548131130304329\n",
      "Iteration 13401: loss = 0.0025481194950367205\n",
      "Iteration 13402: loss = 0.0025481078598977827\n",
      "Iteration 13403: loss = 0.002548096224887515\n",
      "Iteration 13404: loss = 0.002548084590005914\n",
      "Iteration 13405: loss = 0.002548072955252977\n",
      "Iteration 13406: loss = 0.002548061320628701\n",
      "Iteration 13407: loss = 0.002548049686133085\n",
      "Iteration 13408: loss = 0.0025480380517661247\n",
      "Iteration 13409: loss = 0.002548026417527818\n",
      "Iteration 13410: loss = 0.002548014783418163\n",
      "Iteration 13411: loss = 0.0025480031494371563\n",
      "Iteration 13412: loss = 0.0025479915155847965\n",
      "Iteration 13413: loss = 0.0025479798818610795\n",
      "Iteration 13414: loss = 0.002547968248266004\n",
      "Iteration 13415: loss = 0.0025479566147995673\n",
      "Iteration 13416: loss = 0.0025479449814617655\n",
      "Iteration 13417: loss = 0.002547933348252598\n",
      "Iteration 13418: loss = 0.0025479217151720608\n",
      "Iteration 13419: loss = 0.0025479100822201526\n",
      "Iteration 13420: loss = 0.00254789844939687\n",
      "Iteration 13421: loss = 0.00254788681670221\n",
      "Iteration 13422: loss = 0.0025478751841361705\n",
      "Iteration 13423: loss = 0.0025478635516987497\n",
      "Iteration 13424: loss = 0.0025478519193899447\n",
      "Iteration 13425: loss = 0.0025478402872097516\n",
      "Iteration 13426: loss = 0.00254782865515817\n",
      "Iteration 13427: loss = 0.002547817023235196\n",
      "Iteration 13428: loss = 0.0025478053914408274\n",
      "Iteration 13429: loss = 0.002547793759775062\n",
      "Iteration 13430: loss = 0.002547782128237896\n",
      "Iteration 13431: loss = 0.0025477704968293285\n",
      "Iteration 13432: loss = 0.0025477588655493555\n",
      "Iteration 13433: loss = 0.002547747234397976\n",
      "Iteration 13434: loss = 0.0025477356033751864\n",
      "Iteration 13435: loss = 0.002547723972480984\n",
      "Iteration 13436: loss = 0.0025477123417153675\n",
      "Iteration 13437: loss = 0.002547700711078333\n",
      "Iteration 13438: loss = 0.0025476890805698786\n",
      "Iteration 13439: loss = 0.0025476774501900024\n",
      "Iteration 13440: loss = 0.0025476658199387\n",
      "Iteration 13441: loss = 0.002547654189815971\n",
      "Iteration 13442: loss = 0.0025476425598218114\n",
      "Iteration 13443: loss = 0.002547630929956219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13444: loss = 0.002547619300219192\n",
      "Iteration 13445: loss = 0.002547607670610727\n",
      "Iteration 13446: loss = 0.0025475960411308223\n",
      "Iteration 13447: loss = 0.0025475844117794745\n",
      "Iteration 13448: loss = 0.0025475727825566815\n",
      "Iteration 13449: loss = 0.0025475611534624405\n",
      "Iteration 13450: loss = 0.0025475495244967495\n",
      "Iteration 13451: loss = 0.0025475378956596063\n",
      "Iteration 13452: loss = 0.002547526266951007\n",
      "Iteration 13453: loss = 0.0025475146383709497\n",
      "Iteration 13454: loss = 0.002547503009919433\n",
      "Iteration 13455: loss = 0.0025474913815964526\n",
      "Iteration 13456: loss = 0.002547479753402008\n",
      "Iteration 13457: loss = 0.002547468125336094\n",
      "Iteration 13458: loss = 0.00254745649739871\n",
      "Iteration 13459: loss = 0.002547444869589854\n",
      "Iteration 13460: loss = 0.0025474332419095215\n",
      "Iteration 13461: loss = 0.0025474216143577116\n",
      "Iteration 13462: loss = 0.002547409986934422\n",
      "Iteration 13463: loss = 0.0025473983596396486\n",
      "Iteration 13464: loss = 0.00254738673247339\n",
      "Iteration 13465: loss = 0.0025473751054356434\n",
      "Iteration 13466: loss = 0.0025473634785264064\n",
      "Iteration 13467: loss = 0.002547351851745677\n",
      "Iteration 13468: loss = 0.002547340225093451\n",
      "Iteration 13469: loss = 0.002547328598569728\n",
      "Iteration 13470: loss = 0.0025473169721745045\n",
      "Iteration 13471: loss = 0.0025473053459077774\n",
      "Iteration 13472: loss = 0.0025472937197695456\n",
      "Iteration 13473: loss = 0.0025472820937598056\n",
      "Iteration 13474: loss = 0.0025472704678785553\n",
      "Iteration 13475: loss = 0.002547258842125792\n",
      "Iteration 13476: loss = 0.002547247216501513\n",
      "Iteration 13477: loss = 0.0025472355910057167\n",
      "Iteration 13478: loss = 0.0025472239656383995\n",
      "Iteration 13479: loss = 0.0025472123403995593\n",
      "Iteration 13480: loss = 0.002547200715289194\n",
      "Iteration 13481: loss = 0.0025471890903073003\n",
      "Iteration 13482: loss = 0.002547177465453877\n",
      "Iteration 13483: loss = 0.0025471658407289206\n",
      "Iteration 13484: loss = 0.0025471542161324286\n",
      "Iteration 13485: loss = 0.0025471425916643994\n",
      "Iteration 13486: loss = 0.002547130967324829\n",
      "Iteration 13487: loss = 0.0025471193431137163\n",
      "Iteration 13488: loss = 0.0025471077190310585\n",
      "Iteration 13489: loss = 0.002547096095076853\n",
      "Iteration 13490: loss = 0.002547084471251097\n",
      "Iteration 13491: loss = 0.002547072847553788\n",
      "Iteration 13492: loss = 0.0025470612239849246\n",
      "Iteration 13493: loss = 0.002547049600544503\n",
      "Iteration 13494: loss = 0.002547037977232521\n",
      "Iteration 13495: loss = 0.002547026354048977\n",
      "Iteration 13496: loss = 0.0025470147309938677\n",
      "Iteration 13497: loss = 0.002547003108067191\n",
      "Iteration 13498: loss = 0.0025469914852689437\n",
      "Iteration 13499: loss = 0.0025469798625991245\n",
      "Iteration 13500: loss = 0.00254696824005773\n",
      "Iteration 13501: loss = 0.002546956617644758\n",
      "Iteration 13502: loss = 0.002546944995360206\n",
      "Iteration 13503: loss = 0.0025469333732040717\n",
      "Iteration 13504: loss = 0.002546921751176352\n",
      "Iteration 13505: loss = 0.0025469101292770464\n",
      "Iteration 13506: loss = 0.00254689850750615\n",
      "Iteration 13507: loss = 0.0025468868858636614\n",
      "Iteration 13508: loss = 0.0025468752643495783\n",
      "Iteration 13509: loss = 0.0025468636429638977\n",
      "Iteration 13510: loss = 0.002546852021706618\n",
      "Iteration 13511: loss = 0.0025468404005777358\n",
      "Iteration 13512: loss = 0.0025468287795772497\n",
      "Iteration 13513: loss = 0.0025468171587051556\n",
      "Iteration 13514: loss = 0.002546805537961452\n",
      "Iteration 13515: loss = 0.002546793917346137\n",
      "Iteration 13516: loss = 0.0025467822968592074\n",
      "Iteration 13517: loss = 0.002546770676500661\n",
      "Iteration 13518: loss = 0.002546759056270495\n",
      "Iteration 13519: loss = 0.0025467474361687077\n",
      "Iteration 13520: loss = 0.002546735816195296\n",
      "Iteration 13521: loss = 0.002546724196350258\n",
      "Iteration 13522: loss = 0.0025467125766335903\n",
      "Iteration 13523: loss = 0.0025467009570452914\n",
      "Iteration 13524: loss = 0.002546689337585358\n",
      "Iteration 13525: loss = 0.002546677718253789\n",
      "Iteration 13526: loss = 0.00254666609905058\n",
      "Iteration 13527: loss = 0.002546654479975731\n",
      "Iteration 13528: loss = 0.0025466428610292375\n",
      "Iteration 13529: loss = 0.0025466312422110973\n",
      "Iteration 13530: loss = 0.0025466196235213088\n",
      "Iteration 13531: loss = 0.002546608004959869\n",
      "Iteration 13532: loss = 0.0025465963865267754\n",
      "Iteration 13533: loss = 0.002546584768222026\n",
      "Iteration 13534: loss = 0.0025465731500456186\n",
      "Iteration 13535: loss = 0.0025465615319975503\n",
      "Iteration 13536: loss = 0.002546549914077818\n",
      "Iteration 13537: loss = 0.002546538296286421\n",
      "Iteration 13538: loss = 0.002546526678623355\n",
      "Iteration 13539: loss = 0.002546515061088618\n",
      "Iteration 13540: loss = 0.0025465034436822085\n",
      "Iteration 13541: loss = 0.0025464918264041236\n",
      "Iteration 13542: loss = 0.00254648020925436\n",
      "Iteration 13543: loss = 0.002546468592232917\n",
      "Iteration 13544: loss = 0.0025464569753397906\n",
      "Iteration 13545: loss = 0.002546445358574979\n",
      "Iteration 13546: loss = 0.00254643374193848\n",
      "Iteration 13547: loss = 0.002546422125430291\n",
      "Iteration 13548: loss = 0.002546410509050409\n",
      "Iteration 13549: loss = 0.0025463988927988323\n",
      "Iteration 13550: loss = 0.0025463872766755585\n",
      "Iteration 13551: loss = 0.002546375660680584\n",
      "Iteration 13552: loss = 0.0025463640448139084\n",
      "Iteration 13553: loss = 0.0025463524290755277\n",
      "Iteration 13554: loss = 0.00254634081346544\n",
      "Iteration 13555: loss = 0.002546329197983643\n",
      "Iteration 13556: loss = 0.002546317582630134\n",
      "Iteration 13557: loss = 0.00254630596740491\n",
      "Iteration 13558: loss = 0.0025462943523079703\n",
      "Iteration 13559: loss = 0.0025462827373393107\n",
      "Iteration 13560: loss = 0.002546271122498929\n",
      "Iteration 13561: loss = 0.0025462595077868247\n",
      "Iteration 13562: loss = 0.002546247893202993\n",
      "Iteration 13563: loss = 0.0025462362787474326\n",
      "Iteration 13564: loss = 0.0025462246644201414\n",
      "Iteration 13565: loss = 0.0025462130502211166\n",
      "Iteration 13566: loss = 0.0025462014361503555\n",
      "Iteration 13567: loss = 0.002546189822207856\n",
      "Iteration 13568: loss = 0.0025461782083936154\n",
      "Iteration 13569: loss = 0.002546166594707632\n",
      "Iteration 13570: loss = 0.0025461549811499023\n",
      "Iteration 13571: loss = 0.0025461433677204247\n",
      "Iteration 13572: loss = 0.0025461317544191965\n",
      "Iteration 13573: loss = 0.002546120141246216\n",
      "Iteration 13574: loss = 0.0025461085282014796\n",
      "Iteration 13575: loss = 0.002546096915284985\n",
      "Iteration 13576: loss = 0.0025460853024967312\n",
      "Iteration 13577: loss = 0.0025460736898367145\n",
      "Iteration 13578: loss = 0.002546062077304933\n",
      "Iteration 13579: loss = 0.002546050464901384\n",
      "Iteration 13580: loss = 0.0025460388526260657\n",
      "Iteration 13581: loss = 0.002546027240478975\n",
      "Iteration 13582: loss = 0.00254601562846011\n",
      "Iteration 13583: loss = 0.0025460040165694673\n",
      "Iteration 13584: loss = 0.002545992404807046\n",
      "Iteration 13585: loss = 0.0025459807931728427\n",
      "Iteration 13586: loss = 0.0025459691816668555\n",
      "Iteration 13587: loss = 0.0025459575702890813\n",
      "Iteration 13588: loss = 0.002545945959039519\n",
      "Iteration 13589: loss = 0.0025459343479181645\n",
      "Iteration 13590: loss = 0.002545922736925017\n",
      "Iteration 13591: loss = 0.002545911126060073\n",
      "Iteration 13592: loss = 0.0025458995153233312\n",
      "Iteration 13593: loss = 0.0025458879047147885\n",
      "Iteration 13594: loss = 0.0025458762942344422\n",
      "Iteration 13595: loss = 0.0025458646838822903\n",
      "Iteration 13596: loss = 0.002545853073658331\n",
      "Iteration 13597: loss = 0.002545841463562561\n",
      "Iteration 13598: loss = 0.0025458298535949776\n",
      "Iteration 13599: loss = 0.00254581824375558\n",
      "Iteration 13600: loss = 0.0025458066340443637\n",
      "Iteration 13601: loss = 0.002545795024461329\n",
      "Iteration 13602: loss = 0.002545783415006471\n",
      "Iteration 13603: loss = 0.0025457718056797886\n",
      "Iteration 13604: loss = 0.002545760196481279\n",
      "Iteration 13605: loss = 0.00254574858741094\n",
      "Iteration 13606: loss = 0.00254573697846877\n",
      "Iteration 13607: loss = 0.0025457253696547647\n",
      "Iteration 13608: loss = 0.002545713760968923\n",
      "Iteration 13609: loss = 0.002545702152411243\n",
      "Iteration 13610: loss = 0.0025456905439817214\n",
      "Iteration 13611: loss = 0.002545678935680356\n",
      "Iteration 13612: loss = 0.0025456673275071453\n",
      "Iteration 13613: loss = 0.0025456557194620854\n",
      "Iteration 13614: loss = 0.0025456441115451743\n",
      "Iteration 13615: loss = 0.0025456325037564106\n",
      "Iteration 13616: loss = 0.002545620896095792\n",
      "Iteration 13617: loss = 0.0025456092885633146\n",
      "Iteration 13618: loss = 0.0025455976811589775\n",
      "Iteration 13619: loss = 0.0025455860738827774\n",
      "Iteration 13620: loss = 0.0025455744667347126\n",
      "Iteration 13621: loss = 0.00254556285971478\n",
      "Iteration 13622: loss = 0.0025455512528229783\n",
      "Iteration 13623: loss = 0.0025455396460593045\n",
      "Iteration 13624: loss = 0.0025455280394237555\n",
      "Iteration 13625: loss = 0.0025455164329163307\n",
      "Iteration 13626: loss = 0.002545504826537026\n",
      "Iteration 13627: loss = 0.0025454932202858397\n",
      "Iteration 13628: loss = 0.00254548161416277\n",
      "Iteration 13629: loss = 0.002545470008167814\n",
      "Iteration 13630: loss = 0.0025454584023009695\n",
      "Iteration 13631: loss = 0.0025454467965622344\n",
      "Iteration 13632: loss = 0.0025454351909516048\n",
      "Iteration 13633: loss = 0.002545423585469081\n",
      "Iteration 13634: loss = 0.0025454119801146583\n",
      "Iteration 13635: loss = 0.002545400374888335\n",
      "Iteration 13636: loss = 0.0025453887697901097\n",
      "Iteration 13637: loss = 0.002545377164819979\n",
      "Iteration 13638: loss = 0.002545365559977941\n",
      "Iteration 13639: loss = 0.002545353955263993\n",
      "Iteration 13640: loss = 0.002545342350678133\n",
      "Iteration 13641: loss = 0.002545330746220359\n",
      "Iteration 13642: loss = 0.002545319141890668\n",
      "Iteration 13643: loss = 0.002545307537689057\n",
      "Iteration 13644: loss = 0.002545295933615526\n",
      "Iteration 13645: loss = 0.00254528432967007\n",
      "Iteration 13646: loss = 0.0025452727258526886\n",
      "Iteration 13647: loss = 0.0025452611221633776\n",
      "Iteration 13648: loss = 0.002545249518602137\n",
      "Iteration 13649: loss = 0.002545237915168962\n",
      "Iteration 13650: loss = 0.002545226311863852\n",
      "Iteration 13651: loss = 0.002545214708686804\n",
      "Iteration 13652: loss = 0.0025452031056378164\n",
      "Iteration 13653: loss = 0.002545191502716886\n",
      "Iteration 13654: loss = 0.00254517989992401\n",
      "Iteration 13655: loss = 0.0025451682972591873\n",
      "Iteration 13656: loss = 0.0025451566947224157\n",
      "Iteration 13657: loss = 0.002545145092313691\n",
      "Iteration 13658: loss = 0.0025451334900330126\n",
      "Iteration 13659: loss = 0.0025451218878803777\n",
      "Iteration 13660: loss = 0.002545110285855784\n",
      "Iteration 13661: loss = 0.0025450986839592286\n",
      "Iteration 13662: loss = 0.00254508708219071\n",
      "Iteration 13663: loss = 0.0025450754805502254\n",
      "Iteration 13664: loss = 0.0025450638790377724\n",
      "Iteration 13665: loss = 0.0025450522776533494\n",
      "Iteration 13666: loss = 0.0025450406763969534\n",
      "Iteration 13667: loss = 0.0025450290752685816\n",
      "Iteration 13668: loss = 0.0025450174742682324\n",
      "Iteration 13669: loss = 0.0025450058733959037\n",
      "Iteration 13670: loss = 0.002544994272651593\n",
      "Iteration 13671: loss = 0.002544982672035297\n",
      "Iteration 13672: loss = 0.0025449710715470144\n",
      "Iteration 13673: loss = 0.002544959471186743\n",
      "Iteration 13674: loss = 0.00254494787095448\n",
      "Iteration 13675: loss = 0.0025449362708502236\n",
      "Iteration 13676: loss = 0.002544924670873971\n",
      "Iteration 13677: loss = 0.00254491307102572\n",
      "Iteration 13678: loss = 0.0025449014713054673\n",
      "Iteration 13679: loss = 0.0025448898717132127\n",
      "Iteration 13680: loss = 0.002544878272248952\n",
      "Iteration 13681: loss = 0.002544866672912684\n",
      "Iteration 13682: loss = 0.002544855073704406\n",
      "Iteration 13683: loss = 0.0025448434746241154\n",
      "Iteration 13684: loss = 0.0025448318756718102\n",
      "Iteration 13685: loss = 0.0025448202768474882\n",
      "Iteration 13686: loss = 0.002544808678151147\n",
      "Iteration 13687: loss = 0.0025447970795827845\n",
      "Iteration 13688: loss = 0.002544785481142397\n",
      "Iteration 13689: loss = 0.002544773882829985\n",
      "Iteration 13690: loss = 0.0025447622846455436\n",
      "Iteration 13691: loss = 0.0025447506865890713\n",
      "Iteration 13692: loss = 0.002544739088660566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13693: loss = 0.0025447274908600255\n",
      "Iteration 13694: loss = 0.002544715893187447\n",
      "Iteration 13695: loss = 0.0025447042956428284\n",
      "Iteration 13696: loss = 0.002544692698226168\n",
      "Iteration 13697: loss = 0.0025446811009374624\n",
      "Iteration 13698: loss = 0.0025446695037767107\n",
      "Iteration 13699: loss = 0.0025446579067439088\n",
      "Iteration 13700: loss = 0.0025446463098390557\n",
      "Iteration 13701: loss = 0.002544634713062149\n",
      "Iteration 13702: loss = 0.002544623116413186\n",
      "Iteration 13703: loss = 0.0025446115198921645\n",
      "Iteration 13704: loss = 0.0025445999234990826\n",
      "Iteration 13705: loss = 0.0025445883272339374\n",
      "Iteration 13706: loss = 0.0025445767310967268\n",
      "Iteration 13707: loss = 0.0025445651350874485\n",
      "Iteration 13708: loss = 0.002544553539206101\n",
      "Iteration 13709: loss = 0.0025445419434526806\n",
      "Iteration 13710: loss = 0.002544530347827186\n",
      "Iteration 13711: loss = 0.0025445187523296144\n",
      "Iteration 13712: loss = 0.0025445071569599642\n",
      "Iteration 13713: loss = 0.0025444955617182326\n",
      "Iteration 13714: loss = 0.002544483966604417\n",
      "Iteration 13715: loss = 0.0025444723716185154\n",
      "Iteration 13716: loss = 0.0025444607767605254\n",
      "Iteration 13717: loss = 0.0025444491820304452\n",
      "Iteration 13718: loss = 0.0025444375874282723\n",
      "Iteration 13719: loss = 0.0025444259929540045\n",
      "Iteration 13720: loss = 0.0025444143986076387\n",
      "Iteration 13721: loss = 0.0025444028043891736\n",
      "Iteration 13722: loss = 0.0025443912102986067\n",
      "Iteration 13723: loss = 0.0025443796163359357\n",
      "Iteration 13724: loss = 0.002544368022501158\n",
      "Iteration 13725: loss = 0.0025443564287942716\n",
      "Iteration 13726: loss = 0.0025443448352152737\n",
      "Iteration 13727: loss = 0.0025443332417641627\n",
      "Iteration 13728: loss = 0.002544321648440936\n",
      "Iteration 13729: loss = 0.0025443100552455917\n",
      "Iteration 13730: loss = 0.002544298462178127\n",
      "Iteration 13731: loss = 0.0025442868692385394\n",
      "Iteration 13732: loss = 0.0025442752764268275\n",
      "Iteration 13733: loss = 0.002544263683742989\n",
      "Iteration 13734: loss = 0.0025442520911870214\n",
      "Iteration 13735: loss = 0.002544240498758922\n",
      "Iteration 13736: loss = 0.0025442289064586876\n",
      "Iteration 13737: loss = 0.002544217314286318\n",
      "Iteration 13738: loss = 0.0025442057222418105\n",
      "Iteration 13739: loss = 0.0025441941303251617\n",
      "Iteration 13740: loss = 0.00254418253853637\n",
      "Iteration 13741: loss = 0.0025441709468754336\n",
      "Iteration 13742: loss = 0.0025441593553423496\n",
      "Iteration 13743: loss = 0.0025441477639371155\n",
      "Iteration 13744: loss = 0.0025441361726597296\n",
      "Iteration 13745: loss = 0.0025441245815101894\n",
      "Iteration 13746: loss = 0.0025441129904884927\n",
      "Iteration 13747: loss = 0.0025441013995946373\n",
      "Iteration 13748: loss = 0.002544089808828621\n",
      "Iteration 13749: loss = 0.002544078218190442\n",
      "Iteration 13750: loss = 0.002544066627680096\n",
      "Iteration 13751: loss = 0.0025440550372975825\n",
      "Iteration 13752: loss = 0.0025440434470429\n",
      "Iteration 13753: loss = 0.002544031856916044\n",
      "Iteration 13754: loss = 0.0025440202669170142\n",
      "Iteration 13755: loss = 0.002544008677045807\n",
      "Iteration 13756: loss = 0.002543997087302421\n",
      "Iteration 13757: loss = 0.0025439854976868534\n",
      "Iteration 13758: loss = 0.002543973908199102\n",
      "Iteration 13759: loss = 0.002543962318839165\n",
      "Iteration 13760: loss = 0.00254395072960704\n",
      "Iteration 13761: loss = 0.0025439391405027243\n",
      "Iteration 13762: loss = 0.002543927551526216\n",
      "Iteration 13763: loss = 0.002543915962677513\n",
      "Iteration 13764: loss = 0.0025439043739566127\n",
      "Iteration 13765: loss = 0.002543892785363513\n",
      "Iteration 13766: loss = 0.0025438811968982113\n",
      "Iteration 13767: loss = 0.002543869608560706\n",
      "Iteration 13768: loss = 0.0025438580203509943\n",
      "Iteration 13769: loss = 0.002543846432269075\n",
      "Iteration 13770: loss = 0.002543834844314945\n",
      "Iteration 13771: loss = 0.0025438232564886014\n",
      "Iteration 13772: loss = 0.002543811668790042\n",
      "Iteration 13773: loss = 0.0025438000812192663\n",
      "Iteration 13774: loss = 0.0025437884937762707\n",
      "Iteration 13775: loss = 0.002543776906461053\n",
      "Iteration 13776: loss = 0.0025437653192736114\n",
      "Iteration 13777: loss = 0.0025437537322139437\n",
      "Iteration 13778: loss = 0.0025437421452820467\n",
      "Iteration 13779: loss = 0.0025437305584779194\n",
      "Iteration 13780: loss = 0.002543718971801559\n",
      "Iteration 13781: loss = 0.0025437073852529633\n",
      "Iteration 13782: loss = 0.0025436957988321297\n",
      "Iteration 13783: loss = 0.0025436842125390563\n",
      "Iteration 13784: loss = 0.0025436726263737414\n",
      "Iteration 13785: loss = 0.0025436610403361815\n",
      "Iteration 13786: loss = 0.002543649454426376\n",
      "Iteration 13787: loss = 0.0025436378686443206\n",
      "Iteration 13788: loss = 0.002543626282990015\n",
      "Iteration 13789: loss = 0.002543614697463456\n",
      "Iteration 13790: loss = 0.002543603112064641\n",
      "Iteration 13791: loss = 0.0025435915267935693\n",
      "Iteration 13792: loss = 0.0025435799416502368\n",
      "Iteration 13793: loss = 0.0025435683566346428\n",
      "Iteration 13794: loss = 0.0025435567717467843\n",
      "Iteration 13795: loss = 0.0025435451869866587\n",
      "Iteration 13796: loss = 0.002543533602354265\n",
      "Iteration 13797: loss = 0.0025435220178495997\n",
      "Iteration 13798: loss = 0.0025435104334726615\n",
      "Iteration 13799: loss = 0.0025434988492234476\n",
      "Iteration 13800: loss = 0.0025434872651019557\n",
      "Iteration 13801: loss = 0.002543475681108184\n",
      "Iteration 13802: loss = 0.0025434640972421298\n",
      "Iteration 13803: loss = 0.002543452513503792\n",
      "Iteration 13804: loss = 0.002543440929893167\n",
      "Iteration 13805: loss = 0.002543429346410253\n",
      "Iteration 13806: loss = 0.002543417763055049\n",
      "Iteration 13807: loss = 0.00254340617982755\n",
      "Iteration 13808: loss = 0.0025433945967277562\n",
      "Iteration 13809: loss = 0.002543383013755665\n",
      "Iteration 13810: loss = 0.0025433714309112732\n",
      "Iteration 13811: loss = 0.00254335984819458\n",
      "Iteration 13812: loss = 0.0025433482656055818\n",
      "Iteration 13813: loss = 0.0025433366831442775\n",
      "Iteration 13814: loss = 0.002543325100810664\n",
      "Iteration 13815: loss = 0.0025433135186047396\n",
      "Iteration 13816: loss = 0.0025433019365265017\n",
      "Iteration 13817: loss = 0.0025432903545759486\n",
      "Iteration 13818: loss = 0.0025432787727530776\n",
      "Iteration 13819: loss = 0.0025432671910578866\n",
      "Iteration 13820: loss = 0.002543255609490374\n",
      "Iteration 13821: loss = 0.0025432440280505363\n",
      "Iteration 13822: loss = 0.0025432324467383727\n",
      "Iteration 13823: loss = 0.0025432208655538804\n",
      "Iteration 13824: loss = 0.002543209284497057\n",
      "Iteration 13825: loss = 0.0025431977035679\n",
      "Iteration 13826: loss = 0.0025431861227664084\n",
      "Iteration 13827: loss = 0.0025431745420925788\n",
      "Iteration 13828: loss = 0.0025431629615464096\n",
      "Iteration 13829: loss = 0.002543151381127898\n",
      "Iteration 13830: loss = 0.0025431398008370423\n",
      "Iteration 13831: loss = 0.0025431282206738403\n",
      "Iteration 13832: loss = 0.0025431166406382896\n",
      "Iteration 13833: loss = 0.002543105060730388\n",
      "Iteration 13834: loss = 0.0025430934809501336\n",
      "Iteration 13835: loss = 0.0025430819012975244\n",
      "Iteration 13836: loss = 0.0025430703217725575\n",
      "Iteration 13837: loss = 0.002543058742375231\n",
      "Iteration 13838: loss = 0.002543047163105542\n",
      "Iteration 13839: loss = 0.0025430355839634895\n",
      "Iteration 13840: loss = 0.0025430240049490716\n",
      "Iteration 13841: loss = 0.0025430124260622843\n",
      "Iteration 13842: loss = 0.002543000847303127\n",
      "Iteration 13843: loss = 0.0025429892686715965\n",
      "Iteration 13844: loss = 0.002542977690167691\n",
      "Iteration 13845: loss = 0.002542966111791408\n",
      "Iteration 13846: loss = 0.002542954533542746\n",
      "Iteration 13847: loss = 0.002542942955421703\n",
      "Iteration 13848: loss = 0.0025429313774282757\n",
      "Iteration 13849: loss = 0.002542919799562462\n",
      "Iteration 13850: loss = 0.0025429082218242608\n",
      "Iteration 13851: loss = 0.002542896644213669\n",
      "Iteration 13852: loss = 0.002542885066730685\n",
      "Iteration 13853: loss = 0.0025428734893753057\n",
      "Iteration 13854: loss = 0.00254286191214753\n",
      "Iteration 13855: loss = 0.0025428503350473546\n",
      "Iteration 13856: loss = 0.0025428387580747783\n",
      "Iteration 13857: loss = 0.0025428271812297986\n",
      "Iteration 13858: loss = 0.0025428156045124136\n",
      "Iteration 13859: loss = 0.00254280402792262\n",
      "Iteration 13860: loss = 0.0025427924514604167\n",
      "Iteration 13861: loss = 0.002542780875125801\n",
      "Iteration 13862: loss = 0.002542769298918772\n",
      "Iteration 13863: loss = 0.002542757722839325\n",
      "Iteration 13864: loss = 0.0025427461468874595\n",
      "Iteration 13865: loss = 0.002542734571063174\n",
      "Iteration 13866: loss = 0.0025427229953664645\n",
      "Iteration 13867: loss = 0.00254271141979733\n",
      "Iteration 13868: loss = 0.002542699844355768\n",
      "Iteration 13869: loss = 0.0025426882690417763\n",
      "Iteration 13870: loss = 0.002542676693855353\n",
      "Iteration 13871: loss = 0.0025426651187964956\n",
      "Iteration 13872: loss = 0.0025426535438652018\n",
      "Iteration 13873: loss = 0.0025426419690614704\n",
      "Iteration 13874: loss = 0.0025426303943852976\n",
      "Iteration 13875: loss = 0.002542618819836682\n",
      "Iteration 13876: loss = 0.002542607245415622\n",
      "Iteration 13877: loss = 0.0025425956711221144\n",
      "Iteration 13878: loss = 0.002542584096956158\n",
      "Iteration 13879: loss = 0.0025425725229177502\n",
      "Iteration 13880: loss = 0.002542560949006889\n",
      "Iteration 13881: loss = 0.002542549375223572\n",
      "Iteration 13882: loss = 0.0025425378015677966\n",
      "Iteration 13883: loss = 0.0025425262280395614\n",
      "Iteration 13884: loss = 0.0025425146546388644\n",
      "Iteration 13885: loss = 0.0025425030813657026\n",
      "Iteration 13886: loss = 0.0025424915082200737\n",
      "Iteration 13887: loss = 0.002542479935201977\n",
      "Iteration 13888: loss = 0.0025424683623114087\n",
      "Iteration 13889: loss = 0.002542456789548368\n",
      "Iteration 13890: loss = 0.0025424452169128518\n",
      "Iteration 13891: loss = 0.0025424336444048574\n",
      "Iteration 13892: loss = 0.002542422072024384\n",
      "Iteration 13893: loss = 0.0025424104997714293\n",
      "Iteration 13894: loss = 0.00254239892764599\n",
      "Iteration 13895: loss = 0.002542387355648065\n",
      "Iteration 13896: loss = 0.002542375783777652\n",
      "Iteration 13897: loss = 0.002542364212034748\n",
      "Iteration 13898: loss = 0.0025423526404193524\n",
      "Iteration 13899: loss = 0.002542341068931461\n",
      "Iteration 13900: loss = 0.0025423294975710736\n",
      "Iteration 13901: loss = 0.0025423179263381864\n",
      "Iteration 13902: loss = 0.0025423063552327987\n",
      "Iteration 13903: loss = 0.0025422947842549076\n",
      "Iteration 13904: loss = 0.0025422832134045113\n",
      "Iteration 13905: loss = 0.0025422716426816067\n",
      "Iteration 13906: loss = 0.0025422600720861927\n",
      "Iteration 13907: loss = 0.002542248501618266\n",
      "Iteration 13908: loss = 0.002542236931277826\n",
      "Iteration 13909: loss = 0.00254222536106487\n",
      "Iteration 13910: loss = 0.002542213790979395\n",
      "Iteration 13911: loss = 0.0025422022210213994\n",
      "Iteration 13912: loss = 0.0025421906511908813\n",
      "Iteration 13913: loss = 0.002542179081487839\n",
      "Iteration 13914: loss = 0.0025421675119122688\n",
      "Iteration 13915: loss = 0.0025421559424641696\n",
      "Iteration 13916: loss = 0.0025421443731435387\n",
      "Iteration 13917: loss = 0.0025421328039503754\n",
      "Iteration 13918: loss = 0.002542121234884676\n",
      "Iteration 13919: loss = 0.0025421096659464393\n",
      "Iteration 13920: loss = 0.002542098097135662\n",
      "Iteration 13921: loss = 0.002542086528452343\n",
      "Iteration 13922: loss = 0.0025420749598964796\n",
      "Iteration 13923: loss = 0.00254206339146807\n",
      "Iteration 13924: loss = 0.002542051823167112\n",
      "Iteration 13925: loss = 0.0025420402549936032\n",
      "Iteration 13926: loss = 0.0025420286869475427\n",
      "Iteration 13927: loss = 0.002542017119028926\n",
      "Iteration 13928: loss = 0.002542005551237753\n",
      "Iteration 13929: loss = 0.0025419939835740206\n",
      "Iteration 13930: loss = 0.002541982416037727\n",
      "Iteration 13931: loss = 0.00254197084862887\n",
      "Iteration 13932: loss = 0.0025419592813474474\n",
      "Iteration 13933: loss = 0.0025419477141934567\n",
      "Iteration 13934: loss = 0.0025419361471668966\n",
      "Iteration 13935: loss = 0.0025419245802677645\n",
      "Iteration 13936: loss = 0.0025419130134960582\n",
      "Iteration 13937: loss = 0.0025419014468517753\n",
      "Iteration 13938: loss = 0.002541889880334915\n",
      "Iteration 13939: loss = 0.002541878313945473\n",
      "Iteration 13940: loss = 0.002541866747683449\n",
      "Iteration 13941: loss = 0.0025418551815488406\n",
      "Iteration 13942: loss = 0.002541843615541645\n",
      "Iteration 13943: loss = 0.0025418320496618596\n",
      "Iteration 13944: loss = 0.0025418204839094834\n",
      "Iteration 13945: loss = 0.0025418089182845144\n",
      "Iteration 13946: loss = 0.0025417973527869495\n",
      "Iteration 13947: loss = 0.0025417857874167876\n",
      "Iteration 13948: loss = 0.002541774222174026\n",
      "Iteration 13949: loss = 0.0025417626570586613\n",
      "Iteration 13950: loss = 0.0025417510920706936\n",
      "Iteration 13951: loss = 0.00254173952721012\n",
      "Iteration 13952: loss = 0.002541727962476938\n",
      "Iteration 13953: loss = 0.0025417163978711456\n",
      "Iteration 13954: loss = 0.0025417048333927408\n",
      "Iteration 13955: loss = 0.0025416932690417214\n",
      "Iteration 13956: loss = 0.0025416817048180855\n",
      "Iteration 13957: loss = 0.0025416701407218307\n",
      "Iteration 13958: loss = 0.002541658576752955\n",
      "Iteration 13959: loss = 0.0025416470129114564\n",
      "Iteration 13960: loss = 0.002541635449197332\n",
      "Iteration 13961: loss = 0.0025416238856105808\n",
      "Iteration 13962: loss = 0.0025416123221512\n",
      "Iteration 13963: loss = 0.0025416007588191882\n",
      "Iteration 13964: loss = 0.002541589195614542\n",
      "Iteration 13965: loss = 0.00254157763253726\n",
      "Iteration 13966: loss = 0.002541566069587341\n",
      "Iteration 13967: loss = 0.0025415545067647817\n",
      "Iteration 13968: loss = 0.0025415429440695803\n",
      "Iteration 13969: loss = 0.0025415313815017344\n",
      "Iteration 13970: loss = 0.002541519819061242\n",
      "Iteration 13971: loss = 0.002541508256748101\n",
      "Iteration 13972: loss = 0.0025414966945623103\n",
      "Iteration 13973: loss = 0.002541485132503867\n",
      "Iteration 13974: loss = 0.0025414735705727677\n",
      "Iteration 13975: loss = 0.0025414620087690124\n",
      "Iteration 13976: loss = 0.002541450447092598\n",
      "Iteration 13977: loss = 0.0025414388855435222\n",
      "Iteration 13978: loss = 0.0025414273241217834\n",
      "Iteration 13979: loss = 0.002541415762827379\n",
      "Iteration 13980: loss = 0.0025414042016603075\n",
      "Iteration 13981: loss = 0.002541392640620566\n",
      "Iteration 13982: loss = 0.0025413810797081534\n",
      "Iteration 13983: loss = 0.0025413695189230668\n",
      "Iteration 13984: loss = 0.0025413579582653045\n",
      "Iteration 13985: loss = 0.0025413463977348636\n",
      "Iteration 13986: loss = 0.002541334837331743\n",
      "Iteration 13987: loss = 0.0025413232770559405\n",
      "Iteration 13988: loss = 0.0025413117169074528\n",
      "Iteration 13989: loss = 0.00254130015688628\n",
      "Iteration 13990: loss = 0.002541288596992418\n",
      "Iteration 13991: loss = 0.0025412770372258654\n",
      "Iteration 13992: loss = 0.00254126547758662\n",
      "Iteration 13993: loss = 0.0025412539180746803\n",
      "Iteration 13994: loss = 0.0025412423586900434\n",
      "Iteration 13995: loss = 0.0025412307994327078\n",
      "Iteration 13996: loss = 0.0025412192403026705\n",
      "Iteration 13997: loss = 0.0025412076812999303\n",
      "Iteration 13998: loss = 0.002541196122424485\n",
      "Iteration 13999: loss = 0.002541184563676332\n",
      "Iteration 14000: loss = 0.00254117300505547\n",
      "Iteration 14001: loss = 0.002541161446561896\n",
      "Iteration 14002: loss = 0.0025411498881956086\n",
      "Iteration 14003: loss = 0.0025411383299566047\n",
      "Iteration 14004: loss = 0.0025411267718448843\n",
      "Iteration 14005: loss = 0.0025411152138604424\n",
      "Iteration 14006: loss = 0.0025411036560032798\n",
      "Iteration 14007: loss = 0.0025410920982733925\n",
      "Iteration 14008: loss = 0.002541080540670779\n",
      "Iteration 14009: loss = 0.002541068983195437\n",
      "Iteration 14010: loss = 0.0025410574258473645\n",
      "Iteration 14011: loss = 0.00254104586862656\n",
      "Iteration 14012: loss = 0.0025410343115330205\n",
      "Iteration 14013: loss = 0.002541022754566744\n",
      "Iteration 14014: loss = 0.0025410111977277297\n",
      "Iteration 14015: loss = 0.002540999641015974\n",
      "Iteration 14016: loss = 0.0025409880844314754\n",
      "Iteration 14017: loss = 0.002540976527974232\n",
      "Iteration 14018: loss = 0.0025409649716442414\n",
      "Iteration 14019: loss = 0.0025409534154415014\n",
      "Iteration 14020: loss = 0.0025409418593660103\n",
      "Iteration 14021: loss = 0.0025409303034177656\n",
      "Iteration 14022: loss = 0.0025409187475967654\n",
      "Iteration 14023: loss = 0.0025409071919030085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14024: loss = 0.002540895636336491\n",
      "Iteration 14025: loss = 0.0025408840808972127\n",
      "Iteration 14026: loss = 0.0025408725255851704\n",
      "Iteration 14027: loss = 0.0025408609704003614\n",
      "Iteration 14028: loss = 0.002540849415342785\n",
      "Iteration 14029: loss = 0.002540837860412439\n",
      "Iteration 14030: loss = 0.0025408263056093203\n",
      "Iteration 14031: loss = 0.0025408147509334276\n",
      "Iteration 14032: loss = 0.002540803196384759\n",
      "Iteration 14033: loss = 0.0025407916419633116\n",
      "Iteration 14034: loss = 0.0025407800876690845\n",
      "Iteration 14035: loss = 0.0025407685335020742\n",
      "Iteration 14036: loss = 0.00254075697946228\n",
      "Iteration 14037: loss = 0.0025407454255496984\n",
      "Iteration 14038: loss = 0.0025407338717643286\n",
      "Iteration 14039: loss = 0.0025407223181061677\n",
      "Iteration 14040: loss = 0.0025407107645752146\n",
      "Iteration 14041: loss = 0.0025406992111714657\n",
      "Iteration 14042: loss = 0.0025406876578949206\n",
      "Iteration 14043: loss = 0.0025406761047455763\n",
      "Iteration 14044: loss = 0.002540664551723431\n",
      "Iteration 14045: loss = 0.0025406529988284817\n",
      "Iteration 14046: loss = 0.002540641446060728\n",
      "Iteration 14047: loss = 0.002540629893420167\n",
      "Iteration 14048: loss = 0.0025406183409067955\n",
      "Iteration 14049: loss = 0.0025406067885206133\n",
      "Iteration 14050: loss = 0.0025405952362616176\n",
      "Iteration 14051: loss = 0.002540583684129806\n",
      "Iteration 14052: loss = 0.0025405721321251776\n",
      "Iteration 14053: loss = 0.002540560580247728\n",
      "Iteration 14054: loss = 0.0025405490284974575\n",
      "Iteration 14055: loss = 0.002540537476874363\n",
      "Iteration 14056: loss = 0.002540525925378443\n",
      "Iteration 14057: loss = 0.002540514374009695\n",
      "Iteration 14058: loss = 0.002540502822768116\n",
      "Iteration 14059: loss = 0.002540491271653706\n",
      "Iteration 14060: loss = 0.002540479720666461\n",
      "Iteration 14061: loss = 0.0025404681698063805\n",
      "Iteration 14062: loss = 0.002540456619073461\n",
      "Iteration 14063: loss = 0.0025404450684677012\n",
      "Iteration 14064: loss = 0.002540433517989099\n",
      "Iteration 14065: loss = 0.0025404219676376525\n",
      "Iteration 14066: loss = 0.0025404104174133594\n",
      "Iteration 14067: loss = 0.0025403988673162177\n",
      "Iteration 14068: loss = 0.0025403873173462255\n",
      "Iteration 14069: loss = 0.002540375767503381\n",
      "Iteration 14070: loss = 0.0025403642177876805\n",
      "Iteration 14071: loss = 0.0025403526681991246\n",
      "Iteration 14072: loss = 0.0025403411187377087\n",
      "Iteration 14073: loss = 0.0025403295694034324\n",
      "Iteration 14074: loss = 0.002540318020196293\n",
      "Iteration 14075: loss = 0.0025403064711162883\n",
      "Iteration 14076: loss = 0.0025402949221634166\n",
      "Iteration 14077: loss = 0.0025402833733376763\n",
      "Iteration 14078: loss = 0.0025402718246390647\n",
      "Iteration 14079: loss = 0.0025402602760675797\n",
      "Iteration 14080: loss = 0.0025402487276232196\n",
      "Iteration 14081: loss = 0.002540237179305982\n",
      "Iteration 14082: loss = 0.0025402256311158646\n",
      "Iteration 14083: loss = 0.002540214083052866\n",
      "Iteration 14084: loss = 0.0025402025351169842\n",
      "Iteration 14085: loss = 0.0025401909873082164\n",
      "Iteration 14086: loss = 0.0025401794396265618\n",
      "Iteration 14087: loss = 0.002540167892072017\n",
      "Iteration 14088: loss = 0.0025401563446445802\n",
      "Iteration 14089: loss = 0.0025401447973442503\n",
      "Iteration 14090: loss = 0.002540133250171024\n",
      "Iteration 14091: loss = 0.0025401217031249005\n",
      "Iteration 14092: loss = 0.002540110156205877\n",
      "Iteration 14093: loss = 0.0025400986094139518\n",
      "Iteration 14094: loss = 0.002540087062749122\n",
      "Iteration 14095: loss = 0.002540075516211387\n",
      "Iteration 14096: loss = 0.0025400639698007436\n",
      "Iteration 14097: loss = 0.00254005242351719\n",
      "Iteration 14098: loss = 0.002540040877360724\n",
      "Iteration 14099: loss = 0.002540029331331345\n",
      "Iteration 14100: loss = 0.0025400177854290494\n",
      "Iteration 14101: loss = 0.0025400062396538346\n",
      "Iteration 14102: loss = 0.0025399946940057004\n",
      "Iteration 14103: loss = 0.002539983148484644\n",
      "Iteration 14104: loss = 0.0025399716030906627\n",
      "Iteration 14105: loss = 0.0025399600578237557\n",
      "Iteration 14106: loss = 0.00253994851268392\n",
      "Iteration 14107: loss = 0.0025399369676711533\n",
      "Iteration 14108: loss = 0.0025399254227854553\n",
      "Iteration 14109: loss = 0.0025399138780268214\n",
      "Iteration 14110: loss = 0.0025399023333952516\n",
      "Iteration 14111: loss = 0.0025398907888907435\n",
      "Iteration 14112: loss = 0.002539879244513294\n",
      "Iteration 14113: loss = 0.002539867700262903\n",
      "Iteration 14114: loss = 0.0025398561561395666\n",
      "Iteration 14115: loss = 0.002539844612143284\n",
      "Iteration 14116: loss = 0.002539833068274052\n",
      "Iteration 14117: loss = 0.0025398215245318696\n",
      "Iteration 14118: loss = 0.002539809980916734\n",
      "Iteration 14119: loss = 0.002539798437428644\n",
      "Iteration 14120: loss = 0.002539786894067597\n",
      "Iteration 14121: loss = 0.0025397753508335907\n",
      "Iteration 14122: loss = 0.002539763807726624\n",
      "Iteration 14123: loss = 0.0025397522647466943\n",
      "Iteration 14124: loss = 0.0025397407218937995\n",
      "Iteration 14125: loss = 0.0025397291791679384\n",
      "Iteration 14126: loss = 0.0025397176365691073\n",
      "Iteration 14127: loss = 0.0025397060940973055\n",
      "Iteration 14128: loss = 0.0025396945517525303\n",
      "Iteration 14129: loss = 0.0025396830095347806\n",
      "Iteration 14130: loss = 0.0025396714674440534\n",
      "Iteration 14131: loss = 0.0025396599254803474\n",
      "Iteration 14132: loss = 0.00253964838364366\n",
      "Iteration 14133: loss = 0.002539636841933989\n",
      "Iteration 14134: loss = 0.002539625300351334\n",
      "Iteration 14135: loss = 0.0025396137588956907\n",
      "Iteration 14136: loss = 0.0025396022175670583\n",
      "Iteration 14137: loss = 0.002539590676365435\n",
      "Iteration 14138: loss = 0.0025395791352908178\n",
      "Iteration 14139: loss = 0.0025395675943432057\n",
      "Iteration 14140: loss = 0.0025395560535225966\n",
      "Iteration 14141: loss = 0.002539544512828988\n",
      "Iteration 14142: loss = 0.0025395329722623775\n",
      "Iteration 14143: loss = 0.0025395214318227636\n",
      "Iteration 14144: loss = 0.002539509891510145\n",
      "Iteration 14145: loss = 0.0025394983513245184\n",
      "Iteration 14146: loss = 0.002539486811265883\n",
      "Iteration 14147: loss = 0.002539475271334236\n",
      "Iteration 14148: loss = 0.0025394637315295752\n",
      "Iteration 14149: loss = 0.002539452191851899\n",
      "Iteration 14150: loss = 0.0025394406523012057\n",
      "Iteration 14151: loss = 0.0025394291128774922\n",
      "Iteration 14152: loss = 0.0025394175735807575\n",
      "Iteration 14153: loss = 0.002539406034411\n",
      "Iteration 14154: loss = 0.002539394495368216\n",
      "Iteration 14155: loss = 0.002539382956452405\n",
      "Iteration 14156: loss = 0.0025393714176635647\n",
      "Iteration 14157: loss = 0.0025393598790016923\n",
      "Iteration 14158: loss = 0.002539348340466786\n",
      "Iteration 14159: loss = 0.0025393368020588444\n",
      "Iteration 14160: loss = 0.002539325263777866\n",
      "Iteration 14161: loss = 0.002539313725623847\n",
      "Iteration 14162: loss = 0.002539302187596787\n",
      "Iteration 14163: loss = 0.0025392906496966836\n",
      "Iteration 14164: loss = 0.002539279111923534\n",
      "Iteration 14165: loss = 0.002539267574277337\n",
      "Iteration 14166: loss = 0.0025392560367580906\n",
      "Iteration 14167: loss = 0.0025392444993657926\n",
      "Iteration 14168: loss = 0.0025392329621004403\n",
      "Iteration 14169: loss = 0.002539221424962033\n",
      "Iteration 14170: loss = 0.002539209887950568\n",
      "Iteration 14171: loss = 0.0025391983510660433\n",
      "Iteration 14172: loss = 0.0025391868143084567\n",
      "Iteration 14173: loss = 0.002539175277677807\n",
      "Iteration 14174: loss = 0.0025391637411740914\n",
      "Iteration 14175: loss = 0.0025391522047973074\n",
      "Iteration 14176: loss = 0.002539140668547455\n",
      "Iteration 14177: loss = 0.0025391291324245303\n",
      "Iteration 14178: loss = 0.002539117596428532\n",
      "Iteration 14179: loss = 0.0025391060605594587\n",
      "Iteration 14180: loss = 0.0025390945248173064\n",
      "Iteration 14181: loss = 0.0025390829892020755\n",
      "Iteration 14182: loss = 0.0025390714537137636\n",
      "Iteration 14183: loss = 0.0025390599183523666\n",
      "Iteration 14184: loss = 0.0025390483831178846\n",
      "Iteration 14185: loss = 0.0025390368480103153\n",
      "Iteration 14186: loss = 0.002539025313029656\n",
      "Iteration 14187: loss = 0.0025390137781759053\n",
      "Iteration 14188: loss = 0.0025390022434490607\n",
      "Iteration 14189: loss = 0.0025389907088491205\n",
      "Iteration 14190: loss = 0.0025389791743760833\n",
      "Iteration 14191: loss = 0.0025389676400299458\n",
      "Iteration 14192: loss = 0.002538956105810707\n",
      "Iteration 14193: loss = 0.0025389445717183654\n",
      "Iteration 14194: loss = 0.002538933037752917\n",
      "Iteration 14195: loss = 0.0025389215039143616\n",
      "Iteration 14196: loss = 0.002538909970202697\n",
      "Iteration 14197: loss = 0.0025388984366179206\n",
      "Iteration 14198: loss = 0.0025388869031600305\n",
      "Iteration 14199: loss = 0.002538875369829025\n",
      "Iteration 14200: loss = 0.0025388638366249027\n",
      "Iteration 14201: loss = 0.00253885230354766\n",
      "Iteration 14202: loss = 0.0025388407705972966\n",
      "Iteration 14203: loss = 0.0025388292377738087\n",
      "Iteration 14204: loss = 0.002538817705077196\n",
      "Iteration 14205: loss = 0.0025388061725074556\n",
      "Iteration 14206: loss = 0.0025387946400645864\n",
      "Iteration 14207: loss = 0.002538783107748586\n",
      "Iteration 14208: loss = 0.0025387715755594517\n",
      "Iteration 14209: loss = 0.0025387600434971817\n",
      "Iteration 14210: loss = 0.002538748511561775\n",
      "Iteration 14211: loss = 0.0025387369797532284\n",
      "Iteration 14212: loss = 0.0025387254480715407\n",
      "Iteration 14213: loss = 0.0025387139165167103\n",
      "Iteration 14214: loss = 0.0025387023850887342\n",
      "Iteration 14215: loss = 0.0025386908537876106\n",
      "Iteration 14216: loss = 0.0025386793226133382\n",
      "Iteration 14217: loss = 0.002538667791565914\n",
      "Iteration 14218: loss = 0.002538656260645338\n",
      "Iteration 14219: loss = 0.0025386447298516054\n",
      "Iteration 14220: loss = 0.002538633199184716\n",
      "Iteration 14221: loss = 0.002538621668644668\n",
      "Iteration 14222: loss = 0.002538610138231458\n",
      "Iteration 14223: loss = 0.0025385986079450857\n",
      "Iteration 14224: loss = 0.002538587077785548\n",
      "Iteration 14225: loss = 0.002538575547752844\n",
      "Iteration 14226: loss = 0.00253856401784697\n",
      "Iteration 14227: loss = 0.002538552488067925\n",
      "Iteration 14228: loss = 0.0025385409584157076\n",
      "Iteration 14229: loss = 0.0025385294288903155\n",
      "Iteration 14230: loss = 0.002538517899491746\n",
      "Iteration 14231: loss = 0.002538506370219998\n",
      "Iteration 14232: loss = 0.0025384948410750686\n",
      "Iteration 14233: loss = 0.002538483312056957\n",
      "Iteration 14234: loss = 0.0025384717831656604\n",
      "Iteration 14235: loss = 0.0025384602544011774\n",
      "Iteration 14236: loss = 0.0025384487257635044\n",
      "Iteration 14237: loss = 0.0025384371972526428\n",
      "Iteration 14238: loss = 0.002538425668868587\n",
      "Iteration 14239: loss = 0.002538414140611337\n",
      "Iteration 14240: loss = 0.00253840261248089\n",
      "Iteration 14241: loss = 0.0025383910844772447\n",
      "Iteration 14242: loss = 0.002538379556600399\n",
      "Iteration 14243: loss = 0.002538368028850351\n",
      "Iteration 14244: loss = 0.0025383565012270983\n",
      "Iteration 14245: loss = 0.0025383449737306383\n",
      "Iteration 14246: loss = 0.0025383334463609714\n",
      "Iteration 14247: loss = 0.0025383219191180933\n",
      "Iteration 14248: loss = 0.002538310392002003\n",
      "Iteration 14249: loss = 0.0025382988650126987\n",
      "Iteration 14250: loss = 0.0025382873381501777\n",
      "Iteration 14251: loss = 0.0025382758114144387\n",
      "Iteration 14252: loss = 0.0025382642848054797\n",
      "Iteration 14253: loss = 0.002538252758323298\n",
      "Iteration 14254: loss = 0.0025382412319678925\n",
      "Iteration 14255: loss = 0.0025382297057392617\n",
      "Iteration 14256: loss = 0.002538218179637402\n",
      "Iteration 14257: loss = 0.002538206653662312\n",
      "Iteration 14258: loss = 0.002538195127813991\n",
      "Iteration 14259: loss = 0.002538183602092435\n",
      "Iteration 14260: loss = 0.002538172076497644\n",
      "Iteration 14261: loss = 0.002538160551029615\n",
      "Iteration 14262: loss = 0.0025381490256883464\n",
      "Iteration 14263: loss = 0.0025381375004738353\n",
      "Iteration 14264: loss = 0.002538125975386081\n",
      "Iteration 14265: loss = 0.0025381144504250815\n",
      "Iteration 14266: loss = 0.002538102925590834\n",
      "Iteration 14267: loss = 0.0025380914008833366\n",
      "Iteration 14268: loss = 0.0025380798763025877\n",
      "Iteration 14269: loss = 0.002538068351848586\n",
      "Iteration 14270: loss = 0.0025380568275213283\n",
      "Iteration 14271: loss = 0.0025380453033208134\n",
      "Iteration 14272: loss = 0.0025380337792470387\n",
      "Iteration 14273: loss = 0.0025380222553000034\n",
      "Iteration 14274: loss = 0.0025380107314797043\n",
      "Iteration 14275: loss = 0.0025379992077861403\n",
      "Iteration 14276: loss = 0.002537987684219309\n",
      "Iteration 14277: loss = 0.002537976160779209\n",
      "Iteration 14278: loss = 0.0025379646374658377\n",
      "Iteration 14279: loss = 0.0025379531142791937\n",
      "Iteration 14280: loss = 0.002537941591219274\n",
      "Iteration 14281: loss = 0.0025379300682860776\n",
      "Iteration 14282: loss = 0.002537918545479603\n",
      "Iteration 14283: loss = 0.0025379070227998474\n",
      "Iteration 14284: loss = 0.0025378955002468085\n",
      "Iteration 14285: loss = 0.0025378839778204855\n",
      "Iteration 14286: loss = 0.0025378724555208754\n",
      "Iteration 14287: loss = 0.0025378609333479765\n",
      "Iteration 14288: loss = 0.002537849411301788\n",
      "Iteration 14289: loss = 0.002537837889382306\n",
      "Iteration 14290: loss = 0.0025378263675895304\n",
      "Iteration 14291: loss = 0.0025378148459234582\n",
      "Iteration 14292: loss = 0.0025378033243840872\n",
      "Iteration 14293: loss = 0.002537791802971417\n",
      "Iteration 14294: loss = 0.0025377802816854435\n",
      "Iteration 14295: loss = 0.002537768760526166\n",
      "Iteration 14296: loss = 0.002537757239493583\n",
      "Iteration 14297: loss = 0.002537745718587691\n",
      "Iteration 14298: loss = 0.00253773419780849\n",
      "Iteration 14299: loss = 0.002537722677155977\n",
      "Iteration 14300: loss = 0.00253771115663015\n",
      "Iteration 14301: loss = 0.0025376996362310076\n",
      "Iteration 14302: loss = 0.0025376881159585468\n",
      "Iteration 14303: loss = 0.0025376765958127667\n",
      "Iteration 14304: loss = 0.002537665075793665\n",
      "Iteration 14305: loss = 0.002537653555901239\n",
      "Iteration 14306: loss = 0.0025376420361354884\n",
      "Iteration 14307: loss = 0.00253763051649641\n",
      "Iteration 14308: loss = 0.0025376189969840025\n",
      "Iteration 14309: loss = 0.0025376074775982638\n",
      "Iteration 14310: loss = 0.0025375959583391916\n",
      "Iteration 14311: loss = 0.0025375844392067836\n",
      "Iteration 14312: loss = 0.002537572920201039\n",
      "Iteration 14313: loss = 0.0025375614013219564\n",
      "Iteration 14314: loss = 0.0025375498825695314\n",
      "Iteration 14315: loss = 0.0025375383639437642\n",
      "Iteration 14316: loss = 0.0025375268454446522\n",
      "Iteration 14317: loss = 0.002537515327072193\n",
      "Iteration 14318: loss = 0.0025375038088263856\n",
      "Iteration 14319: loss = 0.002537492290707227\n",
      "Iteration 14320: loss = 0.002537480772714716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14321: loss = 0.0025374692548488505\n",
      "Iteration 14322: loss = 0.002537457737109629\n",
      "Iteration 14323: loss = 0.0025374462194970486\n",
      "Iteration 14324: loss = 0.002537434702011108\n",
      "Iteration 14325: loss = 0.002537423184651805\n",
      "Iteration 14326: loss = 0.002537411667419138\n",
      "Iteration 14327: loss = 0.0025374001503131053\n",
      "Iteration 14328: loss = 0.0025373886333337043\n",
      "Iteration 14329: loss = 0.0025373771164809334\n",
      "Iteration 14330: loss = 0.0025373655997547907\n",
      "Iteration 14331: loss = 0.0025373540831552742\n",
      "Iteration 14332: loss = 0.002537342566682382\n",
      "Iteration 14333: loss = 0.002537331050336112\n",
      "Iteration 14334: loss = 0.002537319534116462\n",
      "Iteration 14335: loss = 0.0025373080180234315\n",
      "Iteration 14336: loss = 0.0025372965020570176\n",
      "Iteration 14337: loss = 0.002537284986217218\n",
      "Iteration 14338: loss = 0.0025372734705040303\n",
      "Iteration 14339: loss = 0.002537261954917454\n",
      "Iteration 14340: loss = 0.002537250439457487\n",
      "Iteration 14341: loss = 0.0025372389241241264\n",
      "Iteration 14342: loss = 0.0025372274089173713\n",
      "Iteration 14343: loss = 0.002537215893837219\n",
      "Iteration 14344: loss = 0.0025372043788836683\n",
      "Iteration 14345: loss = 0.0025371928640567165\n",
      "Iteration 14346: loss = 0.002537181349356362\n",
      "Iteration 14347: loss = 0.0025371698347826034\n",
      "Iteration 14348: loss = 0.0025371583203354377\n",
      "Iteration 14349: loss = 0.002537146806014864\n",
      "Iteration 14350: loss = 0.00253713529182088\n",
      "Iteration 14351: loss = 0.0025371237777534836\n",
      "Iteration 14352: loss = 0.0025371122638126738\n",
      "Iteration 14353: loss = 0.0025371007499984466\n",
      "Iteration 14354: loss = 0.0025370892363108023\n",
      "Iteration 14355: loss = 0.0025370777227497377\n",
      "Iteration 14356: loss = 0.0025370662093152515\n",
      "Iteration 14357: loss = 0.002537054696007341\n",
      "Iteration 14358: loss = 0.0025370431828260057\n",
      "Iteration 14359: loss = 0.0025370316697712423\n",
      "Iteration 14360: loss = 0.00253702015684305\n",
      "Iteration 14361: loss = 0.0025370086440414255\n",
      "Iteration 14362: loss = 0.0025369971313663683\n",
      "Iteration 14363: loss = 0.0025369856188178753\n",
      "Iteration 14364: loss = 0.0025369741063959454\n",
      "Iteration 14365: loss = 0.0025369625941005767\n",
      "Iteration 14366: loss = 0.002536951081931767\n",
      "Iteration 14367: loss = 0.0025369395698895143\n",
      "Iteration 14368: loss = 0.002536928057973817\n",
      "Iteration 14369: loss = 0.0025369165461846727\n",
      "Iteration 14370: loss = 0.00253690503452208\n",
      "Iteration 14371: loss = 0.002536893522986037\n",
      "Iteration 14372: loss = 0.0025368820115765412\n",
      "Iteration 14373: loss = 0.0025368705002935914\n",
      "Iteration 14374: loss = 0.002536858989137185\n",
      "Iteration 14375: loss = 0.0025368474781073203\n",
      "Iteration 14376: loss = 0.002536835967203996\n",
      "Iteration 14377: loss = 0.002536824456427209\n",
      "Iteration 14378: loss = 0.002536812945776959\n",
      "Iteration 14379: loss = 0.002536801435253243\n",
      "Iteration 14380: loss = 0.002536789924856059\n",
      "Iteration 14381: loss = 0.0025367784145854063\n",
      "Iteration 14382: loss = 0.002536766904441281\n",
      "Iteration 14383: loss = 0.002536755394423683\n",
      "Iteration 14384: loss = 0.0025367438845326096\n",
      "Iteration 14385: loss = 0.0025367323747680593\n",
      "Iteration 14386: loss = 0.002536720865130029\n",
      "Iteration 14387: loss = 0.002536709355618518\n",
      "Iteration 14388: loss = 0.0025366978462335247\n",
      "Iteration 14389: loss = 0.0025366863369750458\n",
      "Iteration 14390: loss = 0.002536674827843081\n",
      "Iteration 14391: loss = 0.0025366633188376266\n",
      "Iteration 14392: loss = 0.0025366518099586826\n",
      "Iteration 14393: loss = 0.002536640301206245\n",
      "Iteration 14394: loss = 0.0025366287925803143\n",
      "Iteration 14395: loss = 0.002536617284080887\n",
      "Iteration 14396: loss = 0.0025366057757079618\n",
      "Iteration 14397: loss = 0.002536594267461536\n",
      "Iteration 14398: loss = 0.0025365827593416086\n",
      "Iteration 14399: loss = 0.0025365712513481776\n",
      "Iteration 14400: loss = 0.0025365597434812404\n",
      "Iteration 14401: loss = 0.002536548235740796\n",
      "Iteration 14402: loss = 0.0025365367281268416\n",
      "Iteration 14403: loss = 0.0025365252206393765\n",
      "Iteration 14404: loss = 0.0025365137132783975\n",
      "Iteration 14405: loss = 0.0025365022060439037\n",
      "Iteration 14406: loss = 0.002536490698935893\n",
      "Iteration 14407: loss = 0.0025364791919543625\n",
      "Iteration 14408: loss = 0.0025364676850993116\n",
      "Iteration 14409: loss = 0.0025364561783707378\n",
      "Iteration 14410: loss = 0.0025364446717686395\n",
      "Iteration 14411: loss = 0.0025364331652930148\n",
      "Iteration 14412: loss = 0.0025364216589438614\n",
      "Iteration 14413: loss = 0.0025364101527211775\n",
      "Iteration 14414: loss = 0.0025363986466249615\n",
      "Iteration 14415: loss = 0.002536387140655211\n",
      "Iteration 14416: loss = 0.0025363756348119257\n",
      "Iteration 14417: loss = 0.0025363641290951015\n",
      "Iteration 14418: loss = 0.002536352623504738\n",
      "Iteration 14419: loss = 0.002536341118040832\n",
      "Iteration 14420: loss = 0.002536329612703383\n",
      "Iteration 14421: loss = 0.0025363181074923888\n",
      "Iteration 14422: loss = 0.002536306602407847\n",
      "Iteration 14423: loss = 0.0025362950974497553\n",
      "Iteration 14424: loss = 0.0025362835926181133\n",
      "Iteration 14425: loss = 0.0025362720879129183\n",
      "Iteration 14426: loss = 0.002536260583334168\n",
      "Iteration 14427: loss = 0.002536249078881861\n",
      "Iteration 14428: loss = 0.002536237574555995\n",
      "Iteration 14429: loss = 0.0025362260703565686\n",
      "Iteration 14430: loss = 0.0025362145662835805\n",
      "Iteration 14431: loss = 0.0025362030623370274\n",
      "Iteration 14432: loss = 0.0025361915585169078\n",
      "Iteration 14433: loss = 0.002536180054823221\n",
      "Iteration 14434: loss = 0.0025361685512559633\n",
      "Iteration 14435: loss = 0.0025361570478151345\n",
      "Iteration 14436: loss = 0.002536145544500731\n",
      "Iteration 14437: loss = 0.0025361340413127525\n",
      "Iteration 14438: loss = 0.0025361225382511962\n",
      "Iteration 14439: loss = 0.0025361110353160615\n",
      "Iteration 14440: loss = 0.002536099532507344\n",
      "Iteration 14441: loss = 0.0025360880298250445\n",
      "Iteration 14442: loss = 0.0025360765272691593\n",
      "Iteration 14443: loss = 0.002536065024839687\n",
      "Iteration 14444: loss = 0.002536053522536626\n",
      "Iteration 14445: loss = 0.002536042020359975\n",
      "Iteration 14446: loss = 0.002536030518309731\n",
      "Iteration 14447: loss = 0.0025360190163858927\n",
      "Iteration 14448: loss = 0.002536007514588458\n",
      "Iteration 14449: loss = 0.0025359960129174245\n",
      "Iteration 14450: loss = 0.0025359845113727916\n",
      "Iteration 14451: loss = 0.0025359730099545567\n",
      "Iteration 14452: loss = 0.002535961508662718\n",
      "Iteration 14453: loss = 0.0025359500074972736\n",
      "Iteration 14454: loss = 0.0025359385064582214\n",
      "Iteration 14455: loss = 0.00253592700554556\n",
      "Iteration 14456: loss = 0.002535915504759287\n",
      "Iteration 14457: loss = 0.002535904004099401\n",
      "Iteration 14458: loss = 0.0025358925035659\n",
      "Iteration 14459: loss = 0.0025358810031587816\n",
      "Iteration 14460: loss = 0.002535869502878045\n",
      "Iteration 14461: loss = 0.002535858002723687\n",
      "Iteration 14462: loss = 0.0025358465026957067\n",
      "Iteration 14463: loss = 0.002535835002794102\n",
      "Iteration 14464: loss = 0.002535823503018871\n",
      "Iteration 14465: loss = 0.002535812003370012\n",
      "Iteration 14466: loss = 0.0025358005038475222\n",
      "Iteration 14467: loss = 0.002535789004451401\n",
      "Iteration 14468: loss = 0.0025357775051816464\n",
      "Iteration 14469: loss = 0.0025357660060382555\n",
      "Iteration 14470: loss = 0.002535754507021227\n",
      "Iteration 14471: loss = 0.002535743008130559\n",
      "Iteration 14472: loss = 0.002535731509366251\n",
      "Iteration 14473: loss = 0.0025357200107282984\n",
      "Iteration 14474: loss = 0.002535708512216701\n",
      "Iteration 14475: loss = 0.002535697013831457\n",
      "Iteration 14476: loss = 0.0025356855155725644\n",
      "Iteration 14477: loss = 0.002535674017440021\n",
      "Iteration 14478: loss = 0.002535662519433825\n",
      "Iteration 14479: loss = 0.0025356510215539755\n",
      "Iteration 14480: loss = 0.0025356395238004684\n",
      "Iteration 14481: loss = 0.002535628026173304\n",
      "Iteration 14482: loss = 0.0025356165286724796\n",
      "Iteration 14483: loss = 0.002535605031297993\n",
      "Iteration 14484: loss = 0.0025355935340498435\n",
      "Iteration 14485: loss = 0.002535582036928028\n",
      "Iteration 14486: loss = 0.0025355705399325453\n",
      "Iteration 14487: loss = 0.0025355590430633924\n",
      "Iteration 14488: loss = 0.0025355475463205695\n",
      "Iteration 14489: loss = 0.002535536049704073\n",
      "Iteration 14490: loss = 0.002535524553213902\n",
      "Iteration 14491: loss = 0.0025355130568500534\n",
      "Iteration 14492: loss = 0.0025355015606125272\n",
      "Iteration 14493: loss = 0.0025354900645013206\n",
      "Iteration 14494: loss = 0.002535478568516431\n",
      "Iteration 14495: loss = 0.0025354670726578576\n",
      "Iteration 14496: loss = 0.0025354555769255987\n",
      "Iteration 14497: loss = 0.002535444081319651\n",
      "Iteration 14498: loss = 0.002535432585840014\n",
      "Iteration 14499: loss = 0.0025354210904866854\n",
      "Iteration 14500: loss = 0.0025354095952596634\n",
      "Iteration 14501: loss = 0.0025353981001589457\n",
      "Iteration 14502: loss = 0.002535386605184531\n",
      "Iteration 14503: loss = 0.0025353751103364174\n",
      "Iteration 14504: loss = 0.002535363615614603\n",
      "Iteration 14505: loss = 0.0025353521210190856\n",
      "Iteration 14506: loss = 0.0025353406265498633\n",
      "Iteration 14507: loss = 0.0025353291322069357\n",
      "Iteration 14508: loss = 0.0025353176379902987\n",
      "Iteration 14509: loss = 0.002535306143899952\n",
      "Iteration 14510: loss = 0.002535294649935893\n",
      "Iteration 14511: loss = 0.0025352831560981207\n",
      "Iteration 14512: loss = 0.0025352716623866325\n",
      "Iteration 14513: loss = 0.002535260168801426\n",
      "Iteration 14514: loss = 0.0025352486753425006\n",
      "Iteration 14515: loss = 0.002535237182009854\n",
      "Iteration 14516: loss = 0.002535225688803484\n",
      "Iteration 14517: loss = 0.002535214195723389\n",
      "Iteration 14518: loss = 0.0025352027027695672\n",
      "Iteration 14519: loss = 0.002535191209942017\n",
      "Iteration 14520: loss = 0.002535179717240736\n",
      "Iteration 14521: loss = 0.002535168224665723\n",
      "Iteration 14522: loss = 0.002535156732216975\n",
      "Iteration 14523: loss = 0.0025351452398944914\n",
      "Iteration 14524: loss = 0.00253513374769827\n",
      "Iteration 14525: loss = 0.002535122255628308\n",
      "Iteration 14526: loss = 0.0025351107636846053\n",
      "Iteration 14527: loss = 0.0025350992718671584\n",
      "Iteration 14528: loss = 0.0025350877801759665\n",
      "Iteration 14529: loss = 0.0025350762886110273\n",
      "Iteration 14530: loss = 0.0025350647971723395\n",
      "Iteration 14531: loss = 0.0025350533058599\n",
      "Iteration 14532: loss = 0.002535041814673708\n",
      "Iteration 14533: loss = 0.002535030323613762\n",
      "Iteration 14534: loss = 0.0025350188326800594\n",
      "Iteration 14535: loss = 0.002535007341872598\n",
      "Iteration 14536: loss = 0.0025349958511913773\n",
      "Iteration 14537: loss = 0.002534984360636394\n",
      "Iteration 14538: loss = 0.002534972870207647\n",
      "Iteration 14539: loss = 0.0025349613799051348\n",
      "Iteration 14540: loss = 0.0025349498897288543\n",
      "Iteration 14541: loss = 0.0025349383996788053\n",
      "Iteration 14542: loss = 0.0025349269097549847\n",
      "Iteration 14543: loss = 0.002534915419957391\n",
      "Iteration 14544: loss = 0.002534903930286023\n",
      "Iteration 14545: loss = 0.0025348924407408773\n",
      "Iteration 14546: loss = 0.0025348809513219546\n",
      "Iteration 14547: loss = 0.0025348694620292494\n",
      "Iteration 14548: loss = 0.002534857972862764\n",
      "Iteration 14549: loss = 0.0025348464838224937\n",
      "Iteration 14550: loss = 0.0025348349949084375\n",
      "Iteration 14551: loss = 0.002534823506120593\n",
      "Iteration 14552: loss = 0.0025348120174589596\n",
      "Iteration 14553: loss = 0.002534800528923535\n",
      "Iteration 14554: loss = 0.0025347890405143167\n",
      "Iteration 14555: loss = 0.0025347775522313034\n",
      "Iteration 14556: loss = 0.0025347660640744935\n",
      "Iteration 14557: loss = 0.002534754576043884\n",
      "Iteration 14558: loss = 0.0025347430881394743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14559: loss = 0.0025347316003612626\n",
      "Iteration 14560: loss = 0.0025347201127092467\n",
      "Iteration 14561: loss = 0.002534708625183424\n",
      "Iteration 14562: loss = 0.0025346971377837933\n",
      "Iteration 14563: loss = 0.002534685650510353\n",
      "Iteration 14564: loss = 0.002534674163363101\n",
      "Iteration 14565: loss = 0.0025346626763420362\n",
      "Iteration 14566: loss = 0.0025346511894471556\n",
      "Iteration 14567: loss = 0.002534639702678458\n",
      "Iteration 14568: loss = 0.0025346282160359414\n",
      "Iteration 14569: loss = 0.002534616729519604\n",
      "Iteration 14570: loss = 0.0025346052431294438\n",
      "Iteration 14571: loss = 0.002534593756865459\n",
      "Iteration 14572: loss = 0.0025345822707276483\n",
      "Iteration 14573: loss = 0.0025345707847160095\n",
      "Iteration 14574: loss = 0.00253455929883054\n",
      "Iteration 14575: loss = 0.0025345478130712395\n",
      "Iteration 14576: loss = 0.0025345363274381053\n",
      "Iteration 14577: loss = 0.0025345248419311353\n",
      "Iteration 14578: loss = 0.0025345133565503287\n",
      "Iteration 14579: loss = 0.0025345018712956825\n",
      "Iteration 14580: loss = 0.0025344903861671954\n",
      "Iteration 14581: loss = 0.0025344789011648656\n",
      "Iteration 14582: loss = 0.0025344674162886913\n",
      "Iteration 14583: loss = 0.00253445593153867\n",
      "Iteration 14584: loss = 0.0025344444469148015\n",
      "Iteration 14585: loss = 0.0025344329624170823\n",
      "Iteration 14586: loss = 0.002534421478045511\n",
      "Iteration 14587: loss = 0.0025344099938000857\n",
      "Iteration 14588: loss = 0.002534398509680806\n",
      "Iteration 14589: loss = 0.002534387025687668\n",
      "Iteration 14590: loss = 0.0025343755418206714\n",
      "Iteration 14591: loss = 0.0025343640580798134\n",
      "Iteration 14592: loss = 0.0025343525744650924\n",
      "Iteration 14593: loss = 0.002534341090976507\n",
      "Iteration 14594: loss = 0.0025343296076140547\n",
      "Iteration 14595: loss = 0.0025343181243777345\n",
      "Iteration 14596: loss = 0.0025343066412675435\n",
      "Iteration 14597: loss = 0.002534295158283481\n",
      "Iteration 14598: loss = 0.0025342836754255454\n",
      "Iteration 14599: loss = 0.002534272192693733\n",
      "Iteration 14600: loss = 0.002534260710088044\n",
      "Iteration 14601: loss = 0.002534249227608475\n",
      "Iteration 14602: loss = 0.002534237745255025\n",
      "Iteration 14603: loss = 0.0025342262630276926\n",
      "Iteration 14604: loss = 0.002534214780926475\n",
      "Iteration 14605: loss = 0.002534203298951371\n",
      "Iteration 14606: loss = 0.002534191817102379\n",
      "Iteration 14607: loss = 0.0025341803353794963\n",
      "Iteration 14608: loss = 0.002534168853782722\n",
      "Iteration 14609: loss = 0.0025341573723120536\n",
      "Iteration 14610: loss = 0.00253414589096749\n",
      "Iteration 14611: loss = 0.002534134409749028\n",
      "Iteration 14612: loss = 0.002534122928656668\n",
      "Iteration 14613: loss = 0.002534111447690406\n",
      "Iteration 14614: loss = 0.0025340999668502407\n",
      "Iteration 14615: loss = 0.002534088486136171\n",
      "Iteration 14616: loss = 0.0025340770055481947\n",
      "Iteration 14617: loss = 0.00253406552508631\n",
      "Iteration 14618: loss = 0.0025340540447505162\n",
      "Iteration 14619: loss = 0.0025340425645408097\n",
      "Iteration 14620: loss = 0.0025340310844571886\n",
      "Iteration 14621: loss = 0.0025340196044996524\n",
      "Iteration 14622: loss = 0.0025340081246681984\n",
      "Iteration 14623: loss = 0.002533996644962826\n",
      "Iteration 14624: loss = 0.002533985165383532\n",
      "Iteration 14625: loss = 0.0025339736859303153\n",
      "Iteration 14626: loss = 0.002533962206603174\n",
      "Iteration 14627: loss = 0.0025339507274021057\n",
      "Iteration 14628: loss = 0.002533939248327109\n",
      "Iteration 14629: loss = 0.0025339277693781823\n",
      "Iteration 14630: loss = 0.0025339162905553244\n",
      "Iteration 14631: loss = 0.002533904811858532\n",
      "Iteration 14632: loss = 0.0025338933332878038\n",
      "Iteration 14633: loss = 0.002533881854843139\n",
      "Iteration 14634: loss = 0.002533870376524534\n",
      "Iteration 14635: loss = 0.002533858898331989\n",
      "Iteration 14636: loss = 0.0025338474202655004\n",
      "Iteration 14637: loss = 0.0025338359423250673\n",
      "Iteration 14638: loss = 0.002533824464510688\n",
      "Iteration 14639: loss = 0.0025338129868223602\n",
      "Iteration 14640: loss = 0.0025338015092600825\n",
      "Iteration 14641: loss = 0.0025337900318238528\n",
      "Iteration 14642: loss = 0.0025337785545136698\n",
      "Iteration 14643: loss = 0.002533767077329531\n",
      "Iteration 14644: loss = 0.002533755600271435\n",
      "Iteration 14645: loss = 0.00253374412333938\n",
      "Iteration 14646: loss = 0.0025337326465333636\n",
      "Iteration 14647: loss = 0.002533721169853385\n",
      "Iteration 14648: loss = 0.002533709693299441\n",
      "Iteration 14649: loss = 0.0025336982168715316\n",
      "Iteration 14650: loss = 0.002533686740569654\n",
      "Iteration 14651: loss = 0.002533675264393806\n",
      "Iteration 14652: loss = 0.0025336637883439867\n",
      "Iteration 14653: loss = 0.0025336523124201934\n",
      "Iteration 14654: loss = 0.002533640836622425\n",
      "Iteration 14655: loss = 0.0025336293609506794\n",
      "Iteration 14656: loss = 0.0025336178854049552\n",
      "Iteration 14657: loss = 0.00253360640998525\n",
      "Iteration 14658: loss = 0.0025335949346915617\n",
      "Iteration 14659: loss = 0.0025335834595238897\n",
      "Iteration 14660: loss = 0.002533571984482231\n",
      "Iteration 14661: loss = 0.002533560509566585\n",
      "Iteration 14662: loss = 0.0025335490347769486\n",
      "Iteration 14663: loss = 0.002533537560113321\n",
      "Iteration 14664: loss = 0.0025335260855757\n",
      "Iteration 14665: loss = 0.002533514611164084\n",
      "Iteration 14666: loss = 0.0025335031368784702\n",
      "Iteration 14667: loss = 0.0025334916627188587\n",
      "Iteration 14668: loss = 0.0025334801886852457\n",
      "Iteration 14669: loss = 0.0025334687147776304\n",
      "Iteration 14670: loss = 0.0025334572409960116\n",
      "Iteration 14671: loss = 0.0025334457673403866\n",
      "Iteration 14672: loss = 0.0025334342938107533\n",
      "Iteration 14673: loss = 0.0025334228204071108\n",
      "Iteration 14674: loss = 0.002533411347129457\n",
      "Iteration 14675: loss = 0.00253339987397779\n",
      "Iteration 14676: loss = 0.0025333884009521076\n",
      "Iteration 14677: loss = 0.002533376928052409\n",
      "Iteration 14678: loss = 0.0025333654552786916\n",
      "Iteration 14679: loss = 0.002533353982630954\n",
      "Iteration 14680: loss = 0.002533342510109194\n",
      "Iteration 14681: loss = 0.0025333310377134103\n",
      "Iteration 14682: loss = 0.002533319565443601\n",
      "Iteration 14683: loss = 0.0025333080932997635\n",
      "Iteration 14684: loss = 0.002533296621281897\n",
      "Iteration 14685: loss = 0.0025332851493899996\n",
      "Iteration 14686: loss = 0.0025332736776240692\n",
      "Iteration 14687: loss = 0.002533262205984104\n",
      "Iteration 14688: loss = 0.0025332507344701027\n",
      "Iteration 14689: loss = 0.002533239263082062\n",
      "Iteration 14690: loss = 0.0025332277918199827\n",
      "Iteration 14691: loss = 0.00253321632068386\n",
      "Iteration 14692: loss = 0.002533204849673695\n",
      "Iteration 14693: loss = 0.0025331933787894833\n",
      "Iteration 14694: loss = 0.0025331819080312255\n",
      "Iteration 14695: loss = 0.0025331704373989183\n",
      "Iteration 14696: loss = 0.0025331589668925597\n",
      "Iteration 14697: loss = 0.002533147496512149\n",
      "Iteration 14698: loss = 0.0025331360262576827\n",
      "Iteration 14699: loss = 0.0025331245561291617\n",
      "Iteration 14700: loss = 0.002533113086126582\n",
      "Iteration 14701: loss = 0.0025331016162499424\n",
      "Iteration 14702: loss = 0.002533090146499242\n",
      "Iteration 14703: loss = 0.0025330786768744773\n",
      "Iteration 14704: loss = 0.002533067207375648\n",
      "Iteration 14705: loss = 0.0025330557380027516\n",
      "Iteration 14706: loss = 0.002533044268755786\n",
      "Iteration 14707: loss = 0.002533032799634751\n",
      "Iteration 14708: loss = 0.002533021330639643\n",
      "Iteration 14709: loss = 0.002533009861770461\n",
      "Iteration 14710: loss = 0.002532998393027203\n",
      "Iteration 14711: loss = 0.0025329869244098673\n",
      "Iteration 14712: loss = 0.0025329754559184525\n",
      "Iteration 14713: loss = 0.0025329639875529557\n",
      "Iteration 14714: loss = 0.002532952519313377\n",
      "Iteration 14715: loss = 0.0025329410511997124\n",
      "Iteration 14716: loss = 0.0025329295832119615\n",
      "Iteration 14717: loss = 0.0025329181153501226\n",
      "Iteration 14718: loss = 0.0025329066476141933\n",
      "Iteration 14719: loss = 0.0025328951800041724\n",
      "Iteration 14720: loss = 0.002532883712520057\n",
      "Iteration 14721: loss = 0.0025328722451618472\n",
      "Iteration 14722: loss = 0.002532860777929539\n",
      "Iteration 14723: loss = 0.002532849310823132\n",
      "Iteration 14724: loss = 0.0025328378438426243\n",
      "Iteration 14725: loss = 0.002532826376988014\n",
      "Iteration 14726: loss = 0.0025328149102592998\n",
      "Iteration 14727: loss = 0.0025328034436564784\n",
      "Iteration 14728: loss = 0.0025327919771795495\n",
      "Iteration 14729: loss = 0.0025327805108285107\n",
      "Iteration 14730: loss = 0.0025327690446033608\n",
      "Iteration 14731: loss = 0.002532757578504097\n",
      "Iteration 14732: loss = 0.002532746112530718\n",
      "Iteration 14733: loss = 0.0025327346466832226\n",
      "Iteration 14734: loss = 0.0025327231809616087\n",
      "Iteration 14735: loss = 0.002532711715365874\n",
      "Iteration 14736: loss = 0.002532700249896017\n",
      "Iteration 14737: loss = 0.0025326887845520357\n",
      "Iteration 14738: loss = 0.0025326773193339296\n",
      "Iteration 14739: loss = 0.0025326658542416947\n",
      "Iteration 14740: loss = 0.0025326543892753317\n",
      "Iteration 14741: loss = 0.002532642924434837\n",
      "Iteration 14742: loss = 0.0025326314597202094\n",
      "Iteration 14743: loss = 0.002532619995131447\n",
      "Iteration 14744: loss = 0.0025326085306685486\n",
      "Iteration 14745: loss = 0.0025325970663315116\n",
      "Iteration 14746: loss = 0.002532585602120335\n",
      "Iteration 14747: loss = 0.0025325741380350167\n",
      "Iteration 14748: loss = 0.0025325626740755544\n",
      "Iteration 14749: loss = 0.0025325512102419463\n",
      "Iteration 14750: loss = 0.0025325397465341924\n",
      "Iteration 14751: loss = 0.002532528282952289\n",
      "Iteration 14752: loss = 0.0025325168194962346\n",
      "Iteration 14753: loss = 0.002532505356166028\n",
      "Iteration 14754: loss = 0.002532493892961668\n",
      "Iteration 14755: loss = 0.002532482429883151\n",
      "Iteration 14756: loss = 0.002532470966930477\n",
      "Iteration 14757: loss = 0.002532459504103643\n",
      "Iteration 14758: loss = 0.002532448041402648\n",
      "Iteration 14759: loss = 0.00253243657882749\n",
      "Iteration 14760: loss = 0.002532425116378167\n",
      "Iteration 14761: loss = 0.0025324136540546775\n",
      "Iteration 14762: loss = 0.0025324021918570194\n",
      "Iteration 14763: loss = 0.0025323907297851913\n",
      "Iteration 14764: loss = 0.0025323792678391918\n",
      "Iteration 14765: loss = 0.002532367806019018\n",
      "Iteration 14766: loss = 0.002532356344324669\n",
      "Iteration 14767: loss = 0.0025323448827561427\n",
      "Iteration 14768: loss = 0.0025323334213134376\n",
      "Iteration 14769: loss = 0.0025323219599965516\n",
      "Iteration 14770: loss = 0.0025323104988054838\n",
      "Iteration 14771: loss = 0.0025322990377402307\n",
      "Iteration 14772: loss = 0.002532287576800792\n",
      "Iteration 14773: loss = 0.0025322761159871656\n",
      "Iteration 14774: loss = 0.0025322646552993493\n",
      "Iteration 14775: loss = 0.002532253194737342\n",
      "Iteration 14776: loss = 0.0025322417343011413\n",
      "Iteration 14777: loss = 0.0025322302739907462\n",
      "Iteration 14778: loss = 0.002532218813806154\n",
      "Iteration 14779: loss = 0.0025322073537473633\n",
      "Iteration 14780: loss = 0.0025321958938143724\n",
      "Iteration 14781: loss = 0.00253218443400718\n",
      "Iteration 14782: loss = 0.0025321729743257838\n",
      "Iteration 14783: loss = 0.002532161514770182\n",
      "Iteration 14784: loss = 0.002532150055340373\n",
      "Iteration 14785: loss = 0.0025321385960363546\n",
      "Iteration 14786: loss = 0.002532127136858126\n",
      "Iteration 14787: loss = 0.002532115677805685\n",
      "Iteration 14788: loss = 0.0025321042188790293\n",
      "Iteration 14789: loss = 0.0025320927600781576\n",
      "Iteration 14790: loss = 0.002532081301403068\n",
      "Iteration 14791: loss = 0.002532069842853759\n",
      "Iteration 14792: loss = 0.0025320583844302283\n",
      "Iteration 14793: loss = 0.002532046926132475\n",
      "Iteration 14794: loss = 0.002532035467960497\n",
      "Iteration 14795: loss = 0.002532024009914292\n",
      "Iteration 14796: loss = 0.0025320125519938588\n",
      "Iteration 14797: loss = 0.002532001094199195\n",
      "Iteration 14798: loss = 0.0025319896365303\n",
      "Iteration 14799: loss = 0.0025319781789871704\n",
      "Iteration 14800: loss = 0.002531966721569806\n",
      "Iteration 14801: loss = 0.002531955264278204\n",
      "Iteration 14802: loss = 0.002531943807112364\n",
      "Iteration 14803: loss = 0.0025319323500722826\n",
      "Iteration 14804: loss = 0.002531920893157959\n",
      "Iteration 14805: loss = 0.0025319094363693904\n",
      "Iteration 14806: loss = 0.0025318979797065767\n",
      "Iteration 14807: loss = 0.002531886523169515\n",
      "Iteration 14808: loss = 0.0025318750667582043\n",
      "Iteration 14809: loss = 0.002531863610472642\n",
      "Iteration 14810: loss = 0.002531852154312826\n",
      "Iteration 14811: loss = 0.0025318406982787565\n",
      "Iteration 14812: loss = 0.0025318292423704295\n",
      "Iteration 14813: loss = 0.0025318177865878448\n",
      "Iteration 14814: loss = 0.002531806330931\n",
      "Iteration 14815: loss = 0.0025317948753998927\n",
      "Iteration 14816: loss = 0.002531783419994523\n",
      "Iteration 14817: loss = 0.002531771964714887\n",
      "Iteration 14818: loss = 0.0025317605095609846\n",
      "Iteration 14819: loss = 0.002531749054532813\n",
      "Iteration 14820: loss = 0.0025317375996303714\n",
      "Iteration 14821: loss = 0.0025317261448536575\n",
      "Iteration 14822: loss = 0.002531714690202669\n",
      "Iteration 14823: loss = 0.002531703235677405\n",
      "Iteration 14824: loss = 0.0025316917812778627\n",
      "Iteration 14825: loss = 0.002531680327004042\n",
      "Iteration 14826: loss = 0.0025316688728559402\n",
      "Iteration 14827: loss = 0.0025316574188335547\n",
      "Iteration 14828: loss = 0.002531645964936885\n",
      "Iteration 14829: loss = 0.00253163451116593\n",
      "Iteration 14830: loss = 0.0025316230575206863\n",
      "Iteration 14831: loss = 0.002531611604001152\n",
      "Iteration 14832: loss = 0.0025316001506073267\n",
      "Iteration 14833: loss = 0.0025315886973392085\n",
      "Iteration 14834: loss = 0.002531577244196795\n",
      "Iteration 14835: loss = 0.002531565791180084\n",
      "Iteration 14836: loss = 0.002531554338289075\n",
      "Iteration 14837: loss = 0.0025315428855237653\n",
      "Iteration 14838: loss = 0.0025315314328841544\n",
      "Iteration 14839: loss = 0.002531519980370239\n",
      "Iteration 14840: loss = 0.002531508527982018\n",
      "Iteration 14841: loss = 0.00253149707571949\n",
      "Iteration 14842: loss = 0.0025314856235826525\n",
      "Iteration 14843: loss = 0.0025314741715715045\n",
      "Iteration 14844: loss = 0.0025314627196860438\n",
      "Iteration 14845: loss = 0.002531451267926269\n",
      "Iteration 14846: loss = 0.002531439816292178\n",
      "Iteration 14847: loss = 0.002531428364783769\n",
      "Iteration 14848: loss = 0.0025314169134010404\n",
      "Iteration 14849: loss = 0.002531405462143991\n",
      "Iteration 14850: loss = 0.0025313940110126185\n",
      "Iteration 14851: loss = 0.002531382560006921\n",
      "Iteration 14852: loss = 0.0025313711091268965\n",
      "Iteration 14853: loss = 0.002531359658372545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14854: loss = 0.002531348207743862\n",
      "Iteration 14855: loss = 0.0025313367572408485\n",
      "Iteration 14856: loss = 0.0025313253068635013\n",
      "Iteration 14857: loss = 0.0025313138566118182\n",
      "Iteration 14858: loss = 0.002531302406485798\n",
      "Iteration 14859: loss = 0.0025312909564854393\n",
      "Iteration 14860: loss = 0.0025312795066107403\n",
      "Iteration 14861: loss = 0.0025312680568616995\n",
      "Iteration 14862: loss = 0.002531256607238314\n",
      "Iteration 14863: loss = 0.0025312451577405837\n",
      "Iteration 14864: loss = 0.002531233708368505\n",
      "Iteration 14865: loss = 0.002531222259122077\n",
      "Iteration 14866: loss = 0.0025312108100012986\n",
      "Iteration 14867: loss = 0.0025311993610061677\n",
      "Iteration 14868: loss = 0.0025311879121366824\n",
      "Iteration 14869: loss = 0.0025311764633928406\n",
      "Iteration 14870: loss = 0.002531165014774641\n",
      "Iteration 14871: loss = 0.0025311535662820815\n",
      "Iteration 14872: loss = 0.0025311421179151613\n",
      "Iteration 14873: loss = 0.0025311306696738775\n",
      "Iteration 14874: loss = 0.002531119221558229\n",
      "Iteration 14875: loss = 0.0025311077735682137\n",
      "Iteration 14876: loss = 0.0025310963257038303\n",
      "Iteration 14877: loss = 0.002531084877965077\n",
      "Iteration 14878: loss = 0.002531073430351951\n",
      "Iteration 14879: loss = 0.0025310619828644524\n",
      "Iteration 14880: loss = 0.002531050535502578\n",
      "Iteration 14881: loss = 0.002531039088266327\n",
      "Iteration 14882: loss = 0.002531027641155697\n",
      "Iteration 14883: loss = 0.0025310161941706866\n",
      "Iteration 14884: loss = 0.0025310047473112943\n",
      "Iteration 14885: loss = 0.002530993300577517\n",
      "Iteration 14886: loss = 0.002530981853969355\n",
      "Iteration 14887: loss = 0.002530970407486805\n",
      "Iteration 14888: loss = 0.0025309589611298665\n",
      "Iteration 14889: loss = 0.0025309475148985365\n",
      "Iteration 14890: loss = 0.002530936068792815\n",
      "Iteration 14891: loss = 0.002530924622812697\n",
      "Iteration 14892: loss = 0.0025309131769581847\n",
      "Iteration 14893: loss = 0.0025309017312292736\n",
      "Iteration 14894: loss = 0.002530890285625963\n",
      "Iteration 14895: loss = 0.002530878840148252\n",
      "Iteration 14896: loss = 0.002530867394796137\n",
      "Iteration 14897: loss = 0.002530855949569618\n",
      "Iteration 14898: loss = 0.002530844504468692\n",
      "Iteration 14899: loss = 0.002530833059493358\n",
      "Iteration 14900: loss = 0.0025308216146436133\n",
      "Iteration 14901: loss = 0.002530810169919457\n",
      "Iteration 14902: loss = 0.002530798725320888\n",
      "Iteration 14903: loss = 0.0025307872808479036\n",
      "Iteration 14904: loss = 0.0025307758365005024\n",
      "Iteration 14905: loss = 0.002530764392278682\n",
      "Iteration 14906: loss = 0.0025307529481824417\n",
      "Iteration 14907: loss = 0.0025307415042117795\n",
      "Iteration 14908: loss = 0.002530730060366693\n",
      "Iteration 14909: loss = 0.0025307186166471816\n",
      "Iteration 14910: loss = 0.0025307071730532424\n",
      "Iteration 14911: loss = 0.002530695729584874\n",
      "Iteration 14912: loss = 0.0025306842862420752\n",
      "Iteration 14913: loss = 0.0025306728430248442\n",
      "Iteration 14914: loss = 0.0025306613999331786\n",
      "Iteration 14915: loss = 0.002530649956967077\n",
      "Iteration 14916: loss = 0.002530638514126538\n",
      "Iteration 14917: loss = 0.0025306270714115595\n",
      "Iteration 14918: loss = 0.00253061562882214\n",
      "Iteration 14919: loss = 0.002530604186358278\n",
      "Iteration 14920: loss = 0.002530592744019971\n",
      "Iteration 14921: loss = 0.0025305813018072173\n",
      "Iteration 14922: loss = 0.0025305698597200164\n",
      "Iteration 14923: loss = 0.0025305584177583653\n",
      "Iteration 14924: loss = 0.002530546975922263\n",
      "Iteration 14925: loss = 0.002530535534211707\n",
      "Iteration 14926: loss = 0.002530524092626697\n",
      "Iteration 14927: loss = 0.0025305126511672295\n",
      "Iteration 14928: loss = 0.002530501209833304\n",
      "Iteration 14929: loss = 0.002530489768624918\n",
      "Iteration 14930: loss = 0.002530478327542071\n",
      "Iteration 14931: loss = 0.00253046688658476\n",
      "Iteration 14932: loss = 0.002530455445752983\n",
      "Iteration 14933: loss = 0.0025304440050467406\n",
      "Iteration 14934: loss = 0.0025304325644660286\n",
      "Iteration 14935: loss = 0.0025304211240108456\n",
      "Iteration 14936: loss = 0.0025304096836811915\n",
      "Iteration 14937: loss = 0.002530398243477063\n",
      "Iteration 14938: loss = 0.0025303868033984592\n",
      "Iteration 14939: loss = 0.0025303753634453776\n",
      "Iteration 14940: loss = 0.0025303639236178175\n",
      "Iteration 14941: loss = 0.002530352483915776\n",
      "Iteration 14942: loss = 0.002530341044339252\n",
      "Iteration 14943: loss = 0.0025303296048882445\n",
      "Iteration 14944: loss = 0.002530318165562751\n",
      "Iteration 14945: loss = 0.0025303067263627694\n",
      "Iteration 14946: loss = 0.0025302952872882987\n",
      "Iteration 14947: loss = 0.002530283848339337\n",
      "Iteration 14948: loss = 0.0025302724095158822\n",
      "Iteration 14949: loss = 0.002530260970817933\n",
      "Iteration 14950: loss = 0.0025302495322454877\n",
      "Iteration 14951: loss = 0.002530238093798544\n",
      "Iteration 14952: loss = 0.002530226655477101\n",
      "Iteration 14953: loss = 0.0025302152172811563\n",
      "Iteration 14954: loss = 0.002530203779210709\n",
      "Iteration 14955: loss = 0.0025301923412657562\n",
      "Iteration 14956: loss = 0.0025301809034462973\n",
      "Iteration 14957: loss = 0.00253016946575233\n",
      "Iteration 14958: loss = 0.0025301580281838526\n",
      "Iteration 14959: loss = 0.002530146590740864\n",
      "Iteration 14960: loss = 0.002530135153423361\n",
      "Iteration 14961: loss = 0.0025301237162313433\n",
      "Iteration 14962: loss = 0.002530112279164809\n",
      "Iteration 14963: loss = 0.002530100842223756\n",
      "Iteration 14964: loss = 0.0025300894054081824\n",
      "Iteration 14965: loss = 0.002530077968718087\n",
      "Iteration 14966: loss = 0.002530066532153469\n",
      "Iteration 14967: loss = 0.002530055095714324\n",
      "Iteration 14968: loss = 0.002530043659400652\n",
      "Iteration 14969: loss = 0.0025300322232124513\n",
      "Iteration 14970: loss = 0.0025300207871497204\n",
      "Iteration 14971: loss = 0.002530009351212457\n",
      "Iteration 14972: loss = 0.0025299979154006593\n",
      "Iteration 14973: loss = 0.0025299864797143265\n",
      "Iteration 14974: loss = 0.002529975044153456\n",
      "Iteration 14975: loss = 0.002529963608718046\n",
      "Iteration 14976: loss = 0.0025299521734080954\n",
      "Iteration 14977: loss = 0.002529940738223603\n",
      "Iteration 14978: loss = 0.0025299293031645653\n",
      "Iteration 14979: loss = 0.002529917868230982\n",
      "Iteration 14980: loss = 0.0025299064334228507\n",
      "Iteration 14981: loss = 0.00252989499874017\n",
      "Iteration 14982: loss = 0.0025298835641829383\n",
      "Iteration 14983: loss = 0.002529872129751154\n",
      "Iteration 14984: loss = 0.0025298606954448142\n",
      "Iteration 14985: loss = 0.002529849261263919\n",
      "Iteration 14986: loss = 0.002529837827208466\n",
      "Iteration 14987: loss = 0.002529826393278453\n",
      "Iteration 14988: loss = 0.0025298149594738784\n",
      "Iteration 14989: loss = 0.002529803525794741\n",
      "Iteration 14990: loss = 0.0025297920922410385\n",
      "Iteration 14991: loss = 0.0025297806588127692\n",
      "Iteration 14992: loss = 0.002529769225509932\n",
      "Iteration 14993: loss = 0.002529757792332525\n",
      "Iteration 14994: loss = 0.0025297463592805465\n",
      "Iteration 14995: loss = 0.002529734926353994\n",
      "Iteration 14996: loss = 0.002529723493552867\n",
      "Iteration 14997: loss = 0.002529712060877163\n",
      "Iteration 14998: loss = 0.0025297006283268807\n",
      "Iteration 14999: loss = 0.0025296891959020185\n",
      "Iteration 15000: loss = 0.0025296777636025737\n",
      "Iteration 15001: loss = 0.002529666331428545\n",
      "Iteration 15002: loss = 0.002529654899379932\n",
      "Iteration 15003: loss = 0.0025296434674567316\n",
      "Iteration 15004: loss = 0.0025296320356589424\n",
      "Iteration 15005: loss = 0.002529620603986563\n",
      "Iteration 15006: loss = 0.002529609172439591\n",
      "Iteration 15007: loss = 0.002529597741018025\n",
      "Iteration 15008: loss = 0.0025295863097218644\n",
      "Iteration 15009: loss = 0.002529574878551106\n",
      "Iteration 15010: loss = 0.002529563447505749\n",
      "Iteration 15011: loss = 0.0025295520165857905\n",
      "Iteration 15012: loss = 0.0025295405857912305\n",
      "Iteration 15013: loss = 0.0025295291551220658\n",
      "Iteration 15014: loss = 0.002529517724578295\n",
      "Iteration 15015: loss = 0.0025295062941599173\n",
      "Iteration 15016: loss = 0.002529494863866931\n",
      "Iteration 15017: loss = 0.0025294834336993333\n",
      "Iteration 15018: loss = 0.002529472003657123\n",
      "Iteration 15019: loss = 0.0025294605737402986\n",
      "Iteration 15020: loss = 0.0025294491439488576\n",
      "Iteration 15021: loss = 0.002529437714282799\n",
      "Iteration 15022: loss = 0.0025294262847421214\n",
      "Iteration 15023: loss = 0.0025294148553268226\n",
      "Iteration 15024: loss = 0.002529403426036901\n",
      "Iteration 15025: loss = 0.0025293919968723553\n",
      "Iteration 15026: loss = 0.0025293805678331827\n",
      "Iteration 15027: loss = 0.0025293691389193826\n",
      "Iteration 15028: loss = 0.002529357710130953\n",
      "Iteration 15029: loss = 0.0025293462814678916\n",
      "Iteration 15030: loss = 0.0025293348529301977\n",
      "Iteration 15031: loss = 0.0025293234245178688\n",
      "Iteration 15032: loss = 0.002529311996230904\n",
      "Iteration 15033: loss = 0.0025293005680693\n",
      "Iteration 15034: loss = 0.0025292891400330573\n",
      "Iteration 15035: loss = 0.0025292777121221728\n",
      "Iteration 15036: loss = 0.002529266284336645\n",
      "Iteration 15037: loss = 0.002529254856676472\n",
      "Iteration 15038: loss = 0.0025292434291416532\n",
      "Iteration 15039: loss = 0.0025292320017321853\n",
      "Iteration 15040: loss = 0.0025292205744480677\n",
      "Iteration 15041: loss = 0.0025292091472892985\n",
      "Iteration 15042: loss = 0.0025291977202558766\n",
      "Iteration 15043: loss = 0.0025291862933477984\n",
      "Iteration 15044: loss = 0.002529174866565064\n",
      "Iteration 15045: loss = 0.0025291634399076707\n",
      "Iteration 15046: loss = 0.0025291520133756177\n",
      "Iteration 15047: loss = 0.0025291405869689028\n",
      "Iteration 15048: loss = 0.0025291291606875242\n",
      "Iteration 15049: loss = 0.0025291177345314808\n",
      "Iteration 15050: loss = 0.00252910630850077\n",
      "Iteration 15051: loss = 0.0025290948825953904\n",
      "Iteration 15052: loss = 0.0025290834568153404\n",
      "Iteration 15053: loss = 0.002529072031160619\n",
      "Iteration 15054: loss = 0.0025290606056312236\n",
      "Iteration 15055: loss = 0.0025290491802271524\n",
      "Iteration 15056: loss = 0.002529037754948404\n",
      "Iteration 15057: loss = 0.0025290263297949775\n",
      "Iteration 15058: loss = 0.0025290149047668704\n",
      "Iteration 15059: loss = 0.002529003479864081\n",
      "Iteration 15060: loss = 0.002528992055086607\n",
      "Iteration 15061: loss = 0.0025289806304344487\n",
      "Iteration 15062: loss = 0.002528969205907602\n",
      "Iteration 15063: loss = 0.002528957781506067\n",
      "Iteration 15064: loss = 0.0025289463572298406\n",
      "Iteration 15065: loss = 0.0025289349330789228\n",
      "Iteration 15066: loss = 0.00252892350905331\n",
      "Iteration 15067: loss = 0.0025289120851530026\n",
      "Iteration 15068: loss = 0.002528900661377997\n",
      "Iteration 15069: loss = 0.0025288892377282922\n",
      "Iteration 15070: loss = 0.0025288778142038867\n",
      "Iteration 15071: loss = 0.002528866390804779\n",
      "Iteration 15072: loss = 0.002528854967530967\n",
      "Iteration 15073: loss = 0.0025288435443824486\n",
      "Iteration 15074: loss = 0.0025288321213592234\n",
      "Iteration 15075: loss = 0.0025288206984612886\n",
      "Iteration 15076: loss = 0.002528809275688643\n",
      "Iteration 15077: loss = 0.002528797853041284\n",
      "Iteration 15078: loss = 0.0025287864305192113\n",
      "Iteration 15079: loss = 0.002528775008122423\n",
      "Iteration 15080: loss = 0.0025287635858509162\n",
      "Iteration 15081: loss = 0.00252875216370469\n",
      "Iteration 15082: loss = 0.0025287407416837437\n",
      "Iteration 15083: loss = 0.002528729319788074\n",
      "Iteration 15084: loss = 0.0025287178980176795\n",
      "Iteration 15085: loss = 0.0025287064763725594\n",
      "Iteration 15086: loss = 0.0025286950548527114\n",
      "Iteration 15087: loss = 0.002528683633458134\n",
      "Iteration 15088: loss = 0.002528672212188825\n",
      "Iteration 15089: loss = 0.0025286607910447833\n",
      "Iteration 15090: loss = 0.002528649370026007\n",
      "Iteration 15091: loss = 0.002528637949132494\n",
      "Iteration 15092: loss = 0.002528626528364244\n",
      "Iteration 15093: loss = 0.0025286151077212535\n",
      "Iteration 15094: loss = 0.002528603687203522\n",
      "Iteration 15095: loss = 0.0025285922668110473\n",
      "Iteration 15096: loss = 0.0025285808465438284\n",
      "Iteration 15097: loss = 0.0025285694264018626\n",
      "Iteration 15098: loss = 0.002528558006385149\n",
      "Iteration 15099: loss = 0.0025285465864936857\n",
      "Iteration 15100: loss = 0.0025285351667274705\n",
      "Iteration 15101: loss = 0.002528523747086503\n",
      "Iteration 15102: loss = 0.00252851232757078\n",
      "Iteration 15103: loss = 0.0025285009081803013\n",
      "Iteration 15104: loss = 0.002528489488915064\n",
      "Iteration 15105: loss = 0.0025284780697750666\n",
      "Iteration 15106: loss = 0.0025284666507603077\n",
      "Iteration 15107: loss = 0.002528455231870786\n",
      "Iteration 15108: loss = 0.0025284438131064992\n",
      "Iteration 15109: loss = 0.0025284323944674454\n",
      "Iteration 15110: loss = 0.0025284209759536234\n",
      "Iteration 15111: loss = 0.002528409557565032\n",
      "Iteration 15112: loss = 0.002528398139301669\n",
      "Iteration 15113: loss = 0.002528386721163532\n",
      "Iteration 15114: loss = 0.002528375303150621\n",
      "Iteration 15115: loss = 0.0025283638852629325\n",
      "Iteration 15116: loss = 0.0025283524675004665\n",
      "Iteration 15117: loss = 0.00252834104986322\n",
      "Iteration 15118: loss = 0.0025283296323511915\n",
      "Iteration 15119: loss = 0.00252831821496438\n",
      "Iteration 15120: loss = 0.002528306797702783\n",
      "Iteration 15121: loss = 0.0025282953805663994\n",
      "Iteration 15122: loss = 0.002528283963555228\n",
      "Iteration 15123: loss = 0.0025282725466692657\n",
      "Iteration 15124: loss = 0.002528261129908512\n",
      "Iteration 15125: loss = 0.0025282497132729643\n",
      "Iteration 15126: loss = 0.002528238296762622\n",
      "Iteration 15127: loss = 0.0025282268803774833\n",
      "Iteration 15128: loss = 0.002528215464117546\n",
      "Iteration 15129: loss = 0.002528204047982808\n",
      "Iteration 15130: loss = 0.002528192631973268\n",
      "Iteration 15131: loss = 0.002528181216088925\n",
      "Iteration 15132: loss = 0.0025281698003297762\n",
      "Iteration 15133: loss = 0.002528158384695821\n",
      "Iteration 15134: loss = 0.002528146969187057\n",
      "Iteration 15135: loss = 0.002528135553803483\n",
      "Iteration 15136: loss = 0.0025281241385450965\n",
      "Iteration 15137: loss = 0.0025281127234118967\n",
      "Iteration 15138: loss = 0.002528101308403882\n",
      "Iteration 15139: loss = 0.0025280898935210504\n",
      "Iteration 15140: loss = 0.0025280784787634\n",
      "Iteration 15141: loss = 0.002528067064130929\n",
      "Iteration 15142: loss = 0.0025280556496236365\n",
      "Iteration 15143: loss = 0.00252804423524152\n",
      "Iteration 15144: loss = 0.0025280328209845784\n",
      "Iteration 15145: loss = 0.0025280214068528098\n",
      "Iteration 15146: loss = 0.0025280099928462127\n",
      "Iteration 15147: loss = 0.002527998578964785\n",
      "Iteration 15148: loss = 0.0025279871652085254\n",
      "Iteration 15149: loss = 0.002527975751577432\n",
      "Iteration 15150: loss = 0.0025279643380715032\n",
      "Iteration 15151: loss = 0.0025279529246907376\n",
      "Iteration 15152: loss = 0.002527941511435133\n",
      "Iteration 15153: loss = 0.002527930098304688\n",
      "Iteration 15154: loss = 0.002527918685299401\n",
      "Iteration 15155: loss = 0.0025279072724192705\n",
      "Iteration 15156: loss = 0.0025278958596642947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15157: loss = 0.0025278844470344717\n",
      "Iteration 15158: loss = 0.0025278730345297995\n",
      "Iteration 15159: loss = 0.002527861622150278\n",
      "Iteration 15160: loss = 0.0025278502098959035\n",
      "Iteration 15161: loss = 0.002527838797766675\n",
      "Iteration 15162: loss = 0.0025278273857625914\n",
      "Iteration 15163: loss = 0.002527815973883651\n",
      "Iteration 15164: loss = 0.0025278045621298516\n",
      "Iteration 15165: loss = 0.002527793150501192\n",
      "Iteration 15166: loss = 0.00252778173899767\n",
      "Iteration 15167: loss = 0.002527770327619284\n",
      "Iteration 15168: loss = 0.002527758916366033\n",
      "Iteration 15169: loss = 0.002527747505237915\n",
      "Iteration 15170: loss = 0.002527736094234928\n",
      "Iteration 15171: loss = 0.002527724683357071\n",
      "Iteration 15172: loss = 0.002527713272604341\n",
      "Iteration 15173: loss = 0.0025277018619767376\n",
      "Iteration 15174: loss = 0.0025276904514742586\n",
      "Iteration 15175: loss = 0.0025276790410969025\n",
      "Iteration 15176: loss = 0.002527667630844668\n",
      "Iteration 15177: loss = 0.002527656220717553\n",
      "Iteration 15178: loss = 0.0025276448107155553\n",
      "Iteration 15179: loss = 0.002527633400838675\n",
      "Iteration 15180: loss = 0.002527621991086908\n",
      "Iteration 15181: loss = 0.0025276105814602537\n",
      "Iteration 15182: loss = 0.002527599171958711\n",
      "Iteration 15183: loss = 0.0025275877625822786\n",
      "Iteration 15184: loss = 0.002527576353330953\n",
      "Iteration 15185: loss = 0.0025275649442047347\n",
      "Iteration 15186: loss = 0.00252755353520362\n",
      "Iteration 15187: loss = 0.0025275421263276083\n",
      "Iteration 15188: loss = 0.0025275307175766978\n",
      "Iteration 15189: loss = 0.0025275193089508876\n",
      "Iteration 15190: loss = 0.0025275079004501745\n",
      "Iteration 15191: loss = 0.0025274964920745575\n",
      "Iteration 15192: loss = 0.0025274850838240357\n",
      "Iteration 15193: loss = 0.002527473675698606\n",
      "Iteration 15194: loss = 0.0025274622676982684\n",
      "Iteration 15195: loss = 0.0025274508598230197\n",
      "Iteration 15196: loss = 0.0025274394520728594\n",
      "Iteration 15197: loss = 0.0025274280444477844\n",
      "Iteration 15198: loss = 0.002527416636947795\n",
      "Iteration 15199: loss = 0.0025274052295728884\n",
      "Iteration 15200: loss = 0.002527393822323062\n",
      "Iteration 15201: loss = 0.0025273824151983157\n",
      "Iteration 15202: loss = 0.002527371008198648\n",
      "Iteration 15203: loss = 0.002527359601324056\n",
      "Iteration 15204: loss = 0.0025273481945745383\n",
      "Iteration 15205: loss = 0.002527336787950094\n",
      "Iteration 15206: loss = 0.002527325381450721\n",
      "Iteration 15207: loss = 0.0025273139750764174\n",
      "Iteration 15208: loss = 0.0025273025688271815\n",
      "Iteration 15209: loss = 0.002527291162703012\n",
      "Iteration 15210: loss = 0.0025272797567039073\n",
      "Iteration 15211: loss = 0.002527268350829865\n",
      "Iteration 15212: loss = 0.002527256945080886\n",
      "Iteration 15213: loss = 0.002527245539456964\n",
      "Iteration 15214: loss = 0.0025272341339581013\n",
      "Iteration 15215: loss = 0.002527222728584294\n",
      "Iteration 15216: loss = 0.0025272113233355425\n",
      "Iteration 15217: loss = 0.0025271999182118433\n",
      "Iteration 15218: loss = 0.002527188513213196\n",
      "Iteration 15219: loss = 0.0025271771083395984\n",
      "Iteration 15220: loss = 0.0025271657035910483\n",
      "Iteration 15221: loss = 0.0025271542989675445\n",
      "Iteration 15222: loss = 0.0025271428944690856\n",
      "Iteration 15223: loss = 0.0025271314900956695\n",
      "Iteration 15224: loss = 0.002527120085847295\n",
      "Iteration 15225: loss = 0.002527108681723961\n",
      "Iteration 15226: loss = 0.0025270972777256635\n",
      "Iteration 15227: loss = 0.002527085873852403\n",
      "Iteration 15228: loss = 0.002527074470104178\n",
      "Iteration 15229: loss = 0.002527063066480985\n",
      "Iteration 15230: loss = 0.0025270516629828242\n",
      "Iteration 15231: loss = 0.002527040259609693\n",
      "Iteration 15232: loss = 0.0025270288563615895\n",
      "Iteration 15233: loss = 0.002527017453238513\n",
      "Iteration 15234: loss = 0.002527006050240461\n",
      "Iteration 15235: loss = 0.0025269946473674325\n",
      "Iteration 15236: loss = 0.002526983244619425\n",
      "Iteration 15237: loss = 0.0025269718419964377\n",
      "Iteration 15238: loss = 0.002526960439498468\n",
      "Iteration 15239: loss = 0.002526949037125516\n",
      "Iteration 15240: loss = 0.002526937634877578\n",
      "Iteration 15241: loss = 0.0025269262327546537\n",
      "Iteration 15242: loss = 0.00252691483075674\n",
      "Iteration 15243: loss = 0.002526903428883837\n",
      "Iteration 15244: loss = 0.002526892027135942\n",
      "Iteration 15245: loss = 0.002526880625513054\n",
      "Iteration 15246: loss = 0.0025268692240151707\n",
      "Iteration 15247: loss = 0.00252685782264229\n",
      "Iteration 15248: loss = 0.002526846421394412\n",
      "Iteration 15249: loss = 0.002526835020271534\n",
      "Iteration 15250: loss = 0.0025268236192736537\n",
      "Iteration 15251: loss = 0.0025268122184007704\n",
      "Iteration 15252: loss = 0.0025268008176528817\n",
      "Iteration 15253: loss = 0.002526789417029987\n",
      "Iteration 15254: loss = 0.002526778016532084\n",
      "Iteration 15255: loss = 0.0025267666161591703\n",
      "Iteration 15256: loss = 0.0025267552159112454\n",
      "Iteration 15257: loss = 0.0025267438157883073\n",
      "Iteration 15258: loss = 0.0025267324157903543\n",
      "Iteration 15259: loss = 0.002526721015917385\n",
      "Iteration 15260: loss = 0.0025267096161693974\n",
      "Iteration 15261: loss = 0.00252669821654639\n",
      "Iteration 15262: loss = 0.0025266868170483606\n",
      "Iteration 15263: loss = 0.0025266754176753083\n",
      "Iteration 15264: loss = 0.0025266640184272316\n",
      "Iteration 15265: loss = 0.0025266526193041283\n",
      "Iteration 15266: loss = 0.002526641220305997\n",
      "Iteration 15267: loss = 0.0025266298214328357\n",
      "Iteration 15268: loss = 0.0025266184226846425\n",
      "Iteration 15269: loss = 0.0025266070240614166\n",
      "Iteration 15270: loss = 0.002526595625563157\n",
      "Iteration 15271: loss = 0.00252658422718986\n",
      "Iteration 15272: loss = 0.0025265728289415256\n",
      "Iteration 15273: loss = 0.0025265614308181503\n",
      "Iteration 15274: loss = 0.002526550032819735\n",
      "Iteration 15275: loss = 0.002526538634946276\n",
      "Iteration 15276: loss = 0.002526527237197773\n",
      "Iteration 15277: loss = 0.002526515839574223\n",
      "Iteration 15278: loss = 0.002526504442075626\n",
      "Iteration 15279: loss = 0.0025264930447019794\n",
      "Iteration 15280: loss = 0.002526481647453281\n",
      "Iteration 15281: loss = 0.002526470250329531\n",
      "Iteration 15282: loss = 0.0025264588533307247\n",
      "Iteration 15283: loss = 0.0025264474564568634\n",
      "Iteration 15284: loss = 0.0025264360597079443\n",
      "Iteration 15285: loss = 0.0025264246630839655\n",
      "Iteration 15286: loss = 0.0025264132665849255\n",
      "Iteration 15287: loss = 0.0025264018702108233\n",
      "Iteration 15288: loss = 0.0025263904739616567\n",
      "Iteration 15289: loss = 0.002526379077837424\n",
      "Iteration 15290: loss = 0.002526367681838123\n",
      "Iteration 15291: loss = 0.0025263562859637535\n",
      "Iteration 15292: loss = 0.002526344890214313\n",
      "Iteration 15293: loss = 0.0025263334945897996\n",
      "Iteration 15294: loss = 0.002526322099090212\n",
      "Iteration 15295: loss = 0.0025263107037155484\n",
      "Iteration 15296: loss = 0.002526299308465808\n",
      "Iteration 15297: loss = 0.002526287913340988\n",
      "Iteration 15298: loss = 0.002526276518341087\n",
      "Iteration 15299: loss = 0.0025262651234661034\n",
      "Iteration 15300: loss = 0.0025262537287160363\n",
      "Iteration 15301: loss = 0.002526242334090883\n",
      "Iteration 15302: loss = 0.0025262309395906427\n",
      "Iteration 15303: loss = 0.0025262195452153127\n",
      "Iteration 15304: loss = 0.0025262081509648928\n",
      "Iteration 15305: loss = 0.00252619675683938\n",
      "Iteration 15306: loss = 0.002526185362838774\n",
      "Iteration 15307: loss = 0.0025261739689630713\n",
      "Iteration 15308: loss = 0.002526162575212272\n",
      "Iteration 15309: loss = 0.002526151181586374\n",
      "Iteration 15310: loss = 0.002526139788085375\n",
      "Iteration 15311: loss = 0.0025261283947092743\n",
      "Iteration 15312: loss = 0.00252611700145807\n",
      "Iteration 15313: loss = 0.0025261056083317594\n",
      "Iteration 15314: loss = 0.002526094215330342\n",
      "Iteration 15315: loss = 0.0025260828224538162\n",
      "Iteration 15316: loss = 0.0025260714297021796\n",
      "Iteration 15317: loss = 0.0025260600370754314\n",
      "Iteration 15318: loss = 0.00252604864457357\n",
      "Iteration 15319: loss = 0.0025260372521965922\n",
      "Iteration 15320: loss = 0.002526025859944498\n",
      "Iteration 15321: loss = 0.002526014467817285\n",
      "Iteration 15322: loss = 0.0025260030758149517\n",
      "Iteration 15323: loss = 0.002525991683937497\n",
      "Iteration 15324: loss = 0.002525980292184919\n",
      "Iteration 15325: loss = 0.002525968900557215\n",
      "Iteration 15326: loss = 0.0025259575090543847\n",
      "Iteration 15327: loss = 0.002525946117676426\n",
      "Iteration 15328: loss = 0.0025259347264233374\n",
      "Iteration 15329: loss = 0.002525923335295117\n",
      "Iteration 15330: loss = 0.0025259119442917637\n",
      "Iteration 15331: loss = 0.002525900553413275\n",
      "Iteration 15332: loss = 0.002525889162659649\n",
      "Iteration 15333: loss = 0.002525877772030886\n",
      "Iteration 15334: loss = 0.0025258663815269818\n",
      "Iteration 15335: loss = 0.002525854991147937\n",
      "Iteration 15336: loss = 0.002525843600893749\n",
      "Iteration 15337: loss = 0.0025258322107644163\n",
      "Iteration 15338: loss = 0.002525820820759937\n",
      "Iteration 15339: loss = 0.0025258094308803093\n",
      "Iteration 15340: loss = 0.002525798041125532\n",
      "Iteration 15341: loss = 0.002525786651495604\n",
      "Iteration 15342: loss = 0.002525775261990522\n",
      "Iteration 15343: loss = 0.002525763872610286\n",
      "Iteration 15344: loss = 0.0025257524833548935\n",
      "Iteration 15345: loss = 0.0025257410942243434\n",
      "Iteration 15346: loss = 0.0025257297052186335\n",
      "Iteration 15347: loss = 0.0025257183163377626\n",
      "Iteration 15348: loss = 0.002525706927581729\n",
      "Iteration 15349: loss = 0.002525695538950531\n",
      "Iteration 15350: loss = 0.002525684150444167\n",
      "Iteration 15351: loss = 0.002525672762062635\n",
      "Iteration 15352: loss = 0.002525661373805934\n",
      "Iteration 15353: loss = 0.002525649985674062\n",
      "Iteration 15354: loss = 0.0025256385976670168\n",
      "Iteration 15355: loss = 0.002525627209784798\n",
      "Iteration 15356: loss = 0.0025256158220274033\n",
      "Iteration 15357: loss = 0.0025256044343948303\n",
      "Iteration 15358: loss = 0.0025255930468870793\n",
      "Iteration 15359: loss = 0.002525581659504147\n",
      "Iteration 15360: loss = 0.0025255702722460326\n",
      "Iteration 15361: loss = 0.0025255588851127338\n",
      "Iteration 15362: loss = 0.002525547498104249\n",
      "Iteration 15363: loss = 0.0025255361112205776\n",
      "Iteration 15364: loss = 0.002525524724461717\n",
      "Iteration 15365: loss = 0.002525513337827666\n",
      "Iteration 15366: loss = 0.0025255019513184228\n",
      "Iteration 15367: loss = 0.002525490564933986\n",
      "Iteration 15368: loss = 0.0025254791786743533\n",
      "Iteration 15369: loss = 0.002525467792539523\n",
      "Iteration 15370: loss = 0.0025254564065294947\n",
      "Iteration 15371: loss = 0.002525445020644266\n",
      "Iteration 15372: loss = 0.002525433634883835\n",
      "Iteration 15373: loss = 0.002525422249248201\n",
      "Iteration 15374: loss = 0.002525410863737361\n",
      "Iteration 15375: loss = 0.0025253994783513145\n",
      "Iteration 15376: loss = 0.002525388093090059\n",
      "Iteration 15377: loss = 0.0025253767079535944\n",
      "Iteration 15378: loss = 0.002525365322941917\n",
      "Iteration 15379: loss = 0.002525353938055027\n",
      "Iteration 15380: loss = 0.0025253425532929218\n",
      "Iteration 15381: loss = 0.0025253311686556\n",
      "Iteration 15382: loss = 0.0025253197841430598\n",
      "Iteration 15383: loss = 0.002525308399755299\n",
      "Iteration 15384: loss = 0.002525297015492317\n",
      "Iteration 15385: loss = 0.0025252856313541126\n",
      "Iteration 15386: loss = 0.0025252742473406828\n",
      "Iteration 15387: loss = 0.0025252628634520264\n",
      "Iteration 15388: loss = 0.002525251479688142\n",
      "Iteration 15389: loss = 0.0025252400960490283\n",
      "Iteration 15390: loss = 0.002525228712534683\n",
      "Iteration 15391: loss = 0.002525217329145105\n",
      "Iteration 15392: loss = 0.002525205945880292\n",
      "Iteration 15393: loss = 0.0025251945627402434\n",
      "Iteration 15394: loss = 0.0025251831797249566\n",
      "Iteration 15395: loss = 0.00252517179683443\n",
      "Iteration 15396: loss = 0.002525160414068663\n",
      "Iteration 15397: loss = 0.0025251490314276534\n",
      "Iteration 15398: loss = 0.002525137648911399\n",
      "Iteration 15399: loss = 0.0025251262665198985\n",
      "Iteration 15400: loss = 0.00252511488425315\n",
      "Iteration 15401: loss = 0.0025251035021111533\n",
      "Iteration 15402: loss = 0.0025250921200939054\n",
      "Iteration 15403: loss = 0.002525080738201405\n",
      "Iteration 15404: loss = 0.0025250693564336504\n",
      "Iteration 15405: loss = 0.00252505797479064\n",
      "Iteration 15406: loss = 0.0025250465932723726\n",
      "Iteration 15407: loss = 0.0025250352118788458\n",
      "Iteration 15408: loss = 0.002525023830610059\n",
      "Iteration 15409: loss = 0.0025250124494660096\n",
      "Iteration 15410: loss = 0.0025250010684466965\n",
      "Iteration 15411: loss = 0.002524989687552118\n",
      "Iteration 15412: loss = 0.0025249783067822725\n",
      "Iteration 15413: loss = 0.002524966926137157\n",
      "Iteration 15414: loss = 0.0025249555456167733\n",
      "Iteration 15415: loss = 0.002524944165221116\n",
      "Iteration 15416: loss = 0.002524932784950186\n",
      "Iteration 15417: loss = 0.00252492140480398\n",
      "Iteration 15418: loss = 0.002524910024782498\n",
      "Iteration 15419: loss = 0.002524898644885737\n",
      "Iteration 15420: loss = 0.002524887265113696\n",
      "Iteration 15421: loss = 0.002524875885466373\n",
      "Iteration 15422: loss = 0.0025248645059437673\n",
      "Iteration 15423: loss = 0.0025248531265458763\n",
      "Iteration 15424: loss = 0.0025248417472726993\n",
      "Iteration 15425: loss = 0.002524830368124234\n",
      "Iteration 15426: loss = 0.0025248189891004785\n",
      "Iteration 15427: loss = 0.002524807610201432\n",
      "Iteration 15428: loss = 0.002524796231427091\n",
      "Iteration 15429: loss = 0.0025247848527774567\n",
      "Iteration 15430: loss = 0.0025247734742525254\n",
      "Iteration 15431: loss = 0.002524762095852297\n",
      "Iteration 15432: loss = 0.0025247507175767685\n",
      "Iteration 15433: loss = 0.0025247393394259394\n",
      "Iteration 15434: loss = 0.0025247279613998066\n",
      "Iteration 15435: loss = 0.0025247165834983705\n",
      "Iteration 15436: loss = 0.0025247052057216276\n",
      "Iteration 15437: loss = 0.0025246938280695766\n",
      "Iteration 15438: loss = 0.002524682450542217\n",
      "Iteration 15439: loss = 0.002524671073139546\n",
      "Iteration 15440: loss = 0.0025246596958615635\n",
      "Iteration 15441: loss = 0.0025246483187082664\n",
      "Iteration 15442: loss = 0.0025246369416796533\n",
      "Iteration 15443: loss = 0.002524625564775723\n",
      "Iteration 15444: loss = 0.002524614187996474\n",
      "Iteration 15445: loss = 0.002524602811341904\n",
      "Iteration 15446: loss = 0.002524591434812012\n",
      "Iteration 15447: loss = 0.0025245800584067957\n",
      "Iteration 15448: loss = 0.002524568682126255\n",
      "Iteration 15449: loss = 0.0025245573059703864\n",
      "Iteration 15450: loss = 0.002524545929939189\n",
      "Iteration 15451: loss = 0.0025245345540326615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15452: loss = 0.0025245231782508024\n",
      "Iteration 15453: loss = 0.002524511802593609\n",
      "Iteration 15454: loss = 0.002524500427061081\n",
      "Iteration 15455: loss = 0.0025244890516532164\n",
      "Iteration 15456: loss = 0.002524477676370013\n",
      "Iteration 15457: loss = 0.0025244663012114698\n",
      "Iteration 15458: loss = 0.002524454926177585\n",
      "Iteration 15459: loss = 0.002524443551268357\n",
      "Iteration 15460: loss = 0.002524432176483784\n",
      "Iteration 15461: loss = 0.002524420801823864\n",
      "Iteration 15462: loss = 0.002524409427288597\n",
      "Iteration 15463: loss = 0.0025243980528779795\n",
      "Iteration 15464: loss = 0.002524386678592011\n",
      "Iteration 15465: loss = 0.002524375304430689\n",
      "Iteration 15466: loss = 0.002524363930394013\n",
      "Iteration 15467: loss = 0.002524352556481981\n",
      "Iteration 15468: loss = 0.0025243411826945906\n",
      "Iteration 15469: loss = 0.0025243298090318407\n",
      "Iteration 15470: loss = 0.0025243184354937303\n",
      "Iteration 15471: loss = 0.0025243070620802575\n",
      "Iteration 15472: loss = 0.00252429568879142\n",
      "Iteration 15473: loss = 0.002524284315627217\n",
      "Iteration 15474: loss = 0.0025242729425876457\n",
      "Iteration 15475: loss = 0.002524261569672706\n",
      "Iteration 15476: loss = 0.0025242501968823954\n",
      "Iteration 15477: loss = 0.0025242388242167125\n",
      "Iteration 15478: loss = 0.0025242274516756556\n",
      "Iteration 15479: loss = 0.0025242160792592234\n",
      "Iteration 15480: loss = 0.002524204706967414\n",
      "Iteration 15481: loss = 0.002524193334800225\n",
      "Iteration 15482: loss = 0.002524181962757657\n",
      "Iteration 15483: loss = 0.002524170590839706\n",
      "Iteration 15484: loss = 0.0025241592190463715\n",
      "Iteration 15485: loss = 0.002524147847377652\n",
      "Iteration 15486: loss = 0.0025241364758335455\n",
      "Iteration 15487: loss = 0.002524125104414051\n",
      "Iteration 15488: loss = 0.0025241137331191658\n",
      "Iteration 15489: loss = 0.0025241023619488896\n",
      "Iteration 15490: loss = 0.00252409099090322\n",
      "Iteration 15491: loss = 0.002524079619982155\n",
      "Iteration 15492: loss = 0.0025240682491856938\n",
      "Iteration 15493: loss = 0.0025240568785138344\n",
      "Iteration 15494: loss = 0.002524045507966575\n",
      "Iteration 15495: loss = 0.0025240341375439143\n",
      "Iteration 15496: loss = 0.0025240227672458514\n",
      "Iteration 15497: loss = 0.0025240113970723833\n",
      "Iteration 15498: loss = 0.002524000027023509\n",
      "Iteration 15499: loss = 0.002523988657099227\n",
      "Iteration 15500: loss = 0.002523977287299536\n",
      "Iteration 15501: loss = 0.002523965917624434\n",
      "Iteration 15502: loss = 0.002523954548073919\n",
      "Iteration 15503: loss = 0.0025239431786479895\n",
      "Iteration 15504: loss = 0.0025239318093466445\n",
      "Iteration 15505: loss = 0.0025239204401698817\n",
      "Iteration 15506: loss = 0.0025239090711177007\n",
      "Iteration 15507: loss = 0.0025238977021900984\n",
      "Iteration 15508: loss = 0.002523886333387074\n",
      "Iteration 15509: loss = 0.0025238749647086257\n",
      "Iteration 15510: loss = 0.0025238635961547518\n",
      "Iteration 15511: loss = 0.0025238522277254505\n",
      "Iteration 15512: loss = 0.0025238408594207206\n",
      "Iteration 15513: loss = 0.0025238294912405613\n",
      "Iteration 15514: loss = 0.0025238181231849693\n",
      "Iteration 15515: loss = 0.002523806755253944\n",
      "Iteration 15516: loss = 0.0025237953874474835\n",
      "Iteration 15517: loss = 0.002523784019765586\n",
      "Iteration 15518: loss = 0.0025237726522082506\n",
      "Iteration 15519: loss = 0.002523761284775475\n",
      "Iteration 15520: loss = 0.002523749917467258\n",
      "Iteration 15521: loss = 0.0025237385502835975\n",
      "Iteration 15522: loss = 0.002523727183224493\n",
      "Iteration 15523: loss = 0.002523715816289941\n",
      "Iteration 15524: loss = 0.002523704449479941\n",
      "Iteration 15525: loss = 0.0025236930827944926\n",
      "Iteration 15526: loss = 0.002523681716233592\n",
      "Iteration 15527: loss = 0.0025236703497972393\n",
      "Iteration 15528: loss = 0.0025236589834854314\n",
      "Iteration 15529: loss = 0.0025236476172981683\n",
      "Iteration 15530: loss = 0.002523636251235447\n",
      "Iteration 15531: loss = 0.0025236248852972664\n",
      "Iteration 15532: loss = 0.0025236135194836255\n",
      "Iteration 15533: loss = 0.0025236021537945216\n",
      "Iteration 15534: loss = 0.002523590788229953\n",
      "Iteration 15535: loss = 0.00252357942278992\n",
      "Iteration 15536: loss = 0.0025235680574744197\n",
      "Iteration 15537: loss = 0.00252355669228345\n",
      "Iteration 15538: loss = 0.00252354532721701\n",
      "Iteration 15539: loss = 0.002523533962275098\n",
      "Iteration 15540: loss = 0.002523522597457712\n",
      "Iteration 15541: loss = 0.002523511232764851\n",
      "Iteration 15542: loss = 0.002523499868196513\n",
      "Iteration 15543: loss = 0.0025234885037526966\n",
      "Iteration 15544: loss = 0.0025234771394334\n",
      "Iteration 15545: loss = 0.002523465775238622\n",
      "Iteration 15546: loss = 0.0025234544111683607\n",
      "Iteration 15547: loss = 0.002523443047222614\n",
      "Iteration 15548: loss = 0.002523431683401381\n",
      "Iteration 15549: loss = 0.00252342031970466\n",
      "Iteration 15550: loss = 0.0025234089561324497\n",
      "Iteration 15551: loss = 0.0025233975926847473\n",
      "Iteration 15552: loss = 0.0025233862293615517\n",
      "Iteration 15553: loss = 0.0025233748661628625\n",
      "Iteration 15554: loss = 0.002523363503088677\n",
      "Iteration 15555: loss = 0.0025233521401389937\n",
      "Iteration 15556: loss = 0.0025233407773138108\n",
      "Iteration 15557: loss = 0.0025233294146131268\n",
      "Iteration 15558: loss = 0.0025233180520369413\n",
      "Iteration 15559: loss = 0.002523306689585251\n",
      "Iteration 15560: loss = 0.002523295327258055\n",
      "Iteration 15561: loss = 0.0025232839650553515\n",
      "Iteration 15562: loss = 0.002523272602977139\n",
      "Iteration 15563: loss = 0.0025232612410234167\n",
      "Iteration 15564: loss = 0.0025232498791941814\n",
      "Iteration 15565: loss = 0.0025232385174894325\n",
      "Iteration 15566: loss = 0.0025232271559091687\n",
      "Iteration 15567: loss = 0.0025232157944533877\n",
      "Iteration 15568: loss = 0.0025232044331220884\n",
      "Iteration 15569: loss = 0.002523193071915269\n",
      "Iteration 15570: loss = 0.0025231817108329275\n",
      "Iteration 15571: loss = 0.002523170349875063\n",
      "Iteration 15572: loss = 0.002523158989041673\n",
      "Iteration 15573: loss = 0.002523147628332757\n",
      "Iteration 15574: loss = 0.0025231362677483123\n",
      "Iteration 15575: loss = 0.0025231249072883385\n",
      "Iteration 15576: loss = 0.002523113546952833\n",
      "Iteration 15577: loss = 0.0025231021867417945\n",
      "Iteration 15578: loss = 0.002523090826655222\n",
      "Iteration 15579: loss = 0.0025230794666931127\n",
      "Iteration 15580: loss = 0.0025230681068554666\n",
      "Iteration 15581: loss = 0.00252305674714228\n",
      "Iteration 15582: loss = 0.0025230453875535537\n",
      "Iteration 15583: loss = 0.002523034028089284\n",
      "Iteration 15584: loss = 0.00252302266874947\n",
      "Iteration 15585: loss = 0.0025230113095341114\n",
      "Iteration 15586: loss = 0.0025229999504432043\n",
      "Iteration 15587: loss = 0.002522988591476749\n",
      "Iteration 15588: loss = 0.002522977232634743\n",
      "Iteration 15589: loss = 0.002522965873917185\n",
      "Iteration 15590: loss = 0.0025229545153240733\n",
      "Iteration 15591: loss = 0.002522943156855406\n",
      "Iteration 15592: loss = 0.0025229317985111816\n",
      "Iteration 15593: loss = 0.0025229204402914\n",
      "Iteration 15594: loss = 0.002522909082196057\n",
      "Iteration 15595: loss = 0.002522897724225153\n",
      "Iteration 15596: loss = 0.002522886366378685\n",
      "Iteration 15597: loss = 0.002522875008656652\n",
      "Iteration 15598: loss = 0.0025228636510590536\n",
      "Iteration 15599: loss = 0.002522852293585887\n",
      "Iteration 15600: loss = 0.0025228409362371505\n",
      "Iteration 15601: loss = 0.002522829579012842\n",
      "Iteration 15602: loss = 0.0025228182219129616\n",
      "Iteration 15603: loss = 0.002522806864937506\n",
      "Iteration 15604: loss = 0.0025227955080864752\n",
      "Iteration 15605: loss = 0.002522784151359866\n",
      "Iteration 15606: loss = 0.0025227727947576785\n",
      "Iteration 15607: loss = 0.0025227614382799094\n",
      "Iteration 15608: loss = 0.002522750081926558\n",
      "Iteration 15609: loss = 0.0025227387256976223\n",
      "Iteration 15610: loss = 0.0025227273695931016\n",
      "Iteration 15611: loss = 0.0025227160136129935\n",
      "Iteration 15612: loss = 0.0025227046577572963\n",
      "Iteration 15613: loss = 0.002522693302026009\n",
      "Iteration 15614: loss = 0.00252268194641913\n",
      "Iteration 15615: loss = 0.002522670590936657\n",
      "Iteration 15616: loss = 0.002522659235578589\n",
      "Iteration 15617: loss = 0.0025226478803449244\n",
      "Iteration 15618: loss = 0.002522636525235661\n",
      "Iteration 15619: loss = 0.0025226251702507975\n",
      "Iteration 15620: loss = 0.0025226138153903333\n",
      "Iteration 15621: loss = 0.0025226024606542653\n",
      "Iteration 15622: loss = 0.0025225911060425925\n",
      "Iteration 15623: loss = 0.002522579751555314\n",
      "Iteration 15624: loss = 0.002522568397192427\n",
      "Iteration 15625: loss = 0.0025225570429539307\n",
      "Iteration 15626: loss = 0.002522545688839823\n",
      "Iteration 15627: loss = 0.0025225343348501033\n",
      "Iteration 15628: loss = 0.0025225229809847692\n",
      "Iteration 15629: loss = 0.0025225116272438184\n",
      "Iteration 15630: loss = 0.002522500273627251\n",
      "Iteration 15631: loss = 0.0025224889201350643\n",
      "Iteration 15632: loss = 0.0025224775667672572\n",
      "Iteration 15633: loss = 0.002522466213523827\n",
      "Iteration 15634: loss = 0.002522454860404774\n",
      "Iteration 15635: loss = 0.0025224435074100954\n",
      "Iteration 15636: loss = 0.002522432154539789\n",
      "Iteration 15637: loss = 0.002522420801793855\n",
      "Iteration 15638: loss = 0.00252240944917229\n",
      "Iteration 15639: loss = 0.0025223980966750936\n",
      "Iteration 15640: loss = 0.002522386744302264\n",
      "Iteration 15641: loss = 0.0025223753920537995\n",
      "Iteration 15642: loss = 0.002522364039929698\n",
      "Iteration 15643: loss = 0.0025223526879299585\n",
      "Iteration 15644: loss = 0.0025223413360545796\n",
      "Iteration 15645: loss = 0.002522329984303559\n",
      "Iteration 15646: loss = 0.0025223186326768956\n",
      "Iteration 15647: loss = 0.002522307281174588\n",
      "Iteration 15648: loss = 0.002522295929796634\n",
      "Iteration 15649: loss = 0.002522284578543032\n",
      "Iteration 15650: loss = 0.0025222732274137816\n",
      "Iteration 15651: loss = 0.00252226187640888\n",
      "Iteration 15652: loss = 0.002522250525528326\n",
      "Iteration 15653: loss = 0.0025222391747721177\n",
      "Iteration 15654: loss = 0.002522227824140254\n",
      "Iteration 15655: loss = 0.0025222164736327327\n",
      "Iteration 15656: loss = 0.002522205123249553\n",
      "Iteration 15657: loss = 0.0025221937729907134\n",
      "Iteration 15658: loss = 0.002522182422856211\n",
      "Iteration 15659: loss = 0.0025221710728460453\n",
      "Iteration 15660: loss = 0.0025221597229602146\n",
      "Iteration 15661: loss = 0.0025221483731987173\n",
      "Iteration 15662: loss = 0.0025221370235615514\n",
      "Iteration 15663: loss = 0.0025221256740487153\n",
      "Iteration 15664: loss = 0.0025221143246602082\n",
      "Iteration 15665: loss = 0.0025221029753960283\n",
      "Iteration 15666: loss = 0.002522091626256173\n",
      "Iteration 15667: loss = 0.002522080277240642\n",
      "Iteration 15668: loss = 0.0025220689283494335\n",
      "Iteration 15669: loss = 0.0025220575795825446\n",
      "Iteration 15670: loss = 0.0025220462309399755\n",
      "Iteration 15671: loss = 0.002522034882421723\n",
      "Iteration 15672: loss = 0.0025220235340277877\n",
      "Iteration 15673: loss = 0.0025220121857581655\n",
      "Iteration 15674: loss = 0.002522000837612856\n",
      "Iteration 15675: loss = 0.002521989489591858\n",
      "Iteration 15676: loss = 0.0025219781416951693\n",
      "Iteration 15677: loss = 0.0025219667939227887\n",
      "Iteration 15678: loss = 0.002521955446274714\n",
      "Iteration 15679: loss = 0.0025219440987509444\n",
      "Iteration 15680: loss = 0.002521932751351478\n",
      "Iteration 15681: loss = 0.002521921404076313\n",
      "Iteration 15682: loss = 0.002521910056925448\n",
      "Iteration 15683: loss = 0.0025218987098988813\n",
      "Iteration 15684: loss = 0.0025218873629966115\n",
      "Iteration 15685: loss = 0.0025218760162186364\n",
      "Iteration 15686: loss = 0.0025218646695649555\n",
      "Iteration 15687: loss = 0.0025218533230355667\n",
      "Iteration 15688: loss = 0.0025218419766304683\n",
      "Iteration 15689: loss = 0.0025218306303496584\n",
      "Iteration 15690: loss = 0.002521819284193137\n",
      "Iteration 15691: loss = 0.0025218079381609\n",
      "Iteration 15692: loss = 0.0025217965922529483\n",
      "Iteration 15693: loss = 0.002521785246469278\n",
      "Iteration 15694: loss = 0.0025217739008098895\n",
      "Iteration 15695: loss = 0.00252176255527478\n",
      "Iteration 15696: loss = 0.0025217512098639483\n",
      "Iteration 15697: loss = 0.002521739864577393\n",
      "Iteration 15698: loss = 0.002521728519415112\n",
      "Iteration 15699: loss = 0.0025217171743771045\n",
      "Iteration 15700: loss = 0.002521705829463368\n",
      "Iteration 15701: loss = 0.0025216944846739022\n",
      "Iteration 15702: loss = 0.002521683140008704\n",
      "Iteration 15703: loss = 0.0025216717954677733\n",
      "Iteration 15704: loss = 0.002521660451051107\n",
      "Iteration 15705: loss = 0.0025216491067587047\n",
      "Iteration 15706: loss = 0.002521637762590564\n",
      "Iteration 15707: loss = 0.0025216264185466838\n",
      "Iteration 15708: loss = 0.0025216150746270633\n",
      "Iteration 15709: loss = 0.002521603730831699\n",
      "Iteration 15710: loss = 0.0025215923871605905\n",
      "Iteration 15711: loss = 0.0025215810436137365\n",
      "Iteration 15712: loss = 0.0025215697001911342\n",
      "Iteration 15713: loss = 0.0025215583568927846\n",
      "Iteration 15714: loss = 0.0025215470137186823\n",
      "Iteration 15715: loss = 0.002521535670668829\n",
      "Iteration 15716: loss = 0.0025215243277432217\n",
      "Iteration 15717: loss = 0.0025215129849418586\n",
      "Iteration 15718: loss = 0.002521501642264739\n",
      "Iteration 15719: loss = 0.0025214902997118607\n",
      "Iteration 15720: loss = 0.002521478957283222\n",
      "Iteration 15721: loss = 0.002521467614978822\n",
      "Iteration 15722: loss = 0.0025214562727986583\n",
      "Iteration 15723: loss = 0.00252144493074273\n",
      "Iteration 15724: loss = 0.0025214335888110355\n",
      "Iteration 15725: loss = 0.002521422247003573\n",
      "Iteration 15726: loss = 0.0025214109053203404\n",
      "Iteration 15727: loss = 0.0025213995637613368\n",
      "Iteration 15728: loss = 0.00252138822232656\n",
      "Iteration 15729: loss = 0.00252137688101601\n",
      "Iteration 15730: loss = 0.0025213655398296834\n",
      "Iteration 15731: loss = 0.002521354198767579\n",
      "Iteration 15732: loss = 0.0025213428578296955\n",
      "Iteration 15733: loss = 0.0025213315170160323\n",
      "Iteration 15734: loss = 0.002521320176326586\n",
      "Iteration 15735: loss = 0.0025213088357613563\n",
      "Iteration 15736: loss = 0.002521297495320341\n",
      "Iteration 15737: loss = 0.0025212861550035393\n",
      "Iteration 15738: loss = 0.0025212748148109483\n",
      "Iteration 15739: loss = 0.002521263474742568\n",
      "Iteration 15740: loss = 0.0025212521347983954\n",
      "Iteration 15741: loss = 0.0025212407949784296\n",
      "Iteration 15742: loss = 0.0025212294552826686\n",
      "Iteration 15743: loss = 0.002521218115711112\n",
      "Iteration 15744: loss = 0.002521206776263757\n",
      "Iteration 15745: loss = 0.002521195436940602\n",
      "Iteration 15746: loss = 0.002521184097741647\n",
      "Iteration 15747: loss = 0.0025211727586668883\n",
      "Iteration 15748: loss = 0.0025211614197163256\n",
      "Iteration 15749: loss = 0.0025211500808899573\n",
      "Iteration 15750: loss = 0.002521138742187781\n",
      "Iteration 15751: loss = 0.0025211274036097964\n",
      "Iteration 15752: loss = 0.0025211160651560003\n",
      "Iteration 15753: loss = 0.0025211047268263926\n",
      "Iteration 15754: loss = 0.0025210933886209708\n",
      "Iteration 15755: loss = 0.002521082050539734\n",
      "Iteration 15756: loss = 0.00252107071258268\n",
      "Iteration 15757: loss = 0.002521059374749808\n",
      "Iteration 15758: loss = 0.0025210480370411155\n",
      "Iteration 15759: loss = 0.0025210366994566015\n",
      "Iteration 15760: loss = 0.0025210253619962646\n",
      "Iteration 15761: loss = 0.0025210140246601028\n",
      "Iteration 15762: loss = 0.0025210026874481145\n",
      "Iteration 15763: loss = 0.002520991350360298\n",
      "Iteration 15764: loss = 0.0025209800133966526\n",
      "Iteration 15765: loss = 0.0025209686765571755\n",
      "Iteration 15766: loss = 0.0025209573398418668\n",
      "Iteration 15767: loss = 0.0025209460032507226\n",
      "Iteration 15768: loss = 0.002520934666783743\n",
      "Iteration 15769: loss = 0.002520923330440927\n",
      "Iteration 15770: loss = 0.0025209119942222707\n",
      "Iteration 15771: loss = 0.002520900658127775\n",
      "Iteration 15772: loss = 0.0025208893221574363\n",
      "Iteration 15773: loss = 0.002520877986311255\n",
      "Iteration 15774: loss = 0.002520866650589228\n",
      "Iteration 15775: loss = 0.002520855314991354\n",
      "Iteration 15776: loss = 0.0025208439795176315\n",
      "Iteration 15777: loss = 0.002520832644168059\n",
      "Iteration 15778: loss = 0.0025208213089426353\n",
      "Iteration 15779: loss = 0.002520809973841359\n",
      "Iteration 15780: loss = 0.002520798638864227\n",
      "Iteration 15781: loss = 0.0025207873040112394\n",
      "Iteration 15782: loss = 0.0025207759692823938\n",
      "Iteration 15783: loss = 0.002520764634677689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15784: loss = 0.002520753300197123\n",
      "Iteration 15785: loss = 0.0025207419658406947\n",
      "Iteration 15786: loss = 0.0025207306316084024\n",
      "Iteration 15787: loss = 0.0025207192975002443\n",
      "Iteration 15788: loss = 0.0025207079635162187\n",
      "Iteration 15789: loss = 0.0025206966296563247\n",
      "Iteration 15790: loss = 0.0025206852959205606\n",
      "Iteration 15791: loss = 0.0025206739623089237\n",
      "Iteration 15792: loss = 0.0025206626288214137\n",
      "Iteration 15793: loss = 0.0025206512954580288\n",
      "Iteration 15794: loss = 0.002520639962218767\n",
      "Iteration 15795: loss = 0.002520628629103627\n",
      "Iteration 15796: loss = 0.0025206172961126073\n",
      "Iteration 15797: loss = 0.002520605963245706\n",
      "Iteration 15798: loss = 0.002520594630502922\n",
      "Iteration 15799: loss = 0.002520583297884254\n",
      "Iteration 15800: loss = 0.0025205719653896993\n",
      "Iteration 15801: loss = 0.002520560633019257\n",
      "Iteration 15802: loss = 0.0025205493007729256\n",
      "Iteration 15803: loss = 0.002520537968650703\n",
      "Iteration 15804: loss = 0.0025205266366525887\n",
      "Iteration 15805: loss = 0.0025205153047785805\n",
      "Iteration 15806: loss = 0.002520503973028676\n",
      "Iteration 15807: loss = 0.002520492641402875\n",
      "Iteration 15808: loss = 0.0025204813099011752\n",
      "Iteration 15809: loss = 0.0025204699785235755\n",
      "Iteration 15810: loss = 0.002520458647270074\n",
      "Iteration 15811: loss = 0.002520447316140669\n",
      "Iteration 15812: loss = 0.0025204359851353593\n",
      "Iteration 15813: loss = 0.002520424654254143\n",
      "Iteration 15814: loss = 0.002520413323497019\n",
      "Iteration 15815: loss = 0.002520401992863985\n",
      "Iteration 15816: loss = 0.0025203906623550397\n",
      "Iteration 15817: loss = 0.002520379331970182\n",
      "Iteration 15818: loss = 0.00252036800170941\n",
      "Iteration 15819: loss = 0.0025203566715727216\n",
      "Iteration 15820: loss = 0.0025203453415601164\n",
      "Iteration 15821: loss = 0.002520334011671592\n",
      "Iteration 15822: loss = 0.002520322681907147\n",
      "Iteration 15823: loss = 0.00252031135226678\n",
      "Iteration 15824: loss = 0.002520300022750489\n",
      "Iteration 15825: loss = 0.002520288693358273\n",
      "Iteration 15826: loss = 0.00252027736409013\n",
      "Iteration 15827: loss = 0.0025202660349460584\n",
      "Iteration 15828: loss = 0.002520254705926057\n",
      "Iteration 15829: loss = 0.0025202433770301243\n",
      "Iteration 15830: loss = 0.0025202320482582584\n",
      "Iteration 15831: loss = 0.0025202207196104577\n",
      "Iteration 15832: loss = 0.002520209391086721\n",
      "Iteration 15833: loss = 0.002520198062687046\n",
      "Iteration 15834: loss = 0.002520186734411432\n",
      "Iteration 15835: loss = 0.0025201754062598772\n",
      "Iteration 15836: loss = 0.0025201640782323803\n",
      "Iteration 15837: loss = 0.0025201527503289386\n",
      "Iteration 15838: loss = 0.002520141422549551\n",
      "Iteration 15839: loss = 0.002520130094894217\n",
      "Iteration 15840: loss = 0.0025201187673629336\n",
      "Iteration 15841: loss = 0.0025201074399557\n",
      "Iteration 15842: loss = 0.0025200961126725153\n",
      "Iteration 15843: loss = 0.0025200847855133763\n",
      "Iteration 15844: loss = 0.002520073458478283\n",
      "Iteration 15845: loss = 0.002520062131567232\n",
      "Iteration 15846: loss = 0.0025200508047802236\n",
      "Iteration 15847: loss = 0.002520039478117256\n",
      "Iteration 15848: loss = 0.0025200281515783264\n",
      "Iteration 15849: loss = 0.002520016825163434\n",
      "Iteration 15850: loss = 0.002520005498872577\n",
      "Iteration 15851: loss = 0.002519994172705755\n",
      "Iteration 15852: loss = 0.0025199828466629646\n",
      "Iteration 15853: loss = 0.0025199715207442053\n",
      "Iteration 15854: loss = 0.0025199601949494753\n",
      "Iteration 15855: loss = 0.0025199488692787732\n",
      "Iteration 15856: loss = 0.0025199375437320974\n",
      "Iteration 15857: loss = 0.002519926218309446\n",
      "Iteration 15858: loss = 0.002519914893010818\n",
      "Iteration 15859: loss = 0.002519903567836211\n",
      "Iteration 15860: loss = 0.0025198922427856246\n",
      "Iteration 15861: loss = 0.002519880917859057\n",
      "Iteration 15862: loss = 0.002519869593056505\n",
      "Iteration 15863: loss = 0.002519858268377969\n",
      "Iteration 15864: loss = 0.002519846943823447\n",
      "Iteration 15865: loss = 0.0025198356193929365\n",
      "Iteration 15866: loss = 0.002519824295086437\n",
      "Iteration 15867: loss = 0.002519812970903947\n",
      "Iteration 15868: loss = 0.0025198016468454634\n",
      "Iteration 15869: loss = 0.002519790322910986\n",
      "Iteration 15870: loss = 0.0025197789991005136\n",
      "Iteration 15871: loss = 0.0025197676754140433\n",
      "Iteration 15872: loss = 0.0025197563518515745\n",
      "Iteration 15873: loss = 0.0025197450284131055\n",
      "Iteration 15874: loss = 0.0025197337050986346\n",
      "Iteration 15875: loss = 0.00251972238190816\n",
      "Iteration 15876: loss = 0.0025197110588416807\n",
      "Iteration 15877: loss = 0.0025196997358991947\n",
      "Iteration 15878: loss = 0.0025196884130807007\n",
      "Iteration 15879: loss = 0.0025196770903861965\n",
      "Iteration 15880: loss = 0.002519665767815681\n",
      "Iteration 15881: loss = 0.002519654445369153\n",
      "Iteration 15882: loss = 0.0025196431230466113\n",
      "Iteration 15883: loss = 0.002519631800848053\n",
      "Iteration 15884: loss = 0.0025196204787734772\n",
      "Iteration 15885: loss = 0.0025196091568228827\n",
      "Iteration 15886: loss = 0.0025195978349962667\n",
      "Iteration 15887: loss = 0.0025195865132936293\n",
      "Iteration 15888: loss = 0.0025195751917149677\n",
      "Iteration 15889: loss = 0.0025195638702602816\n",
      "Iteration 15890: loss = 0.002519552548929568\n",
      "Iteration 15891: loss = 0.002519541227722826\n",
      "Iteration 15892: loss = 0.0025195299066400542\n",
      "Iteration 15893: loss = 0.002519518585681251\n",
      "Iteration 15894: loss = 0.002519507264846414\n",
      "Iteration 15895: loss = 0.0025194959441355433\n",
      "Iteration 15896: loss = 0.0025194846235486357\n",
      "Iteration 15897: loss = 0.002519473303085691\n",
      "Iteration 15898: loss = 0.0025194619827467067\n",
      "Iteration 15899: loss = 0.0025194506625316805\n",
      "Iteration 15900: loss = 0.0025194393424406132\n",
      "Iteration 15901: loss = 0.0025194280224735016\n",
      "Iteration 15902: loss = 0.0025194167026303445\n",
      "Iteration 15903: loss = 0.0025194053829111396\n",
      "Iteration 15904: loss = 0.002519394063315887\n",
      "Iteration 15905: loss = 0.0025193827438445837\n",
      "Iteration 15906: loss = 0.002519371424497229\n",
      "Iteration 15907: loss = 0.0025193601052738205\n",
      "Iteration 15908: loss = 0.002519348786174357\n",
      "Iteration 15909: loss = 0.0025193374671988373\n",
      "Iteration 15910: loss = 0.0025193261483472595\n",
      "Iteration 15911: loss = 0.002519314829619622\n",
      "Iteration 15912: loss = 0.0025193035110159237\n",
      "Iteration 15913: loss = 0.0025192921925361626\n",
      "Iteration 15914: loss = 0.002519280874180337\n",
      "Iteration 15915: loss = 0.002519269555948446\n",
      "Iteration 15916: loss = 0.0025192582378404877\n",
      "Iteration 15917: loss = 0.0025192469198564603\n",
      "Iteration 15918: loss = 0.0025192356019963624\n",
      "Iteration 15919: loss = 0.0025192242842601922\n",
      "Iteration 15920: loss = 0.002519212966647949\n",
      "Iteration 15921: loss = 0.00251920164915963\n",
      "Iteration 15922: loss = 0.0025191903317952347\n",
      "Iteration 15923: loss = 0.002519179014554761\n",
      "Iteration 15924: loss = 0.0025191676974382076\n",
      "Iteration 15925: loss = 0.0025191563804455724\n",
      "Iteration 15926: loss = 0.0025191450635768556\n",
      "Iteration 15927: loss = 0.002519133746832053\n",
      "Iteration 15928: loss = 0.0025191224302111652\n",
      "Iteration 15929: loss = 0.002519111113714189\n",
      "Iteration 15930: loss = 0.0025190997973411245\n",
      "Iteration 15931: loss = 0.002519088481091969\n",
      "Iteration 15932: loss = 0.002519077164966721\n",
      "Iteration 15933: loss = 0.0025190658489653794\n",
      "Iteration 15934: loss = 0.002519054533087943\n",
      "Iteration 15935: loss = 0.002519043217334409\n",
      "Iteration 15936: loss = 0.002519031901704777\n",
      "Iteration 15937: loss = 0.0025190205861990445\n",
      "Iteration 15938: loss = 0.0025190092708172107\n",
      "Iteration 15939: loss = 0.0025189979555592735\n",
      "Iteration 15940: loss = 0.0025189866404252315\n",
      "Iteration 15941: loss = 0.002518975325415084\n",
      "Iteration 15942: loss = 0.002518964010528829\n",
      "Iteration 15943: loss = 0.002518952695766463\n",
      "Iteration 15944: loss = 0.0025189413811279875\n",
      "Iteration 15945: loss = 0.002518930066613399\n",
      "Iteration 15946: loss = 0.0025189187522226967\n",
      "Iteration 15947: loss = 0.0025189074379558794\n",
      "Iteration 15948: loss = 0.0025188961238129443\n",
      "Iteration 15949: loss = 0.002518884809793891\n",
      "Iteration 15950: loss = 0.0025188734958987167\n",
      "Iteration 15951: loss = 0.002518862182127421\n",
      "Iteration 15952: loss = 0.002518850868480002\n",
      "Iteration 15953: loss = 0.002518839554956459\n",
      "Iteration 15954: loss = 0.0025188282415567883\n",
      "Iteration 15955: loss = 0.0025188169282809904\n",
      "Iteration 15956: loss = 0.002518805615129063\n",
      "Iteration 15957: loss = 0.0025187943021010043\n",
      "Iteration 15958: loss = 0.002518782989196813\n",
      "Iteration 15959: loss = 0.002518771676416487\n",
      "Iteration 15960: loss = 0.002518760363760026\n",
      "Iteration 15961: loss = 0.002518749051227427\n",
      "Iteration 15962: loss = 0.00251873773881869\n",
      "Iteration 15963: loss = 0.0025187264265338124\n",
      "Iteration 15964: loss = 0.002518715114372793\n",
      "Iteration 15965: loss = 0.002518703802335629\n",
      "Iteration 15966: loss = 0.0025186924904223216\n",
      "Iteration 15967: loss = 0.002518681178632867\n",
      "Iteration 15968: loss = 0.0025186698669672643\n",
      "Iteration 15969: loss = 0.002518658555425511\n",
      "Iteration 15970: loss = 0.0025186472440076072\n",
      "Iteration 15971: loss = 0.002518635932713551\n",
      "Iteration 15972: loss = 0.0025186246215433395\n",
      "Iteration 15973: loss = 0.0025186133104969726\n",
      "Iteration 15974: loss = 0.0025186019995744485\n",
      "Iteration 15975: loss = 0.002518590688775765\n",
      "Iteration 15976: loss = 0.002518579378100921\n",
      "Iteration 15977: loss = 0.0025185680675499154\n",
      "Iteration 15978: loss = 0.002518556757122746\n",
      "Iteration 15979: loss = 0.0025185454468194114\n",
      "Iteration 15980: loss = 0.00251853413663991\n",
      "Iteration 15981: loss = 0.00251852282658424\n",
      "Iteration 15982: loss = 0.0025185115166524003\n",
      "Iteration 15983: loss = 0.0025185002068443896\n",
      "Iteration 15984: loss = 0.0025184888971602052\n",
      "Iteration 15985: loss = 0.002518477587599847\n",
      "Iteration 15986: loss = 0.0025184662781633127\n",
      "Iteration 15987: loss = 0.0025184549688506003\n",
      "Iteration 15988: loss = 0.002518443659661709\n",
      "Iteration 15989: loss = 0.002518432350596638\n",
      "Iteration 15990: loss = 0.0025184210416553834\n",
      "Iteration 15991: loss = 0.0025184097328379454\n",
      "Iteration 15992: loss = 0.002518398424144322\n",
      "Iteration 15993: loss = 0.002518387115574512\n",
      "Iteration 15994: loss = 0.002518375807128514\n",
      "Iteration 15995: loss = 0.002518364498806325\n",
      "Iteration 15996: loss = 0.002518353190607945\n",
      "Iteration 15997: loss = 0.0025183418825333722\n",
      "Iteration 15998: loss = 0.0025183305745826046\n",
      "Iteration 15999: loss = 0.0025183192667556403\n",
      "Iteration 16000: loss = 0.002518307959052479\n",
      "Iteration 16001: loss = 0.0025182966514731182\n",
      "Iteration 16002: loss = 0.0025182853440175565\n",
      "Iteration 16003: loss = 0.002518274036685792\n",
      "Iteration 16004: loss = 0.0025182627294778248\n",
      "Iteration 16005: loss = 0.0025182514223936517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16006: loss = 0.0025182401154332716\n",
      "Iteration 16007: loss = 0.002518228808596682\n",
      "Iteration 16008: loss = 0.002518217501883883\n",
      "Iteration 16009: loss = 0.002518206195294873\n",
      "Iteration 16010: loss = 0.002518194888829649\n",
      "Iteration 16011: loss = 0.0025181835824882107\n",
      "Iteration 16012: loss = 0.002518172276270556\n",
      "Iteration 16013: loss = 0.002518160970176683\n",
      "Iteration 16014: loss = 0.002518149664206591\n",
      "Iteration 16015: loss = 0.002518138358360278\n",
      "Iteration 16016: loss = 0.002518127052637743\n",
      "Iteration 16017: loss = 0.0025181157470389836\n",
      "Iteration 16018: loss = 0.002518104441563999\n",
      "Iteration 16019: loss = 0.0025180931362127866\n",
      "Iteration 16020: loss = 0.002518081830985346\n",
      "Iteration 16021: loss = 0.002518070525881675\n",
      "Iteration 16022: loss = 0.0025180592209017726\n",
      "Iteration 16023: loss = 0.0025180479160456367\n",
      "Iteration 16024: loss = 0.002518036611313266\n",
      "Iteration 16025: loss = 0.002518025306704659\n",
      "Iteration 16026: loss = 0.002518014002219814\n",
      "Iteration 16027: loss = 0.0025180026978587297\n",
      "Iteration 16028: loss = 0.002517991393621405\n",
      "Iteration 16029: loss = 0.0025179800895078364\n",
      "Iteration 16030: loss = 0.0025179687855180245\n",
      "Iteration 16031: loss = 0.0025179574816519665\n",
      "Iteration 16032: loss = 0.002517946177909662\n",
      "Iteration 16033: loss = 0.002517934874291108\n",
      "Iteration 16034: loss = 0.0025179235707963044\n",
      "Iteration 16035: loss = 0.002517912267425249\n",
      "Iteration 16036: loss = 0.0025179009641779393\n",
      "Iteration 16037: loss = 0.0025178896610543755\n",
      "Iteration 16038: loss = 0.002517878358054555\n",
      "Iteration 16039: loss = 0.002517867055178477\n",
      "Iteration 16040: loss = 0.002517855752426139\n",
      "Iteration 16041: loss = 0.00251784444979754\n",
      "Iteration 16042: loss = 0.0025178331472926778\n",
      "Iteration 16043: loss = 0.0025178218449115526\n",
      "Iteration 16044: loss = 0.0025178105426541604\n",
      "Iteration 16045: loss = 0.0025177992405205018\n",
      "Iteration 16046: loss = 0.0025177879385105744\n",
      "Iteration 16047: loss = 0.0025177766366243767\n",
      "Iteration 16048: loss = 0.002517765334861907\n",
      "Iteration 16049: loss = 0.0025177540332231635\n",
      "Iteration 16050: loss = 0.0025177427317081454\n",
      "Iteration 16051: loss = 0.0025177314303168508\n",
      "Iteration 16052: loss = 0.002517720129049278\n",
      "Iteration 16053: loss = 0.0025177088279054257\n",
      "Iteration 16054: loss = 0.0025176975268852925\n",
      "Iteration 16055: loss = 0.002517686225988876\n",
      "Iteration 16056: loss = 0.002517674925216176\n",
      "Iteration 16057: loss = 0.00251766362456719\n",
      "Iteration 16058: loss = 0.0025176523240419166\n",
      "Iteration 16059: loss = 0.002517641023640354\n",
      "Iteration 16060: loss = 0.002517629723362501\n",
      "Iteration 16061: loss = 0.0025176184232083565\n",
      "Iteration 16062: loss = 0.0025176071231779182\n",
      "Iteration 16063: loss = 0.0025175958232711852\n",
      "Iteration 16064: loss = 0.0025175845234881554\n",
      "Iteration 16065: loss = 0.002517573223828828\n",
      "Iteration 16066: loss = 0.0025175619242932008\n",
      "Iteration 16067: loss = 0.002517550624881272\n",
      "Iteration 16068: loss = 0.0025175393255930413\n",
      "Iteration 16069: loss = 0.002517528026428505\n",
      "Iteration 16070: loss = 0.002517516727387664\n",
      "Iteration 16071: loss = 0.0025175054284705158\n",
      "Iteration 16072: loss = 0.002517494129677058\n",
      "Iteration 16073: loss = 0.00251748283100729\n",
      "Iteration 16074: loss = 0.0025174715324612095\n",
      "Iteration 16075: loss = 0.002517460234038816\n",
      "Iteration 16076: loss = 0.002517448935740108\n",
      "Iteration 16077: loss = 0.002517437637565083\n",
      "Iteration 16078: loss = 0.0025174263395137393\n",
      "Iteration 16079: loss = 0.002517415041586077\n",
      "Iteration 16080: loss = 0.002517403743782093\n",
      "Iteration 16081: loss = 0.002517392446101786\n",
      "Iteration 16082: loss = 0.002517381148545155\n",
      "Iteration 16083: loss = 0.0025173698511121983\n",
      "Iteration 16084: loss = 0.0025173585538029136\n",
      "Iteration 16085: loss = 0.002517347256617301\n",
      "Iteration 16086: loss = 0.002517335959555357\n",
      "Iteration 16087: loss = 0.0025173246626170815\n",
      "Iteration 16088: loss = 0.002517313365802472\n",
      "Iteration 16089: loss = 0.0025173020691115285\n",
      "Iteration 16090: loss = 0.0025172907725442475\n",
      "Iteration 16091: loss = 0.0025172794761006284\n",
      "Iteration 16092: loss = 0.0025172681797806695\n",
      "Iteration 16093: loss = 0.0025172568835843696\n",
      "Iteration 16094: loss = 0.0025172455875117276\n",
      "Iteration 16095: loss = 0.0025172342915627407\n",
      "Iteration 16096: loss = 0.0025172229957374074\n",
      "Iteration 16097: loss = 0.002517211700035728\n",
      "Iteration 16098: loss = 0.002517200404457699\n",
      "Iteration 16099: loss = 0.00251718910900332\n",
      "Iteration 16100: loss = 0.002517177813672588\n",
      "Iteration 16101: loss = 0.002517166518465503\n",
      "Iteration 16102: loss = 0.0025171552233820634\n",
      "Iteration 16103: loss = 0.0025171439284222665\n",
      "Iteration 16104: loss = 0.002517132633586112\n",
      "Iteration 16105: loss = 0.002517121338873598\n",
      "Iteration 16106: loss = 0.002517110044284722\n",
      "Iteration 16107: loss = 0.002517098749819484\n",
      "Iteration 16108: loss = 0.002517087455477881\n",
      "Iteration 16109: loss = 0.0025170761612599126\n",
      "Iteration 16110: loss = 0.002517064867165577\n",
      "Iteration 16111: loss = 0.0025170535731948726\n",
      "Iteration 16112: loss = 0.002517042279347797\n",
      "Iteration 16113: loss = 0.0025170309856243508\n",
      "Iteration 16114: loss = 0.00251701969202453\n",
      "Iteration 16115: loss = 0.0025170083985483344\n",
      "Iteration 16116: loss = 0.0025169971051957622\n",
      "Iteration 16117: loss = 0.0025169858119668117\n",
      "Iteration 16118: loss = 0.002516974518861482\n",
      "Iteration 16119: loss = 0.0025169632258797703\n",
      "Iteration 16120: loss = 0.0025169519330216768\n",
      "Iteration 16121: loss = 0.0025169406402871988\n",
      "Iteration 16122: loss = 0.002516929347676335\n",
      "Iteration 16123: loss = 0.0025169180551890833\n",
      "Iteration 16124: loss = 0.0025169067628254436\n",
      "Iteration 16125: loss = 0.002516895470585413\n",
      "Iteration 16126: loss = 0.0025168841784689905\n",
      "Iteration 16127: loss = 0.0025168728864761744\n",
      "Iteration 16128: loss = 0.0025168615946069635\n",
      "Iteration 16129: loss = 0.0025168503028613564\n",
      "Iteration 16130: loss = 0.002516839011239351\n",
      "Iteration 16131: loss = 0.002516827719740946\n",
      "Iteration 16132: loss = 0.0025168164283661393\n",
      "Iteration 16133: loss = 0.0025168051371149305\n",
      "Iteration 16134: loss = 0.002516793845987317\n",
      "Iteration 16135: loss = 0.002516782554983298\n",
      "Iteration 16136: loss = 0.002516771264102872\n",
      "Iteration 16137: loss = 0.002516759973346037\n",
      "Iteration 16138: loss = 0.0025167486827127918\n",
      "Iteration 16139: loss = 0.0025167373922031343\n",
      "Iteration 16140: loss = 0.0025167261018170634\n",
      "Iteration 16141: loss = 0.0025167148115545776\n",
      "Iteration 16142: loss = 0.002516703521415676\n",
      "Iteration 16143: loss = 0.0025166922314003555\n",
      "Iteration 16144: loss = 0.0025166809415086157\n",
      "Iteration 16145: loss = 0.0025166696517404546\n",
      "Iteration 16146: loss = 0.0025166583620958713\n",
      "Iteration 16147: loss = 0.0025166470725748635\n",
      "Iteration 16148: loss = 0.00251663578317743\n",
      "Iteration 16149: loss = 0.0025166244939035694\n",
      "Iteration 16150: loss = 0.00251661320475328\n",
      "Iteration 16151: loss = 0.002516601915726561\n",
      "Iteration 16152: loss = 0.002516590626823409\n",
      "Iteration 16153: loss = 0.002516579338043824\n",
      "Iteration 16154: loss = 0.0025165680493878045\n",
      "Iteration 16155: loss = 0.002516556760855348\n",
      "Iteration 16156: loss = 0.002516545472446454\n",
      "Iteration 16157: loss = 0.0025165341841611203\n",
      "Iteration 16158: loss = 0.0025165228959993457\n",
      "Iteration 16159: loss = 0.002516511607961129\n",
      "Iteration 16160: loss = 0.0025165003200464673\n",
      "Iteration 16161: loss = 0.00251648903225536\n",
      "Iteration 16162: loss = 0.002516477744587806\n",
      "Iteration 16163: loss = 0.002516466457043803\n",
      "Iteration 16164: loss = 0.00251645516962335\n",
      "Iteration 16165: loss = 0.002516443882326445\n",
      "Iteration 16166: loss = 0.0025164325951530873\n",
      "Iteration 16167: loss = 0.0025164213081032743\n",
      "Iteration 16168: loss = 0.0025164100211770043\n",
      "Iteration 16169: loss = 0.002516398734374278\n",
      "Iteration 16170: loss = 0.002516387447695091\n",
      "Iteration 16171: loss = 0.0025163761611394437\n",
      "Iteration 16172: loss = 0.0025163648747073336\n",
      "Iteration 16173: loss = 0.0025163535883987597\n",
      "Iteration 16174: loss = 0.0025163423022137197\n",
      "Iteration 16175: loss = 0.002516331016152214\n",
      "Iteration 16176: loss = 0.0025163197302142384\n",
      "Iteration 16177: loss = 0.0025163084443997927\n",
      "Iteration 16178: loss = 0.0025162971587088757\n",
      "Iteration 16179: loss = 0.002516285873141486\n",
      "Iteration 16180: loss = 0.002516274587697621\n",
      "Iteration 16181: loss = 0.0025162633023772794\n",
      "Iteration 16182: loss = 0.0025162520171804603\n",
      "Iteration 16183: loss = 0.0025162407321071617\n",
      "Iteration 16184: loss = 0.0025162294471573828\n",
      "Iteration 16185: loss = 0.0025162181623311214\n",
      "Iteration 16186: loss = 0.0025162068776283754\n",
      "Iteration 16187: loss = 0.002516195593049145\n",
      "Iteration 16188: loss = 0.0025161843085934267\n",
      "Iteration 16189: loss = 0.0025161730242612202\n",
      "Iteration 16190: loss = 0.002516161740052524\n",
      "Iteration 16191: loss = 0.002516150455967336\n",
      "Iteration 16192: loss = 0.0025161391720056546\n",
      "Iteration 16193: loss = 0.002516127888167479\n",
      "Iteration 16194: loss = 0.0025161166044528074\n",
      "Iteration 16195: loss = 0.002516105320861638\n",
      "Iteration 16196: loss = 0.002516094037393969\n",
      "Iteration 16197: loss = 0.0025160827540498\n",
      "Iteration 16198: loss = 0.0025160714708291274\n",
      "Iteration 16199: loss = 0.0025160601877319525\n",
      "Iteration 16200: loss = 0.0025160489047582712\n",
      "Iteration 16201: loss = 0.0025160376219080837\n",
      "Iteration 16202: loss = 0.0025160263391813876\n",
      "Iteration 16203: loss = 0.002516015056578181\n",
      "Iteration 16204: loss = 0.002516003774098464\n",
      "Iteration 16205: loss = 0.0025159924917422336\n",
      "Iteration 16206: loss = 0.0025159812095094884\n",
      "Iteration 16207: loss = 0.002515969927400227\n",
      "Iteration 16208: loss = 0.002515958645414449\n",
      "Iteration 16209: loss = 0.002515947363552151\n",
      "Iteration 16210: loss = 0.002515936081813333\n",
      "Iteration 16211: loss = 0.0025159248001979926\n",
      "Iteration 16212: loss = 0.002515913518706129\n",
      "Iteration 16213: loss = 0.002515902237337739\n",
      "Iteration 16214: loss = 0.002515890956092823\n",
      "Iteration 16215: loss = 0.002515879674971379\n",
      "Iteration 16216: loss = 0.0025158683939734048\n",
      "Iteration 16217: loss = 0.0025158571130989\n",
      "Iteration 16218: loss = 0.0025158458323478613\n",
      "Iteration 16219: loss = 0.002515834551720289\n",
      "Iteration 16220: loss = 0.002515823271216181\n",
      "Iteration 16221: loss = 0.0025158119908355345\n",
      "Iteration 16222: loss = 0.00251580071057835\n",
      "Iteration 16223: loss = 0.002515789430444625\n",
      "Iteration 16224: loss = 0.0025157781504343574\n",
      "Iteration 16225: loss = 0.002515766870547547\n",
      "Iteration 16226: loss = 0.0025157555907841904\n",
      "Iteration 16227: loss = 0.002515744311144288\n",
      "Iteration 16228: loss = 0.0025157330316278375\n",
      "Iteration 16229: loss = 0.0025157217522348376\n",
      "Iteration 16230: loss = 0.002515710472965286\n",
      "Iteration 16231: loss = 0.0025156991938191818\n",
      "Iteration 16232: loss = 0.002515687914796523\n",
      "Iteration 16233: loss = 0.0025156766358973093\n",
      "Iteration 16234: loss = 0.0025156653571215383\n",
      "Iteration 16235: loss = 0.0025156540784692085\n",
      "Iteration 16236: loss = 0.0025156427999403178\n",
      "Iteration 16237: loss = 0.0025156315215348656\n",
      "Iteration 16238: loss = 0.0025156202432528503\n",
      "Iteration 16239: loss = 0.002515608965094269\n",
      "Iteration 16240: loss = 0.0025155976870591223\n",
      "Iteration 16241: loss = 0.0025155864091474076\n",
      "Iteration 16242: loss = 0.002515575131359123\n",
      "Iteration 16243: loss = 0.0025155638536942674\n",
      "Iteration 16244: loss = 0.0025155525761528397\n",
      "Iteration 16245: loss = 0.002515541298734838\n",
      "Iteration 16246: loss = 0.002515530021440261\n",
      "Iteration 16247: loss = 0.0025155187442691056\n",
      "Iteration 16248: loss = 0.002515507467221373\n",
      "Iteration 16249: loss = 0.0025154961902970592\n",
      "Iteration 16250: loss = 0.002515484913496164\n",
      "Iteration 16251: loss = 0.0025154736368186857\n",
      "Iteration 16252: loss = 0.002515462360264623\n",
      "Iteration 16253: loss = 0.002515451083833973\n",
      "Iteration 16254: loss = 0.002515439807526737\n",
      "Iteration 16255: loss = 0.0025154285313429103\n",
      "Iteration 16256: loss = 0.0025154172552824925\n",
      "Iteration 16257: loss = 0.0025154059793454833\n",
      "Iteration 16258: loss = 0.00251539470353188\n",
      "Iteration 16259: loss = 0.0025153834278416813\n",
      "Iteration 16260: loss = 0.0025153721522748855\n",
      "Iteration 16261: loss = 0.002515360876831491\n",
      "Iteration 16262: loss = 0.002515349601511497\n",
      "Iteration 16263: loss = 0.0025153383263149015\n",
      "Iteration 16264: loss = 0.0025153270512417026\n",
      "Iteration 16265: loss = 0.002515315776291899\n",
      "Iteration 16266: loss = 0.00251530450146549\n",
      "Iteration 16267: loss = 0.0025152932267624734\n",
      "Iteration 16268: loss = 0.0025152819521828476\n",
      "Iteration 16269: loss = 0.002515270677726611\n",
      "Iteration 16270: loss = 0.002515259403393762\n",
      "Iteration 16271: loss = 0.0025152481291842994\n",
      "Iteration 16272: loss = 0.002515236855098222\n",
      "Iteration 16273: loss = 0.0025152255811355276\n",
      "Iteration 16274: loss = 0.002515214307296215\n",
      "Iteration 16275: loss = 0.0025152030335802824\n",
      "Iteration 16276: loss = 0.0025151917599877285\n",
      "Iteration 16277: loss = 0.002515180486518552\n",
      "Iteration 16278: loss = 0.002515169213172751\n",
      "Iteration 16279: loss = 0.0025151579399503242\n",
      "Iteration 16280: loss = 0.00251514666685127\n",
      "Iteration 16281: loss = 0.002515135393875587\n",
      "Iteration 16282: loss = 0.002515124121023274\n",
      "Iteration 16283: loss = 0.002515112848294328\n",
      "Iteration 16284: loss = 0.0025151015756887493\n",
      "Iteration 16285: loss = 0.0025150903032065356\n",
      "Iteration 16286: loss = 0.0025150790308476846\n",
      "Iteration 16287: loss = 0.0025150677586121962\n",
      "Iteration 16288: loss = 0.0025150564865000684\n",
      "Iteration 16289: loss = 0.0025150452145112994\n",
      "Iteration 16290: loss = 0.002515033942645888\n",
      "Iteration 16291: loss = 0.002515022670903831\n",
      "Iteration 16292: loss = 0.00251501139928513\n",
      "Iteration 16293: loss = 0.0025150001277897812\n",
      "Iteration 16294: loss = 0.002514988856417784\n",
      "Iteration 16295: loss = 0.0025149775851691365\n",
      "Iteration 16296: loss = 0.002514966314043837\n",
      "Iteration 16297: loss = 0.0025149550430418836\n",
      "Iteration 16298: loss = 0.0025149437721632764\n",
      "Iteration 16299: loss = 0.002514932501408013\n",
      "Iteration 16300: loss = 0.002514921230776091\n",
      "Iteration 16301: loss = 0.0025149099602675105\n",
      "Iteration 16302: loss = 0.0025148986898822687\n",
      "Iteration 16303: loss = 0.0025148874196203644\n",
      "Iteration 16304: loss = 0.002514876149481797\n",
      "Iteration 16305: loss = 0.002514864879466563\n",
      "Iteration 16306: loss = 0.002514853609574663\n",
      "Iteration 16307: loss = 0.0025148423398060936\n",
      "Iteration 16308: loss = 0.0025148310701608545\n",
      "Iteration 16309: loss = 0.002514819800638944\n",
      "Iteration 16310: loss = 0.0025148085312403606\n",
      "Iteration 16311: loss = 0.0025147972619651024\n",
      "Iteration 16312: loss = 0.0025147859928131683\n",
      "Iteration 16313: loss = 0.002514774723784557\n",
      "Iteration 16314: loss = 0.0025147634548792664\n",
      "Iteration 16315: loss = 0.002514752186097295\n",
      "Iteration 16316: loss = 0.0025147409174386412\n",
      "Iteration 16317: loss = 0.002514729648903304\n",
      "Iteration 16318: loss = 0.0025147183804912817\n",
      "Iteration 16319: loss = 0.002514707112202572\n",
      "Iteration 16320: loss = 0.002514695844037175\n",
      "Iteration 16321: loss = 0.002514684575995088\n",
      "Iteration 16322: loss = 0.0025146733080763095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16323: loss = 0.0025146620402808394\n",
      "Iteration 16324: loss = 0.002514650772608673\n",
      "Iteration 16325: loss = 0.0025146395050598116\n",
      "Iteration 16326: loss = 0.002514628237634253\n",
      "Iteration 16327: loss = 0.0025146169703319956\n",
      "Iteration 16328: loss = 0.0025146057031530376\n",
      "Iteration 16329: loss = 0.002514594436097378\n",
      "Iteration 16330: loss = 0.0025145831691650144\n",
      "Iteration 16331: loss = 0.0025145719023559466\n",
      "Iteration 16332: loss = 0.002514560635670172\n",
      "Iteration 16333: loss = 0.0025145493691076898\n",
      "Iteration 16334: loss = 0.0025145381026684976\n",
      "Iteration 16335: loss = 0.002514526836352595\n",
      "Iteration 16336: loss = 0.002514515570159979\n",
      "Iteration 16337: loss = 0.0025145043040906492\n",
      "Iteration 16338: loss = 0.0025144930381446043\n",
      "Iteration 16339: loss = 0.002514481772321842\n",
      "Iteration 16340: loss = 0.0025144705066223612\n",
      "Iteration 16341: loss = 0.00251445924104616\n",
      "Iteration 16342: loss = 0.0025144479755932377\n",
      "Iteration 16343: loss = 0.0025144367102635915\n",
      "Iteration 16344: loss = 0.002514425445057221\n",
      "Iteration 16345: loss = 0.0025144141799741246\n",
      "Iteration 16346: loss = 0.0025144029150143002\n",
      "Iteration 16347: loss = 0.0025143916501777466\n",
      "Iteration 16348: loss = 0.002514380385464462\n",
      "Iteration 16349: loss = 0.0025143691208744457\n",
      "Iteration 16350: loss = 0.0025143578564076957\n",
      "Iteration 16351: loss = 0.00251434659206421\n",
      "Iteration 16352: loss = 0.002514335327843988\n",
      "Iteration 16353: loss = 0.0025143240637470264\n",
      "Iteration 16354: loss = 0.002514312799773326\n",
      "Iteration 16355: loss = 0.0025143015359228842\n",
      "Iteration 16356: loss = 0.0025142902721956993\n",
      "Iteration 16357: loss = 0.00251427900859177\n",
      "Iteration 16358: loss = 0.0025142677451110945\n",
      "Iteration 16359: loss = 0.002514256481753672\n",
      "Iteration 16360: loss = 0.0025142452185195007\n",
      "Iteration 16361: loss = 0.002514233955408579\n",
      "Iteration 16362: loss = 0.0025142226924209045\n",
      "Iteration 16363: loss = 0.0025142114295564773\n",
      "Iteration 16364: loss = 0.0025142001668152945\n",
      "Iteration 16365: loss = 0.0025141889041973555\n",
      "Iteration 16366: loss = 0.0025141776417026587\n",
      "Iteration 16367: loss = 0.0025141663793312023\n",
      "Iteration 16368: loss = 0.002514155117082985\n",
      "Iteration 16369: loss = 0.0025141438549580045\n",
      "Iteration 16370: loss = 0.00251413259295626\n",
      "Iteration 16371: loss = 0.00251412133107775\n",
      "Iteration 16372: loss = 0.0025141100693224735\n",
      "Iteration 16373: loss = 0.0025140988076904273\n",
      "Iteration 16374: loss = 0.002514087546181611\n",
      "Iteration 16375: loss = 0.002514076284796024\n",
      "Iteration 16376: loss = 0.002514065023533663\n",
      "Iteration 16377: loss = 0.0025140537623945273\n",
      "Iteration 16378: loss = 0.002514042501378616\n",
      "Iteration 16379: loss = 0.002514031240485926\n",
      "Iteration 16380: loss = 0.0025140199797164574\n",
      "Iteration 16381: loss = 0.0025140087190702086\n",
      "Iteration 16382: loss = 0.002513997458547176\n",
      "Iteration 16383: loss = 0.0025139861981473607\n",
      "Iteration 16384: loss = 0.0025139749378707598\n",
      "Iteration 16385: loss = 0.002513963677717372\n",
      "Iteration 16386: loss = 0.0025139524176871963\n",
      "Iteration 16387: loss = 0.0025139411577802303\n",
      "Iteration 16388: loss = 0.002513929897996473\n",
      "Iteration 16389: loss = 0.0025139186383359227\n",
      "Iteration 16390: loss = 0.0025139073787985786\n",
      "Iteration 16391: loss = 0.0025138961193844375\n",
      "Iteration 16392: loss = 0.0025138848600935\n",
      "Iteration 16393: loss = 0.002513873600925763\n",
      "Iteration 16394: loss = 0.0025138623418812255\n",
      "Iteration 16395: loss = 0.0025138510829598865\n",
      "Iteration 16396: loss = 0.0025138398241617435\n",
      "Iteration 16397: loss = 0.002513828565486796\n",
      "Iteration 16398: loss = 0.002513817306935042\n",
      "Iteration 16399: loss = 0.002513806048506479\n",
      "Iteration 16400: loss = 0.0025137947902011074\n",
      "Iteration 16401: loss = 0.002513783532018925\n",
      "Iteration 16402: loss = 0.002513772273959929\n",
      "Iteration 16403: loss = 0.0025137610160241193\n",
      "Iteration 16404: loss = 0.002513749758211494\n",
      "Iteration 16405: loss = 0.0025137385005220524\n",
      "Iteration 16406: loss = 0.0025137272429557917\n",
      "Iteration 16407: loss = 0.00251371598551271\n",
      "Iteration 16408: loss = 0.002513704728192807\n",
      "Iteration 16409: loss = 0.002513693470996082\n",
      "Iteration 16410: loss = 0.0025136822139225318\n",
      "Iteration 16411: loss = 0.0025136709569721547\n",
      "Iteration 16412: loss = 0.00251365970014495\n",
      "Iteration 16413: loss = 0.0025136484434409166\n",
      "Iteration 16414: loss = 0.002513637186860053\n",
      "Iteration 16415: loss = 0.0025136259304023565\n",
      "Iteration 16416: loss = 0.0025136146740678262\n",
      "Iteration 16417: loss = 0.0025136034178564605\n",
      "Iteration 16418: loss = 0.0025135921617682583\n",
      "Iteration 16419: loss = 0.002513580905803218\n",
      "Iteration 16420: loss = 0.002513569649961337\n",
      "Iteration 16421: loss = 0.0025135583942426155\n",
      "Iteration 16422: loss = 0.0025135471386470514\n",
      "Iteration 16423: loss = 0.0025135358831746423\n",
      "Iteration 16424: loss = 0.0025135246278253877\n",
      "Iteration 16425: loss = 0.002513513372599286\n",
      "Iteration 16426: loss = 0.002513502117496335\n",
      "Iteration 16427: loss = 0.002513490862516534\n",
      "Iteration 16428: loss = 0.0025134796076598806\n",
      "Iteration 16429: loss = 0.002513468352926374\n",
      "Iteration 16430: loss = 0.002513457098316013\n",
      "Iteration 16431: loss = 0.002513445843828795\n",
      "Iteration 16432: loss = 0.0025134345894647194\n",
      "Iteration 16433: loss = 0.0025134233352237844\n",
      "Iteration 16434: loss = 0.002513412081105988\n",
      "Iteration 16435: loss = 0.0025134008271113293\n",
      "Iteration 16436: loss = 0.0025133895732398067\n",
      "Iteration 16437: loss = 0.0025133783194914187\n",
      "Iteration 16438: loss = 0.0025133670658661635\n",
      "Iteration 16439: loss = 0.0025133558123640403\n",
      "Iteration 16440: loss = 0.0025133445589850464\n",
      "Iteration 16441: loss = 0.002513333305729181\n",
      "Iteration 16442: loss = 0.002513322052596443\n",
      "Iteration 16443: loss = 0.0025133107995868303\n",
      "Iteration 16444: loss = 0.0025132995467003418\n",
      "Iteration 16445: loss = 0.0025132882939369753\n",
      "Iteration 16446: loss = 0.00251327704129673\n",
      "Iteration 16447: loss = 0.0025132657887796035\n",
      "Iteration 16448: loss = 0.002513254536385595\n",
      "Iteration 16449: loss = 0.0025132432841147036\n",
      "Iteration 16450: loss = 0.0025132320319669267\n",
      "Iteration 16451: loss = 0.002513220779942264\n",
      "Iteration 16452: loss = 0.002513209528040712\n",
      "Iteration 16453: loss = 0.0025131982762622708\n",
      "Iteration 16454: loss = 0.0025131870246069385\n",
      "Iteration 16455: loss = 0.002513175773074713\n",
      "Iteration 16456: loss = 0.002513164521665594\n",
      "Iteration 16457: loss = 0.0025131532703795787\n",
      "Iteration 16458: loss = 0.0025131420192166668\n",
      "Iteration 16459: loss = 0.002513130768176856\n",
      "Iteration 16460: loss = 0.0025131195172601447\n",
      "Iteration 16461: loss = 0.002513108266466532\n",
      "Iteration 16462: loss = 0.0025130970157960157\n",
      "Iteration 16463: loss = 0.0025130857652485954\n",
      "Iteration 16464: loss = 0.0025130745148242684\n",
      "Iteration 16465: loss = 0.002513063264523033\n",
      "Iteration 16466: loss = 0.002513052014344889\n",
      "Iteration 16467: loss = 0.0025130407642898344\n",
      "Iteration 16468: loss = 0.002513029514357867\n",
      "Iteration 16469: loss = 0.002513018264548986\n",
      "Iteration 16470: loss = 0.00251300701486319\n",
      "Iteration 16471: loss = 0.002512995765300477\n",
      "Iteration 16472: loss = 0.002512984515860845\n",
      "Iteration 16473: loss = 0.002512973266544294\n",
      "Iteration 16474: loss = 0.002512962017350822\n",
      "Iteration 16475: loss = 0.0025129507682804257\n",
      "Iteration 16476: loss = 0.002512939519333106\n",
      "Iteration 16477: loss = 0.0025129282705088607\n",
      "Iteration 16478: loss = 0.0025129170218076875\n",
      "Iteration 16479: loss = 0.002512905773229586\n",
      "Iteration 16480: loss = 0.0025128945247745536\n",
      "Iteration 16481: loss = 0.00251288327644259\n",
      "Iteration 16482: loss = 0.002512872028233692\n",
      "Iteration 16483: loss = 0.002512860780147859\n",
      "Iteration 16484: loss = 0.0025128495321850904\n",
      "Iteration 16485: loss = 0.002512838284345384\n",
      "Iteration 16486: loss = 0.0025128270366287374\n",
      "Iteration 16487: loss = 0.0025128157890351504\n",
      "Iteration 16488: loss = 0.002512804541564621\n",
      "Iteration 16489: loss = 0.0025127932942171473\n",
      "Iteration 16490: loss = 0.002512782046992728\n",
      "Iteration 16491: loss = 0.002512770799891362\n",
      "Iteration 16492: loss = 0.002512759552913047\n",
      "Iteration 16493: loss = 0.002512748306057783\n",
      "Iteration 16494: loss = 0.0025127370593255667\n",
      "Iteration 16495: loss = 0.002512725812716398\n",
      "Iteration 16496: loss = 0.0025127145662302746\n",
      "Iteration 16497: loss = 0.002512703319867195\n",
      "Iteration 16498: loss = 0.002512692073627158\n",
      "Iteration 16499: loss = 0.002512680827510162\n",
      "Iteration 16500: loss = 0.0025126695815162053\n",
      "Iteration 16501: loss = 0.002512658335645287\n",
      "Iteration 16502: loss = 0.002512647089897405\n",
      "Iteration 16503: loss = 0.0025126358442725573\n",
      "Iteration 16504: loss = 0.0025126245987707437\n",
      "Iteration 16505: loss = 0.0025126133533919623\n",
      "Iteration 16506: loss = 0.002512602108136211\n",
      "Iteration 16507: loss = 0.002512590863003488\n",
      "Iteration 16508: loss = 0.002512579617993793\n",
      "Iteration 16509: loss = 0.0025125683731071235\n",
      "Iteration 16510: loss = 0.002512557128343479\n",
      "Iteration 16511: loss = 0.0025125458837028576\n",
      "Iteration 16512: loss = 0.002512534639185257\n",
      "Iteration 16513: loss = 0.0025125233947906763\n",
      "Iteration 16514: loss = 0.002512512150519114\n",
      "Iteration 16515: loss = 0.0025125009063705684\n",
      "Iteration 16516: loss = 0.0025124896623450388\n",
      "Iteration 16517: loss = 0.002512478418442523\n",
      "Iteration 16518: loss = 0.0025124671746630193\n",
      "Iteration 16519: loss = 0.0025124559310065265\n",
      "Iteration 16520: loss = 0.0025124446874730433\n",
      "Iteration 16521: loss = 0.0025124334440625676\n",
      "Iteration 16522: loss = 0.002512422200775098\n",
      "Iteration 16523: loss = 0.0025124109576106337\n",
      "Iteration 16524: loss = 0.002512399714569173\n",
      "Iteration 16525: loss = 0.0025123884716507132\n",
      "Iteration 16526: loss = 0.002512377228855255\n",
      "Iteration 16527: loss = 0.0025123659861827946\n",
      "Iteration 16528: loss = 0.002512354743633332\n",
      "Iteration 16529: loss = 0.0025123435012068653\n",
      "Iteration 16530: loss = 0.0025123322589033917\n",
      "Iteration 16531: loss = 0.002512321016722912\n",
      "Iteration 16532: loss = 0.002512309774665424\n",
      "Iteration 16533: loss = 0.0025122985327309245\n",
      "Iteration 16534: loss = 0.0025122872909194146\n",
      "Iteration 16535: loss = 0.0025122760492308906\n",
      "Iteration 16536: loss = 0.002512264807665352\n",
      "Iteration 16537: loss = 0.0025122535662227973\n",
      "Iteration 16538: loss = 0.002512242324903225\n",
      "Iteration 16539: loss = 0.0025122310837066334\n",
      "Iteration 16540: loss = 0.0025122198426330206\n",
      "Iteration 16541: loss = 0.0025122086016823864\n",
      "Iteration 16542: loss = 0.002512197360854728\n",
      "Iteration 16543: loss = 0.0025121861201500444\n",
      "Iteration 16544: loss = 0.0025121748795683342\n",
      "Iteration 16545: loss = 0.002512163639109595\n",
      "Iteration 16546: loss = 0.002512152398773827\n",
      "Iteration 16547: loss = 0.0025121411585610274\n",
      "Iteration 16548: loss = 0.002512129918471195\n",
      "Iteration 16549: loss = 0.0025121186785043285\n",
      "Iteration 16550: loss = 0.0025121074386604257\n",
      "Iteration 16551: loss = 0.0025120961989394857\n",
      "Iteration 16552: loss = 0.002512084959341507\n",
      "Iteration 16553: loss = 0.002512073719866488\n",
      "Iteration 16554: loss = 0.002512062480514428\n",
      "Iteration 16555: loss = 0.0025120512412853237\n",
      "Iteration 16556: loss = 0.0025120400021791758\n",
      "Iteration 16557: loss = 0.0025120287631959807\n",
      "Iteration 16558: loss = 0.0025120175243357377\n",
      "Iteration 16559: loss = 0.0025120062855984457\n",
      "Iteration 16560: loss = 0.002511995046984103\n",
      "Iteration 16561: loss = 0.0025119838084927074\n",
      "Iteration 16562: loss = 0.002511972570124259\n",
      "Iteration 16563: loss = 0.002511961331878754\n",
      "Iteration 16564: loss = 0.0025119500937561936\n",
      "Iteration 16565: loss = 0.0025119388557565737\n",
      "Iteration 16566: loss = 0.002511927617879895\n",
      "Iteration 16567: loss = 0.002511916380126154\n",
      "Iteration 16568: loss = 0.0025119051424953506\n",
      "Iteration 16569: loss = 0.0025118939049874832\n",
      "Iteration 16570: loss = 0.0025118826676025496\n",
      "Iteration 16571: loss = 0.0025118714303405485\n",
      "Iteration 16572: loss = 0.002511860193201479\n",
      "Iteration 16573: loss = 0.0025118489561853394\n",
      "Iteration 16574: loss = 0.002511837719292127\n",
      "Iteration 16575: loss = 0.002511826482521842\n",
      "Iteration 16576: loss = 0.002511815245874482\n",
      "Iteration 16577: loss = 0.0025118040093500453\n",
      "Iteration 16578: loss = 0.0025117927729485312\n",
      "Iteration 16579: loss = 0.002511781536669938\n",
      "Iteration 16580: loss = 0.0025117703005142636\n",
      "Iteration 16581: loss = 0.0025117590644815066\n",
      "Iteration 16582: loss = 0.002511747828571666\n",
      "Iteration 16583: loss = 0.0025117365927847402\n",
      "Iteration 16584: loss = 0.0025117253571207273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16585: loss = 0.0025117141215796265\n",
      "Iteration 16586: loss = 0.0025117028861614352\n",
      "Iteration 16587: loss = 0.0025116916508661534\n",
      "Iteration 16588: loss = 0.002511680415693778\n",
      "Iteration 16589: loss = 0.0025116691806443088\n",
      "Iteration 16590: loss = 0.0025116579457177433\n",
      "Iteration 16591: loss = 0.0025116467109140804\n",
      "Iteration 16592: loss = 0.0025116354762333183\n",
      "Iteration 16593: loss = 0.0025116242416754566\n",
      "Iteration 16594: loss = 0.002511613007240493\n",
      "Iteration 16595: loss = 0.002511601772928426\n",
      "Iteration 16596: loss = 0.0025115905387392543\n",
      "Iteration 16597: loss = 0.002511579304672976\n",
      "Iteration 16598: loss = 0.0025115680707295898\n",
      "Iteration 16599: loss = 0.002511556836909094\n",
      "Iteration 16600: loss = 0.0025115456032114877\n",
      "Iteration 16601: loss = 0.002511534369636769\n",
      "Iteration 16602: loss = 0.0025115231361849365\n",
      "Iteration 16603: loss = 0.002511511902855988\n",
      "Iteration 16604: loss = 0.0025115006696499235\n",
      "Iteration 16605: loss = 0.0025114894365667402\n",
      "Iteration 16606: loss = 0.002511478203606437\n",
      "Iteration 16607: loss = 0.002511466970769013\n",
      "Iteration 16608: loss = 0.0025114557380544657\n",
      "Iteration 16609: loss = 0.002511444505462794\n",
      "Iteration 16610: loss = 0.0025114332729939963\n",
      "Iteration 16611: loss = 0.002511422040648072\n",
      "Iteration 16612: loss = 0.0025114108084250183\n",
      "Iteration 16613: loss = 0.0025113995763248334\n",
      "Iteration 16614: loss = 0.002511388344347518\n",
      "Iteration 16615: loss = 0.0025113771124930686\n",
      "Iteration 16616: loss = 0.002511365880761485\n",
      "Iteration 16617: loss = 0.0025113546491527645\n",
      "Iteration 16618: loss = 0.002511343417666906\n",
      "Iteration 16619: loss = 0.0025113321863039087\n",
      "Iteration 16620: loss = 0.00251132095506377\n",
      "Iteration 16621: loss = 0.002511309723946489\n",
      "Iteration 16622: loss = 0.0025112984929520645\n",
      "Iteration 16623: loss = 0.0025112872620804945\n",
      "Iteration 16624: loss = 0.0025112760313317777\n",
      "Iteration 16625: loss = 0.0025112648007059126\n",
      "Iteration 16626: loss = 0.0025112535702028977\n",
      "Iteration 16627: loss = 0.002511242339822731\n",
      "Iteration 16628: loss = 0.0025112311095654115\n",
      "Iteration 16629: loss = 0.0025112198794309385\n",
      "Iteration 16630: loss = 0.0025112086494193087\n",
      "Iteration 16631: loss = 0.0025111974195305225\n",
      "Iteration 16632: loss = 0.0025111861897645768\n",
      "Iteration 16633: loss = 0.0025111749601214707\n",
      "Iteration 16634: loss = 0.002511163730601203\n",
      "Iteration 16635: loss = 0.002511152501203772\n",
      "Iteration 16636: loss = 0.0025111412719291764\n",
      "Iteration 16637: loss = 0.0025111300427774143\n",
      "Iteration 16638: loss = 0.002511118813748484\n",
      "Iteration 16639: loss = 0.002511107584842385\n",
      "Iteration 16640: loss = 0.002511096356059115\n",
      "Iteration 16641: loss = 0.0025110851273986725\n",
      "Iteration 16642: loss = 0.0025110738988610563\n",
      "Iteration 16643: loss = 0.002511062670446265\n",
      "Iteration 16644: loss = 0.0025110514421542965\n",
      "Iteration 16645: loss = 0.00251104021398515\n",
      "Iteration 16646: loss = 0.002511028985938824\n",
      "Iteration 16647: loss = 0.002511017758015316\n",
      "Iteration 16648: loss = 0.0025110065302146258\n",
      "Iteration 16649: loss = 0.0025109953025367507\n",
      "Iteration 16650: loss = 0.00251098407498169\n",
      "Iteration 16651: loss = 0.0025109728475494423\n",
      "Iteration 16652: loss = 0.002510961620240006\n",
      "Iteration 16653: loss = 0.0025109503930533785\n",
      "Iteration 16654: loss = 0.0025109391659895604\n",
      "Iteration 16655: loss = 0.0025109279390485486\n",
      "Iteration 16656: loss = 0.0025109167122303417\n",
      "Iteration 16657: loss = 0.0025109054855349386\n",
      "Iteration 16658: loss = 0.002510894258962338\n",
      "Iteration 16659: loss = 0.002510883032512538\n",
      "Iteration 16660: loss = 0.0025108718061855376\n",
      "Iteration 16661: loss = 0.002510860579981334\n",
      "Iteration 16662: loss = 0.0025108493538999275\n",
      "Iteration 16663: loss = 0.002510838127941315\n",
      "Iteration 16664: loss = 0.0025108269021054965\n",
      "Iteration 16665: loss = 0.00251081567639247\n",
      "Iteration 16666: loss = 0.0025108044508022334\n",
      "Iteration 16667: loss = 0.0025107932253347854\n",
      "Iteration 16668: loss = 0.0025107819999901247\n",
      "Iteration 16669: loss = 0.00251077077476825\n",
      "Iteration 16670: loss = 0.0025107595496691587\n",
      "Iteration 16671: loss = 0.0025107483246928513\n",
      "Iteration 16672: loss = 0.0025107370998393246\n",
      "Iteration 16673: loss = 0.002510725875108578\n",
      "Iteration 16674: loss = 0.00251071465050061\n",
      "Iteration 16675: loss = 0.0025107034260154182\n",
      "Iteration 16676: loss = 0.002510692201653002\n",
      "Iteration 16677: loss = 0.0025106809774133594\n",
      "Iteration 16678: loss = 0.002510669753296489\n",
      "Iteration 16679: loss = 0.00251065852930239\n",
      "Iteration 16680: loss = 0.00251064730543106\n",
      "Iteration 16681: loss = 0.002510636081682498\n",
      "Iteration 16682: loss = 0.002510624858056702\n",
      "Iteration 16683: loss = 0.002510613634553671\n",
      "Iteration 16684: loss = 0.0025106024111734033\n",
      "Iteration 16685: loss = 0.0025105911879158976\n",
      "Iteration 16686: loss = 0.002510579964781152\n",
      "Iteration 16687: loss = 0.0025105687417691655\n",
      "Iteration 16688: loss = 0.0025105575188799365\n",
      "Iteration 16689: loss = 0.0025105462961134634\n",
      "Iteration 16690: loss = 0.002510535073469744\n",
      "Iteration 16691: loss = 0.0025105238509487786\n",
      "Iteration 16692: loss = 0.002510512628550563\n",
      "Iteration 16693: loss = 0.0025105014062750987\n",
      "Iteration 16694: loss = 0.0025104901841223825\n",
      "Iteration 16695: loss = 0.002510478962092413\n",
      "Iteration 16696: loss = 0.0025104677401851893\n",
      "Iteration 16697: loss = 0.002510456518400709\n",
      "Iteration 16698: loss = 0.002510445296738971\n",
      "Iteration 16699: loss = 0.0025104340751999736\n",
      "Iteration 16700: loss = 0.002510422853783717\n",
      "Iteration 16701: loss = 0.0025104116324901975\n",
      "Iteration 16702: loss = 0.002510400411319414\n",
      "Iteration 16703: loss = 0.002510389190271367\n",
      "Iteration 16704: loss = 0.0025103779693460516\n",
      "Iteration 16705: loss = 0.0025103667485434694\n",
      "Iteration 16706: loss = 0.002510355527863617\n",
      "Iteration 16707: loss = 0.0025103443073064943\n",
      "Iteration 16708: loss = 0.002510333086872098\n",
      "Iteration 16709: loss = 0.0025103218665604287\n",
      "Iteration 16710: loss = 0.0025103106463714832\n",
      "Iteration 16711: loss = 0.002510299426305261\n",
      "Iteration 16712: loss = 0.0025102882063617604\n",
      "Iteration 16713: loss = 0.0025102769865409796\n",
      "Iteration 16714: loss = 0.0025102657668429175\n",
      "Iteration 16715: loss = 0.0025102545472675724\n",
      "Iteration 16716: loss = 0.002510243327814943\n",
      "Iteration 16717: loss = 0.0025102321084850276\n",
      "Iteration 16718: loss = 0.002510220889277824\n",
      "Iteration 16719: loss = 0.0025102096701933325\n",
      "Iteration 16720: loss = 0.0025101984512315503\n",
      "Iteration 16721: loss = 0.0025101872323924756\n",
      "Iteration 16722: loss = 0.002510176013676108\n",
      "Iteration 16723: loss = 0.0025101647950824453\n",
      "Iteration 16724: loss = 0.002510153576611486\n",
      "Iteration 16725: loss = 0.002510142358263229\n",
      "Iteration 16726: loss = 0.0025101311400376734\n",
      "Iteration 16727: loss = 0.0025101199219348157\n",
      "Iteration 16728: loss = 0.002510108703954656\n",
      "Iteration 16729: loss = 0.0025100974860971926\n",
      "Iteration 16730: loss = 0.0025100862683624233\n",
      "Iteration 16731: loss = 0.002510075050750348\n",
      "Iteration 16732: loss = 0.0025100638332609636\n",
      "Iteration 16733: loss = 0.0025100526158942696\n",
      "Iteration 16734: loss = 0.0025100413986502645\n",
      "Iteration 16735: loss = 0.0025100301815289463\n",
      "Iteration 16736: loss = 0.002510018964530314\n",
      "Iteration 16737: loss = 0.002510007747654366\n",
      "Iteration 16738: loss = 0.0025099965309011\n",
      "Iteration 16739: loss = 0.002509985314270516\n",
      "Iteration 16740: loss = 0.0025099740977626113\n",
      "Iteration 16741: loss = 0.002509962881377385\n",
      "Iteration 16742: loss = 0.002509951665114835\n",
      "Iteration 16743: loss = 0.0025099404489749605\n",
      "Iteration 16744: loss = 0.0025099292329577596\n",
      "Iteration 16745: loss = 0.0025099180170632313\n",
      "Iteration 16746: loss = 0.0025099068012913736\n",
      "Iteration 16747: loss = 0.002509895585642185\n",
      "Iteration 16748: loss = 0.0025098843701156642\n",
      "Iteration 16749: loss = 0.0025098731547118103\n",
      "Iteration 16750: loss = 0.0025098619394306205\n",
      "Iteration 16751: loss = 0.0025098507242720947\n",
      "Iteration 16752: loss = 0.00250983950923623\n",
      "Iteration 16753: loss = 0.002509828294323026\n",
      "Iteration 16754: loss = 0.0025098170795324806\n",
      "Iteration 16755: loss = 0.002509805864864592\n",
      "Iteration 16756: loss = 0.0025097946503193605\n",
      "Iteration 16757: loss = 0.0025097834358967825\n",
      "Iteration 16758: loss = 0.002509772221596857\n",
      "Iteration 16759: loss = 0.002509761007419584\n",
      "Iteration 16760: loss = 0.0025097497933649605\n",
      "Iteration 16761: loss = 0.0025097385794329856\n",
      "Iteration 16762: loss = 0.002509727365623657\n",
      "Iteration 16763: loss = 0.0025097161519369742\n",
      "Iteration 16764: loss = 0.002509704938372935\n",
      "Iteration 16765: loss = 0.0025096937249315383\n",
      "Iteration 16766: loss = 0.0025096825116127826\n",
      "Iteration 16767: loss = 0.0025096712984166666\n",
      "Iteration 16768: loss = 0.0025096600853431877\n",
      "Iteration 16769: loss = 0.002509648872392346\n",
      "Iteration 16770: loss = 0.0025096376595641397\n",
      "Iteration 16771: loss = 0.002509626446858566\n",
      "Iteration 16772: loss = 0.0025096152342756247\n",
      "Iteration 16773: loss = 0.0025096040218153142\n",
      "Iteration 16774: loss = 0.0025095928094776322\n",
      "Iteration 16775: loss = 0.0025095815972625782\n",
      "Iteration 16776: loss = 0.00250957038517015\n",
      "Iteration 16777: loss = 0.002509559173200346\n",
      "Iteration 16778: loss = 0.0025095479613531655\n",
      "Iteration 16779: loss = 0.0025095367496286067\n",
      "Iteration 16780: loss = 0.0025095255380266674\n",
      "Iteration 16781: loss = 0.002509514326547347\n",
      "Iteration 16782: loss = 0.002509503115190643\n",
      "Iteration 16783: loss = 0.0025094919039565563\n",
      "Iteration 16784: loss = 0.0025094806928450827\n",
      "Iteration 16785: loss = 0.0025094694818562214\n",
      "Iteration 16786: loss = 0.0025094582709899713\n",
      "Iteration 16787: loss = 0.0025094470602463315\n",
      "Iteration 16788: loss = 0.0025094358496252993\n",
      "Iteration 16789: loss = 0.0025094246391268735\n",
      "Iteration 16790: loss = 0.002509413428751054\n",
      "Iteration 16791: loss = 0.0025094022184978374\n",
      "Iteration 16792: loss = 0.0025093910083672233\n",
      "Iteration 16793: loss = 0.0025093797983592094\n",
      "Iteration 16794: loss = 0.0025093685884737954\n",
      "Iteration 16795: loss = 0.0025093573787109786\n",
      "Iteration 16796: loss = 0.0025093461690707582\n",
      "Iteration 16797: loss = 0.002509334959553133\n",
      "Iteration 16798: loss = 0.002509323750158101\n",
      "Iteration 16799: loss = 0.00250931254088566\n",
      "Iteration 16800: loss = 0.0025093013317358096\n",
      "Iteration 16801: loss = 0.002509290122708548\n",
      "Iteration 16802: loss = 0.002509278913803875\n",
      "Iteration 16803: loss = 0.002509267705021786\n",
      "Iteration 16804: loss = 0.002509256496362282\n",
      "Iteration 16805: loss = 0.002509245287825361\n",
      "Iteration 16806: loss = 0.002509234079411021\n",
      "Iteration 16807: loss = 0.002509222871119261\n",
      "Iteration 16808: loss = 0.0025092116629500797\n",
      "Iteration 16809: loss = 0.002509200454903475\n",
      "Iteration 16810: loss = 0.0025091892469794462\n",
      "Iteration 16811: loss = 0.0025091780391779906\n",
      "Iteration 16812: loss = 0.002509166831499108\n",
      "Iteration 16813: loss = 0.002509155623942796\n",
      "Iteration 16814: loss = 0.002509144416509054\n",
      "Iteration 16815: loss = 0.0025091332091978793\n",
      "Iteration 16816: loss = 0.0025091220020092716\n",
      "Iteration 16817: loss = 0.0025091107949432286\n",
      "Iteration 16818: loss = 0.002509099587999749\n",
      "Iteration 16819: loss = 0.002509088381178831\n",
      "Iteration 16820: loss = 0.0025090771744804745\n",
      "Iteration 16821: loss = 0.0025090659679046765\n",
      "Iteration 16822: loss = 0.0025090547614514363\n",
      "Iteration 16823: loss = 0.002509043555120752\n",
      "Iteration 16824: loss = 0.0025090323489126223\n",
      "Iteration 16825: loss = 0.0025090211428270455\n",
      "Iteration 16826: loss = 0.0025090099368640204\n",
      "Iteration 16827: loss = 0.0025089987310235456\n",
      "Iteration 16828: loss = 0.0025089875253056196\n",
      "Iteration 16829: loss = 0.00250897631971024\n",
      "Iteration 16830: loss = 0.002508965114237407\n",
      "Iteration 16831: loss = 0.002508953908887118\n",
      "Iteration 16832: loss = 0.002508942703659371\n",
      "Iteration 16833: loss = 0.0025089314985541652\n",
      "Iteration 16834: loss = 0.0025089202935714996\n",
      "Iteration 16835: loss = 0.0025089090887113726\n",
      "Iteration 16836: loss = 0.0025088978839737813\n",
      "Iteration 16837: loss = 0.002508886679358726\n",
      "Iteration 16838: loss = 0.0025088754748662043\n",
      "Iteration 16839: loss = 0.002508864270496215\n",
      "Iteration 16840: loss = 0.002508853066248756\n",
      "Iteration 16841: loss = 0.002508841862123827\n",
      "Iteration 16842: loss = 0.0025088306581214246\n",
      "Iteration 16843: loss = 0.0025088194542415496\n",
      "Iteration 16844: loss = 0.0025088082504841993\n",
      "Iteration 16845: loss = 0.002508797046849372\n",
      "Iteration 16846: loss = 0.002508785843337067\n",
      "Iteration 16847: loss = 0.0025087746399472823\n",
      "Iteration 16848: loss = 0.002508763436680016\n",
      "Iteration 16849: loss = 0.0025087522335352675\n",
      "Iteration 16850: loss = 0.002508741030513035\n",
      "Iteration 16851: loss = 0.0025087298276133168\n",
      "Iteration 16852: loss = 0.0025087186248361117\n",
      "Iteration 16853: loss = 0.0025087074221814176\n",
      "Iteration 16854: loss = 0.002508696219649234\n",
      "Iteration 16855: loss = 0.002508685017239559\n",
      "Iteration 16856: loss = 0.0025086738149523906\n",
      "Iteration 16857: loss = 0.0025086626127877277\n",
      "Iteration 16858: loss = 0.0025086514107455693\n",
      "Iteration 16859: loss = 0.002508640208825913\n",
      "Iteration 16860: loss = 0.002508629007028758\n",
      "Iteration 16861: loss = 0.0025086178053541023\n",
      "Iteration 16862: loss = 0.0025086066038019447\n",
      "Iteration 16863: loss = 0.002508595402372284\n",
      "Iteration 16864: loss = 0.002508584201065118\n",
      "Iteration 16865: loss = 0.0025085729998804464\n",
      "Iteration 16866: loss = 0.002508561798818266\n",
      "Iteration 16867: loss = 0.002508550597878577\n",
      "Iteration 16868: loss = 0.002508539397061377\n",
      "Iteration 16869: loss = 0.0025085281963666646\n",
      "Iteration 16870: loss = 0.0025085169957944384\n",
      "Iteration 16871: loss = 0.002508505795344697\n",
      "Iteration 16872: loss = 0.0025084945950174385\n",
      "Iteration 16873: loss = 0.0025084833948126623\n",
      "Iteration 16874: loss = 0.002508472194730366\n",
      "Iteration 16875: loss = 0.002508460994770549\n",
      "Iteration 16876: loss = 0.0025084497949332085\n",
      "Iteration 16877: loss = 0.0025084385952183442\n",
      "Iteration 16878: loss = 0.002508427395625954\n",
      "Iteration 16879: loss = 0.0025084161961560366\n",
      "Iteration 16880: loss = 0.0025084049968085915\n",
      "Iteration 16881: loss = 0.0025083937975836153\n",
      "Iteration 16882: loss = 0.002508382598481108\n",
      "Iteration 16883: loss = 0.0025083713995010673\n",
      "Iteration 16884: loss = 0.002508360200643492\n",
      "Iteration 16885: loss = 0.0025083490019083806\n",
      "Iteration 16886: loss = 0.002508337803295732\n",
      "Iteration 16887: loss = 0.0025083266048055446\n",
      "Iteration 16888: loss = 0.0025083154064378162\n",
      "Iteration 16889: loss = 0.002508304208192546\n",
      "Iteration 16890: loss = 0.0025082930100697324\n",
      "Iteration 16891: loss = 0.0025082818120693733\n",
      "Iteration 16892: loss = 0.002508270614191468\n",
      "Iteration 16893: loss = 0.002508259416436015\n",
      "Iteration 16894: loss = 0.0025082482188030124\n",
      "Iteration 16895: loss = 0.002508237021292459\n",
      "Iteration 16896: loss = 0.002508225823904353\n",
      "Iteration 16897: loss = 0.002508214626638693\n",
      "Iteration 16898: loss = 0.0025082034294954783\n",
      "Iteration 16899: loss = 0.002508192232474706\n",
      "Iteration 16900: loss = 0.002508181035576376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16901: loss = 0.002508169838800486\n",
      "Iteration 16902: loss = 0.0025081586421470347\n",
      "Iteration 16903: loss = 0.0025081474456160207\n",
      "Iteration 16904: loss = 0.0025081362492074422\n",
      "Iteration 16905: loss = 0.002508125052921298\n",
      "Iteration 16906: loss = 0.0025081138567575865\n",
      "Iteration 16907: loss = 0.0025081026607163067\n",
      "Iteration 16908: loss = 0.0025080914647974567\n",
      "Iteration 16909: loss = 0.0025080802690010348\n",
      "Iteration 16910: loss = 0.0025080690733270397\n",
      "Iteration 16911: loss = 0.0025080578777754697\n",
      "Iteration 16912: loss = 0.0025080466823463243\n",
      "Iteration 16913: loss = 0.0025080354870396005\n",
      "Iteration 16914: loss = 0.0025080242918552984\n",
      "Iteration 16915: loss = 0.0025080130967934148\n",
      "Iteration 16916: loss = 0.0025080019018539497\n",
      "Iteration 16917: loss = 0.0025079907070369016\n",
      "Iteration 16918: loss = 0.0025079795123422676\n",
      "Iteration 16919: loss = 0.0025079683177700474\n",
      "Iteration 16920: loss = 0.0025079571233202393\n",
      "Iteration 16921: loss = 0.002507945928992842\n",
      "Iteration 16922: loss = 0.002507934734787853\n",
      "Iteration 16923: loss = 0.0025079235407052722\n",
      "Iteration 16924: loss = 0.002507912346745097\n",
      "Iteration 16925: loss = 0.002507901152907327\n",
      "Iteration 16926: loss = 0.0025078899591919595\n",
      "Iteration 16927: loss = 0.002507878765598994\n",
      "Iteration 16928: loss = 0.002507867572128429\n",
      "Iteration 16929: loss = 0.002507856378780262\n",
      "Iteration 16930: loss = 0.0025078451855544926\n",
      "Iteration 16931: loss = 0.002507833992451119\n",
      "Iteration 16932: loss = 0.002507822799470139\n",
      "Iteration 16933: loss = 0.0025078116066115518\n",
      "Iteration 16934: loss = 0.002507800413875356\n",
      "Iteration 16935: loss = 0.0025077892212615506\n",
      "Iteration 16936: loss = 0.0025077780287701325\n",
      "Iteration 16937: loss = 0.002507766836401102\n",
      "Iteration 16938: loss = 0.002507755644154457\n",
      "Iteration 16939: loss = 0.0025077444520301955\n",
      "Iteration 16940: loss = 0.0025077332600283164\n",
      "Iteration 16941: loss = 0.0025077220681488177\n",
      "Iteration 16942: loss = 0.0025077108763916986\n",
      "Iteration 16943: loss = 0.002507699684756958\n",
      "Iteration 16944: loss = 0.0025076884932445933\n",
      "Iteration 16945: loss = 0.002507677301854604\n",
      "Iteration 16946: loss = 0.002507666110586988\n",
      "Iteration 16947: loss = 0.002507654919441744\n",
      "Iteration 16948: loss = 0.0025076437284188703\n",
      "Iteration 16949: loss = 0.0025076325375183663\n",
      "Iteration 16950: loss = 0.0025076213467402292\n",
      "Iteration 16951: loss = 0.0025076101560844587\n",
      "Iteration 16952: loss = 0.0025075989655510525\n",
      "Iteration 16953: loss = 0.0025075877751400094\n",
      "Iteration 16954: loss = 0.0025075765848513276\n",
      "Iteration 16955: loss = 0.0025075653946850063\n",
      "Iteration 16956: loss = 0.0025075542046410436\n",
      "Iteration 16957: loss = 0.002507543014719438\n",
      "Iteration 16958: loss = 0.0025075318249201885\n",
      "Iteration 16959: loss = 0.002507520635243293\n",
      "Iteration 16960: loss = 0.00250750944568875\n",
      "Iteration 16961: loss = 0.0025074982562565584\n",
      "Iteration 16962: loss = 0.002507487066946717\n",
      "Iteration 16963: loss = 0.0025074758777592233\n",
      "Iteration 16964: loss = 0.002507464688694077\n",
      "Iteration 16965: loss = 0.0025074534997512753\n",
      "Iteration 16966: loss = 0.002507442310930818\n",
      "Iteration 16967: loss = 0.0025074311222327028\n",
      "Iteration 16968: loss = 0.002507419933656929\n",
      "Iteration 16969: loss = 0.002507408745203494\n",
      "Iteration 16970: loss = 0.0025073975568723978\n",
      "Iteration 16971: loss = 0.0025073863686636373\n",
      "Iteration 16972: loss = 0.0025073751805772117\n",
      "Iteration 16973: loss = 0.00250736399261312\n",
      "Iteration 16974: loss = 0.0025073528047713602\n",
      "Iteration 16975: loss = 0.002507341617051931\n",
      "Iteration 16976: loss = 0.0025073304294548305\n",
      "Iteration 16977: loss = 0.0025073192419800584\n",
      "Iteration 16978: loss = 0.0025073080546276117\n",
      "Iteration 16979: loss = 0.0025072968673974893\n",
      "Iteration 16980: loss = 0.002507285680289691\n",
      "Iteration 16981: loss = 0.002507274493304214\n",
      "Iteration 16982: loss = 0.002507263306441057\n",
      "Iteration 16983: loss = 0.002507252119700219\n",
      "Iteration 16984: loss = 0.0025072409330816974\n",
      "Iteration 16985: loss = 0.0025072297465854924\n",
      "Iteration 16986: loss = 0.002507218560211602\n",
      "Iteration 16987: loss = 0.0025072073739600235\n",
      "Iteration 16988: loss = 0.0025071961878307563\n",
      "Iteration 16989: loss = 0.0025071850018237997\n",
      "Iteration 16990: loss = 0.002507173815939151\n",
      "Iteration 16991: loss = 0.002507162630176809\n",
      "Iteration 16992: loss = 0.002507151444536773\n",
      "Iteration 16993: loss = 0.0025071402590190403\n",
      "Iteration 16994: loss = 0.0025071290736236097\n",
      "Iteration 16995: loss = 0.00250711788835048\n",
      "Iteration 16996: loss = 0.002507106703199651\n",
      "Iteration 16997: loss = 0.002507095518171119\n",
      "Iteration 16998: loss = 0.0025070843332648837\n",
      "Iteration 16999: loss = 0.002507073148480944\n",
      "Iteration 17000: loss = 0.002507061963819297\n",
      "Iteration 17001: loss = 0.0025070507792799425\n",
      "Iteration 17002: loss = 0.002507039594862878\n",
      "Iteration 17003: loss = 0.0025070284105681035\n",
      "Iteration 17004: loss = 0.002507017226395617\n",
      "Iteration 17005: loss = 0.0025070060423454154\n",
      "Iteration 17006: loss = 0.0025069948584174996\n",
      "Iteration 17007: loss = 0.0025069836746118663\n",
      "Iteration 17008: loss = 0.002506972490928514\n",
      "Iteration 17009: loss = 0.002506961307367444\n",
      "Iteration 17010: loss = 0.002506950123928651\n",
      "Iteration 17011: loss = 0.002506938940612136\n",
      "Iteration 17012: loss = 0.0025069277574178966\n",
      "Iteration 17013: loss = 0.002506916574345932\n",
      "Iteration 17014: loss = 0.0025069053913962394\n",
      "Iteration 17015: loss = 0.0025068942085688184\n",
      "Iteration 17016: loss = 0.002506883025863668\n",
      "Iteration 17017: loss = 0.0025068718432807854\n",
      "Iteration 17018: loss = 0.00250686066082017\n",
      "Iteration 17019: loss = 0.00250684947848182\n",
      "Iteration 17020: loss = 0.0025068382962657337\n",
      "Iteration 17021: loss = 0.0025068271141719107\n",
      "Iteration 17022: loss = 0.002506815932200348\n",
      "Iteration 17023: loss = 0.002506804750351045\n",
      "Iteration 17024: loss = 0.0025067935686239997\n",
      "Iteration 17025: loss = 0.0025067823870192113\n",
      "Iteration 17026: loss = 0.0025067712055366785\n",
      "Iteration 17027: loss = 0.002506760024176399\n",
      "Iteration 17028: loss = 0.002506748842938372\n",
      "Iteration 17029: loss = 0.0025067376618225948\n",
      "Iteration 17030: loss = 0.002506726480829067\n",
      "Iteration 17031: loss = 0.0025067152999577874\n",
      "Iteration 17032: loss = 0.002506704119208754\n",
      "Iteration 17033: loss = 0.0025066929385819652\n",
      "Iteration 17034: loss = 0.0025066817580774196\n",
      "Iteration 17035: loss = 0.0025066705776951165\n",
      "Iteration 17036: loss = 0.002506659397435053\n",
      "Iteration 17037: loss = 0.0025066482172972286\n",
      "Iteration 17038: loss = 0.0025066370372816412\n",
      "Iteration 17039: loss = 0.0025066258573882903\n",
      "Iteration 17040: loss = 0.002506614677617174\n",
      "Iteration 17041: loss = 0.0025066034979682903\n",
      "Iteration 17042: loss = 0.002506592318441638\n",
      "Iteration 17043: loss = 0.0025065811390372152\n",
      "Iteration 17044: loss = 0.0025065699597550217\n",
      "Iteration 17045: loss = 0.002506558780595055\n",
      "Iteration 17046: loss = 0.002506547601557314\n",
      "Iteration 17047: loss = 0.002506536422641797\n",
      "Iteration 17048: loss = 0.0025065252438485025\n",
      "Iteration 17049: loss = 0.002506514065177429\n",
      "Iteration 17050: loss = 0.0025065028866285755\n",
      "Iteration 17051: loss = 0.0025064917082019402\n",
      "Iteration 17052: loss = 0.002506480529897522\n",
      "Iteration 17053: loss = 0.002506469351715318\n",
      "Iteration 17054: loss = 0.002506458173655329\n",
      "Iteration 17055: loss = 0.0025064469957175512\n",
      "Iteration 17056: loss = 0.002506435817901985\n",
      "Iteration 17057: loss = 0.002506424640208627\n",
      "Iteration 17058: loss = 0.0025064134626374772\n",
      "Iteration 17059: loss = 0.0025064022851885343\n",
      "Iteration 17060: loss = 0.002506391107861797\n",
      "Iteration 17061: loss = 0.0025063799306572617\n",
      "Iteration 17062: loss = 0.002506368753574929\n",
      "Iteration 17063: loss = 0.0025063575766147966\n",
      "Iteration 17064: loss = 0.0025063463997768635\n",
      "Iteration 17065: loss = 0.002506335223061128\n",
      "Iteration 17066: loss = 0.002506324046467588\n",
      "Iteration 17067: loss = 0.002506312869996243\n",
      "Iteration 17068: loss = 0.0025063016936470906\n",
      "Iteration 17069: loss = 0.00250629051742013\n",
      "Iteration 17070: loss = 0.00250627934131536\n",
      "Iteration 17071: loss = 0.002506268165332778\n",
      "Iteration 17072: loss = 0.0025062569894723835\n",
      "Iteration 17073: loss = 0.0025062458137341745\n",
      "Iteration 17074: loss = 0.00250623463811815\n",
      "Iteration 17075: loss = 0.0025062234626243084\n",
      "Iteration 17076: loss = 0.0025062122872526477\n",
      "Iteration 17077: loss = 0.0025062011120031667\n",
      "Iteration 17078: loss = 0.002506189936875865\n",
      "Iteration 17079: loss = 0.0025061787618707396\n",
      "Iteration 17080: loss = 0.0025061675869877892\n",
      "Iteration 17081: loss = 0.002506156412227013\n",
      "Iteration 17082: loss = 0.0025061452375884094\n",
      "Iteration 17083: loss = 0.002506134063071976\n",
      "Iteration 17084: loss = 0.0025061228886777134\n",
      "Iteration 17085: loss = 0.0025061117144056177\n",
      "Iteration 17086: loss = 0.002506100540255689\n",
      "Iteration 17087: loss = 0.0025060893662279247\n",
      "Iteration 17088: loss = 0.0025060781923223244\n",
      "Iteration 17089: loss = 0.002506067018538887\n",
      "Iteration 17090: loss = 0.0025060558448776094\n",
      "Iteration 17091: loss = 0.002506044671338491\n",
      "Iteration 17092: loss = 0.002506033497921531\n",
      "Iteration 17093: loss = 0.0025060223246267262\n",
      "Iteration 17094: loss = 0.0025060111514540765\n",
      "Iteration 17095: loss = 0.00250599997840358\n",
      "Iteration 17096: loss = 0.0025059888054752353\n",
      "Iteration 17097: loss = 0.002505977632669041\n",
      "Iteration 17098: loss = 0.0025059664599849955\n",
      "Iteration 17099: loss = 0.0025059552874230973\n",
      "Iteration 17100: loss = 0.002505944114983345\n",
      "Iteration 17101: loss = 0.0025059329426657373\n",
      "Iteration 17102: loss = 0.0025059217704702726\n",
      "Iteration 17103: loss = 0.0025059105983969493\n",
      "Iteration 17104: loss = 0.0025058994264457657\n",
      "Iteration 17105: loss = 0.002505888254616721\n",
      "Iteration 17106: loss = 0.002505877082909813\n",
      "Iteration 17107: loss = 0.00250586591132504\n",
      "Iteration 17108: loss = 0.002505854739862402\n",
      "Iteration 17109: loss = 0.0025058435685218964\n",
      "Iteration 17110: loss = 0.002505832397303522\n",
      "Iteration 17111: loss = 0.0025058212262072774\n",
      "Iteration 17112: loss = 0.0025058100552331607\n",
      "Iteration 17113: loss = 0.0025057988843811703\n",
      "Iteration 17114: loss = 0.0025057877136513054\n",
      "Iteration 17115: loss = 0.002505776543043565\n",
      "Iteration 17116: loss = 0.002505765372557947\n",
      "Iteration 17117: loss = 0.002505754202194449\n",
      "Iteration 17118: loss = 0.00250574303195307\n",
      "Iteration 17119: loss = 0.00250573186183381\n",
      "Iteration 17120: loss = 0.002505720691836665\n",
      "Iteration 17121: loss = 0.002505709521961636\n",
      "Iteration 17122: loss = 0.0025056983522087203\n",
      "Iteration 17123: loss = 0.0025056871825779163\n",
      "Iteration 17124: loss = 0.0025056760130692233\n",
      "Iteration 17125: loss = 0.002505664843682639\n",
      "Iteration 17126: loss = 0.0025056536744181624\n",
      "Iteration 17127: loss = 0.0025056425052757915\n",
      "Iteration 17128: loss = 0.0025056313362555255\n",
      "Iteration 17129: loss = 0.0025056201673573633\n",
      "Iteration 17130: loss = 0.0025056089985813013\n",
      "Iteration 17131: loss = 0.0025055978299273404\n",
      "Iteration 17132: loss = 0.0025055866613954785\n",
      "Iteration 17133: loss = 0.0025055754929857134\n",
      "Iteration 17134: loss = 0.0025055643246980437\n",
      "Iteration 17135: loss = 0.002505553156532469\n",
      "Iteration 17136: loss = 0.0025055419884889864\n",
      "Iteration 17137: loss = 0.0025055308205675962\n",
      "Iteration 17138: loss = 0.002505519652768295\n",
      "Iteration 17139: loss = 0.002505508485091083\n",
      "Iteration 17140: loss = 0.002505497317535957\n",
      "Iteration 17141: loss = 0.002505486150102917\n",
      "Iteration 17142: loss = 0.002505474982791961\n",
      "Iteration 17143: loss = 0.002505463815603087\n",
      "Iteration 17144: loss = 0.002505452648536294\n",
      "Iteration 17145: loss = 0.002505441481591581\n",
      "Iteration 17146: loss = 0.002505430314768947\n",
      "Iteration 17147: loss = 0.0025054191480683883\n",
      "Iteration 17148: loss = 0.0025054079814899053\n",
      "Iteration 17149: loss = 0.0025053968150334956\n",
      "Iteration 17150: loss = 0.002505385648699159\n",
      "Iteration 17151: loss = 0.002505374482486892\n",
      "Iteration 17152: loss = 0.002505363316396695\n",
      "Iteration 17153: loss = 0.0025053521504285656\n",
      "Iteration 17154: loss = 0.0025053409845825023\n",
      "Iteration 17155: loss = 0.0025053298188585037\n",
      "Iteration 17156: loss = 0.002505318653256569\n",
      "Iteration 17157: loss = 0.0025053074877766962\n",
      "Iteration 17158: loss = 0.002505296322418884\n",
      "Iteration 17159: loss = 0.0025052851571831297\n",
      "Iteration 17160: loss = 0.0025052739920694337\n",
      "Iteration 17161: loss = 0.0025052628270777937\n",
      "Iteration 17162: loss = 0.0025052516622082084\n",
      "Iteration 17163: loss = 0.0025052404974606757\n",
      "Iteration 17164: loss = 0.002505229332835195\n",
      "Iteration 17165: loss = 0.002505218168331764\n",
      "Iteration 17166: loss = 0.0025052070039503823\n",
      "Iteration 17167: loss = 0.002505195839691047\n",
      "Iteration 17168: loss = 0.0025051846755537576\n",
      "Iteration 17169: loss = 0.0025051735115385124\n",
      "Iteration 17170: loss = 0.0025051623476453106\n",
      "Iteration 17171: loss = 0.0025051511838741496\n",
      "Iteration 17172: loss = 0.0025051400202250286\n",
      "Iteration 17173: loss = 0.002505128856697946\n",
      "Iteration 17174: loss = 0.0025051176932929\n",
      "Iteration 17175: loss = 0.0025051065300098897\n",
      "Iteration 17176: loss = 0.0025050953668489134\n",
      "Iteration 17177: loss = 0.002505084203809969\n",
      "Iteration 17178: loss = 0.002505073040893056\n",
      "Iteration 17179: loss = 0.0025050618780981726\n",
      "Iteration 17180: loss = 0.002505050715425317\n",
      "Iteration 17181: loss = 0.002505039552874488\n",
      "Iteration 17182: loss = 0.0025050283904456843\n",
      "Iteration 17183: loss = 0.002505017228138904\n",
      "Iteration 17184: loss = 0.002505006065954146\n",
      "Iteration 17185: loss = 0.0025049949038914083\n",
      "Iteration 17186: loss = 0.0025049837419506907\n",
      "Iteration 17187: loss = 0.0025049725801319905\n",
      "Iteration 17188: loss = 0.0025049614184353064\n",
      "Iteration 17189: loss = 0.0025049502568606375\n",
      "Iteration 17190: loss = 0.0025049390954079817\n",
      "Iteration 17191: loss = 0.0025049279340773373\n",
      "Iteration 17192: loss = 0.0025049167728687038\n",
      "Iteration 17193: loss = 0.002504905611782079\n",
      "Iteration 17194: loss = 0.002504894450817462\n",
      "Iteration 17195: loss = 0.0025048832899748513\n",
      "Iteration 17196: loss = 0.002504872129254244\n",
      "Iteration 17197: loss = 0.0025048609686556408\n",
      "Iteration 17198: loss = 0.0025048498081790384\n",
      "Iteration 17199: loss = 0.002504838647824436\n",
      "Iteration 17200: loss = 0.002504827487591833\n",
      "Iteration 17201: loss = 0.002504816327481227\n",
      "Iteration 17202: loss = 0.0025048051674926163\n",
      "Iteration 17203: loss = 0.002504794007626\n",
      "Iteration 17204: loss = 0.0025047828478813768\n",
      "Iteration 17205: loss = 0.0025047716882587445\n",
      "Iteration 17206: loss = 0.0025047605287581024\n",
      "Iteration 17207: loss = 0.002504749369379448\n",
      "Iteration 17208: loss = 0.002504738210122781\n",
      "Iteration 17209: loss = 0.002504727050988099\n",
      "Iteration 17210: loss = 0.002504715891975401\n",
      "Iteration 17211: loss = 0.002504704733084686\n",
      "Iteration 17212: loss = 0.0025046935743159517\n",
      "Iteration 17213: loss = 0.0025046824156691964\n",
      "Iteration 17214: loss = 0.00250467125714442\n",
      "Iteration 17215: loss = 0.0025046600987416196\n",
      "Iteration 17216: loss = 0.0025046489404607947\n",
      "Iteration 17217: loss = 0.0025046377823019434\n",
      "Iteration 17218: loss = 0.002504626624265064\n",
      "Iteration 17219: loss = 0.002504615466350155\n",
      "Iteration 17220: loss = 0.002504604308557216\n",
      "Iteration 17221: loss = 0.002504593150886244\n",
      "Iteration 17222: loss = 0.002504581993337239\n",
      "Iteration 17223: loss = 0.002504570835910198\n",
      "Iteration 17224: loss = 0.0025045596786051213\n",
      "Iteration 17225: loss = 0.0025045485214220064\n",
      "Iteration 17226: loss = 0.002504537364360851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17227: loss = 0.0025045262074216553\n",
      "Iteration 17228: loss = 0.002504515050604417\n",
      "Iteration 17229: loss = 0.0025045038939091346\n",
      "Iteration 17230: loss = 0.0025044927373358066\n",
      "Iteration 17231: loss = 0.002504481580884432\n",
      "Iteration 17232: loss = 0.0025044704245550087\n",
      "Iteration 17233: loss = 0.0025044592683475353\n",
      "Iteration 17234: loss = 0.002504448112262011\n",
      "Iteration 17235: loss = 0.0025044369562984337\n",
      "Iteration 17236: loss = 0.0025044258004568026\n",
      "Iteration 17237: loss = 0.002504414644737115\n",
      "Iteration 17238: loss = 0.0025044034891393706\n",
      "Iteration 17239: loss = 0.002504392333663568\n",
      "Iteration 17240: loss = 0.0025043811783097046\n",
      "Iteration 17241: loss = 0.0025043700230777794\n",
      "Iteration 17242: loss = 0.0025043588679677918\n",
      "Iteration 17243: loss = 0.002504347712979739\n",
      "Iteration 17244: loss = 0.0025043365581136207\n",
      "Iteration 17245: loss = 0.0025043254033694346\n",
      "Iteration 17246: loss = 0.00250431424874718\n",
      "Iteration 17247: loss = 0.002504303094246854\n",
      "Iteration 17248: loss = 0.002504291939868457\n",
      "Iteration 17249: loss = 0.002504280785611986\n",
      "Iteration 17250: loss = 0.0025042696314774406\n",
      "Iteration 17251: loss = 0.0025042584774648186\n",
      "Iteration 17252: loss = 0.002504247323574119\n",
      "Iteration 17253: loss = 0.00250423616980534\n",
      "Iteration 17254: loss = 0.0025042250161584805\n",
      "Iteration 17255: loss = 0.0025042138626335383\n",
      "Iteration 17256: loss = 0.002504202709230513\n",
      "Iteration 17257: loss = 0.0025041915559494026\n",
      "Iteration 17258: loss = 0.0025041804027902057\n",
      "Iteration 17259: loss = 0.0025041692497529203\n",
      "Iteration 17260: loss = 0.0025041580968375456\n",
      "Iteration 17261: loss = 0.0025041469440440795\n",
      "Iteration 17262: loss = 0.002504135791372522\n",
      "Iteration 17263: loss = 0.00250412463882287\n",
      "Iteration 17264: loss = 0.002504113486395122\n",
      "Iteration 17265: loss = 0.0025041023340892777\n",
      "Iteration 17266: loss = 0.002504091181905335\n",
      "Iteration 17267: loss = 0.002504080029843293\n",
      "Iteration 17268: loss = 0.0025040688779031485\n",
      "Iteration 17269: loss = 0.002504057726084902\n",
      "Iteration 17270: loss = 0.002504046574388552\n",
      "Iteration 17271: loss = 0.0025040354228140953\n",
      "Iteration 17272: loss = 0.0025040242713615323\n",
      "Iteration 17273: loss = 0.0025040131200308596\n",
      "Iteration 17274: loss = 0.0025040019688220773\n",
      "Iteration 17275: loss = 0.0025039908177351836\n",
      "Iteration 17276: loss = 0.0025039796667701776\n",
      "Iteration 17277: loss = 0.002503968515927056\n",
      "Iteration 17278: loss = 0.0025039573652058185\n",
      "Iteration 17279: loss = 0.002503946214606464\n",
      "Iteration 17280: loss = 0.002503935064128991\n",
      "Iteration 17281: loss = 0.0025039239137733967\n",
      "Iteration 17282: loss = 0.002503912763539681\n",
      "Iteration 17283: loss = 0.0025039016134278426\n",
      "Iteration 17284: loss = 0.0025038904634378796\n",
      "Iteration 17285: loss = 0.002503879313569789\n",
      "Iteration 17286: loss = 0.0025038681638235713\n",
      "Iteration 17287: loss = 0.0025038570141992247\n",
      "Iteration 17288: loss = 0.0025038458646967476\n",
      "Iteration 17289: loss = 0.0025038347153161383\n",
      "Iteration 17290: loss = 0.0025038235660573955\n",
      "Iteration 17291: loss = 0.002503812416920518\n",
      "Iteration 17292: loss = 0.0025038012679055033\n",
      "Iteration 17293: loss = 0.0025037901190123504\n",
      "Iteration 17294: loss = 0.002503778970241059\n",
      "Iteration 17295: loss = 0.002503767821591626\n",
      "Iteration 17296: loss = 0.0025037566730640513\n",
      "Iteration 17297: loss = 0.0025037455246583323\n",
      "Iteration 17298: loss = 0.002503734376374468\n",
      "Iteration 17299: loss = 0.002503723228212457\n",
      "Iteration 17300: loss = 0.002503712080172299\n",
      "Iteration 17301: loss = 0.002503700932253989\n",
      "Iteration 17302: loss = 0.0025036897844575292\n",
      "Iteration 17303: loss = 0.0025036786367829173\n",
      "Iteration 17304: loss = 0.00250366748923015\n",
      "Iteration 17305: loss = 0.0025036563417992277\n",
      "Iteration 17306: loss = 0.002503645194490149\n",
      "Iteration 17307: loss = 0.002503634047302911\n",
      "Iteration 17308: loss = 0.0025036229002375135\n",
      "Iteration 17309: loss = 0.0025036117532939543\n",
      "Iteration 17310: loss = 0.0025036006064722326\n",
      "Iteration 17311: loss = 0.002503589459772346\n",
      "Iteration 17312: loss = 0.0025035783131942936\n",
      "Iteration 17313: loss = 0.0025035671667380747\n",
      "Iteration 17314: loss = 0.0025035560204036867\n",
      "Iteration 17315: loss = 0.002503544874191128\n",
      "Iteration 17316: loss = 0.002503533728100398\n",
      "Iteration 17317: loss = 0.002503522582131495\n",
      "Iteration 17318: loss = 0.0025035114362844174\n",
      "Iteration 17319: loss = 0.002503500290559163\n",
      "Iteration 17320: loss = 0.0025034891449557317\n",
      "Iteration 17321: loss = 0.0025034779994741216\n",
      "Iteration 17322: loss = 0.0025034668541143307\n",
      "Iteration 17323: loss = 0.0025034557088763577\n",
      "Iteration 17324: loss = 0.0025034445637602014\n",
      "Iteration 17325: loss = 0.0025034334187658603\n",
      "Iteration 17326: loss = 0.002503422273893333\n",
      "Iteration 17327: loss = 0.0025034111291426173\n",
      "Iteration 17328: loss = 0.002503399984513713\n",
      "Iteration 17329: loss = 0.002503388840006618\n",
      "Iteration 17330: loss = 0.00250337769562133\n",
      "Iteration 17331: loss = 0.002503366551357849\n",
      "Iteration 17332: loss = 0.002503355407216173\n",
      "Iteration 17333: loss = 0.0025033442631963\n",
      "Iteration 17334: loss = 0.002503333119298229\n",
      "Iteration 17335: loss = 0.002503321975521958\n",
      "Iteration 17336: loss = 0.0025033108318674867\n",
      "Iteration 17337: loss = 0.002503299688334813\n",
      "Iteration 17338: loss = 0.002503288544923935\n",
      "Iteration 17339: loss = 0.0025032774016348516\n",
      "Iteration 17340: loss = 0.0025032662584675617\n",
      "Iteration 17341: loss = 0.002503255115422063\n",
      "Iteration 17342: loss = 0.0025032439724983545\n",
      "Iteration 17343: loss = 0.0025032328296964355\n",
      "Iteration 17344: loss = 0.0025032216870163036\n",
      "Iteration 17345: loss = 0.002503210544457957\n",
      "Iteration 17346: loss = 0.0025031994020213947\n",
      "Iteration 17347: loss = 0.0025031882597066155\n",
      "Iteration 17348: loss = 0.0025031771175136174\n",
      "Iteration 17349: loss = 0.0025031659754424\n",
      "Iteration 17350: loss = 0.0025031548334929613\n",
      "Iteration 17351: loss = 0.0025031436916652986\n",
      "Iteration 17352: loss = 0.002503132549959412\n",
      "Iteration 17353: loss = 0.0025031214083752994\n",
      "Iteration 17354: loss = 0.0025031102669129594\n",
      "Iteration 17355: loss = 0.0025030991255723905\n",
      "Iteration 17356: loss = 0.002503087984353592\n",
      "Iteration 17357: loss = 0.002503076843256561\n",
      "Iteration 17358: loss = 0.002503065702281297\n",
      "Iteration 17359: loss = 0.002503054561427798\n",
      "Iteration 17360: loss = 0.0025030434206960634\n",
      "Iteration 17361: loss = 0.0025030322800860907\n",
      "Iteration 17362: loss = 0.002503021139597879\n",
      "Iteration 17363: loss = 0.002503009999231427\n",
      "Iteration 17364: loss = 0.0025029988589867326\n",
      "Iteration 17365: loss = 0.0025029877188637952\n",
      "Iteration 17366: loss = 0.0025029765788626124\n",
      "Iteration 17367: loss = 0.0025029654389831834\n",
      "Iteration 17368: loss = 0.0025029542992255067\n",
      "Iteration 17369: loss = 0.002502943159589581\n",
      "Iteration 17370: loss = 0.002502932020075404\n",
      "Iteration 17371: loss = 0.0025029208806829745\n",
      "Iteration 17372: loss = 0.002502909741412291\n",
      "Iteration 17373: loss = 0.0025028986022633533\n",
      "Iteration 17374: loss = 0.0025028874632361587\n",
      "Iteration 17375: loss = 0.0025028763243307053\n",
      "Iteration 17376: loss = 0.002502865185546993\n",
      "Iteration 17377: loss = 0.00250285404688502\n",
      "Iteration 17378: loss = 0.0025028429083447834\n",
      "Iteration 17379: loss = 0.0025028317699262834\n",
      "Iteration 17380: loss = 0.002502820631629518\n",
      "Iteration 17381: loss = 0.0025028094934544855\n",
      "Iteration 17382: loss = 0.0025027983554011847\n",
      "Iteration 17383: loss = 0.0025027872174696137\n",
      "Iteration 17384: loss = 0.0025027760796597727\n",
      "Iteration 17385: loss = 0.002502764941971658\n",
      "Iteration 17386: loss = 0.0025027538044052693\n",
      "Iteration 17387: loss = 0.0025027426669606044\n",
      "Iteration 17388: loss = 0.0025027315296376634\n",
      "Iteration 17389: loss = 0.0025027203924364427\n",
      "Iteration 17390: loss = 0.0025027092553569423\n",
      "Iteration 17391: loss = 0.0025026981183991605\n",
      "Iteration 17392: loss = 0.0025026869815630956\n",
      "Iteration 17393: loss = 0.0025026758448487463\n",
      "Iteration 17394: loss = 0.002502664708256111\n",
      "Iteration 17395: loss = 0.0025026535717851883\n",
      "Iteration 17396: loss = 0.0025026424354359766\n",
      "Iteration 17397: loss = 0.0025026312992084744\n",
      "Iteration 17398: loss = 0.002502620163102681\n",
      "Iteration 17399: loss = 0.0025026090271185945\n",
      "Iteration 17400: loss = 0.0025025978912562134\n",
      "Iteration 17401: loss = 0.0025025867555155353\n",
      "Iteration 17402: loss = 0.00250257561989656\n",
      "Iteration 17403: loss = 0.0025025644843992855\n",
      "Iteration 17404: loss = 0.0025025533490237103\n",
      "Iteration 17405: loss = 0.0025025422137698334\n",
      "Iteration 17406: loss = 0.002502531078637653\n",
      "Iteration 17407: loss = 0.002502519943627167\n",
      "Iteration 17408: loss = 0.002502508808738375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17409: loss = 0.002502497673971276\n",
      "Iteration 17410: loss = 0.0025024865393258665\n",
      "Iteration 17411: loss = 0.0025024754048021468\n",
      "Iteration 17412: loss = 0.0025024642704001144\n",
      "Iteration 17413: loss = 0.002502453136119769\n",
      "Iteration 17414: loss = 0.002502442001961108\n",
      "Iteration 17415: loss = 0.0025024308679241296\n",
      "Iteration 17416: loss = 0.0025024197340088343\n",
      "Iteration 17417: loss = 0.0025024086002152185\n",
      "Iteration 17418: loss = 0.0025023974665432823\n",
      "Iteration 17419: loss = 0.002502386332993024\n",
      "Iteration 17420: loss = 0.0025023751995644408\n",
      "Iteration 17421: loss = 0.002502364066257532\n",
      "Iteration 17422: loss = 0.002502352933072297\n",
      "Iteration 17423: loss = 0.002502341800008734\n",
      "Iteration 17424: loss = 0.0025023306670668402\n",
      "Iteration 17425: loss = 0.0025023195342466155\n",
      "Iteration 17426: loss = 0.002502308401548058\n",
      "Iteration 17427: loss = 0.0025022972689711666\n",
      "Iteration 17428: loss = 0.002502286136515939\n",
      "Iteration 17429: loss = 0.0025022750041823754\n",
      "Iteration 17430: loss = 0.002502263871970472\n",
      "Iteration 17431: loss = 0.0025022527398802295\n",
      "Iteration 17432: loss = 0.0025022416079116447\n",
      "Iteration 17433: loss = 0.0025022304760647174\n",
      "Iteration 17434: loss = 0.0025022193443394454\n",
      "Iteration 17435: loss = 0.002502208212735828\n",
      "Iteration 17436: loss = 0.002502197081253862\n",
      "Iteration 17437: loss = 0.0025021859498935485\n",
      "Iteration 17438: loss = 0.002502174818654884\n",
      "Iteration 17439: loss = 0.002502163687537868\n",
      "Iteration 17440: loss = 0.002502152556542499\n",
      "Iteration 17441: loss = 0.0025021414256687752\n",
      "Iteration 17442: loss = 0.0025021302949166956\n",
      "Iteration 17443: loss = 0.002502119164286258\n",
      "Iteration 17444: loss = 0.002502108033777461\n",
      "Iteration 17445: loss = 0.0025020969033903036\n",
      "Iteration 17446: loss = 0.0025020857731247854\n",
      "Iteration 17447: loss = 0.0025020746429809024\n",
      "Iteration 17448: loss = 0.0025020635129586553\n",
      "Iteration 17449: loss = 0.0025020523830580417\n",
      "Iteration 17450: loss = 0.0025020412532790596\n",
      "Iteration 17451: loss = 0.002502030123621709\n",
      "Iteration 17452: loss = 0.0025020189940859875\n",
      "Iteration 17453: loss = 0.002502007864671893\n",
      "Iteration 17454: loss = 0.002501996735379426\n",
      "Iteration 17455: loss = 0.0025019856062085833\n",
      "Iteration 17456: loss = 0.0025019744771593642\n",
      "Iteration 17457: loss = 0.002501963348231767\n",
      "Iteration 17458: loss = 0.0025019522194257905\n",
      "Iteration 17459: loss = 0.0025019410907414323\n",
      "Iteration 17460: loss = 0.002501929962178692\n",
      "Iteration 17461: loss = 0.002501918833737568\n",
      "Iteration 17462: loss = 0.002501907705418059\n",
      "Iteration 17463: loss = 0.0025018965772201627\n",
      "Iteration 17464: loss = 0.0025018854491438776\n",
      "Iteration 17465: loss = 0.002501874321189204\n",
      "Iteration 17466: loss = 0.0025018631933561384\n",
      "Iteration 17467: loss = 0.0025018520656446804\n",
      "Iteration 17468: loss = 0.002501840938054828\n",
      "Iteration 17469: loss = 0.0025018298105865806\n",
      "Iteration 17470: loss = 0.0025018186832399354\n",
      "Iteration 17471: loss = 0.002501807556014892\n",
      "Iteration 17472: loss = 0.002501796428911448\n",
      "Iteration 17473: loss = 0.002501785301929604\n",
      "Iteration 17474: loss = 0.0025017741750693557\n",
      "Iteration 17475: loss = 0.002501763048330704\n",
      "Iteration 17476: loss = 0.0025017519217136464\n",
      "Iteration 17477: loss = 0.002501740795218182\n",
      "Iteration 17478: loss = 0.0025017296688443075\n",
      "Iteration 17479: loss = 0.002501718542592024\n",
      "Iteration 17480: loss = 0.0025017074164613282\n",
      "Iteration 17481: loss = 0.0025016962904522194\n",
      "Iteration 17482: loss = 0.0025016851645646965\n",
      "Iteration 17483: loss = 0.002501674038798757\n",
      "Iteration 17484: loss = 0.0025016629131544006\n",
      "Iteration 17485: loss = 0.0025016517876316246\n",
      "Iteration 17486: loss = 0.0025016406622304283\n",
      "Iteration 17487: loss = 0.00250162953695081\n",
      "Iteration 17488: loss = 0.002501618411792769\n",
      "Iteration 17489: loss = 0.002501607286756303\n",
      "Iteration 17490: loss = 0.0025015961618414105\n",
      "Iteration 17491: loss = 0.0025015850370480905\n",
      "Iteration 17492: loss = 0.0025015739123763416\n",
      "Iteration 17493: loss = 0.0025015627878261612\n",
      "Iteration 17494: loss = 0.0025015516633975486\n",
      "Iteration 17495: loss = 0.002501540539090503\n",
      "Iteration 17496: loss = 0.0025015294149050233\n",
      "Iteration 17497: loss = 0.002501518290841106\n",
      "Iteration 17498: loss = 0.0025015071668987514\n",
      "Iteration 17499: loss = 0.002501496043077957\n",
      "Iteration 17500: loss = 0.0025014849193787216\n",
      "Iteration 17501: loss = 0.002501473795801044\n",
      "Iteration 17502: loss = 0.0025014626723449227\n",
      "Iteration 17503: loss = 0.0025014515490103563\n",
      "Iteration 17504: loss = 0.0025014404257973433\n",
      "Iteration 17505: loss = 0.002501429302705882\n",
      "Iteration 17506: loss = 0.002501418179735971\n",
      "Iteration 17507: loss = 0.002501407056887609\n",
      "Iteration 17508: loss = 0.002501395934160794\n",
      "Iteration 17509: loss = 0.0025013848115555256\n",
      "Iteration 17510: loss = 0.0025013736890718017\n",
      "Iteration 17511: loss = 0.0025013625667096207\n",
      "Iteration 17512: loss = 0.0025013514444689818\n",
      "Iteration 17513: loss = 0.002501340322349882\n",
      "Iteration 17514: loss = 0.002501329200352322\n",
      "Iteration 17515: loss = 0.002501318078476299\n",
      "Iteration 17516: loss = 0.002501306956721812\n",
      "Iteration 17517: loss = 0.002501295835088859\n",
      "Iteration 17518: loss = 0.002501284713577439\n",
      "Iteration 17519: loss = 0.0025012735921875508\n",
      "Iteration 17520: loss = 0.0025012624709191914\n",
      "Iteration 17521: loss = 0.0025012513497723616\n",
      "Iteration 17522: loss = 0.0025012402287470582\n",
      "Iteration 17523: loss = 0.0025012291078432805\n",
      "Iteration 17524: loss = 0.0025012179870610276\n",
      "Iteration 17525: loss = 0.0025012068664002967\n",
      "Iteration 17526: loss = 0.0025011957458610868\n",
      "Iteration 17527: loss = 0.0025011846254433968\n",
      "Iteration 17528: loss = 0.0025011735051472255\n",
      "Iteration 17529: loss = 0.002501162384972571\n",
      "Iteration 17530: loss = 0.002501151264919431\n",
      "Iteration 17531: loss = 0.002501140144987806\n",
      "Iteration 17532: loss = 0.0025011290251776928\n",
      "Iteration 17533: loss = 0.002501117905489091\n",
      "Iteration 17534: loss = 0.0025011067859219984\n",
      "Iteration 17535: loss = 0.002501095666476414\n",
      "Iteration 17536: loss = 0.0025010845471523367\n",
      "Iteration 17537: loss = 0.002501073427949764\n",
      "Iteration 17538: loss = 0.002501062308868695\n",
      "Iteration 17539: loss = 0.002501051189909128\n",
      "Iteration 17540: loss = 0.0025010400710710626\n",
      "Iteration 17541: loss = 0.002501028952354496\n",
      "Iteration 17542: loss = 0.0025010178337594277\n",
      "Iteration 17543: loss = 0.002501006715285855\n",
      "Iteration 17544: loss = 0.002500995596933778\n",
      "Iteration 17545: loss = 0.0025009844787031938\n",
      "Iteration 17546: loss = 0.002500973360594102\n",
      "Iteration 17547: loss = 0.002500962242606501\n",
      "Iteration 17548: loss = 0.002500951124740389\n",
      "Iteration 17549: loss = 0.0025009400069957643\n",
      "Iteration 17550: loss = 0.0025009288893726262\n",
      "Iteration 17551: loss = 0.0025009177718709726\n",
      "Iteration 17552: loss = 0.0025009066544908025\n",
      "Iteration 17553: loss = 0.002500895537232114\n",
      "Iteration 17554: loss = 0.002500884420094906\n",
      "Iteration 17555: loss = 0.002500873303079177\n",
      "Iteration 17556: loss = 0.0025008621861849255\n",
      "Iteration 17557: loss = 0.00250085106941215\n",
      "Iteration 17558: loss = 0.002500839952760849\n",
      "Iteration 17559: loss = 0.0025008288362310207\n",
      "Iteration 17560: loss = 0.002500817719822664\n",
      "Iteration 17561: loss = 0.002500806603535778\n",
      "Iteration 17562: loss = 0.0025007954873703607\n",
      "Iteration 17563: loss = 0.0025007843713264095\n",
      "Iteration 17564: loss = 0.0025007732554039254\n",
      "Iteration 17565: loss = 0.0025007621396029053\n",
      "Iteration 17566: loss = 0.0025007510239233475\n",
      "Iteration 17567: loss = 0.002500739908365252\n",
      "Iteration 17568: loss = 0.002500728792928616\n",
      "Iteration 17569: loss = 0.0025007176776134384\n",
      "Iteration 17570: loss = 0.002500706562419718\n",
      "Iteration 17571: loss = 0.002500695447347453\n",
      "Iteration 17572: loss = 0.0025006843323966427\n",
      "Iteration 17573: loss = 0.0025006732175672846\n",
      "Iteration 17574: loss = 0.0025006621028593775\n",
      "Iteration 17575: loss = 0.002500650988272921\n",
      "Iteration 17576: loss = 0.002500639873807912\n",
      "Iteration 17577: loss = 0.00250062875946435\n",
      "Iteration 17578: loss = 0.002500617645242234\n",
      "Iteration 17579: loss = 0.002500606531141561\n",
      "Iteration 17580: loss = 0.0025005954171623303\n",
      "Iteration 17581: loss = 0.0025005843033045418\n",
      "Iteration 17582: loss = 0.0025005731895681917\n",
      "Iteration 17583: loss = 0.00250056207595328\n",
      "Iteration 17584: loss = 0.0025005509624598053\n",
      "Iteration 17585: loss = 0.0025005398490877658\n",
      "Iteration 17586: loss = 0.00250052873583716\n",
      "Iteration 17587: loss = 0.002500517622707986\n",
      "Iteration 17588: loss = 0.0025005065097002434\n",
      "Iteration 17589: loss = 0.00250049539681393\n",
      "Iteration 17590: loss = 0.002500484284049044\n",
      "Iteration 17591: loss = 0.0025004731714055855\n",
      "Iteration 17592: loss = 0.002500462058883551\n",
      "Iteration 17593: loss = 0.00250045094648294\n",
      "Iteration 17594: loss = 0.0025004398342037518\n",
      "Iteration 17595: loss = 0.0025004287220459836\n",
      "Iteration 17596: loss = 0.002500417610009635\n",
      "Iteration 17597: loss = 0.0025004064980947037\n",
      "Iteration 17598: loss = 0.0025003953863011884\n",
      "Iteration 17599: loss = 0.0025003842746290886\n",
      "Iteration 17600: loss = 0.0025003731630784017\n",
      "Iteration 17601: loss = 0.0025003620516491267\n",
      "Iteration 17602: loss = 0.0025003509403412625\n",
      "Iteration 17603: loss = 0.002500339829154807\n",
      "Iteration 17604: loss = 0.002500328718089759\n",
      "Iteration 17605: loss = 0.002500317607146117\n",
      "Iteration 17606: loss = 0.0025003064963238794\n",
      "Iteration 17607: loss = 0.0025002953856230452\n",
      "Iteration 17608: loss = 0.0025002842750436126\n",
      "Iteration 17609: loss = 0.0025002731645855803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17610: loss = 0.0025002620542489465\n",
      "Iteration 17611: loss = 0.00250025094403371\n",
      "Iteration 17612: loss = 0.00250023983393987\n",
      "Iteration 17613: loss = 0.0025002287239674235\n",
      "Iteration 17614: loss = 0.00250021761411637\n",
      "Iteration 17615: loss = 0.002500206504386709\n",
      "Iteration 17616: loss = 0.002500195394778437\n",
      "Iteration 17617: loss = 0.002500184285291554\n",
      "Iteration 17618: loss = 0.002500173175926058\n",
      "Iteration 17619: loss = 0.002500162066681948\n",
      "Iteration 17620: loss = 0.0025001509575592216\n",
      "Iteration 17621: loss = 0.0025001398485578787\n",
      "Iteration 17622: loss = 0.0025001287396779165\n",
      "Iteration 17623: loss = 0.0025001176309193346\n",
      "Iteration 17624: loss = 0.0025001065222821305\n",
      "Iteration 17625: loss = 0.0025000954137663037\n",
      "Iteration 17626: loss = 0.0025000843053718516\n",
      "Iteration 17627: loss = 0.002500073197098774\n",
      "Iteration 17628: loss = 0.0025000620889470697\n",
      "Iteration 17629: loss = 0.0025000509809167356\n",
      "Iteration 17630: loss = 0.002500039873007772\n",
      "Iteration 17631: loss = 0.0025000287652201755\n",
      "Iteration 17632: loss = 0.0025000176575539464\n",
      "Iteration 17633: loss = 0.0025000065500090825\n",
      "Iteration 17634: loss = 0.002499995442585582\n",
      "Iteration 17635: loss = 0.0024999843352834445\n",
      "Iteration 17636: loss = 0.0024999732281026678\n",
      "Iteration 17637: loss = 0.00249996212104325\n",
      "Iteration 17638: loss = 0.0024999510141051904\n",
      "Iteration 17639: loss = 0.0024999399072884875\n",
      "Iteration 17640: loss = 0.00249992880059314\n",
      "Iteration 17641: loss = 0.0024999176940191455\n",
      "Iteration 17642: loss = 0.0024999065875665034\n",
      "Iteration 17643: loss = 0.0024998954812352125\n",
      "Iteration 17644: loss = 0.0024998843750252708\n",
      "Iteration 17645: loss = 0.002499873268936676\n",
      "Iteration 17646: loss = 0.0024998621629694286\n",
      "Iteration 17647: loss = 0.0024998510571235255\n",
      "Iteration 17648: loss = 0.0024998399513989655\n",
      "Iteration 17649: loss = 0.002499828845795748\n",
      "Iteration 17650: loss = 0.0024998177403138715\n",
      "Iteration 17651: loss = 0.0024998066349533332\n",
      "Iteration 17652: loss = 0.0024997955297141328\n",
      "Iteration 17653: loss = 0.002499784424596269\n",
      "Iteration 17654: loss = 0.0024997733195997393\n",
      "Iteration 17655: loss = 0.002499762214724543\n",
      "Iteration 17656: loss = 0.0024997511099706785\n",
      "Iteration 17657: loss = 0.0024997400053381447\n",
      "Iteration 17658: loss = 0.0024997289008269396\n",
      "Iteration 17659: loss = 0.002499717796437062\n",
      "Iteration 17660: loss = 0.0024997066921685096\n",
      "Iteration 17661: loss = 0.0024996955880212833\n",
      "Iteration 17662: loss = 0.0024996844839953788\n",
      "Iteration 17663: loss = 0.002499673380090796\n",
      "Iteration 17664: loss = 0.002499662276307534\n",
      "Iteration 17665: loss = 0.0024996511726455904\n",
      "Iteration 17666: loss = 0.0024996400691049635\n",
      "Iteration 17667: loss = 0.0024996289656856532\n",
      "Iteration 17668: loss = 0.0024996178623876565\n",
      "Iteration 17669: loss = 0.0024996067592109733\n",
      "Iteration 17670: loss = 0.0024995956561556015\n",
      "Iteration 17671: loss = 0.0024995845532215398\n",
      "Iteration 17672: loss = 0.002499573450408786\n",
      "Iteration 17673: loss = 0.0024995623477173396\n",
      "Iteration 17674: loss = 0.0024995512451471986\n",
      "Iteration 17675: loss = 0.002499540142698362\n",
      "Iteration 17676: loss = 0.0024995290403708285\n",
      "Iteration 17677: loss = 0.0024995179381645956\n",
      "Iteration 17678: loss = 0.002499506836079663\n",
      "Iteration 17679: loss = 0.0024994957341160284\n",
      "Iteration 17680: loss = 0.002499484632273691\n",
      "Iteration 17681: loss = 0.0024994735305526486\n",
      "Iteration 17682: loss = 0.0024994624289529006\n",
      "Iteration 17683: loss = 0.002499451327474445\n",
      "Iteration 17684: loss = 0.0024994402261172806\n",
      "Iteration 17685: loss = 0.002499429124881405\n",
      "Iteration 17686: loss = 0.002499418023766818\n",
      "Iteration 17687: loss = 0.002499406922773518\n",
      "Iteration 17688: loss = 0.002499395821901503\n",
      "Iteration 17689: loss = 0.002499384721150772\n",
      "Iteration 17690: loss = 0.0024993736205213236\n",
      "Iteration 17691: loss = 0.002499362520013156\n",
      "Iteration 17692: loss = 0.0024993514196262672\n",
      "Iteration 17693: loss = 0.002499340319360657\n",
      "Iteration 17694: loss = 0.0024993292192163233\n",
      "Iteration 17695: loss = 0.002499318119193264\n",
      "Iteration 17696: loss = 0.002499307019291479\n",
      "Iteration 17697: loss = 0.002499295919510966\n",
      "Iteration 17698: loss = 0.002499284819851724\n",
      "Iteration 17699: loss = 0.0024992737203137503\n",
      "Iteration 17700: loss = 0.0024992626208970455\n",
      "Iteration 17701: loss = 0.002499251521601607\n",
      "Iteration 17702: loss = 0.0024992404224274327\n",
      "Iteration 17703: loss = 0.002499229323374522\n",
      "Iteration 17704: loss = 0.0024992182244428737\n",
      "Iteration 17705: loss = 0.0024992071256324856\n",
      "Iteration 17706: loss = 0.002499196026943357\n",
      "Iteration 17707: loss = 0.0024991849283754856\n",
      "Iteration 17708: loss = 0.0024991738299288704\n",
      "Iteration 17709: loss = 0.0024991627316035105\n",
      "Iteration 17710: loss = 0.0024991516333994035\n",
      "Iteration 17711: loss = 0.002499140535316548\n",
      "Iteration 17712: loss = 0.0024991294373549427\n",
      "Iteration 17713: loss = 0.002499118339514587\n",
      "Iteration 17714: loss = 0.002499107241795478\n",
      "Iteration 17715: loss = 0.002499096144197616\n",
      "Iteration 17716: loss = 0.0024990850467209976\n",
      "Iteration 17717: loss = 0.0024990739493656227\n",
      "Iteration 17718: loss = 0.0024990628521314897\n",
      "Iteration 17719: loss = 0.0024990517550185962\n",
      "Iteration 17720: loss = 0.0024990406580269425\n",
      "Iteration 17721: loss = 0.002499029561156525\n",
      "Iteration 17722: loss = 0.0024990184644073437\n",
      "Iteration 17723: loss = 0.0024990073677793974\n",
      "Iteration 17724: loss = 0.002498996271272683\n",
      "Iteration 17725: loss = 0.0024989851748872006\n",
      "Iteration 17726: loss = 0.002498974078622948\n",
      "Iteration 17727: loss = 0.002498962982479924\n",
      "Iteration 17728: loss = 0.0024989518864581284\n",
      "Iteration 17729: loss = 0.0024989407905575568\n",
      "Iteration 17730: loss = 0.0024989296947782102\n",
      "Iteration 17731: loss = 0.002498918599120086\n",
      "Iteration 17732: loss = 0.0024989075035831833\n",
      "Iteration 17733: loss = 0.0024988964081675002\n",
      "Iteration 17734: loss = 0.0024988853128730354\n",
      "Iteration 17735: loss = 0.0024988742176997878\n",
      "Iteration 17736: loss = 0.0024988631226477557\n",
      "Iteration 17737: loss = 0.0024988520277169374\n",
      "Iteration 17738: loss = 0.0024988409329073326\n",
      "Iteration 17739: loss = 0.0024988298382189376\n",
      "Iteration 17740: loss = 0.002498818743651753\n",
      "Iteration 17741: loss = 0.0024988076492057764\n",
      "Iteration 17742: loss = 0.0024987965548810063\n",
      "Iteration 17743: loss = 0.0024987854606774423\n",
      "Iteration 17744: loss = 0.0024987743665950816\n",
      "Iteration 17745: loss = 0.002498763272633924\n",
      "Iteration 17746: loss = 0.002498752178793967\n",
      "Iteration 17747: loss = 0.002498741085075209\n",
      "Iteration 17748: loss = 0.0024987299914776494\n",
      "Iteration 17749: loss = 0.0024987188980012866\n",
      "Iteration 17750: loss = 0.002498707804646119\n",
      "Iteration 17751: loss = 0.0024986967114121452\n",
      "Iteration 17752: loss = 0.002498685618299363\n",
      "Iteration 17753: loss = 0.002498674525307772\n",
      "Iteration 17754: loss = 0.00249866343243737\n",
      "Iteration 17755: loss = 0.0024986523396881566\n",
      "Iteration 17756: loss = 0.002498641247060129\n",
      "Iteration 17757: loss = 0.0024986301545532863\n",
      "Iteration 17758: loss = 0.002498619062167628\n",
      "Iteration 17759: loss = 0.0024986079699031508\n",
      "Iteration 17760: loss = 0.002498596877759855\n",
      "Iteration 17761: loss = 0.002498585785737738\n",
      "Iteration 17762: loss = 0.0024985746938367987\n",
      "Iteration 17763: loss = 0.002498563602057036\n",
      "Iteration 17764: loss = 0.002498552510398448\n",
      "Iteration 17765: loss = 0.002498541418861033\n",
      "Iteration 17766: loss = 0.00249853032744479\n",
      "Iteration 17767: loss = 0.0024985192361497177\n",
      "Iteration 17768: loss = 0.0024985081449758143\n",
      "Iteration 17769: loss = 0.002498497053923079\n",
      "Iteration 17770: loss = 0.002498485962991509\n",
      "Iteration 17771: loss = 0.0024984748721811043\n",
      "Iteration 17772: loss = 0.0024984637814918624\n",
      "Iteration 17773: loss = 0.002498452690923782\n",
      "Iteration 17774: loss = 0.002498441600476862\n",
      "Iteration 17775: loss = 0.0024984305101511013\n",
      "Iteration 17776: loss = 0.0024984194199464986\n",
      "Iteration 17777: loss = 0.0024984083298630506\n",
      "Iteration 17778: loss = 0.0024983972399007575\n",
      "Iteration 17779: loss = 0.0024983861500596175\n",
      "Iteration 17780: loss = 0.0024983750603396293\n",
      "Iteration 17781: loss = 0.002498363970740791\n",
      "Iteration 17782: loss = 0.0024983528812631017\n",
      "Iteration 17783: loss = 0.0024983417919065597\n",
      "Iteration 17784: loss = 0.0024983307026711626\n",
      "Iteration 17785: loss = 0.0024983196135569107\n",
      "Iteration 17786: loss = 0.0024983085245638015\n",
      "Iteration 17787: loss = 0.0024982974356918333\n",
      "Iteration 17788: loss = 0.0024982863469410056\n",
      "Iteration 17789: loss = 0.0024982752583113167\n",
      "Iteration 17790: loss = 0.002498264169802764\n",
      "Iteration 17791: loss = 0.0024982530814153477\n",
      "Iteration 17792: loss = 0.002498241993149065\n",
      "Iteration 17793: loss = 0.0024982309050039153\n",
      "Iteration 17794: loss = 0.0024982198169798964\n",
      "Iteration 17795: loss = 0.0024982087290770084\n",
      "Iteration 17796: loss = 0.002498197641295248\n",
      "Iteration 17797: loss = 0.0024981865536346146\n",
      "Iteration 17798: loss = 0.002498175466095107\n",
      "Iteration 17799: loss = 0.002498164378676723\n",
      "Iteration 17800: loss = 0.0024981532913794617\n",
      "Iteration 17801: loss = 0.0024981422042033214\n",
      "Iteration 17802: loss = 0.002498131117148301\n",
      "Iteration 17803: loss = 0.002498120030214399\n",
      "Iteration 17804: loss = 0.0024981089434016135\n",
      "Iteration 17805: loss = 0.0024980978567099433\n",
      "Iteration 17806: loss = 0.002498086770139387\n",
      "Iteration 17807: loss = 0.002498075683689943\n",
      "Iteration 17808: loss = 0.00249806459736161\n",
      "Iteration 17809: loss = 0.002498053511154387\n",
      "Iteration 17810: loss = 0.0024980424250682716\n",
      "Iteration 17811: loss = 0.0024980313391032628\n",
      "Iteration 17812: loss = 0.0024980202532593593\n",
      "Iteration 17813: loss = 0.0024980091675365595\n",
      "Iteration 17814: loss = 0.002497998081934862\n",
      "Iteration 17815: loss = 0.0024979869964542652\n",
      "Iteration 17816: loss = 0.0024979759110947677\n",
      "Iteration 17817: loss = 0.002497964825856368\n",
      "Iteration 17818: loss = 0.0024979537407390654\n",
      "Iteration 17819: loss = 0.002497942655742857\n",
      "Iteration 17820: loss = 0.0024979315708677425\n",
      "Iteration 17821: loss = 0.00249792048611372\n",
      "Iteration 17822: loss = 0.0024979094014807883\n",
      "Iteration 17823: loss = 0.0024978983169689456\n",
      "Iteration 17824: loss = 0.0024978872325781914\n",
      "Iteration 17825: loss = 0.0024978761483085226\n",
      "Iteration 17826: loss = 0.002497865064159939\n",
      "Iteration 17827: loss = 0.0024978539801324387\n",
      "Iteration 17828: loss = 0.0024978428962260206\n",
      "Iteration 17829: loss = 0.0024978318124406827\n",
      "Iteration 17830: loss = 0.002497820728776424\n",
      "Iteration 17831: loss = 0.0024978096452332424\n",
      "Iteration 17832: loss = 0.002497798561811138\n",
      "Iteration 17833: loss = 0.002497787478510108\n",
      "Iteration 17834: loss = 0.0024977763953301507\n",
      "Iteration 17835: loss = 0.002497765312271266\n",
      "Iteration 17836: loss = 0.0024977542293334507\n",
      "Iteration 17837: loss = 0.0024977431465167048\n",
      "Iteration 17838: loss = 0.002497732063821026\n",
      "Iteration 17839: loss = 0.0024977209812464137\n",
      "Iteration 17840: loss = 0.002497709898792866\n",
      "Iteration 17841: loss = 0.0024976988164603807\n",
      "Iteration 17842: loss = 0.0024976877342489582\n",
      "Iteration 17843: loss = 0.0024976766521585944\n",
      "Iteration 17844: loss = 0.0024976655701892904\n",
      "Iteration 17845: loss = 0.0024976544883410436\n",
      "Iteration 17846: loss = 0.0024976434066138527\n",
      "Iteration 17847: loss = 0.0024976323250077152\n",
      "Iteration 17848: loss = 0.002497621243522632\n",
      "Iteration 17849: loss = 0.0024976101621585994\n",
      "Iteration 17850: loss = 0.002497599080915617\n",
      "Iteration 17851: loss = 0.002497587999793684\n",
      "Iteration 17852: loss = 0.0024975769187927975\n",
      "Iteration 17853: loss = 0.0024975658379129567\n",
      "Iteration 17854: loss = 0.00249755475715416\n",
      "Iteration 17855: loss = 0.002497543676516406\n",
      "Iteration 17856: loss = 0.0024975325959996943\n",
      "Iteration 17857: loss = 0.0024975215156040217\n",
      "Iteration 17858: loss = 0.002497510435329388\n",
      "Iteration 17859: loss = 0.0024974993551757914\n",
      "Iteration 17860: loss = 0.0024974882751432293\n",
      "Iteration 17861: loss = 0.0024974771952317024\n",
      "Iteration 17862: loss = 0.002497466115441208\n",
      "Iteration 17863: loss = 0.002497455035771744\n",
      "Iteration 17864: loss = 0.002497443956223311\n",
      "Iteration 17865: loss = 0.0024974328767959054\n",
      "Iteration 17866: loss = 0.0024974217974895268\n",
      "Iteration 17867: loss = 0.0024974107183041738\n",
      "Iteration 17868: loss = 0.0024973996392398446\n",
      "Iteration 17869: loss = 0.002497388560296538\n",
      "Iteration 17870: loss = 0.0024973774814742527\n",
      "Iteration 17871: loss = 0.002497366402772987\n",
      "Iteration 17872: loss = 0.002497355324192739\n",
      "Iteration 17873: loss = 0.0024973442457335076\n",
      "Iteration 17874: loss = 0.0024973331673952925\n",
      "Iteration 17875: loss = 0.0024973220891780904\n",
      "Iteration 17876: loss = 0.0024973110110819005\n",
      "Iteration 17877: loss = 0.0024972999331067222\n",
      "Iteration 17878: loss = 0.002497288855252553\n",
      "Iteration 17879: loss = 0.0024972777775193918\n",
      "Iteration 17880: loss = 0.0024972666999072374\n",
      "Iteration 17881: loss = 0.002497255622416088\n",
      "Iteration 17882: loss = 0.0024972445450459424\n",
      "Iteration 17883: loss = 0.0024972334677967984\n",
      "Iteration 17884: loss = 0.0024972223906686557\n",
      "Iteration 17885: loss = 0.0024972113136615126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17886: loss = 0.002497200236775367\n",
      "Iteration 17887: loss = 0.0024971891600102177\n",
      "Iteration 17888: loss = 0.0024971780833660637\n",
      "Iteration 17889: loss = 0.0024971670068429028\n",
      "Iteration 17890: loss = 0.002497155930440735\n",
      "Iteration 17891: loss = 0.0024971448541595566\n",
      "Iteration 17892: loss = 0.0024971337779993675\n",
      "Iteration 17893: loss = 0.002497122701960167\n",
      "Iteration 17894: loss = 0.0024971116260419524\n",
      "Iteration 17895: loss = 0.0024971005502447225\n",
      "Iteration 17896: loss = 0.0024970894745684757\n",
      "Iteration 17897: loss = 0.002497078399013212\n",
      "Iteration 17898: loss = 0.002497067323578928\n",
      "Iteration 17899: loss = 0.0024970562482656226\n",
      "Iteration 17900: loss = 0.0024970451730732955\n",
      "Iteration 17901: loss = 0.0024970340980019438\n",
      "Iteration 17902: loss = 0.0024970230230515677\n",
      "Iteration 17903: loss = 0.0024970119482221652\n",
      "Iteration 17904: loss = 0.0024970008735137332\n",
      "Iteration 17905: loss = 0.0024969897989262717\n",
      "Iteration 17906: loss = 0.0024969787244597803\n",
      "Iteration 17907: loss = 0.002496967650114255\n",
      "Iteration 17908: loss = 0.0024969565758896964\n",
      "Iteration 17909: loss = 0.0024969455017861022\n",
      "Iteration 17910: loss = 0.0024969344278034716\n",
      "Iteration 17911: loss = 0.0024969233539418024\n",
      "Iteration 17912: loss = 0.0024969122802010932\n",
      "Iteration 17913: loss = 0.0024969012065813425\n",
      "Iteration 17914: loss = 0.0024968901330825496\n",
      "Iteration 17915: loss = 0.002496879059704713\n",
      "Iteration 17916: loss = 0.0024968679864478304\n",
      "Iteration 17917: loss = 0.0024968569133119005\n",
      "Iteration 17918: loss = 0.0024968458402969225\n",
      "Iteration 17919: loss = 0.002496834767402894\n",
      "Iteration 17920: loss = 0.002496823694629815\n",
      "Iteration 17921: loss = 0.0024968126219776826\n",
      "Iteration 17922: loss = 0.002496801549446496\n",
      "Iteration 17923: loss = 0.002496790477036254\n",
      "Iteration 17924: loss = 0.0024967794047469545\n",
      "Iteration 17925: loss = 0.0024967683325785965\n",
      "Iteration 17926: loss = 0.0024967572605311787\n",
      "Iteration 17927: loss = 0.002496746188604699\n",
      "Iteration 17928: loss = 0.0024967351167991566\n",
      "Iteration 17929: loss = 0.0024967240451145498\n",
      "Iteration 17930: loss = 0.002496712973550877\n",
      "Iteration 17931: loss = 0.0024967019021081374\n",
      "Iteration 17932: loss = 0.0024966908307863284\n",
      "Iteration 17933: loss = 0.002496679759585449\n",
      "Iteration 17934: loss = 0.0024966686885054988\n",
      "Iteration 17935: loss = 0.002496657617546475\n",
      "Iteration 17936: loss = 0.002496646546708377\n",
      "Iteration 17937: loss = 0.0024966354759912027\n",
      "Iteration 17938: loss = 0.0024966244053949514\n",
      "Iteration 17939: loss = 0.002496613334919621\n",
      "Iteration 17940: loss = 0.0024966022645652106\n",
      "Iteration 17941: loss = 0.0024965911943317176\n",
      "Iteration 17942: loss = 0.002496580124219142\n",
      "Iteration 17943: loss = 0.0024965690542274816\n",
      "Iteration 17944: loss = 0.0024965579843567354\n",
      "Iteration 17945: loss = 0.002496546914606902\n",
      "Iteration 17946: loss = 0.0024965358449779785\n",
      "Iteration 17947: loss = 0.0024965247754699647\n",
      "Iteration 17948: loss = 0.0024965137060828594\n",
      "Iteration 17949: loss = 0.002496502636816661\n",
      "Iteration 17950: loss = 0.002496491567671368\n",
      "Iteration 17951: loss = 0.002496480498646978\n",
      "Iteration 17952: loss = 0.0024964694297434902\n",
      "Iteration 17953: loss = 0.002496458360960904\n",
      "Iteration 17954: loss = 0.0024964472922992162\n",
      "Iteration 17955: loss = 0.0024964362237584283\n",
      "Iteration 17956: loss = 0.002496425155338535\n",
      "Iteration 17957: loss = 0.002496414087039538\n",
      "Iteration 17958: loss = 0.002496403018861434\n",
      "Iteration 17959: loss = 0.0024963919508042224\n",
      "Iteration 17960: loss = 0.0024963808828679015\n",
      "Iteration 17961: loss = 0.0024963698150524695\n",
      "Iteration 17962: loss = 0.0024963587473579257\n",
      "Iteration 17963: loss = 0.002496347679784268\n",
      "Iteration 17964: loss = 0.002496336612331496\n",
      "Iteration 17965: loss = 0.002496325544999607\n",
      "Iteration 17966: loss = 0.0024963144777886003\n",
      "Iteration 17967: loss = 0.002496303410698474\n",
      "Iteration 17968: loss = 0.0024962923437292267\n",
      "Iteration 17969: loss = 0.002496281276880857\n",
      "Iteration 17970: loss = 0.0024962702101533644\n",
      "Iteration 17971: loss = 0.0024962591435467463\n",
      "Iteration 17972: loss = 0.002496248077061001\n",
      "Iteration 17973: loss = 0.0024962370106961286\n",
      "Iteration 17974: loss = 0.002496225944452126\n",
      "Iteration 17975: loss = 0.0024962148783289916\n",
      "Iteration 17976: loss = 0.002496203812326726\n",
      "Iteration 17977: loss = 0.0024961927464453265\n",
      "Iteration 17978: loss = 0.002496181680684791\n",
      "Iteration 17979: loss = 0.0024961706150451197\n",
      "Iteration 17980: loss = 0.00249615954952631\n",
      "Iteration 17981: loss = 0.00249614848412836\n",
      "Iteration 17982: loss = 0.0024961374188512695\n",
      "Iteration 17983: loss = 0.0024961263536950364\n",
      "Iteration 17984: loss = 0.0024961152886596592\n",
      "Iteration 17985: loss = 0.002496104223745137\n",
      "Iteration 17986: loss = 0.0024960931589514674\n",
      "Iteration 17987: loss = 0.0024960820942786493\n",
      "Iteration 17988: loss = 0.002496071029726682\n",
      "Iteration 17989: loss = 0.0024960599652955632\n",
      "Iteration 17990: loss = 0.0024960489009852914\n",
      "Iteration 17991: loss = 0.002496037836795866\n",
      "Iteration 17992: loss = 0.002496026772727285\n",
      "Iteration 17993: loss = 0.0024960157087795475\n",
      "Iteration 17994: loss = 0.0024960046449526508\n",
      "Iteration 17995: loss = 0.0024959935812465944\n",
      "Iteration 17996: loss = 0.0024959825176613763\n",
      "Iteration 17997: loss = 0.0024959714541969956\n",
      "Iteration 17998: loss = 0.0024959603908534514\n",
      "Iteration 17999: loss = 0.0024959493276307406\n",
      "Iteration 18000: loss = 0.0024959382645288633\n",
      "Iteration 18001: loss = 0.002495927201547817\n",
      "Iteration 18002: loss = 0.0024959161386876013\n",
      "Iteration 18003: loss = 0.0024959050759482135\n",
      "Iteration 18004: loss = 0.0024958940133296527\n",
      "Iteration 18005: loss = 0.0024958829508319185\n",
      "Iteration 18006: loss = 0.002495871888455008\n",
      "Iteration 18007: loss = 0.00249586082619892\n",
      "Iteration 18008: loss = 0.0024958497640636536\n",
      "Iteration 18009: loss = 0.002495838702049207\n",
      "Iteration 18010: loss = 0.002495827640155579\n",
      "Iteration 18011: loss = 0.0024958165783827667\n",
      "Iteration 18012: loss = 0.0024958055167307717\n",
      "Iteration 18013: loss = 0.0024957944551995902\n",
      "Iteration 18014: loss = 0.0024957833937892214\n",
      "Iteration 18015: loss = 0.002495772332499663\n",
      "Iteration 18016: loss = 0.002495761271330915\n",
      "Iteration 18017: loss = 0.0024957502102829756\n",
      "Iteration 18018: loss = 0.0024957391493558427\n",
      "Iteration 18019: loss = 0.0024957280885495154\n",
      "Iteration 18020: loss = 0.002495717027863992\n",
      "Iteration 18021: loss = 0.0024957059672992715\n",
      "Iteration 18022: loss = 0.0024956949068553514\n",
      "Iteration 18023: loss = 0.0024956838465322313\n",
      "Iteration 18024: loss = 0.002495672786329909\n",
      "Iteration 18025: loss = 0.0024956617262483836\n",
      "Iteration 18026: loss = 0.002495650666287654\n",
      "Iteration 18027: loss = 0.0024956396064477176\n",
      "Iteration 18028: loss = 0.0024956285467285736\n",
      "Iteration 18029: loss = 0.002495617487130221\n",
      "Iteration 18030: loss = 0.002495606427652658\n",
      "Iteration 18031: loss = 0.0024955953682958827\n",
      "Iteration 18032: loss = 0.0024955843090598935\n",
      "Iteration 18033: loss = 0.002495573249944691\n",
      "Iteration 18034: loss = 0.0024955621909502706\n",
      "Iteration 18035: loss = 0.0024955511320766334\n",
      "Iteration 18036: loss = 0.002495540073323777\n",
      "Iteration 18037: loss = 0.0024955290146917\n",
      "Iteration 18038: loss = 0.0024955179561804005\n",
      "Iteration 18039: loss = 0.0024955068977898782\n",
      "Iteration 18040: loss = 0.0024954958395201304\n",
      "Iteration 18041: loss = 0.002495484781371156\n",
      "Iteration 18042: loss = 0.0024954737233429548\n",
      "Iteration 18043: loss = 0.002495462665435524\n",
      "Iteration 18044: loss = 0.002495451607648862\n",
      "Iteration 18045: loss = 0.002495440549982968\n",
      "Iteration 18046: loss = 0.0024954294924378405\n",
      "Iteration 18047: loss = 0.002495418435013478\n",
      "Iteration 18048: loss = 0.0024954073777098784\n",
      "Iteration 18049: loss = 0.0024953963205270415\n",
      "Iteration 18050: loss = 0.0024953852634649653\n",
      "Iteration 18051: loss = 0.002495374206523648\n",
      "Iteration 18052: loss = 0.002495363149703088\n",
      "Iteration 18053: loss = 0.002495352093003285\n",
      "Iteration 18054: loss = 0.0024953410364242368\n",
      "Iteration 18055: loss = 0.002495329979965942\n",
      "Iteration 18056: loss = 0.0024953189236283984\n",
      "Iteration 18057: loss = 0.0024953078674116056\n",
      "Iteration 18058: loss = 0.002495296811315563\n",
      "Iteration 18059: loss = 0.0024952857553402663\n",
      "Iteration 18060: loss = 0.002495274699485717\n",
      "Iteration 18061: loss = 0.0024952636437519118\n",
      "Iteration 18062: loss = 0.00249525258813885\n",
      "Iteration 18063: loss = 0.0024952415326465302\n",
      "Iteration 18064: loss = 0.002495230477274951\n",
      "Iteration 18065: loss = 0.00249521942202411\n",
      "Iteration 18066: loss = 0.002495208366894007\n",
      "Iteration 18067: loss = 0.0024951973118846403\n",
      "Iteration 18068: loss = 0.0024951862569960077\n",
      "Iteration 18069: loss = 0.002495175202228109\n",
      "Iteration 18070: loss = 0.002495164147580941\n",
      "Iteration 18071: loss = 0.0024951530930545037\n",
      "Iteration 18072: loss = 0.0024951420386487956\n",
      "Iteration 18073: loss = 0.0024951309843638148\n",
      "Iteration 18074: loss = 0.0024951199301995594\n",
      "Iteration 18075: loss = 0.002495108876156029\n",
      "Iteration 18076: loss = 0.0024950978222332216\n",
      "Iteration 18077: loss = 0.0024950867684311357\n",
      "Iteration 18078: loss = 0.0024950757147497696\n",
      "Iteration 18079: loss = 0.002495064661189123\n",
      "Iteration 18080: loss = 0.0024950536077491927\n",
      "Iteration 18081: loss = 0.002495042554429979\n",
      "Iteration 18082: loss = 0.0024950315012314795\n",
      "Iteration 18083: loss = 0.0024950204481536935\n",
      "Iteration 18084: loss = 0.0024950093951966183\n",
      "Iteration 18085: loss = 0.0024949983423602534\n",
      "Iteration 18086: loss = 0.002494987289644597\n",
      "Iteration 18087: loss = 0.002494976237049648\n",
      "Iteration 18088: loss = 0.0024949651845754045\n",
      "Iteration 18089: loss = 0.0024949541322218656\n",
      "Iteration 18090: loss = 0.002494943079989029\n",
      "Iteration 18091: loss = 0.0024949320278768936\n",
      "Iteration 18092: loss = 0.0024949209758854595\n",
      "Iteration 18093: loss = 0.002494909924014723\n",
      "Iteration 18094: loss = 0.0024948988722646834\n",
      "Iteration 18095: loss = 0.0024948878206353395\n",
      "Iteration 18096: loss = 0.0024948767691266903\n",
      "Iteration 18097: loss = 0.0024948657177387332\n",
      "Iteration 18098: loss = 0.0024948546664714678\n",
      "Iteration 18099: loss = 0.0024948436153248922\n",
      "Iteration 18100: loss = 0.002494832564299005\n",
      "Iteration 18101: loss = 0.002494821513393805\n",
      "Iteration 18102: loss = 0.0024948104626092895\n",
      "Iteration 18103: loss = 0.0024947994119454593\n",
      "Iteration 18104: loss = 0.0024947883614023112\n",
      "Iteration 18105: loss = 0.0024947773109798444\n",
      "Iteration 18106: loss = 0.002494766260678057\n",
      "Iteration 18107: loss = 0.0024947552104969484\n",
      "Iteration 18108: loss = 0.002494744160436516\n",
      "Iteration 18109: loss = 0.002494733110496759\n",
      "Iteration 18110: loss = 0.002494722060677677\n",
      "Iteration 18111: loss = 0.0024947110109792667\n",
      "Iteration 18112: loss = 0.002494699961401528\n",
      "Iteration 18113: loss = 0.002494688911944459\n",
      "Iteration 18114: loss = 0.0024946778626080573\n",
      "Iteration 18115: loss = 0.0024946668133923227\n",
      "Iteration 18116: loss = 0.002494655764297254\n",
      "Iteration 18117: loss = 0.0024946447153228486\n",
      "Iteration 18118: loss = 0.0024946336664691056\n",
      "Iteration 18119: loss = 0.0024946226177360244\n",
      "Iteration 18120: loss = 0.0024946115691236014\n",
      "Iteration 18121: loss = 0.002494600520631837\n",
      "Iteration 18122: loss = 0.00249458947226073\n",
      "Iteration 18123: loss = 0.0024945784240102777\n",
      "Iteration 18124: loss = 0.0024945673758804787\n",
      "Iteration 18125: loss = 0.0024945563278713323\n",
      "Iteration 18126: loss = 0.0024945452799828368\n",
      "Iteration 18127: loss = 0.002494534232214991\n",
      "Iteration 18128: loss = 0.0024945231845677927\n",
      "Iteration 18129: loss = 0.002494512137041242\n",
      "Iteration 18130: loss = 0.0024945010896353353\n",
      "Iteration 18131: loss = 0.002494490042350073\n",
      "Iteration 18132: loss = 0.0024944789951854523\n",
      "Iteration 18133: loss = 0.002494467948141473\n",
      "Iteration 18134: loss = 0.002494456901218132\n",
      "Iteration 18135: loss = 0.0024944458544154304\n",
      "Iteration 18136: loss = 0.0024944348077333644\n",
      "Iteration 18137: loss = 0.002494423761171933\n",
      "Iteration 18138: loss = 0.0024944127147311355\n",
      "Iteration 18139: loss = 0.00249440166841097\n",
      "Iteration 18140: loss = 0.002494390622211436\n",
      "Iteration 18141: loss = 0.00249437957613253\n",
      "Iteration 18142: loss = 0.0024943685301742533\n",
      "Iteration 18143: loss = 0.0024943574843366016\n",
      "Iteration 18144: loss = 0.0024943464386195753\n",
      "Iteration 18145: loss = 0.0024943353930231725\n",
      "Iteration 18146: loss = 0.0024943243475473915\n",
      "Iteration 18147: loss = 0.002494313302192231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18148: loss = 0.0024943022569576903\n",
      "Iteration 18149: loss = 0.0024942912118437665\n",
      "Iteration 18150: loss = 0.00249428016685046\n",
      "Iteration 18151: loss = 0.0024942691219777676\n",
      "Iteration 18152: loss = 0.002494258077225688\n",
      "Iteration 18153: loss = 0.002494247032594221\n",
      "Iteration 18154: loss = 0.0024942359880833647\n",
      "Iteration 18155: loss = 0.002494224943693117\n",
      "Iteration 18156: loss = 0.002494213899423477\n",
      "Iteration 18157: loss = 0.0024942028552744433\n",
      "Iteration 18158: loss = 0.0024941918112460137\n",
      "Iteration 18159: loss = 0.0024941807673381886\n",
      "Iteration 18160: loss = 0.0024941697235509644\n",
      "Iteration 18161: loss = 0.002494158679884341\n",
      "Iteration 18162: loss = 0.002494147636338316\n",
      "Iteration 18163: loss = 0.002494136592912889\n",
      "Iteration 18164: loss = 0.0024941255496080577\n",
      "Iteration 18165: loss = 0.0024941145064238212\n",
      "Iteration 18166: loss = 0.002494103463360178\n",
      "Iteration 18167: loss = 0.0024940924204171267\n",
      "Iteration 18168: loss = 0.0024940813775946647\n",
      "Iteration 18169: loss = 0.0024940703348927925\n",
      "Iteration 18170: loss = 0.002494059292311507\n",
      "Iteration 18171: loss = 0.0024940482498508085\n",
      "Iteration 18172: loss = 0.0024940372075106937\n",
      "Iteration 18173: loss = 0.002494026165291162\n",
      "Iteration 18174: loss = 0.0024940151231922125\n",
      "Iteration 18175: loss = 0.0024940040812138427\n",
      "Iteration 18176: loss = 0.0024939930393560517\n",
      "Iteration 18177: loss = 0.0024939819976188384\n",
      "Iteration 18178: loss = 0.0024939709560022004\n",
      "Iteration 18179: loss = 0.002493959914506137\n",
      "Iteration 18180: loss = 0.0024939488731306465\n",
      "Iteration 18181: loss = 0.0024939378318757283\n",
      "Iteration 18182: loss = 0.0024939267907413795\n",
      "Iteration 18183: loss = 0.002493915749727599\n",
      "Iteration 18184: loss = 0.0024939047088343865\n",
      "Iteration 18185: loss = 0.0024938936680617397\n",
      "Iteration 18186: loss = 0.002493882627409657\n",
      "Iteration 18187: loss = 0.0024938715868781376\n",
      "Iteration 18188: loss = 0.002493860546467179\n",
      "Iteration 18189: loss = 0.002493849506176781\n",
      "Iteration 18190: loss = 0.002493838466006941\n",
      "Iteration 18191: loss = 0.0024938274259576573\n",
      "Iteration 18192: loss = 0.0024938163860289313\n",
      "Iteration 18193: loss = 0.002493805346220758\n",
      "Iteration 18194: loss = 0.0024937943065331384\n",
      "Iteration 18195: loss = 0.00249378326696607\n",
      "Iteration 18196: loss = 0.0024937722275195514\n",
      "Iteration 18197: loss = 0.002493761188193581\n",
      "Iteration 18198: loss = 0.002493750148988158\n",
      "Iteration 18199: loss = 0.0024937391099032805\n",
      "Iteration 18200: loss = 0.002493728070938947\n",
      "Iteration 18201: loss = 0.0024937170320951567\n",
      "Iteration 18202: loss = 0.002493705993371907\n",
      "Iteration 18203: loss = 0.0024936949547691976\n",
      "Iteration 18204: loss = 0.0024936839162870267\n",
      "Iteration 18205: loss = 0.0024936728779253923\n",
      "Iteration 18206: loss = 0.0024936618396842938\n",
      "Iteration 18207: loss = 0.002493650801563729\n",
      "Iteration 18208: loss = 0.0024936397635636973\n",
      "Iteration 18209: loss = 0.002493628725684196\n",
      "Iteration 18210: loss = 0.002493617687925225\n",
      "Iteration 18211: loss = 0.002493606650286782\n",
      "Iteration 18212: loss = 0.002493595612768866\n",
      "Iteration 18213: loss = 0.0024935845753714754\n",
      "Iteration 18214: loss = 0.0024935735380946095\n",
      "Iteration 18215: loss = 0.002493562500938265\n",
      "Iteration 18216: loss = 0.0024935514639024417\n",
      "Iteration 18217: loss = 0.002493540426987139\n",
      "Iteration 18218: loss = 0.0024935293901923537\n",
      "Iteration 18219: loss = 0.002493518353518085\n",
      "Iteration 18220: loss = 0.0024935073169643327\n",
      "Iteration 18221: loss = 0.0024934962805310936\n",
      "Iteration 18222: loss = 0.0024934852442183666\n",
      "Iteration 18223: loss = 0.0024934742080261513\n",
      "Iteration 18224: loss = 0.002493463171954445\n",
      "Iteration 18225: loss = 0.002493452136003247\n",
      "Iteration 18226: loss = 0.002493441100172556\n",
      "Iteration 18227: loss = 0.00249343006446237\n",
      "Iteration 18228: loss = 0.0024934190288726874\n",
      "Iteration 18229: loss = 0.0024934079934035075\n",
      "Iteration 18230: loss = 0.0024933969580548288\n",
      "Iteration 18231: loss = 0.002493385922826649\n",
      "Iteration 18232: loss = 0.0024933748877189674\n",
      "Iteration 18233: loss = 0.002493363852731783\n",
      "Iteration 18234: loss = 0.0024933528178650933\n",
      "Iteration 18235: loss = 0.002493341783118897\n",
      "Iteration 18236: loss = 0.0024933307484931934\n",
      "Iteration 18237: loss = 0.0024933197139879806\n",
      "Iteration 18238: loss = 0.0024933086796032574\n",
      "Iteration 18239: loss = 0.0024932976453390216\n",
      "Iteration 18240: loss = 0.0024932866111952723\n",
      "Iteration 18241: loss = 0.0024932755771720086\n",
      "Iteration 18242: loss = 0.002493264543269229\n",
      "Iteration 18243: loss = 0.002493253509486931\n",
      "Iteration 18244: loss = 0.002493242475825113\n",
      "Iteration 18245: loss = 0.0024932314422837754\n",
      "Iteration 18246: loss = 0.0024932204088629144\n",
      "Iteration 18247: loss = 0.002493209375562531\n",
      "Iteration 18248: loss = 0.0024931983423826225\n",
      "Iteration 18249: loss = 0.0024931873093231873\n",
      "Iteration 18250: loss = 0.002493176276384224\n",
      "Iteration 18251: loss = 0.0024931652435657313\n",
      "Iteration 18252: loss = 0.0024931542108677083\n",
      "Iteration 18253: loss = 0.002493143178290153\n",
      "Iteration 18254: loss = 0.002493132145833064\n",
      "Iteration 18255: loss = 0.0024931211134964397\n",
      "Iteration 18256: loss = 0.002493110081280279\n",
      "Iteration 18257: loss = 0.002493099049184581\n",
      "Iteration 18258: loss = 0.002493088017209342\n",
      "Iteration 18259: loss = 0.0024930769853545635\n",
      "Iteration 18260: loss = 0.0024930659536202423\n",
      "Iteration 18261: loss = 0.0024930549220063773\n",
      "Iteration 18262: loss = 0.0024930438905129667\n",
      "Iteration 18263: loss = 0.00249303285914001\n",
      "Iteration 18264: loss = 0.0024930218278875053\n",
      "Iteration 18265: loss = 0.0024930107967554515\n",
      "Iteration 18266: loss = 0.002492999765743846\n",
      "Iteration 18267: loss = 0.002492988734852688\n",
      "Iteration 18268: loss = 0.0024929777040819767\n",
      "Iteration 18269: loss = 0.00249296667343171\n",
      "Iteration 18270: loss = 0.0024929556429018865\n",
      "Iteration 18271: loss = 0.002492944612492505\n",
      "Iteration 18272: loss = 0.002492933582203564\n",
      "Iteration 18273: loss = 0.002492922552035062\n",
      "Iteration 18274: loss = 0.0024929115219869975\n",
      "Iteration 18275: loss = 0.0024929004920593687\n",
      "Iteration 18276: loss = 0.002492889462252175\n",
      "Iteration 18277: loss = 0.0024928784325654145\n",
      "Iteration 18278: loss = 0.0024928674029990852\n",
      "Iteration 18279: loss = 0.0024928563735531874\n",
      "Iteration 18280: loss = 0.0024928453442277176\n",
      "Iteration 18281: loss = 0.0024928343150226757\n",
      "Iteration 18282: loss = 0.002492823285938059\n",
      "Iteration 18283: loss = 0.0024928122569738685\n",
      "Iteration 18284: loss = 0.0024928012281300997\n",
      "Iteration 18285: loss = 0.002492790199406753\n",
      "Iteration 18286: loss = 0.002492779170803827\n",
      "Iteration 18287: loss = 0.0024927681423213195\n",
      "Iteration 18288: loss = 0.002492757113959229\n",
      "Iteration 18289: loss = 0.002492746085717555\n",
      "Iteration 18290: loss = 0.002492735057596295\n",
      "Iteration 18291: loss = 0.002492724029595449\n",
      "Iteration 18292: loss = 0.0024927130017150137\n",
      "Iteration 18293: loss = 0.0024927019739549887\n",
      "Iteration 18294: loss = 0.002492690946315373\n",
      "Iteration 18295: loss = 0.0024926799187961644\n",
      "Iteration 18296: loss = 0.0024926688913973613\n",
      "Iteration 18297: loss = 0.002492657864118963\n",
      "Iteration 18298: loss = 0.0024926468369609674\n",
      "Iteration 18299: loss = 0.002492635809923374\n",
      "Iteration 18300: loss = 0.0024926247830061804\n",
      "Iteration 18301: loss = 0.0024926137562093853\n",
      "Iteration 18302: loss = 0.0024926027295329874\n",
      "Iteration 18303: loss = 0.0024925917029769854\n",
      "Iteration 18304: loss = 0.0024925806765413775\n",
      "Iteration 18305: loss = 0.002492569650226163\n",
      "Iteration 18306: loss = 0.00249255862403134\n",
      "Iteration 18307: loss = 0.002492547597956906\n",
      "Iteration 18308: loss = 0.0024925365720028622\n",
      "Iteration 18309: loss = 0.0024925255461692047\n",
      "Iteration 18310: loss = 0.002492514520455932\n",
      "Iteration 18311: loss = 0.002492503494863045\n",
      "Iteration 18312: loss = 0.0024924924693905406\n",
      "Iteration 18313: loss = 0.002492481444038417\n",
      "Iteration 18314: loss = 0.0024924704188066743\n",
      "Iteration 18315: loss = 0.0024924593936953092\n",
      "Iteration 18316: loss = 0.002492448368704322\n",
      "Iteration 18317: loss = 0.00249243734383371\n",
      "Iteration 18318: loss = 0.002492426319083472\n",
      "Iteration 18319: loss = 0.002492415294453607\n",
      "Iteration 18320: loss = 0.0024924042699441137\n",
      "Iteration 18321: loss = 0.00249239324555499\n",
      "Iteration 18322: loss = 0.002492382221286234\n",
      "Iteration 18323: loss = 0.002492371197137846\n",
      "Iteration 18324: loss = 0.0024923601731098237\n",
      "Iteration 18325: loss = 0.0024923491492021646\n",
      "Iteration 18326: loss = 0.002492338125414869\n",
      "Iteration 18327: loss = 0.002492327101747934\n",
      "Iteration 18328: loss = 0.0024923160782013595\n",
      "Iteration 18329: loss = 0.002492305054775143\n",
      "Iteration 18330: loss = 0.0024922940314692837\n",
      "Iteration 18331: loss = 0.00249228300828378\n",
      "Iteration 18332: loss = 0.00249227198521863\n",
      "Iteration 18333: loss = 0.0024922609622738327\n",
      "Iteration 18334: loss = 0.0024922499394493865\n",
      "Iteration 18335: loss = 0.0024922389167452903\n",
      "Iteration 18336: loss = 0.002492227894161542\n",
      "Iteration 18337: loss = 0.002492216871698141\n",
      "Iteration 18338: loss = 0.002492205849355085\n",
      "Iteration 18339: loss = 0.0024921948271323735\n",
      "Iteration 18340: loss = 0.0024921838050300047\n",
      "Iteration 18341: loss = 0.0024921727830479767\n",
      "Iteration 18342: loss = 0.0024921617611862886\n",
      "Iteration 18343: loss = 0.002492150739444938\n",
      "Iteration 18344: loss = 0.0024921397178239244\n",
      "Iteration 18345: loss = 0.002492128696323247\n",
      "Iteration 18346: loss = 0.002492117674942903\n",
      "Iteration 18347: loss = 0.0024921066536828917\n",
      "Iteration 18348: loss = 0.0024920956325432107\n",
      "Iteration 18349: loss = 0.00249208461152386\n",
      "Iteration 18350: loss = 0.0024920735906248377\n",
      "Iteration 18351: loss = 0.0024920625698461414\n",
      "Iteration 18352: loss = 0.002492051549187771\n",
      "Iteration 18353: loss = 0.0024920405286497243\n",
      "Iteration 18354: loss = 0.0024920295082319997\n",
      "Iteration 18355: loss = 0.0024920184879345963\n",
      "Iteration 18356: loss = 0.002492007467757512\n",
      "Iteration 18357: loss = 0.002491996447700747\n",
      "Iteration 18358: loss = 0.002491985427764298\n",
      "Iteration 18359: loss = 0.0024919744079481645\n",
      "Iteration 18360: loss = 0.002491963388252344\n",
      "Iteration 18361: loss = 0.0024919523686768364\n",
      "Iteration 18362: loss = 0.0024919413492216396\n",
      "Iteration 18363: loss = 0.0024919303298867523\n",
      "Iteration 18364: loss = 0.002491919310672173\n",
      "Iteration 18365: loss = 0.0024919082915779\n",
      "Iteration 18366: loss = 0.0024918972726039328\n",
      "Iteration 18367: loss = 0.0024918862537502695\n",
      "Iteration 18368: loss = 0.002491875235016908\n",
      "Iteration 18369: loss = 0.002491864216403847\n",
      "Iteration 18370: loss = 0.002491853197911086\n",
      "Iteration 18371: loss = 0.0024918421795386224\n",
      "Iteration 18372: loss = 0.0024918311612864563\n",
      "Iteration 18373: loss = 0.002491820143154584\n",
      "Iteration 18374: loss = 0.0024918091251430064\n",
      "Iteration 18375: loss = 0.002491798107251721\n",
      "Iteration 18376: loss = 0.0024917870894807254\n",
      "Iteration 18377: loss = 0.0024917760718300205\n",
      "Iteration 18378: loss = 0.002491765054299603\n",
      "Iteration 18379: loss = 0.0024917540368894716\n",
      "Iteration 18380: loss = 0.002491743019599625\n",
      "Iteration 18381: loss = 0.002491732002430063\n",
      "Iteration 18382: loss = 0.002491720985380782\n",
      "Iteration 18383: loss = 0.0024917099684517822\n",
      "Iteration 18384: loss = 0.002491698951643062\n",
      "Iteration 18385: loss = 0.0024916879349546196\n",
      "Iteration 18386: loss = 0.0024916769183864537\n",
      "Iteration 18387: loss = 0.002491665901938562\n",
      "Iteration 18388: loss = 0.0024916548856109447\n",
      "Iteration 18389: loss = 0.002491643869403599\n",
      "Iteration 18390: loss = 0.002491632853316524\n",
      "Iteration 18391: loss = 0.0024916218373497185\n",
      "Iteration 18392: loss = 0.0024916108215031806\n",
      "Iteration 18393: loss = 0.0024915998057769093\n",
      "Iteration 18394: loss = 0.002491588790170902\n",
      "Iteration 18395: loss = 0.00249157777468516\n",
      "Iteration 18396: loss = 0.002491566759319678\n",
      "Iteration 18397: loss = 0.0024915557440744578\n",
      "Iteration 18398: loss = 0.0024915447289494963\n",
      "Iteration 18399: loss = 0.002491533713944793\n",
      "Iteration 18400: loss = 0.0024915226990603456\n",
      "Iteration 18401: loss = 0.002491511684296153\n",
      "Iteration 18402: loss = 0.0024915006696522137\n",
      "Iteration 18403: loss = 0.0024914896551285265\n",
      "Iteration 18404: loss = 0.00249147864072509\n",
      "Iteration 18405: loss = 0.0024914676264419025\n",
      "Iteration 18406: loss = 0.0024914566122789623\n",
      "Iteration 18407: loss = 0.002491445598236269\n",
      "Iteration 18408: loss = 0.00249143458431382\n",
      "Iteration 18409: loss = 0.002491423570511614\n",
      "Iteration 18410: loss = 0.0024914125568296507\n",
      "Iteration 18411: loss = 0.002491401543267928\n",
      "Iteration 18412: loss = 0.0024913905298264436\n",
      "Iteration 18413: loss = 0.0024913795165051973\n",
      "Iteration 18414: loss = 0.002491368503304187\n",
      "Iteration 18415: loss = 0.0024913574902234114\n",
      "Iteration 18416: loss = 0.0024913464772628687\n",
      "Iteration 18417: loss = 0.0024913354644225585\n",
      "Iteration 18418: loss = 0.0024913244517024784\n",
      "Iteration 18419: loss = 0.0024913134391026273\n",
      "Iteration 18420: loss = 0.002491302426623004\n",
      "Iteration 18421: loss = 0.0024912914142636066\n",
      "Iteration 18422: loss = 0.0024912804020244335\n",
      "Iteration 18423: loss = 0.002491269389905484\n",
      "Iteration 18424: loss = 0.0024912583779067563\n",
      "Iteration 18425: loss = 0.002491247366028249\n",
      "Iteration 18426: loss = 0.00249123635426996\n",
      "Iteration 18427: loss = 0.0024912253426318896\n",
      "Iteration 18428: loss = 0.0024912143311140345\n",
      "Iteration 18429: loss = 0.0024912033197163944\n",
      "Iteration 18430: loss = 0.0024911923084389672\n",
      "Iteration 18431: loss = 0.002491181297281752\n",
      "Iteration 18432: loss = 0.0024911702862447467\n",
      "Iteration 18433: loss = 0.002491159275327951\n",
      "Iteration 18434: loss = 0.0024911482645313615\n",
      "Iteration 18435: loss = 0.002491137253854979\n",
      "Iteration 18436: loss = 0.0024911262432988006\n",
      "Iteration 18437: loss = 0.0024911152328628257\n",
      "Iteration 18438: loss = 0.0024911042225470525\n",
      "Iteration 18439: loss = 0.002491093212351478\n",
      "Iteration 18440: loss = 0.0024910822022761042\n",
      "Iteration 18441: loss = 0.0024910711923209275\n",
      "Iteration 18442: loss = 0.002491060182485946\n",
      "Iteration 18443: loss = 0.0024910491727711593\n",
      "Iteration 18444: loss = 0.0024910381631765657\n",
      "Iteration 18445: loss = 0.002491027153702164\n",
      "Iteration 18446: loss = 0.002491016144347952\n",
      "Iteration 18447: loss = 0.002491005135113929\n",
      "Iteration 18448: loss = 0.002490994126000094\n",
      "Iteration 18449: loss = 0.0024909831170064436\n",
      "Iteration 18450: loss = 0.002490972108132978\n",
      "Iteration 18451: loss = 0.002490961099379696\n",
      "Iteration 18452: loss = 0.002490950090746595\n",
      "Iteration 18453: loss = 0.002490939082233674\n",
      "Iteration 18454: loss = 0.0024909280738409324\n",
      "Iteration 18455: loss = 0.0024909170655683676\n",
      "Iteration 18456: loss = 0.002490906057415979\n",
      "Iteration 18457: loss = 0.002490895049383764\n",
      "Iteration 18458: loss = 0.002490884041471723\n",
      "Iteration 18459: loss = 0.0024908730336798525\n",
      "Iteration 18460: loss = 0.0024908620260081526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18461: loss = 0.0024908510184566217\n",
      "Iteration 18462: loss = 0.002490840011025257\n",
      "Iteration 18463: loss = 0.002490829003714059\n",
      "Iteration 18464: loss = 0.002490817996523025\n",
      "Iteration 18465: loss = 0.0024908069894521534\n",
      "Iteration 18466: loss = 0.002490795982501444\n",
      "Iteration 18467: loss = 0.0024907849756708946\n",
      "Iteration 18468: loss = 0.0024907739689605034\n",
      "Iteration 18469: loss = 0.002490762962370269\n",
      "Iteration 18470: loss = 0.0024907519559001913\n",
      "Iteration 18471: loss = 0.0024907409495502674\n",
      "Iteration 18472: loss = 0.002490729943320496\n",
      "Iteration 18473: loss = 0.0024907189372108765\n",
      "Iteration 18474: loss = 0.0024907079312214073\n",
      "Iteration 18475: loss = 0.002490696925352086\n",
      "Iteration 18476: loss = 0.002490685919602912\n",
      "Iteration 18477: loss = 0.0024906749139738836\n",
      "Iteration 18478: loss = 0.0024906639084649996\n",
      "Iteration 18479: loss = 0.002490652903076258\n",
      "Iteration 18480: loss = 0.002490641897807658\n",
      "Iteration 18481: loss = 0.002490630892659198\n",
      "Iteration 18482: loss = 0.0024906198876308767\n",
      "Iteration 18483: loss = 0.0024906088827226923\n",
      "Iteration 18484: loss = 0.002490597877934643\n",
      "Iteration 18485: loss = 0.0024905868732667284\n",
      "Iteration 18486: loss = 0.0024905758687189467\n",
      "Iteration 18487: loss = 0.002490564864291296\n",
      "Iteration 18488: loss = 0.002490553859983775\n",
      "Iteration 18489: loss = 0.002490542855796382\n",
      "Iteration 18490: loss = 0.002490531851729117\n",
      "Iteration 18491: loss = 0.0024905208477819774\n",
      "Iteration 18492: loss = 0.002490509843954962\n",
      "Iteration 18493: loss = 0.002490498840248069\n",
      "Iteration 18494: loss = 0.0024904878366612975\n",
      "Iteration 18495: loss = 0.0024904768331946454\n",
      "Iteration 18496: loss = 0.002490465829848112\n",
      "Iteration 18497: loss = 0.0024904548266216956\n",
      "Iteration 18498: loss = 0.002490443823515394\n",
      "Iteration 18499: loss = 0.002490432820529207\n",
      "Iteration 18500: loss = 0.002490421817663133\n",
      "Iteration 18501: loss = 0.0024904108149171704\n",
      "Iteration 18502: loss = 0.002490399812291316\n",
      "Iteration 18503: loss = 0.002490388809785572\n",
      "Iteration 18504: loss = 0.0024903778073999336\n",
      "Iteration 18505: loss = 0.0024903668051344014\n",
      "Iteration 18506: loss = 0.0024903558029889723\n",
      "Iteration 18507: loss = 0.0024903448009636468\n",
      "Iteration 18508: loss = 0.0024903337990584223\n",
      "Iteration 18509: loss = 0.0024903227972732974\n",
      "Iteration 18510: loss = 0.00249031179560827\n",
      "Iteration 18511: loss = 0.0024903007940633402\n",
      "Iteration 18512: loss = 0.0024902897926385057\n",
      "Iteration 18513: loss = 0.0024902787913337653\n",
      "Iteration 18514: loss = 0.0024902677901491176\n",
      "Iteration 18515: loss = 0.0024902567890845604\n",
      "Iteration 18516: loss = 0.002490245788140094\n",
      "Iteration 18517: loss = 0.0024902347873157144\n",
      "Iteration 18518: loss = 0.002490223786611422\n",
      "Iteration 18519: loss = 0.002490212786027216\n",
      "Iteration 18520: loss = 0.002490201785563093\n",
      "Iteration 18521: loss = 0.002490190785219053\n",
      "Iteration 18522: loss = 0.002490179784995093\n",
      "Iteration 18523: loss = 0.002490168784891214\n",
      "Iteration 18524: loss = 0.0024901577849074127\n",
      "Iteration 18525: loss = 0.0024901467850436884\n",
      "Iteration 18526: loss = 0.002490135785300039\n",
      "Iteration 18527: loss = 0.0024901247856764634\n",
      "Iteration 18528: loss = 0.0024901137861729605\n",
      "Iteration 18529: loss = 0.0024901027867895286\n",
      "Iteration 18530: loss = 0.0024900917875261665\n",
      "Iteration 18531: loss = 0.0024900807883828724\n",
      "Iteration 18532: loss = 0.002490069789359645\n",
      "Iteration 18533: loss = 0.0024900587904564827\n",
      "Iteration 18534: loss = 0.0024900477916733844\n",
      "Iteration 18535: loss = 0.002490036793010349\n",
      "Iteration 18536: loss = 0.002490025794467374\n",
      "Iteration 18537: loss = 0.0024900147960444587\n",
      "Iteration 18538: loss = 0.0024900037977416017\n",
      "Iteration 18539: loss = 0.0024899927995588014\n",
      "Iteration 18540: loss = 0.002489981801496056\n",
      "Iteration 18541: loss = 0.0024899708035533642\n",
      "Iteration 18542: loss = 0.0024899598057307256\n",
      "Iteration 18543: loss = 0.002489948808028137\n",
      "Iteration 18544: loss = 0.002489937810445599\n",
      "Iteration 18545: loss = 0.0024899268129831083\n",
      "Iteration 18546: loss = 0.0024899158156406645\n",
      "Iteration 18547: loss = 0.0024899048184182653\n",
      "Iteration 18548: loss = 0.0024898938213159106\n",
      "Iteration 18549: loss = 0.002489882824333598\n",
      "Iteration 18550: loss = 0.002489871827471326\n",
      "Iteration 18551: loss = 0.0024898608307290936\n",
      "Iteration 18552: loss = 0.0024898498341068996\n",
      "Iteration 18553: loss = 0.002489838837604742\n",
      "Iteration 18554: loss = 0.0024898278412226194\n",
      "Iteration 18555: loss = 0.0024898168449605314\n",
      "Iteration 18556: loss = 0.0024898058488184746\n",
      "Iteration 18557: loss = 0.0024897948527964494\n",
      "Iteration 18558: loss = 0.002489783856894453\n",
      "Iteration 18559: loss = 0.002489772861112485\n",
      "Iteration 18560: loss = 0.0024897618654505433\n",
      "Iteration 18561: loss = 0.002489750869908627\n",
      "Iteration 18562: loss = 0.0024897398744867343\n",
      "Iteration 18563: loss = 0.0024897288791848634\n",
      "Iteration 18564: loss = 0.002489717884003014\n",
      "Iteration 18565: loss = 0.0024897068889411836\n",
      "Iteration 18566: loss = 0.002489695893999371\n",
      "Iteration 18567: loss = 0.002489684899177575\n",
      "Iteration 18568: loss = 0.0024896739044757946\n",
      "Iteration 18569: loss = 0.002489662909894027\n",
      "Iteration 18570: loss = 0.0024896519154322724\n",
      "Iteration 18571: loss = 0.0024896409210905285\n",
      "Iteration 18572: loss = 0.0024896299268687932\n",
      "Iteration 18573: loss = 0.0024896189327670666\n",
      "Iteration 18574: loss = 0.0024896079387853464\n",
      "Iteration 18575: loss = 0.0024895969449236304\n",
      "Iteration 18576: loss = 0.002489585951181919\n",
      "Iteration 18577: loss = 0.002489574957560209\n",
      "Iteration 18578: loss = 0.0024895639640585008\n",
      "Iteration 18579: loss = 0.0024895529706767906\n",
      "Iteration 18580: loss = 0.0024895419774150795\n",
      "Iteration 18581: loss = 0.002489530984273364\n",
      "Iteration 18582: loss = 0.002489519991251644\n",
      "Iteration 18583: loss = 0.002489508998349917\n",
      "Iteration 18584: loss = 0.0024894980055681828\n",
      "Iteration 18585: loss = 0.002489487012906439\n",
      "Iteration 18586: loss = 0.002489476020364684\n",
      "Iteration 18587: loss = 0.0024894650279429177\n",
      "Iteration 18588: loss = 0.002489454035641137\n",
      "Iteration 18589: loss = 0.002489443043459342\n",
      "Iteration 18590: loss = 0.00248943205139753\n",
      "Iteration 18591: loss = 0.0024894210594557007\n",
      "Iteration 18592: loss = 0.0024894100676338517\n",
      "Iteration 18593: loss = 0.0024893990759319814\n",
      "Iteration 18594: loss = 0.0024893880843500898\n",
      "Iteration 18595: loss = 0.002489377092888174\n",
      "Iteration 18596: loss = 0.0024893661015462335\n",
      "Iteration 18597: loss = 0.002489355110324266\n",
      "Iteration 18598: loss = 0.002489344119222271\n",
      "Iteration 18599: loss = 0.0024893331282402464\n",
      "Iteration 18600: loss = 0.0024893221373781913\n",
      "Iteration 18601: loss = 0.0024893111466361036\n",
      "Iteration 18602: loss = 0.0024893001560139824\n",
      "Iteration 18603: loss = 0.0024892891655118265\n",
      "Iteration 18604: loss = 0.002489278175129633\n",
      "Iteration 18605: loss = 0.002489267184867402\n",
      "Iteration 18606: loss = 0.0024892561947251323\n",
      "Iteration 18607: loss = 0.0024892452047028213\n",
      "Iteration 18608: loss = 0.0024892342148004677\n",
      "Iteration 18609: loss = 0.002489223225018071\n",
      "Iteration 18610: loss = 0.0024892122353556284\n",
      "Iteration 18611: loss = 0.0024892012458131393\n",
      "Iteration 18612: loss = 0.002489190256390603\n",
      "Iteration 18613: loss = 0.002489179267088017\n",
      "Iteration 18614: loss = 0.0024891682779053796\n",
      "Iteration 18615: loss = 0.0024891572888426903\n",
      "Iteration 18616: loss = 0.0024891462998999476\n",
      "Iteration 18617: loss = 0.0024891353110771492\n",
      "Iteration 18618: loss = 0.0024891243223742945\n",
      "Iteration 18619: loss = 0.0024891133337913815\n",
      "Iteration 18620: loss = 0.0024891023453284086\n",
      "Iteration 18621: loss = 0.002489091356985376\n",
      "Iteration 18622: loss = 0.00248908036876228\n",
      "Iteration 18623: loss = 0.0024890693806591205\n",
      "Iteration 18624: loss = 0.0024890583926758963\n",
      "Iteration 18625: loss = 0.0024890474048126043\n",
      "Iteration 18626: loss = 0.0024890364170692455\n",
      "Iteration 18627: loss = 0.0024890254294458168\n",
      "Iteration 18628: loss = 0.002489014441942317\n",
      "Iteration 18629: loss = 0.0024890034545587444\n",
      "Iteration 18630: loss = 0.0024889924672950987\n",
      "Iteration 18631: loss = 0.0024889814801513778\n",
      "Iteration 18632: loss = 0.0024889704931275796\n",
      "Iteration 18633: loss = 0.002488959506223704\n",
      "Iteration 18634: loss = 0.0024889485194397485\n",
      "Iteration 18635: loss = 0.002488937532775712\n",
      "Iteration 18636: loss = 0.002488926546231593\n",
      "Iteration 18637: loss = 0.002488915559807391\n",
      "Iteration 18638: loss = 0.0024889045735031028\n",
      "Iteration 18639: loss = 0.0024888935873187278\n",
      "Iteration 18640: loss = 0.0024888826012542655\n",
      "Iteration 18641: loss = 0.0024888716153097126\n",
      "Iteration 18642: loss = 0.0024888606294850694\n",
      "Iteration 18643: loss = 0.002488849643780334\n",
      "Iteration 18644: loss = 0.0024888386581955048\n",
      "Iteration 18645: loss = 0.0024888276727305795\n",
      "Iteration 18646: loss = 0.0024888166873855582\n",
      "Iteration 18647: loss = 0.0024888057021604385\n",
      "Iteration 18648: loss = 0.0024887947170552185\n",
      "Iteration 18649: loss = 0.0024887837320698982\n",
      "Iteration 18650: loss = 0.0024887727472044756\n",
      "Iteration 18651: loss = 0.0024887617624589488\n",
      "Iteration 18652: loss = 0.002488750777833317\n",
      "Iteration 18653: loss = 0.0024887397933275775\n",
      "Iteration 18654: loss = 0.002488728808941731\n",
      "Iteration 18655: loss = 0.002488717824675774\n",
      "Iteration 18656: loss = 0.002488706840529707\n",
      "Iteration 18657: loss = 0.002488695856503526\n",
      "Iteration 18658: loss = 0.002488684872597232\n",
      "Iteration 18659: loss = 0.002488673888810823\n",
      "Iteration 18660: loss = 0.0024886629051442963\n",
      "Iteration 18661: loss = 0.002488651921597652\n",
      "Iteration 18662: loss = 0.002488640938170888\n",
      "Iteration 18663: loss = 0.0024886299548640026\n",
      "Iteration 18664: loss = 0.002488618971676995\n",
      "Iteration 18665: loss = 0.0024886079886098636\n",
      "Iteration 18666: loss = 0.0024885970056626066\n",
      "Iteration 18667: loss = 0.002488586022835222\n",
      "Iteration 18668: loss = 0.0024885750401277104\n",
      "Iteration 18669: loss = 0.0024885640575400686\n",
      "Iteration 18670: loss = 0.0024885530750722957\n",
      "Iteration 18671: loss = 0.0024885420927243905\n",
      "Iteration 18672: loss = 0.0024885311104963508\n",
      "Iteration 18673: loss = 0.002488520128388176\n",
      "Iteration 18674: loss = 0.0024885091463998643\n",
      "Iteration 18675: loss = 0.0024884981645314145\n",
      "Iteration 18676: loss = 0.0024884871827828246\n",
      "Iteration 18677: loss = 0.002488476201154094\n",
      "Iteration 18678: loss = 0.002488465219645221\n",
      "Iteration 18679: loss = 0.0024884542382562036\n",
      "Iteration 18680: loss = 0.002488443256987041\n",
      "Iteration 18681: loss = 0.0024884322758377316\n",
      "Iteration 18682: loss = 0.002488421294808273\n",
      "Iteration 18683: loss = 0.0024884103138986652\n",
      "Iteration 18684: loss = 0.002488399333108907\n",
      "Iteration 18685: loss = 0.0024883883524389953\n",
      "Iteration 18686: loss = 0.0024883773718889298\n",
      "Iteration 18687: loss = 0.002488366391458709\n",
      "Iteration 18688: loss = 0.002488355411148331\n",
      "Iteration 18689: loss = 0.002488344430957795\n",
      "Iteration 18690: loss = 0.002488333450887099\n",
      "Iteration 18691: loss = 0.002488322470936242\n",
      "Iteration 18692: loss = 0.002488311491105223\n",
      "Iteration 18693: loss = 0.002488300511394039\n",
      "Iteration 18694: loss = 0.00248828953180269\n",
      "Iteration 18695: loss = 0.0024882785523311743\n",
      "Iteration 18696: loss = 0.0024882675729794895\n",
      "Iteration 18697: loss = 0.0024882565937476347\n",
      "Iteration 18698: loss = 0.00248824561463561\n",
      "Iteration 18699: loss = 0.0024882346356434114\n",
      "Iteration 18700: loss = 0.0024882236567710394\n",
      "Iteration 18701: loss = 0.002488212678018492\n",
      "Iteration 18702: loss = 0.002488201699385767\n",
      "Iteration 18703: loss = 0.002488190720872865\n",
      "Iteration 18704: loss = 0.0024881797424797817\n",
      "Iteration 18705: loss = 0.0024881687642065173\n",
      "Iteration 18706: loss = 0.002488157786053071\n",
      "Iteration 18707: loss = 0.00248814680801944\n",
      "Iteration 18708: loss = 0.0024881358301056243\n",
      "Iteration 18709: loss = 0.0024881248523116206\n",
      "Iteration 18710: loss = 0.0024881138746374295\n",
      "Iteration 18711: loss = 0.002488102897083048\n",
      "Iteration 18712: loss = 0.002488091919648475\n",
      "Iteration 18713: loss = 0.0024880809423337095\n",
      "Iteration 18714: loss = 0.0024880699651387505\n",
      "Iteration 18715: loss = 0.0024880589880635954\n",
      "Iteration 18716: loss = 0.0024880480111082433\n",
      "Iteration 18717: loss = 0.002488037034272693\n",
      "Iteration 18718: loss = 0.0024880260575569422\n",
      "Iteration 18719: loss = 0.002488015080960991\n",
      "Iteration 18720: loss = 0.002488004104484837\n",
      "Iteration 18721: loss = 0.0024879931281284787\n",
      "Iteration 18722: loss = 0.0024879821518919145\n",
      "Iteration 18723: loss = 0.0024879711757751434\n",
      "Iteration 18724: loss = 0.002487960199778165\n",
      "Iteration 18725: loss = 0.002487949223900975\n",
      "Iteration 18726: loss = 0.0024879382481435746\n",
      "Iteration 18727: loss = 0.0024879272725059614\n",
      "Iteration 18728: loss = 0.002487916296988134\n",
      "Iteration 18729: loss = 0.0024879053215900913\n",
      "Iteration 18730: loss = 0.002487894346311831\n",
      "Iteration 18731: loss = 0.0024878833711533526\n",
      "Iteration 18732: loss = 0.0024878723961146544\n",
      "Iteration 18733: loss = 0.002487861421195734\n",
      "Iteration 18734: loss = 0.0024878504463965927\n",
      "Iteration 18735: loss = 0.0024878394717172255\n",
      "Iteration 18736: loss = 0.0024878284971576337\n",
      "Iteration 18737: loss = 0.0024878175227178137\n",
      "Iteration 18738: loss = 0.0024878065483977664\n",
      "Iteration 18739: loss = 0.002487795574197489\n",
      "Iteration 18740: loss = 0.0024877846001169802\n",
      "Iteration 18741: loss = 0.0024877736261562384\n",
      "Iteration 18742: loss = 0.002487762652315263\n",
      "Iteration 18743: loss = 0.002487751678594051\n",
      "Iteration 18744: loss = 0.002487740704992603\n",
      "Iteration 18745: loss = 0.002487729731510916\n",
      "Iteration 18746: loss = 0.0024877187581489883\n",
      "Iteration 18747: loss = 0.00248770778490682\n",
      "Iteration 18748: loss = 0.002487696811784409\n",
      "Iteration 18749: loss = 0.002487685838781754\n",
      "Iteration 18750: loss = 0.002487674865898853\n",
      "Iteration 18751: loss = 0.002487663893135705\n",
      "Iteration 18752: loss = 0.002487652920492308\n",
      "Iteration 18753: loss = 0.002487641947968662\n",
      "Iteration 18754: loss = 0.002487630975564764\n",
      "Iteration 18755: loss = 0.0024876200032806135\n",
      "Iteration 18756: loss = 0.0024876090311162086\n",
      "Iteration 18757: loss = 0.002487598059071548\n",
      "Iteration 18758: loss = 0.002487587087146631\n",
      "Iteration 18759: loss = 0.002487576115341455\n",
      "Iteration 18760: loss = 0.0024875651436560182\n",
      "Iteration 18761: loss = 0.0024875541720903206\n",
      "Iteration 18762: loss = 0.002487543200644361\n",
      "Iteration 18763: loss = 0.0024875322293181367\n",
      "Iteration 18764: loss = 0.0024875212581116466\n",
      "Iteration 18765: loss = 0.002487510287024889\n",
      "Iteration 18766: loss = 0.0024874993160578633\n",
      "Iteration 18767: loss = 0.0024874883452105675\n",
      "Iteration 18768: loss = 0.0024874773744830003\n",
      "Iteration 18769: loss = 0.0024874664038751606\n",
      "Iteration 18770: loss = 0.0024874554333870463\n",
      "Iteration 18771: loss = 0.0024874444630186565\n",
      "Iteration 18772: loss = 0.0024874334927699896\n",
      "Iteration 18773: loss = 0.0024874225226410433\n",
      "Iteration 18774: loss = 0.0024874115526318185\n",
      "Iteration 18775: loss = 0.002487400582742311\n",
      "Iteration 18776: loss = 0.002487389612972521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18777: loss = 0.002487378643322447\n",
      "Iteration 18778: loss = 0.002487367673792087\n",
      "Iteration 18779: loss = 0.0024873567043814404\n",
      "Iteration 18780: loss = 0.002487345735090505\n",
      "Iteration 18781: loss = 0.0024873347659192797\n",
      "Iteration 18782: loss = 0.002487323796867763\n",
      "Iteration 18783: loss = 0.0024873128279359526\n",
      "Iteration 18784: loss = 0.002487301859123849\n",
      "Iteration 18785: loss = 0.0024872908904314487\n",
      "Iteration 18786: loss = 0.0024872799218587523\n",
      "Iteration 18787: loss = 0.0024872689534057566\n",
      "Iteration 18788: loss = 0.0024872579850724608\n",
      "Iteration 18789: loss = 0.0024872470168588643\n",
      "Iteration 18790: loss = 0.0024872360487649646\n",
      "Iteration 18791: loss = 0.0024872250807907604\n",
      "Iteration 18792: loss = 0.0024872141129362504\n",
      "Iteration 18793: loss = 0.0024872031452014337\n",
      "Iteration 18794: loss = 0.002487192177586308\n",
      "Iteration 18795: loss = 0.002487181210090872\n",
      "Iteration 18796: loss = 0.0024871702427151253\n",
      "Iteration 18797: loss = 0.0024871592754590654\n",
      "Iteration 18798: loss = 0.002487148308322691\n",
      "Iteration 18799: loss = 0.002487137341306001\n",
      "Iteration 18800: loss = 0.002487126374408994\n",
      "Iteration 18801: loss = 0.0024871154076316682\n",
      "Iteration 18802: loss = 0.002487104440974023\n",
      "Iteration 18803: loss = 0.0024870934744360557\n",
      "Iteration 18804: loss = 0.0024870825080177657\n",
      "Iteration 18805: loss = 0.0024870715417191504\n",
      "Iteration 18806: loss = 0.0024870605755402107\n",
      "Iteration 18807: loss = 0.002487049609480943\n",
      "Iteration 18808: loss = 0.002487038643541347\n",
      "Iteration 18809: loss = 0.0024870276777214212\n",
      "Iteration 18810: loss = 0.002487016712021164\n",
      "Iteration 18811: loss = 0.0024870057464405734\n",
      "Iteration 18812: loss = 0.0024869947809796488\n",
      "Iteration 18813: loss = 0.0024869838156383883\n",
      "Iteration 18814: loss = 0.0024869728504167904\n",
      "Iteration 18815: loss = 0.0024869618853148546\n",
      "Iteration 18816: loss = 0.0024869509203325783\n",
      "Iteration 18817: loss = 0.0024869399554699606\n",
      "Iteration 18818: loss = 0.002486928990727\n",
      "Iteration 18819: loss = 0.0024869180261036946\n",
      "Iteration 18820: loss = 0.002486907061600044\n",
      "Iteration 18821: loss = 0.0024868960972160457\n",
      "Iteration 18822: loss = 0.0024868851329516994\n",
      "Iteration 18823: loss = 0.0024868741688070023\n",
      "Iteration 18824: loss = 0.0024868632047819546\n",
      "Iteration 18825: loss = 0.002486852240876553\n",
      "Iteration 18826: loss = 0.0024868412770907974\n",
      "Iteration 18827: loss = 0.0024868303134246866\n",
      "Iteration 18828: loss = 0.002486819349878218\n",
      "Iteration 18829: loss = 0.002486808386451391\n",
      "Iteration 18830: loss = 0.002486797423144204\n",
      "Iteration 18831: loss = 0.0024867864599566556\n",
      "Iteration 18832: loss = 0.002486775496888744\n",
      "Iteration 18833: loss = 0.002486764533940468\n",
      "Iteration 18834: loss = 0.0024867535711118266\n",
      "Iteration 18835: loss = 0.0024867426084028174\n",
      "Iteration 18836: loss = 0.0024867316458134395\n",
      "Iteration 18837: loss = 0.0024867206833436924\n",
      "Iteration 18838: loss = 0.002486709720993573\n",
      "Iteration 18839: loss = 0.0024866987587630813\n",
      "Iteration 18840: loss = 0.0024866877966522147\n",
      "Iteration 18841: loss = 0.0024866768346609724\n",
      "Iteration 18842: loss = 0.0024866658727893533\n",
      "Iteration 18843: loss = 0.002486654911037355\n",
      "Iteration 18844: loss = 0.002486643949404977\n",
      "Iteration 18845: loss = 0.002486632987892217\n",
      "Iteration 18846: loss = 0.0024866220264990746\n",
      "Iteration 18847: loss = 0.0024866110652255477\n",
      "Iteration 18848: loss = 0.002486600104071635\n",
      "Iteration 18849: loss = 0.002486589143037335\n",
      "Iteration 18850: loss = 0.002486578182122646\n",
      "Iteration 18851: loss = 0.0024865672213275677\n",
      "Iteration 18852: loss = 0.0024865562606520976\n",
      "Iteration 18853: loss = 0.0024865453000962345\n",
      "Iteration 18854: loss = 0.0024865343396599767\n",
      "Iteration 18855: loss = 0.0024865233793433233\n",
      "Iteration 18856: loss = 0.0024865124191462726\n",
      "Iteration 18857: loss = 0.0024865014590688237\n",
      "Iteration 18858: loss = 0.0024864904991109744\n",
      "Iteration 18859: loss = 0.0024864795392727235\n",
      "Iteration 18860: loss = 0.0024864685795540696\n",
      "Iteration 18861: loss = 0.002486457619955012\n",
      "Iteration 18862: loss = 0.0024864466604755473\n",
      "Iteration 18863: loss = 0.0024864357011156763\n",
      "Iteration 18864: loss = 0.002486424741875397\n",
      "Iteration 18865: loss = 0.002486413782754707\n",
      "Iteration 18866: loss = 0.0024864028237536052\n",
      "Iteration 18867: loss = 0.0024863918648720907\n",
      "Iteration 18868: loss = 0.002486380906110162\n",
      "Iteration 18869: loss = 0.0024863699474678177\n",
      "Iteration 18870: loss = 0.0024863589889450553\n",
      "Iteration 18871: loss = 0.002486348030541875\n",
      "Iteration 18872: loss = 0.002486337072258275\n",
      "Iteration 18873: loss = 0.0024863261140942523\n",
      "Iteration 18874: loss = 0.0024863151560498065\n",
      "Iteration 18875: loss = 0.002486304198124938\n",
      "Iteration 18876: loss = 0.0024862932403196423\n",
      "Iteration 18877: loss = 0.00248628228263392\n",
      "Iteration 18878: loss = 0.002486271325067769\n",
      "Iteration 18879: loss = 0.0024862603676211875\n",
      "Iteration 18880: loss = 0.0024862494102941743\n",
      "Iteration 18881: loss = 0.0024862384530867287\n",
      "Iteration 18882: loss = 0.002486227495998849\n",
      "Iteration 18883: loss = 0.0024862165390305327\n",
      "Iteration 18884: loss = 0.00248620558218178\n",
      "Iteration 18885: loss = 0.002486194625452588\n",
      "Iteration 18886: loss = 0.0024861836688429557\n",
      "Iteration 18887: loss = 0.002486172712352882\n",
      "Iteration 18888: loss = 0.002486161755982366\n",
      "Iteration 18889: loss = 0.0024861507997314047\n",
      "Iteration 18890: loss = 0.002486139843599998\n",
      "Iteration 18891: loss = 0.0024861288875881443\n",
      "Iteration 18892: loss = 0.0024861179316958415\n",
      "Iteration 18893: loss = 0.00248610697592309\n",
      "Iteration 18894: loss = 0.0024860960202698853\n",
      "Iteration 18895: loss = 0.002486085064736228\n",
      "Iteration 18896: loss = 0.0024860741093221165\n",
      "Iteration 18897: loss = 0.0024860631540275487\n",
      "Iteration 18898: loss = 0.002486052198852524\n",
      "Iteration 18899: loss = 0.0024860412437970416\n",
      "Iteration 18900: loss = 0.0024860302888610976\n",
      "Iteration 18901: loss = 0.002486019334044693\n",
      "Iteration 18902: loss = 0.0024860083793478253\n",
      "Iteration 18903: loss = 0.0024859974247704924\n",
      "Iteration 18904: loss = 0.0024859864703126945\n",
      "Iteration 18905: loss = 0.0024859755159744295\n",
      "Iteration 18906: loss = 0.002485964561755695\n",
      "Iteration 18907: loss = 0.002485953607656492\n",
      "Iteration 18908: loss = 0.002485942653676816\n",
      "Iteration 18909: loss = 0.0024859316998166675\n",
      "Iteration 18910: loss = 0.0024859207460760442\n",
      "Iteration 18911: loss = 0.0024859097924549455\n",
      "Iteration 18912: loss = 0.0024858988389533697\n",
      "Iteration 18913: loss = 0.002485887885571315\n",
      "Iteration 18914: loss = 0.0024858769323087802\n",
      "Iteration 18915: loss = 0.002485865979165764\n",
      "Iteration 18916: loss = 0.002485855026142265\n",
      "Iteration 18917: loss = 0.002485844073238281\n",
      "Iteration 18918: loss = 0.002485833120453812\n",
      "Iteration 18919: loss = 0.0024858221677888552\n",
      "Iteration 18920: loss = 0.00248581121524341\n",
      "Iteration 18921: loss = 0.0024858002628174746\n",
      "Iteration 18922: loss = 0.002485789310511048\n",
      "Iteration 18923: loss = 0.002485778358324128\n",
      "Iteration 18924: loss = 0.0024857674062567134\n",
      "Iteration 18925: loss = 0.0024857564543088032\n",
      "Iteration 18926: loss = 0.002485745502480396\n",
      "Iteration 18927: loss = 0.0024857345507714903\n",
      "Iteration 18928: loss = 0.002485723599182084\n",
      "Iteration 18929: loss = 0.0024857126477121766\n",
      "Iteration 18930: loss = 0.0024857016963617663\n",
      "Iteration 18931: loss = 0.0024856907451308507\n",
      "Iteration 18932: loss = 0.0024856797940194303\n",
      "Iteration 18933: loss = 0.0024856688430275027\n",
      "Iteration 18934: loss = 0.002485657892155066\n",
      "Iteration 18935: loss = 0.0024856469414021197\n",
      "Iteration 18936: loss = 0.002485635990768661\n",
      "Iteration 18937: loss = 0.00248562504025469\n",
      "Iteration 18938: loss = 0.0024856140898602045\n",
      "Iteration 18939: loss = 0.0024856031395852035\n",
      "Iteration 18940: loss = 0.002485592189429685\n",
      "Iteration 18941: loss = 0.0024855812393936474\n",
      "Iteration 18942: loss = 0.00248557028947709\n",
      "Iteration 18943: loss = 0.0024855593396800116\n",
      "Iteration 18944: loss = 0.00248554839000241\n",
      "Iteration 18945: loss = 0.002485537440444284\n",
      "Iteration 18946: loss = 0.002485526491005632\n",
      "Iteration 18947: loss = 0.0024855155416864524\n",
      "Iteration 18948: loss = 0.0024855045924867454\n",
      "Iteration 18949: loss = 0.002485493643406507\n",
      "Iteration 18950: loss = 0.002485482694445738\n",
      "Iteration 18951: loss = 0.0024854717456044358\n",
      "Iteration 18952: loss = 0.0024854607968825995\n",
      "Iteration 18953: loss = 0.002485449848280227\n",
      "Iteration 18954: loss = 0.0024854388997973174\n",
      "Iteration 18955: loss = 0.0024854279514338684\n",
      "Iteration 18956: loss = 0.0024854170031898804\n",
      "Iteration 18957: loss = 0.0024854060550653506\n",
      "Iteration 18958: loss = 0.0024853951070602787\n",
      "Iteration 18959: loss = 0.0024853841591746606\n",
      "Iteration 18960: loss = 0.0024853732114084983\n",
      "Iteration 18961: loss = 0.002485362263761788\n",
      "Iteration 18962: loss = 0.0024853513162345292\n",
      "Iteration 18963: loss = 0.002485340368826721\n",
      "Iteration 18964: loss = 0.0024853294215383603\n",
      "Iteration 18965: loss = 0.002485318474369447\n",
      "Iteration 18966: loss = 0.0024853075273199797\n",
      "Iteration 18967: loss = 0.0024852965803899563\n",
      "Iteration 18968: loss = 0.002485285633579376\n",
      "Iteration 18969: loss = 0.0024852746868882364\n",
      "Iteration 18970: loss = 0.002485263740316538\n",
      "Iteration 18971: loss = 0.0024852527938642767\n",
      "Iteration 18972: loss = 0.0024852418475314535\n",
      "Iteration 18973: loss = 0.002485230901318065\n",
      "Iteration 18974: loss = 0.0024852199552241115\n",
      "Iteration 18975: loss = 0.0024852090092495906\n",
      "Iteration 18976: loss = 0.002485198063394501\n",
      "Iteration 18977: loss = 0.0024851871176588417\n",
      "Iteration 18978: loss = 0.0024851761720426106\n",
      "Iteration 18979: loss = 0.002485165226545807\n",
      "Iteration 18980: loss = 0.0024851542811684287\n",
      "Iteration 18981: loss = 0.0024851433359104748\n",
      "Iteration 18982: loss = 0.0024851323907719436\n",
      "Iteration 18983: loss = 0.002485121445752834\n",
      "Iteration 18984: loss = 0.0024851105008531437\n",
      "Iteration 18985: loss = 0.002485099556072873\n",
      "Iteration 18986: loss = 0.0024850886114120186\n",
      "Iteration 18987: loss = 0.00248507766687058\n",
      "Iteration 18988: loss = 0.0024850667224485556\n",
      "Iteration 18989: loss = 0.0024850557781459443\n",
      "Iteration 18990: loss = 0.002485044833962744\n",
      "Iteration 18991: loss = 0.002485033889898954\n",
      "Iteration 18992: loss = 0.002485022945954573\n",
      "Iteration 18993: loss = 0.0024850120021295986\n",
      "Iteration 18994: loss = 0.00248500105842403\n",
      "Iteration 18995: loss = 0.002484990114837865\n",
      "Iteration 18996: loss = 0.0024849791713711035\n",
      "Iteration 18997: loss = 0.0024849682280237437\n",
      "Iteration 18998: loss = 0.0024849572847957836\n",
      "Iteration 18999: loss = 0.002484946341687222\n",
      "Iteration 19000: loss = 0.0024849353986980574\n",
      "Iteration 19001: loss = 0.0024849244558282886\n",
      "Iteration 19002: loss = 0.0024849135130779144\n",
      "Iteration 19003: loss = 0.0024849025704469325\n",
      "Iteration 19004: loss = 0.0024848916279353425\n",
      "Iteration 19005: loss = 0.002484880685543142\n",
      "Iteration 19006: loss = 0.0024848697432703305\n",
      "Iteration 19007: loss = 0.002484858801116906\n",
      "Iteration 19008: loss = 0.002484847859082867\n",
      "Iteration 19009: loss = 0.0024848369171682127\n",
      "Iteration 19010: loss = 0.002484825975372941\n",
      "Iteration 19011: loss = 0.0024848150336970504\n",
      "Iteration 19012: loss = 0.0024848040921405406\n",
      "Iteration 19013: loss = 0.002484793150703409\n",
      "Iteration 19014: loss = 0.0024847822093856547\n",
      "Iteration 19015: loss = 0.002484771268187276\n",
      "Iteration 19016: loss = 0.002484760327108271\n",
      "Iteration 19017: loss = 0.0024847493861486396\n",
      "Iteration 19018: loss = 0.00248473844530838\n",
      "Iteration 19019: loss = 0.0024847275045874894\n",
      "Iteration 19020: loss = 0.002484716563985968\n",
      "Iteration 19021: loss = 0.0024847056235038135\n",
      "Iteration 19022: loss = 0.002484694683141025\n",
      "Iteration 19023: loss = 0.0024846837428976008\n",
      "Iteration 19024: loss = 0.0024846728027735398\n",
      "Iteration 19025: loss = 0.0024846618627688394\n",
      "Iteration 19026: loss = 0.0024846509228834994\n",
      "Iteration 19027: loss = 0.0024846399831175182\n",
      "Iteration 19028: loss = 0.002484629043470894\n",
      "Iteration 19029: loss = 0.002484618103943626\n",
      "Iteration 19030: loss = 0.002484607164535712\n",
      "Iteration 19031: loss = 0.0024845962252471507\n",
      "Iteration 19032: loss = 0.002484585286077941\n",
      "Iteration 19033: loss = 0.002484574347028082\n",
      "Iteration 19034: loss = 0.002484563408097571\n",
      "Iteration 19035: loss = 0.0024845524692864067\n",
      "Iteration 19036: loss = 0.002484541530594589\n",
      "Iteration 19037: loss = 0.0024845305920221153\n",
      "Iteration 19038: loss = 0.002484519653568985\n",
      "Iteration 19039: loss = 0.002484508715235195\n",
      "Iteration 19040: loss = 0.0024844977770207464\n",
      "Iteration 19041: loss = 0.0024844868389256357\n",
      "Iteration 19042: loss = 0.0024844759009498623\n",
      "Iteration 19043: loss = 0.002484464963093425\n",
      "Iteration 19044: loss = 0.002484454025356322\n",
      "Iteration 19045: loss = 0.0024844430877385515\n",
      "Iteration 19046: loss = 0.0024844321502401133\n",
      "Iteration 19047: loss = 0.0024844212128610045\n",
      "Iteration 19048: loss = 0.0024844102756012244\n",
      "Iteration 19049: loss = 0.002484399338460772\n",
      "Iteration 19050: loss = 0.0024843884014396447\n",
      "Iteration 19051: loss = 0.0024843774645378425\n",
      "Iteration 19052: loss = 0.0024843665277553624\n",
      "Iteration 19053: loss = 0.002484355591092205\n",
      "Iteration 19054: loss = 0.002484344654548367\n",
      "Iteration 19055: loss = 0.0024843337181238477\n",
      "Iteration 19056: loss = 0.002484322781818645\n",
      "Iteration 19057: loss = 0.002484311845632759\n",
      "Iteration 19058: loss = 0.0024843009095661876\n",
      "Iteration 19059: loss = 0.0024842899736189287\n",
      "Iteration 19060: loss = 0.002484279037790981\n",
      "Iteration 19061: loss = 0.0024842681020823443\n",
      "Iteration 19062: loss = 0.0024842571664930153\n",
      "Iteration 19063: loss = 0.002484246231022995\n",
      "Iteration 19064: loss = 0.0024842352956722788\n",
      "Iteration 19065: loss = 0.0024842243604408674\n",
      "Iteration 19066: loss = 0.00248421342532876\n",
      "Iteration 19067: loss = 0.0024842024903359537\n",
      "Iteration 19068: loss = 0.002484191555462447\n",
      "Iteration 19069: loss = 0.0024841806207082395\n",
      "Iteration 19070: loss = 0.0024841696860733293\n",
      "Iteration 19071: loss = 0.0024841587515577147\n",
      "Iteration 19072: loss = 0.002484147817161395\n",
      "Iteration 19073: loss = 0.002484136882884368\n",
      "Iteration 19074: loss = 0.002484125948726632\n",
      "Iteration 19075: loss = 0.002484115014688187\n",
      "Iteration 19076: loss = 0.002484104080769031\n",
      "Iteration 19077: loss = 0.0024840931469691614\n",
      "Iteration 19078: loss = 0.002484082213288578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19079: loss = 0.002484071279727279\n",
      "Iteration 19080: loss = 0.0024840603462852637\n",
      "Iteration 19081: loss = 0.002484049412962529\n",
      "Iteration 19082: loss = 0.002484038479759075\n",
      "Iteration 19083: loss = 0.0024840275466748996\n",
      "Iteration 19084: loss = 0.0024840166137100016\n",
      "Iteration 19085: loss = 0.0024840056808643797\n",
      "Iteration 19086: loss = 0.0024839947481380318\n",
      "Iteration 19087: loss = 0.002483983815530958\n",
      "Iteration 19088: loss = 0.0024839728830431547\n",
      "Iteration 19089: loss = 0.002483961950674622\n",
      "Iteration 19090: loss = 0.002483951018425358\n",
      "Iteration 19091: loss = 0.002483940086295361\n",
      "Iteration 19092: loss = 0.002483929154284631\n",
      "Iteration 19093: loss = 0.002483918222393164\n",
      "Iteration 19094: loss = 0.002483907290620961\n",
      "Iteration 19095: loss = 0.002483896358968019\n",
      "Iteration 19096: loss = 0.002483885427434338\n",
      "Iteration 19097: loss = 0.0024838744960199157\n",
      "Iteration 19098: loss = 0.0024838635647247503\n",
      "Iteration 19099: loss = 0.002483852633548841\n",
      "Iteration 19100: loss = 0.0024838417024921865\n",
      "Iteration 19101: loss = 0.0024838307715547847\n",
      "Iteration 19102: loss = 0.002483819840736635\n",
      "Iteration 19103: loss = 0.0024838089100377354\n",
      "Iteration 19104: loss = 0.0024837979794580843\n",
      "Iteration 19105: loss = 0.0024837870489976812\n",
      "Iteration 19106: loss = 0.0024837761186565234\n",
      "Iteration 19107: loss = 0.0024837651884346106\n",
      "Iteration 19108: loss = 0.002483754258331941\n",
      "Iteration 19109: loss = 0.0024837433283485125\n",
      "Iteration 19110: loss = 0.002483732398484324\n",
      "Iteration 19111: loss = 0.0024837214687393754\n",
      "Iteration 19112: loss = 0.002483710539113664\n",
      "Iteration 19113: loss = 0.002483699609607188\n",
      "Iteration 19114: loss = 0.002483688680219947\n",
      "Iteration 19115: loss = 0.002483677750951939\n",
      "Iteration 19116: loss = 0.0024836668218031627\n",
      "Iteration 19117: loss = 0.0024836558927736167\n",
      "Iteration 19118: loss = 0.0024836449638633\n",
      "Iteration 19119: loss = 0.00248363403507221\n",
      "Iteration 19120: loss = 0.0024836231064003463\n",
      "Iteration 19121: loss = 0.002483612177847707\n",
      "Iteration 19122: loss = 0.002483601249414291\n",
      "Iteration 19123: loss = 0.0024835903211000974\n",
      "Iteration 19124: loss = 0.0024835793929051234\n",
      "Iteration 19125: loss = 0.0024835684648293684\n",
      "Iteration 19126: loss = 0.002483557536872831\n",
      "Iteration 19127: loss = 0.0024835466090355088\n",
      "Iteration 19128: loss = 0.002483535681317402\n",
      "Iteration 19129: loss = 0.0024835247537185086\n",
      "Iteration 19130: loss = 0.002483513826238827\n",
      "Iteration 19131: loss = 0.0024835028988783553\n",
      "Iteration 19132: loss = 0.0024834919716370924\n",
      "Iteration 19133: loss = 0.0024834810445150375\n",
      "Iteration 19134: loss = 0.002483470117512188\n",
      "Iteration 19135: loss = 0.0024834591906285437\n",
      "Iteration 19136: loss = 0.002483448263864102\n",
      "Iteration 19137: loss = 0.0024834373372188633\n",
      "Iteration 19138: loss = 0.002483426410692824\n",
      "Iteration 19139: loss = 0.0024834154842859834\n",
      "Iteration 19140: loss = 0.002483404557998341\n",
      "Iteration 19141: loss = 0.0024833936318298944\n",
      "Iteration 19142: loss = 0.002483382705780642\n",
      "Iteration 19143: loss = 0.002483371779850584\n",
      "Iteration 19144: loss = 0.002483360854039717\n",
      "Iteration 19145: loss = 0.002483349928348041\n",
      "Iteration 19146: loss = 0.002483339002775553\n",
      "Iteration 19147: loss = 0.002483328077322253\n",
      "Iteration 19148: loss = 0.002483317151988139\n",
      "Iteration 19149: loss = 0.0024833062267732102\n",
      "Iteration 19150: loss = 0.0024832953016774638\n",
      "Iteration 19151: loss = 0.0024832843767009\n",
      "Iteration 19152: loss = 0.0024832734518435164\n",
      "Iteration 19153: loss = 0.0024832625271053116\n",
      "Iteration 19154: loss = 0.002483251602486285\n",
      "Iteration 19155: loss = 0.0024832406779864338\n",
      "Iteration 19156: loss = 0.0024832297536057577\n",
      "Iteration 19157: loss = 0.0024832188293442544\n",
      "Iteration 19158: loss = 0.0024832079052019237\n",
      "Iteration 19159: loss = 0.0024831969811787633\n",
      "Iteration 19160: loss = 0.0024831860572747714\n",
      "Iteration 19161: loss = 0.002483175133489947\n",
      "Iteration 19162: loss = 0.00248316420982429\n",
      "Iteration 19163: loss = 0.0024831532862777967\n",
      "Iteration 19164: loss = 0.002483142362850467\n",
      "Iteration 19165: loss = 0.0024831314395422992\n",
      "Iteration 19166: loss = 0.002483120516353292\n",
      "Iteration 19167: loss = 0.002483109593283443\n",
      "Iteration 19168: loss = 0.0024830986703327524\n",
      "Iteration 19169: loss = 0.0024830877475012177\n",
      "Iteration 19170: loss = 0.002483076824788838\n",
      "Iteration 19171: loss = 0.0024830659021956116\n",
      "Iteration 19172: loss = 0.002483054979721538\n",
      "Iteration 19173: loss = 0.0024830440573666134\n",
      "Iteration 19174: loss = 0.0024830331351308383\n",
      "Iteration 19175: loss = 0.0024830222130142114\n",
      "Iteration 19176: loss = 0.00248301129101673\n",
      "Iteration 19177: loss = 0.0024830003691383943\n",
      "Iteration 19178: loss = 0.0024829894473792015\n",
      "Iteration 19179: loss = 0.0024829785257391504\n",
      "Iteration 19180: loss = 0.0024829676042182405\n",
      "Iteration 19181: loss = 0.0024829566828164693\n",
      "Iteration 19182: loss = 0.002482945761533836\n",
      "Iteration 19183: loss = 0.0024829348403703384\n",
      "Iteration 19184: loss = 0.002482923919325976\n",
      "Iteration 19185: loss = 0.0024829129984007474\n",
      "Iteration 19186: loss = 0.00248290207759465\n",
      "Iteration 19187: loss = 0.0024828911569076836\n",
      "Iteration 19188: loss = 0.002482880236339847\n",
      "Iteration 19189: loss = 0.002482869315891137\n",
      "Iteration 19190: loss = 0.0024828583955615537\n",
      "Iteration 19191: loss = 0.0024828474753510956\n",
      "Iteration 19192: loss = 0.0024828365552597605\n",
      "Iteration 19193: loss = 0.0024828256352875475\n",
      "Iteration 19194: loss = 0.002482814715434455\n",
      "Iteration 19195: loss = 0.002482803795700482\n",
      "Iteration 19196: loss = 0.0024827928760856265\n",
      "Iteration 19197: loss = 0.002482781956589887\n",
      "Iteration 19198: loss = 0.0024827710372132632\n",
      "Iteration 19199: loss = 0.0024827601179557525\n",
      "Iteration 19200: loss = 0.002482749198817354\n",
      "Iteration 19201: loss = 0.0024827382797980653\n",
      "Iteration 19202: loss = 0.002482727360897887\n",
      "Iteration 19203: loss = 0.0024827164421168156\n",
      "Iteration 19204: loss = 0.002482705523454851\n",
      "Iteration 19205: loss = 0.002482694604911991\n",
      "Iteration 19206: loss = 0.002482683686488235\n",
      "Iteration 19207: loss = 0.002482672768183581\n",
      "Iteration 19208: loss = 0.002482661849998028\n",
      "Iteration 19209: loss = 0.0024826509319315735\n",
      "Iteration 19210: loss = 0.0024826400139842165\n",
      "Iteration 19211: loss = 0.002482629096155957\n",
      "Iteration 19212: loss = 0.002482618178446791\n",
      "Iteration 19213: loss = 0.0024826072608567203\n",
      "Iteration 19214: loss = 0.00248259634338574\n",
      "Iteration 19215: loss = 0.0024825854260338517\n",
      "Iteration 19216: loss = 0.002482574508801052\n",
      "Iteration 19217: loss = 0.0024825635916873407\n",
      "Iteration 19218: loss = 0.002482552674692715\n",
      "Iteration 19219: loss = 0.0024825417578171753\n",
      "Iteration 19220: loss = 0.0024825308410607186\n",
      "Iteration 19221: loss = 0.002482519924423344\n",
      "Iteration 19222: loss = 0.0024825090079050502\n",
      "Iteration 19223: loss = 0.0024824980915058364\n",
      "Iteration 19224: loss = 0.0024824871752256996\n",
      "Iteration 19225: loss = 0.0024824762590646395\n",
      "Iteration 19226: loss = 0.0024824653430226547\n",
      "Iteration 19227: loss = 0.002482454427099743\n",
      "Iteration 19228: loss = 0.0024824435112959036\n",
      "Iteration 19229: loss = 0.002482432595611135\n",
      "Iteration 19230: loss = 0.0024824216800454365\n",
      "Iteration 19231: loss = 0.002482410764598805\n",
      "Iteration 19232: loss = 0.0024823998492712403\n",
      "Iteration 19233: loss = 0.0024823889340627407\n",
      "Iteration 19234: loss = 0.002482378018973305\n",
      "Iteration 19235: loss = 0.002482367104002931\n",
      "Iteration 19236: loss = 0.0024823561891516186\n",
      "Iteration 19237: loss = 0.002482345274419365\n",
      "Iteration 19238: loss = 0.002482334359806169\n",
      "Iteration 19239: loss = 0.0024823234453120297\n",
      "Iteration 19240: loss = 0.002482312530936946\n",
      "Iteration 19241: loss = 0.0024823016166809157\n",
      "Iteration 19242: loss = 0.002482290702543938\n",
      "Iteration 19243: loss = 0.0024822797885260105\n",
      "Iteration 19244: loss = 0.0024822688746271326\n",
      "Iteration 19245: loss = 0.002482257960847303\n",
      "Iteration 19246: loss = 0.00248224704718652\n",
      "Iteration 19247: loss = 0.0024822361336447815\n",
      "Iteration 19248: loss = 0.002482225220222087\n",
      "Iteration 19249: loss = 0.002482214306918435\n",
      "Iteration 19250: loss = 0.002482203393733824\n",
      "Iteration 19251: loss = 0.002482192480668252\n",
      "Iteration 19252: loss = 0.002482181567721719\n",
      "Iteration 19253: loss = 0.002482170654894222\n",
      "Iteration 19254: loss = 0.0024821597421857597\n",
      "Iteration 19255: loss = 0.0024821488295963317\n",
      "Iteration 19256: loss = 0.0024821379171259357\n",
      "Iteration 19257: loss = 0.002482127004774571\n",
      "Iteration 19258: loss = 0.0024821160925422355\n",
      "Iteration 19259: loss = 0.0024821051804289284\n",
      "Iteration 19260: loss = 0.0024820942684346476\n",
      "Iteration 19261: loss = 0.0024820833565593918\n",
      "Iteration 19262: loss = 0.0024820724448031605\n",
      "Iteration 19263: loss = 0.0024820615331659513\n",
      "Iteration 19264: loss = 0.002482050621647763\n",
      "Iteration 19265: loss = 0.002482039710248594\n",
      "Iteration 19266: loss = 0.0024820287989684438\n",
      "Iteration 19267: loss = 0.00248201788780731\n",
      "Iteration 19268: loss = 0.0024820069767651906\n",
      "Iteration 19269: loss = 0.002481996065842086\n",
      "Iteration 19270: loss = 0.002481985155037993\n",
      "Iteration 19271: loss = 0.0024819742443529117\n",
      "Iteration 19272: loss = 0.00248196333378684\n",
      "Iteration 19273: loss = 0.0024819524233397764\n",
      "Iteration 19274: loss = 0.002481941513011719\n",
      "Iteration 19275: loss = 0.002481930602802668\n",
      "Iteration 19276: loss = 0.0024819196927126194\n",
      "Iteration 19277: loss = 0.0024819087827415744\n",
      "Iteration 19278: loss = 0.00248189787288953\n",
      "Iteration 19279: loss = 0.002481886963156485\n",
      "Iteration 19280: loss = 0.0024818760535424383\n",
      "Iteration 19281: loss = 0.002481865144047389\n",
      "Iteration 19282: loss = 0.002481854234671334\n",
      "Iteration 19283: loss = 0.0024818433254142737\n",
      "Iteration 19284: loss = 0.0024818324162762056\n",
      "Iteration 19285: loss = 0.002481821507257128\n",
      "Iteration 19286: loss = 0.0024818105983570415\n",
      "Iteration 19287: loss = 0.002481799689575942\n",
      "Iteration 19288: loss = 0.0024817887809138297\n",
      "Iteration 19289: loss = 0.002481777872370703\n",
      "Iteration 19290: loss = 0.00248176696394656\n",
      "Iteration 19291: loss = 0.0024817560556413994\n",
      "Iteration 19292: loss = 0.002481745147455221\n",
      "Iteration 19293: loss = 0.002481734239388021\n",
      "Iteration 19294: loss = 0.0024817233314397996\n",
      "Iteration 19295: loss = 0.002481712423610555\n",
      "Iteration 19296: loss = 0.0024817015159002863\n",
      "Iteration 19297: loss = 0.0024816906083089912\n",
      "Iteration 19298: loss = 0.0024816797008366687\n",
      "Iteration 19299: loss = 0.002481668793483317\n",
      "Iteration 19300: loss = 0.0024816578862489357\n",
      "Iteration 19301: loss = 0.0024816469791335226\n",
      "Iteration 19302: loss = 0.0024816360721370764\n",
      "Iteration 19303: loss = 0.002481625165259596\n",
      "Iteration 19304: loss = 0.0024816142585010787\n",
      "Iteration 19305: loss = 0.0024816033518615245\n",
      "Iteration 19306: loss = 0.0024815924453409316\n",
      "Iteration 19307: loss = 0.002481581538939298\n",
      "Iteration 19308: loss = 0.002481570632656623\n",
      "Iteration 19309: loss = 0.0024815597264929053\n",
      "Iteration 19310: loss = 0.0024815488204481427\n",
      "Iteration 19311: loss = 0.0024815379145223344\n",
      "Iteration 19312: loss = 0.0024815270087154787\n",
      "Iteration 19313: loss = 0.0024815161030275748\n",
      "Iteration 19314: loss = 0.00248150519745862\n",
      "Iteration 19315: loss = 0.002481494292008614\n",
      "Iteration 19316: loss = 0.0024814833866775546\n",
      "Iteration 19317: loss = 0.0024814724814654406\n",
      "Iteration 19318: loss = 0.0024814615763722714\n",
      "Iteration 19319: loss = 0.0024814506713980444\n",
      "Iteration 19320: loss = 0.002481439766542759\n",
      "Iteration 19321: loss = 0.002481428861806413\n",
      "Iteration 19322: loss = 0.002481417957189006\n",
      "Iteration 19323: loss = 0.0024814070526905357\n",
      "Iteration 19324: loss = 0.0024813961483110014\n",
      "Iteration 19325: loss = 0.002481385244050401\n",
      "Iteration 19326: loss = 0.002481374339908733\n",
      "Iteration 19327: loss = 0.0024813634358859965\n",
      "Iteration 19328: loss = 0.00248135253198219\n",
      "Iteration 19329: loss = 0.0024813416281973125\n",
      "Iteration 19330: loss = 0.0024813307245313615\n",
      "Iteration 19331: loss = 0.002481319820984336\n",
      "Iteration 19332: loss = 0.0024813089175562356\n",
      "Iteration 19333: loss = 0.0024812980142470574\n",
      "Iteration 19334: loss = 0.0024812871110568005\n",
      "Iteration 19335: loss = 0.0024812762079854636\n",
      "Iteration 19336: loss = 0.0024812653050330456\n",
      "Iteration 19337: loss = 0.0024812544021995445\n",
      "Iteration 19338: loss = 0.0024812434994849588\n",
      "Iteration 19339: loss = 0.002481232596889288\n",
      "Iteration 19340: loss = 0.0024812216944125292\n",
      "Iteration 19341: loss = 0.0024812107920546824\n",
      "Iteration 19342: loss = 0.0024811998898157457\n",
      "Iteration 19343: loss = 0.0024811889876957174\n",
      "Iteration 19344: loss = 0.002481178085694596\n",
      "Iteration 19345: loss = 0.0024811671838123805\n",
      "Iteration 19346: loss = 0.0024811562820490694\n",
      "Iteration 19347: loss = 0.0024811453804046615\n",
      "Iteration 19348: loss = 0.0024811344788791545\n",
      "Iteration 19349: loss = 0.0024811235774725477\n",
      "Iteration 19350: loss = 0.00248111267618484\n",
      "Iteration 19351: loss = 0.002481101775016029\n",
      "Iteration 19352: loss = 0.0024810908739661135\n",
      "Iteration 19353: loss = 0.0024810799730350932\n",
      "Iteration 19354: loss = 0.0024810690722229653\n",
      "Iteration 19355: loss = 0.002481058171529729\n",
      "Iteration 19356: loss = 0.002481047270955383\n",
      "Iteration 19357: loss = 0.0024810363704999257\n",
      "Iteration 19358: loss = 0.0024810254701633557\n",
      "Iteration 19359: loss = 0.0024810145699456707\n",
      "Iteration 19360: loss = 0.0024810036698468714\n",
      "Iteration 19361: loss = 0.0024809927698669545\n",
      "Iteration 19362: loss = 0.002480981870005919\n",
      "Iteration 19363: loss = 0.0024809709702637637\n",
      "Iteration 19364: loss = 0.0024809600706404865\n",
      "Iteration 19365: loss = 0.0024809491711360876\n",
      "Iteration 19366: loss = 0.0024809382717505646\n",
      "Iteration 19367: loss = 0.0024809273724839156\n",
      "Iteration 19368: loss = 0.00248091647333614\n",
      "Iteration 19369: loss = 0.0024809055743072357\n",
      "Iteration 19370: loss = 0.0024808946753972017\n",
      "Iteration 19371: loss = 0.0024808837766060366\n",
      "Iteration 19372: loss = 0.002480872877933739\n",
      "Iteration 19373: loss = 0.002480861979380307\n",
      "Iteration 19374: loss = 0.0024808510809457398\n",
      "Iteration 19375: loss = 0.002480840182630035\n",
      "Iteration 19376: loss = 0.002480829284433192\n",
      "Iteration 19377: loss = 0.00248081838635521\n",
      "Iteration 19378: loss = 0.002480807488396087\n",
      "Iteration 19379: loss = 0.0024807965905558203\n",
      "Iteration 19380: loss = 0.00248078569283441\n",
      "Iteration 19381: loss = 0.0024807747952318546\n",
      "Iteration 19382: loss = 0.0024807638977481523\n",
      "Iteration 19383: loss = 0.002480753000383301\n",
      "Iteration 19384: loss = 0.0024807421031373014\n",
      "Iteration 19385: loss = 0.00248073120601015\n",
      "Iteration 19386: loss = 0.0024807203090018454\n",
      "Iteration 19387: loss = 0.0024807094121123874\n",
      "Iteration 19388: loss = 0.002480698515341774\n",
      "Iteration 19389: loss = 0.002480687618690004\n",
      "Iteration 19390: loss = 0.0024806767221570757\n",
      "Iteration 19391: loss = 0.0024806658257429874\n",
      "Iteration 19392: loss = 0.002480654929447738\n",
      "Iteration 19393: loss = 0.0024806440332713264\n",
      "Iteration 19394: loss = 0.0024806331372137507\n",
      "Iteration 19395: loss = 0.00248062224127501\n",
      "Iteration 19396: loss = 0.002480611345455102\n",
      "Iteration 19397: loss = 0.002480600449754027\n",
      "Iteration 19398: loss = 0.0024805895541717806\n",
      "Iteration 19399: loss = 0.0024805786587083645\n",
      "Iteration 19400: loss = 0.002480567763363776\n",
      "Iteration 19401: loss = 0.002480556868138013\n",
      "Iteration 19402: loss = 0.0024805459730310743\n",
      "Iteration 19403: loss = 0.0024805350780429604\n",
      "Iteration 19404: loss = 0.0024805241831736668\n",
      "Iteration 19405: loss = 0.002480513288423195\n",
      "Iteration 19406: loss = 0.002480502393791542\n",
      "Iteration 19407: loss = 0.002480491499278706\n",
      "Iteration 19408: loss = 0.0024804806048846864\n",
      "Iteration 19409: loss = 0.0024804697106094813\n",
      "Iteration 19410: loss = 0.00248045881645309\n",
      "Iteration 19411: loss = 0.0024804479224155105\n",
      "Iteration 19412: loss = 0.0024804370284967417\n",
      "Iteration 19413: loss = 0.002480426134696782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19414: loss = 0.0024804152410156295\n",
      "Iteration 19415: loss = 0.0024804043474532834\n",
      "Iteration 19416: loss = 0.002480393454009742\n",
      "Iteration 19417: loss = 0.0024803825606850047\n",
      "Iteration 19418: loss = 0.002480371667479069\n",
      "Iteration 19419: loss = 0.0024803607743919334\n",
      "Iteration 19420: loss = 0.0024803498814235975\n",
      "Iteration 19421: loss = 0.0024803389885740593\n",
      "Iteration 19422: loss = 0.0024803280958433167\n",
      "Iteration 19423: loss = 0.0024803172032313698\n",
      "Iteration 19424: loss = 0.0024803063107382167\n",
      "Iteration 19425: loss = 0.002480295418363855\n",
      "Iteration 19426: loss = 0.002480284526108284\n",
      "Iteration 19427: loss = 0.002480273633971502\n",
      "Iteration 19428: loss = 0.002480262741953508\n",
      "Iteration 19429: loss = 0.0024802518500542997\n",
      "Iteration 19430: loss = 0.0024802409582738772\n",
      "Iteration 19431: loss = 0.0024802300666122382\n",
      "Iteration 19432: loss = 0.0024802191750693814\n",
      "Iteration 19433: loss = 0.002480208283645305\n",
      "Iteration 19434: loss = 0.0024801973923400072\n",
      "Iteration 19435: loss = 0.0024801865011534877\n",
      "Iteration 19436: loss = 0.002480175610085745\n",
      "Iteration 19437: loss = 0.0024801647191367775\n",
      "Iteration 19438: loss = 0.002480153828306583\n",
      "Iteration 19439: loss = 0.0024801429375951606\n",
      "Iteration 19440: loss = 0.002480132047002509\n",
      "Iteration 19441: loss = 0.002480121156528627\n",
      "Iteration 19442: loss = 0.0024801102661735123\n",
      "Iteration 19443: loss = 0.0024800993759371646\n",
      "Iteration 19444: loss = 0.0024800884858195817\n",
      "Iteration 19445: loss = 0.0024800775958207628\n",
      "Iteration 19446: loss = 0.002480066705940706\n",
      "Iteration 19447: loss = 0.0024800558161794098\n",
      "Iteration 19448: loss = 0.002480044926536873\n",
      "Iteration 19449: loss = 0.0024800340370130943\n",
      "Iteration 19450: loss = 0.002480023147608072\n",
      "Iteration 19451: loss = 0.0024800122583218047\n",
      "Iteration 19452: loss = 0.002480001369154291\n",
      "Iteration 19453: loss = 0.00247999048010553\n",
      "Iteration 19454: loss = 0.0024799795911755198\n",
      "Iteration 19455: loss = 0.0024799687023642583\n",
      "Iteration 19456: loss = 0.0024799578136717455\n",
      "Iteration 19457: loss = 0.002479946925097979\n",
      "Iteration 19458: loss = 0.0024799360366429576\n",
      "Iteration 19459: loss = 0.0024799251483066803\n",
      "Iteration 19460: loss = 0.0024799142600891457\n",
      "Iteration 19461: loss = 0.0024799033719903507\n",
      "Iteration 19462: loss = 0.0024798924840102967\n",
      "Iteration 19463: loss = 0.0024798815961489798\n",
      "Iteration 19464: loss = 0.0024798707084063994\n",
      "Iteration 19465: loss = 0.002479859820782555\n",
      "Iteration 19466: loss = 0.002479848933277444\n",
      "Iteration 19467: loss = 0.0024798380458910647\n",
      "Iteration 19468: loss = 0.0024798271586234173\n",
      "Iteration 19469: loss = 0.0024798162714744993\n",
      "Iteration 19470: loss = 0.002479805384444309\n",
      "Iteration 19471: loss = 0.0024797944975328457\n",
      "Iteration 19472: loss = 0.002479783610740108\n",
      "Iteration 19473: loss = 0.0024797727240660934\n",
      "Iteration 19474: loss = 0.002479761837510802\n",
      "Iteration 19475: loss = 0.0024797509510742313\n",
      "Iteration 19476: loss = 0.00247974006475638\n",
      "Iteration 19477: loss = 0.0024797291785572475\n",
      "Iteration 19478: loss = 0.002479718292476831\n",
      "Iteration 19479: loss = 0.00247970740651513\n",
      "Iteration 19480: loss = 0.0024796965206721434\n",
      "Iteration 19481: loss = 0.002479685634947869\n",
      "Iteration 19482: loss = 0.002479674749342306\n",
      "Iteration 19483: loss = 0.0024796638638554525\n",
      "Iteration 19484: loss = 0.0024796529784873065\n",
      "Iteration 19485: loss = 0.002479642093237868\n",
      "Iteration 19486: loss = 0.0024796312081071348\n",
      "Iteration 19487: loss = 0.002479620323095106\n",
      "Iteration 19488: loss = 0.0024796094382017794\n",
      "Iteration 19489: loss = 0.002479598553427154\n",
      "Iteration 19490: loss = 0.0024795876687712277\n",
      "Iteration 19491: loss = 0.0024795767842340005\n",
      "Iteration 19492: loss = 0.00247956589981547\n",
      "Iteration 19493: loss = 0.002479555015515635\n",
      "Iteration 19494: loss = 0.002479544131334494\n",
      "Iteration 19495: loss = 0.002479533247272045\n",
      "Iteration 19496: loss = 0.002479522363328288\n",
      "Iteration 19497: loss = 0.0024795114795032208\n",
      "Iteration 19498: loss = 0.002479500595796842\n",
      "Iteration 19499: loss = 0.00247948971220915\n",
      "Iteration 19500: loss = 0.002479478828740143\n",
      "Iteration 19501: loss = 0.00247946794538982\n",
      "Iteration 19502: loss = 0.002479457062158181\n",
      "Iteration 19503: loss = 0.0024794461790452224\n",
      "Iteration 19504: loss = 0.0024794352960509436\n",
      "Iteration 19505: loss = 0.0024794244131753434\n",
      "Iteration 19506: loss = 0.0024794135304184203\n",
      "Iteration 19507: loss = 0.0024794026477801727\n",
      "Iteration 19508: loss = 0.002479391765260599\n",
      "Iteration 19509: loss = 0.0024793808828596988\n",
      "Iteration 19510: loss = 0.002479370000577469\n",
      "Iteration 19511: loss = 0.0024793591184139097\n",
      "Iteration 19512: loss = 0.0024793482363690183\n",
      "Iteration 19513: loss = 0.0024793373544427953\n",
      "Iteration 19514: loss = 0.0024793264726352365\n",
      "Iteration 19515: loss = 0.0024793155909463424\n",
      "Iteration 19516: loss = 0.0024793047093761116\n",
      "Iteration 19517: loss = 0.0024792938279245415\n",
      "Iteration 19518: loss = 0.0024792829465916317\n",
      "Iteration 19519: loss = 0.0024792720653773804\n",
      "Iteration 19520: loss = 0.0024792611842817864\n",
      "Iteration 19521: loss = 0.002479250303304848\n",
      "Iteration 19522: loss = 0.0024792394224465632\n",
      "Iteration 19523: loss = 0.0024792285417069323\n",
      "Iteration 19524: loss = 0.0024792176610859526\n",
      "Iteration 19525: loss = 0.002479206780583623\n",
      "Iteration 19526: loss = 0.0024791959001999415\n",
      "Iteration 19527: loss = 0.002479185019934908\n",
      "Iteration 19528: loss = 0.0024791741397885195\n",
      "Iteration 19529: loss = 0.0024791632597607758\n",
      "Iteration 19530: loss = 0.002479152379851675\n",
      "Iteration 19531: loss = 0.0024791415000612158\n",
      "Iteration 19532: loss = 0.002479130620389397\n",
      "Iteration 19533: loss = 0.0024791197408362163\n",
      "Iteration 19534: loss = 0.002479108861401673\n",
      "Iteration 19535: loss = 0.0024790979820857656\n",
      "Iteration 19536: loss = 0.0024790871028884925\n",
      "Iteration 19537: loss = 0.002479076223809853\n",
      "Iteration 19538: loss = 0.0024790653448498443\n",
      "Iteration 19539: loss = 0.002479054466008466\n",
      "Iteration 19540: loss = 0.0024790435872857164\n",
      "Iteration 19541: loss = 0.0024790327086815942\n",
      "Iteration 19542: loss = 0.002479021830196098\n",
      "Iteration 19543: loss = 0.002479010951829226\n",
      "Iteration 19544: loss = 0.0024790000735809775\n",
      "Iteration 19545: loss = 0.0024789891954513504\n",
      "Iteration 19546: loss = 0.002478978317440344\n",
      "Iteration 19547: loss = 0.002478967439547956\n",
      "Iteration 19548: loss = 0.0024789565617741854\n",
      "Iteration 19549: loss = 0.0024789456841190307\n",
      "Iteration 19550: loss = 0.002478934806582491\n",
      "Iteration 19551: loss = 0.002478923929164564\n",
      "Iteration 19552: loss = 0.002478913051865249\n",
      "Iteration 19553: loss = 0.002478902174684544\n",
      "Iteration 19554: loss = 0.0024788912976224484\n",
      "Iteration 19555: loss = 0.0024788804206789597\n",
      "Iteration 19556: loss = 0.0024788695438540775\n",
      "Iteration 19557: loss = 0.0024788586671477997\n",
      "Iteration 19558: loss = 0.002478847790560125\n",
      "Iteration 19559: loss = 0.0024788369140910523\n",
      "Iteration 19560: loss = 0.00247882603774058\n",
      "Iteration 19561: loss = 0.0024788151615087062\n",
      "Iteration 19562: loss = 0.00247880428539543\n",
      "Iteration 19563: loss = 0.0024787934094007507\n",
      "Iteration 19564: loss = 0.0024787825335246655\n",
      "Iteration 19565: loss = 0.0024787716577671735\n",
      "Iteration 19566: loss = 0.0024787607821282736\n",
      "Iteration 19567: loss = 0.0024787499066079638\n",
      "Iteration 19568: loss = 0.0024787390312062435\n",
      "Iteration 19569: loss = 0.0024787281559231107\n",
      "Iteration 19570: loss = 0.0024787172807585636\n",
      "Iteration 19571: loss = 0.0024787064057126018\n",
      "Iteration 19572: loss = 0.002478695530785223\n",
      "Iteration 19573: loss = 0.002478684655976427\n",
      "Iteration 19574: loss = 0.0024786737812862105\n",
      "Iteration 19575: loss = 0.0024786629067145736\n",
      "Iteration 19576: loss = 0.0024786520322615134\n",
      "Iteration 19577: loss = 0.0024786411579270306\n",
      "Iteration 19578: loss = 0.002478630283711122\n",
      "Iteration 19579: loss = 0.0024786194096137874\n",
      "Iteration 19580: loss = 0.002478608535635024\n",
      "Iteration 19581: loss = 0.0024785976617748323\n",
      "Iteration 19582: loss = 0.002478586788033209\n",
      "Iteration 19583: loss = 0.0024785759144101533\n",
      "Iteration 19584: loss = 0.0024785650409056646\n",
      "Iteration 19585: loss = 0.0024785541675197403\n",
      "Iteration 19586: loss = 0.0024785432942523796\n",
      "Iteration 19587: loss = 0.002478532421103581\n",
      "Iteration 19588: loss = 0.0024785215480733432\n",
      "Iteration 19589: loss = 0.0024785106751616646\n",
      "Iteration 19590: loss = 0.0024784998023685435\n",
      "Iteration 19591: loss = 0.0024784889296939785\n",
      "Iteration 19592: loss = 0.002478478057137969\n",
      "Iteration 19593: loss = 0.002478467184700513\n",
      "Iteration 19594: loss = 0.0024784563123816093\n",
      "Iteration 19595: loss = 0.0024784454401812557\n",
      "Iteration 19596: loss = 0.0024784345680994523\n",
      "Iteration 19597: loss = 0.002478423696136196\n",
      "Iteration 19598: loss = 0.0024784128242914863\n",
      "Iteration 19599: loss = 0.0024784019525653216\n",
      "Iteration 19600: loss = 0.002478391080957702\n",
      "Iteration 19601: loss = 0.0024783802094686226\n",
      "Iteration 19602: loss = 0.0024783693380980845\n",
      "Iteration 19603: loss = 0.0024783584668460865\n",
      "Iteration 19604: loss = 0.0024783475957126256\n",
      "Iteration 19605: loss = 0.0024783367246977014\n",
      "Iteration 19606: loss = 0.0024783258538013126\n",
      "Iteration 19607: loss = 0.002478314983023458\n",
      "Iteration 19608: loss = 0.002478304112364135\n",
      "Iteration 19609: loss = 0.002478293241823343\n",
      "Iteration 19610: loss = 0.0024782823714010805\n",
      "Iteration 19611: loss = 0.0024782715010973457\n",
      "Iteration 19612: loss = 0.002478260630912138\n",
      "Iteration 19613: loss = 0.0024782497608454553\n",
      "Iteration 19614: loss = 0.0024782388908972963\n",
      "Iteration 19615: loss = 0.002478228021067659\n",
      "Iteration 19616: loss = 0.0024782171513565436\n",
      "Iteration 19617: loss = 0.002478206281763948\n",
      "Iteration 19618: loss = 0.0024781954122898692\n",
      "Iteration 19619: loss = 0.0024781845429343087\n",
      "Iteration 19620: loss = 0.0024781736736972622\n",
      "Iteration 19621: loss = 0.0024781628045787295\n",
      "Iteration 19622: loss = 0.0024781519355787096\n",
      "Iteration 19623: loss = 0.0024781410666972013\n",
      "Iteration 19624: loss = 0.002478130197934202\n",
      "Iteration 19625: loss = 0.002478119329289711\n",
      "Iteration 19626: loss = 0.0024781084607637264\n",
      "Iteration 19627: loss = 0.002478097592356248\n",
      "Iteration 19628: loss = 0.0024780867240672727\n",
      "Iteration 19629: loss = 0.0024780758558968\n",
      "Iteration 19630: loss = 0.002478064987844829\n",
      "Iteration 19631: loss = 0.002478054119911357\n",
      "Iteration 19632: loss = 0.0024780432520963834\n",
      "Iteration 19633: loss = 0.0024780323843999067\n",
      "Iteration 19634: loss = 0.0024780215168219255\n",
      "Iteration 19635: loss = 0.002478010649362438\n",
      "Iteration 19636: loss = 0.0024779997820214435\n",
      "Iteration 19637: loss = 0.0024779889147989397\n",
      "Iteration 19638: loss = 0.002477978047694926\n",
      "Iteration 19639: loss = 0.0024779671807094008\n",
      "Iteration 19640: loss = 0.0024779563138423617\n",
      "Iteration 19641: loss = 0.002477945447093809\n",
      "Iteration 19642: loss = 0.0024779345804637402\n",
      "Iteration 19643: loss = 0.002477923713952153\n",
      "Iteration 19644: loss = 0.0024779128475590486\n",
      "Iteration 19645: loss = 0.0024779019812844233\n",
      "Iteration 19646: loss = 0.0024778911151282766\n",
      "Iteration 19647: loss = 0.0024778802490906062\n",
      "Iteration 19648: loss = 0.0024778693831714114\n",
      "Iteration 19649: loss = 0.0024778585173706917\n",
      "Iteration 19650: loss = 0.002477847651688444\n",
      "Iteration 19651: loss = 0.002477836786124668\n",
      "Iteration 19652: loss = 0.0024778259206793616\n",
      "Iteration 19653: loss = 0.0024778150553525237\n",
      "Iteration 19654: loss = 0.0024778041901441536\n",
      "Iteration 19655: loss = 0.0024777933250542485\n",
      "Iteration 19656: loss = 0.002477782460082807\n",
      "Iteration 19657: loss = 0.002477771595229829\n",
      "Iteration 19658: loss = 0.0024777607304953125\n",
      "Iteration 19659: loss = 0.002477749865879256\n",
      "Iteration 19660: loss = 0.0024777390013816574\n",
      "Iteration 19661: loss = 0.0024777281370025163\n",
      "Iteration 19662: loss = 0.0024777172727418312\n",
      "Iteration 19663: loss = 0.0024777064085996004\n",
      "Iteration 19664: loss = 0.002477695544575822\n",
      "Iteration 19665: loss = 0.002477684680670495\n",
      "Iteration 19666: loss = 0.002477673816883619\n",
      "Iteration 19667: loss = 0.002477662953215191\n",
      "Iteration 19668: loss = 0.00247765208966521\n",
      "Iteration 19669: loss = 0.0024776412262336755\n",
      "Iteration 19670: loss = 0.002477630362920585\n",
      "Iteration 19671: loss = 0.002477619499725937\n",
      "Iteration 19672: loss = 0.002477608636649731\n",
      "Iteration 19673: loss = 0.002477597773691965\n",
      "Iteration 19674: loss = 0.002477586910852638\n",
      "Iteration 19675: loss = 0.002477576048131748\n",
      "Iteration 19676: loss = 0.0024775651855292935\n",
      "Iteration 19677: loss = 0.002477554323045274\n",
      "Iteration 19678: loss = 0.002477543460679687\n",
      "Iteration 19679: loss = 0.002477532598432533\n",
      "Iteration 19680: loss = 0.0024775217363038073\n",
      "Iteration 19681: loss = 0.002477510874293512\n",
      "Iteration 19682: loss = 0.002477500012401643\n",
      "Iteration 19683: loss = 0.002477489150628201\n",
      "Iteration 19684: loss = 0.0024774782889731826\n",
      "Iteration 19685: loss = 0.0024774674274365876\n",
      "Iteration 19686: loss = 0.0024774565660184147\n",
      "Iteration 19687: loss = 0.0024774457047186615\n",
      "Iteration 19688: loss = 0.002477434843537327\n",
      "Iteration 19689: loss = 0.0024774239824744107\n",
      "Iteration 19690: loss = 0.0024774131215299098\n",
      "Iteration 19691: loss = 0.0024774022607038245\n",
      "Iteration 19692: loss = 0.002477391399996151\n",
      "Iteration 19693: loss = 0.0024773805394068898\n",
      "Iteration 19694: loss = 0.0024773696789360394\n",
      "Iteration 19695: loss = 0.002477358818583598\n",
      "Iteration 19696: loss = 0.0024773479583495633\n",
      "Iteration 19697: loss = 0.0024773370982339353\n",
      "Iteration 19698: loss = 0.002477326238236712\n",
      "Iteration 19699: loss = 0.002477315378357892\n",
      "Iteration 19700: loss = 0.0024773045185974736\n",
      "Iteration 19701: loss = 0.0024772936589554555\n",
      "Iteration 19702: loss = 0.002477282799431837\n",
      "Iteration 19703: loss = 0.0024772719400266156\n",
      "Iteration 19704: loss = 0.002477261080739791\n",
      "Iteration 19705: loss = 0.002477250221571361\n",
      "Iteration 19706: loss = 0.002477239362521324\n",
      "Iteration 19707: loss = 0.0024772285035896792\n",
      "Iteration 19708: loss = 0.002477217644776425\n",
      "Iteration 19709: loss = 0.0024772067860815597\n",
      "Iteration 19710: loss = 0.002477195927505082\n",
      "Iteration 19711: loss = 0.0024771850690469906\n",
      "Iteration 19712: loss = 0.0024771742107072845\n",
      "Iteration 19713: loss = 0.0024771633524859615\n",
      "Iteration 19714: loss = 0.0024771524943830205\n",
      "Iteration 19715: loss = 0.00247714163639846\n",
      "Iteration 19716: loss = 0.002477130778532279\n",
      "Iteration 19717: loss = 0.0024771199207844754\n",
      "Iteration 19718: loss = 0.002477109063155049\n",
      "Iteration 19719: loss = 0.0024770982056439967\n",
      "Iteration 19720: loss = 0.0024770873482513186\n",
      "Iteration 19721: loss = 0.0024770764909770116\n",
      "Iteration 19722: loss = 0.0024770656338210764\n",
      "Iteration 19723: loss = 0.00247705477678351\n",
      "Iteration 19724: loss = 0.002477043919864311\n",
      "Iteration 19725: loss = 0.002477033063063479\n",
      "Iteration 19726: loss = 0.0024770222063810123\n",
      "Iteration 19727: loss = 0.0024770113498169084\n",
      "Iteration 19728: loss = 0.0024770004933711673\n",
      "Iteration 19729: loss = 0.0024769896370437864\n",
      "Iteration 19730: loss = 0.0024769787808347652\n",
      "Iteration 19731: loss = 0.002476967924744102\n",
      "Iteration 19732: loss = 0.0024769570687717957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19733: loss = 0.0024769462129178438\n",
      "Iteration 19734: loss = 0.0024769353571822456\n",
      "Iteration 19735: loss = 0.002476924501565\n",
      "Iteration 19736: loss = 0.0024769136460661054\n",
      "Iteration 19737: loss = 0.0024769027906855596\n",
      "Iteration 19738: loss = 0.0024768919354233626\n",
      "Iteration 19739: loss = 0.002476881080279512\n",
      "Iteration 19740: loss = 0.0024768702252540064\n",
      "Iteration 19741: loss = 0.0024768593703468453\n",
      "Iteration 19742: loss = 0.0024768485155580253\n",
      "Iteration 19743: loss = 0.0024768376608875473\n",
      "Iteration 19744: loss = 0.002476826806335408\n",
      "Iteration 19745: loss = 0.0024768159519016073\n",
      "Iteration 19746: loss = 0.002476805097586143\n",
      "Iteration 19747: loss = 0.0024767942433890143\n",
      "Iteration 19748: loss = 0.0024767833893102197\n",
      "Iteration 19749: loss = 0.0024767725353497575\n",
      "Iteration 19750: loss = 0.002476761681507626\n",
      "Iteration 19751: loss = 0.0024767508277838235\n",
      "Iteration 19752: loss = 0.00247673997417835\n",
      "Iteration 19753: loss = 0.002476729120691203\n",
      "Iteration 19754: loss = 0.0024767182673223814\n",
      "Iteration 19755: loss = 0.002476707414071884\n",
      "Iteration 19756: loss = 0.002476696560939709\n",
      "Iteration 19757: loss = 0.0024766857079258553\n",
      "Iteration 19758: loss = 0.0024766748550303207\n",
      "Iteration 19759: loss = 0.002476664002253105\n",
      "Iteration 19760: loss = 0.002476653149594206\n",
      "Iteration 19761: loss = 0.0024766422970536224\n",
      "Iteration 19762: loss = 0.002476631444631353\n",
      "Iteration 19763: loss = 0.0024766205923273956\n",
      "Iteration 19764: loss = 0.00247660974014175\n",
      "Iteration 19765: loss = 0.0024765988880744143\n",
      "Iteration 19766: loss = 0.0024765880361253866\n",
      "Iteration 19767: loss = 0.0024765771842946657\n",
      "Iteration 19768: loss = 0.0024765663325822507\n",
      "Iteration 19769: loss = 0.0024765554809881396\n",
      "Iteration 19770: loss = 0.002476544629512331\n",
      "Iteration 19771: loss = 0.0024765337781548244\n",
      "Iteration 19772: loss = 0.0024765229269156177\n",
      "Iteration 19773: loss = 0.0024765120757947083\n",
      "Iteration 19774: loss = 0.0024765012247920967\n",
      "Iteration 19775: loss = 0.002476490373907781\n",
      "Iteration 19776: loss = 0.0024764795231417593\n",
      "Iteration 19777: loss = 0.002476468672494031\n",
      "Iteration 19778: loss = 0.0024764578219645924\n",
      "Iteration 19779: loss = 0.0024764469715534447\n",
      "Iteration 19780: loss = 0.002476436121260586\n",
      "Iteration 19781: loss = 0.002476425271086014\n",
      "Iteration 19782: loss = 0.0024764144210297277\n",
      "Iteration 19783: loss = 0.002476403571091726\n",
      "Iteration 19784: loss = 0.0024763927212720065\n",
      "Iteration 19785: loss = 0.002476381871570569\n",
      "Iteration 19786: loss = 0.0024763710219874117\n",
      "Iteration 19787: loss = 0.0024763601725225327\n",
      "Iteration 19788: loss = 0.0024763493231759305\n",
      "Iteration 19789: loss = 0.002476338473947605\n",
      "Iteration 19790: loss = 0.002476327624837553\n",
      "Iteration 19791: loss = 0.002476316775845775\n",
      "Iteration 19792: loss = 0.002476305926972268\n",
      "Iteration 19793: loss = 0.002476295078217031\n",
      "Iteration 19794: loss = 0.002476284229580063\n",
      "Iteration 19795: loss = 0.0024762733810613615\n",
      "Iteration 19796: loss = 0.002476262532660927\n",
      "Iteration 19797: loss = 0.002476251684378756\n",
      "Iteration 19798: loss = 0.002476240836214849\n",
      "Iteration 19799: loss = 0.002476229988169203\n",
      "Iteration 19800: loss = 0.0024762191402418177\n",
      "Iteration 19801: loss = 0.0024762082924326906\n",
      "Iteration 19802: loss = 0.0024761974447418214\n",
      "Iteration 19803: loss = 0.0024761865971692086\n",
      "Iteration 19804: loss = 0.0024761757497148494\n",
      "Iteration 19805: loss = 0.002476164902378744\n",
      "Iteration 19806: loss = 0.0024761540551608897\n",
      "Iteration 19807: loss = 0.002476143208061286\n",
      "Iteration 19808: loss = 0.0024761323610799314\n",
      "Iteration 19809: loss = 0.0024761215142168245\n",
      "Iteration 19810: loss = 0.0024761106674719632\n",
      "Iteration 19811: loss = 0.002476099820845347\n",
      "Iteration 19812: loss = 0.0024760889743369735\n",
      "Iteration 19813: loss = 0.002476078127946842\n",
      "Iteration 19814: loss = 0.002476067281674951\n",
      "Iteration 19815: loss = 0.002476056435521299\n",
      "Iteration 19816: loss = 0.002476045589485885\n",
      "Iteration 19817: loss = 0.0024760347435687064\n",
      "Iteration 19818: loss = 0.002476023897769763\n",
      "Iteration 19819: loss = 0.002476013052089053\n",
      "Iteration 19820: loss = 0.0024760022065265746\n",
      "Iteration 19821: loss = 0.0024759913610823267\n",
      "Iteration 19822: loss = 0.0024759805157563083\n",
      "Iteration 19823: loss = 0.0024759696705485175\n",
      "Iteration 19824: loss = 0.002475958825458952\n",
      "Iteration 19825: loss = 0.0024759479804876126\n",
      "Iteration 19826: loss = 0.002475937135634496\n",
      "Iteration 19827: loss = 0.0024759262908996015\n",
      "Iteration 19828: loss = 0.002475915446282928\n",
      "Iteration 19829: loss = 0.002475904601784473\n",
      "Iteration 19830: loss = 0.002475893757404236\n",
      "Iteration 19831: loss = 0.0024758829131422155\n",
      "Iteration 19832: loss = 0.00247587206899841\n",
      "Iteration 19833: loss = 0.0024758612249728175\n",
      "Iteration 19834: loss = 0.002475850381065438\n",
      "Iteration 19835: loss = 0.0024758395372762687\n",
      "Iteration 19836: loss = 0.0024758286936053084\n",
      "Iteration 19837: loss = 0.002475817850052556\n",
      "Iteration 19838: loss = 0.0024758070066180104\n",
      "Iteration 19839: loss = 0.0024757961633016697\n",
      "Iteration 19840: loss = 0.0024757853201035325\n",
      "Iteration 19841: loss = 0.0024757744770235977\n",
      "Iteration 19842: loss = 0.002475763634061863\n",
      "Iteration 19843: loss = 0.0024757527912183286\n",
      "Iteration 19844: loss = 0.0024757419484929917\n",
      "Iteration 19845: loss = 0.002475731105885852\n",
      "Iteration 19846: loss = 0.0024757202633969065\n",
      "Iteration 19847: loss = 0.0024757094210261553\n",
      "Iteration 19848: loss = 0.0024756985787735956\n",
      "Iteration 19849: loss = 0.0024756877366392273\n",
      "Iteration 19850: loss = 0.0024756768946230484\n",
      "Iteration 19851: loss = 0.0024756660527250575\n",
      "Iteration 19852: loss = 0.0024756552109452532\n",
      "Iteration 19853: loss = 0.0024756443692836344\n",
      "Iteration 19854: loss = 0.0024756335277401997\n",
      "Iteration 19855: loss = 0.0024756226863149465\n",
      "Iteration 19856: loss = 0.002475611845007875\n",
      "Iteration 19857: loss = 0.0024756010038189825\n",
      "Iteration 19858: loss = 0.0024755901627482686\n",
      "Iteration 19859: loss = 0.0024755793217957306\n",
      "Iteration 19860: loss = 0.002475568480961369\n",
      "Iteration 19861: loss = 0.0024755576402451804\n",
      "Iteration 19862: loss = 0.002475546799647165\n",
      "Iteration 19863: loss = 0.0024755359591673203\n",
      "Iteration 19864: loss = 0.002475525118805645\n",
      "Iteration 19865: loss = 0.0024755142785621385\n",
      "Iteration 19866: loss = 0.0024755034384367986\n",
      "Iteration 19867: loss = 0.002475492598429624\n",
      "Iteration 19868: loss = 0.0024754817585406134\n",
      "Iteration 19869: loss = 0.0024754709187697656\n",
      "Iteration 19870: loss = 0.002475460079117079\n",
      "Iteration 19871: loss = 0.0024754492395825515\n",
      "Iteration 19872: loss = 0.0024754384001661827\n",
      "Iteration 19873: loss = 0.002475427560867971\n",
      "Iteration 19874: loss = 0.0024754167216879145\n",
      "Iteration 19875: loss = 0.0024754058826260126\n",
      "Iteration 19876: loss = 0.0024753950436822627\n",
      "Iteration 19877: loss = 0.0024753842048566644\n",
      "Iteration 19878: loss = 0.0024753733661492163\n",
      "Iteration 19879: loss = 0.002475362527559916\n",
      "Iteration 19880: loss = 0.0024753516890887632\n",
      "Iteration 19881: loss = 0.0024753408507357556\n",
      "Iteration 19882: loss = 0.0024753300125008922\n",
      "Iteration 19883: loss = 0.002475319174384172\n",
      "Iteration 19884: loss = 0.002475308336385593\n",
      "Iteration 19885: loss = 0.002475297498505154\n",
      "Iteration 19886: loss = 0.0024752866607428534\n",
      "Iteration 19887: loss = 0.0024752758230986898\n",
      "Iteration 19888: loss = 0.0024752649855726617\n",
      "Iteration 19889: loss = 0.0024752541481647683\n",
      "Iteration 19890: loss = 0.0024752433108750074\n",
      "Iteration 19891: loss = 0.002475232473703378\n",
      "Iteration 19892: loss = 0.0024752216366498788\n",
      "Iteration 19893: loss = 0.0024752107997145084\n",
      "Iteration 19894: loss = 0.0024751999628972654\n",
      "Iteration 19895: loss = 0.0024751891261981475\n",
      "Iteration 19896: loss = 0.0024751782896171542\n",
      "Iteration 19897: loss = 0.0024751674531542844\n",
      "Iteration 19898: loss = 0.0024751566168095354\n",
      "Iteration 19899: loss = 0.0024751457805829067\n",
      "Iteration 19900: loss = 0.002475134944474397\n",
      "Iteration 19901: loss = 0.0024751241084840048\n",
      "Iteration 19902: loss = 0.002475113272611728\n",
      "Iteration 19903: loss = 0.002475102436857566\n",
      "Iteration 19904: loss = 0.002475091601221517\n",
      "Iteration 19905: loss = 0.0024750807657035797\n",
      "Iteration 19906: loss = 0.0024750699303037523\n",
      "Iteration 19907: loss = 0.0024750590950220336\n",
      "Iteration 19908: loss = 0.002475048259858423\n",
      "Iteration 19909: loss = 0.0024750374248129177\n",
      "Iteration 19910: loss = 0.0024750265898855176\n",
      "Iteration 19911: loss = 0.0024750157550762204\n",
      "Iteration 19912: loss = 0.002475004920385025\n",
      "Iteration 19913: loss = 0.00247499408581193\n",
      "Iteration 19914: loss = 0.0024749832513569336\n",
      "Iteration 19915: loss = 0.002474972417020035\n",
      "Iteration 19916: loss = 0.0024749615828012325\n",
      "Iteration 19917: loss = 0.0024749507487005244\n",
      "Iteration 19918: loss = 0.0024749399147179097\n",
      "Iteration 19919: loss = 0.0024749290808533868\n",
      "Iteration 19920: loss = 0.002474918247106955\n",
      "Iteration 19921: loss = 0.0024749074134786113\n",
      "Iteration 19922: loss = 0.0024748965799683553\n",
      "Iteration 19923: loss = 0.0024748857465761863\n",
      "Iteration 19924: loss = 0.002474874913302101\n",
      "Iteration 19925: loss = 0.0024748640801460996\n",
      "Iteration 19926: loss = 0.0024748532471081807\n",
      "Iteration 19927: loss = 0.0024748424141883413\n",
      "Iteration 19928: loss = 0.0024748315813865815\n",
      "Iteration 19929: loss = 0.002474820748702899\n",
      "Iteration 19930: loss = 0.0024748099161372933\n",
      "Iteration 19931: loss = 0.0024747990836897628\n",
      "Iteration 19932: loss = 0.0024747882513603057\n",
      "Iteration 19933: loss = 0.00247477741914892\n",
      "Iteration 19934: loss = 0.0024747665870556053\n",
      "Iteration 19935: loss = 0.0024747557550803597\n",
      "Iteration 19936: loss = 0.002474744923223182\n",
      "Iteration 19937: loss = 0.002474734091484071\n",
      "Iteration 19938: loss = 0.0024747232598630246\n",
      "Iteration 19939: loss = 0.0024747124283600415\n",
      "Iteration 19940: loss = 0.002474701596975121\n",
      "Iteration 19941: loss = 0.002474690765708261\n",
      "Iteration 19942: loss = 0.0024746799345594602\n",
      "Iteration 19943: loss = 0.0024746691035287182\n",
      "Iteration 19944: loss = 0.0024746582726160316\n",
      "Iteration 19945: loss = 0.0024746474418214006\n",
      "Iteration 19946: loss = 0.0024746366111448237\n",
      "Iteration 19947: loss = 0.002474625780586298\n",
      "Iteration 19948: loss = 0.0024746149501458235\n",
      "Iteration 19949: loss = 0.002474604119823399\n",
      "Iteration 19950: loss = 0.0024745932896190225\n",
      "Iteration 19951: loss = 0.002474582459532692\n",
      "Iteration 19952: loss = 0.002474571629564407\n",
      "Iteration 19953: loss = 0.0024745607997141658\n",
      "Iteration 19954: loss = 0.0024745499699819666\n",
      "Iteration 19955: loss = 0.0024745391403678094\n",
      "Iteration 19956: loss = 0.0024745283108716905\n",
      "Iteration 19957: loss = 0.0024745174814936105\n",
      "Iteration 19958: loss = 0.0024745066522335666\n",
      "Iteration 19959: loss = 0.0024744958230915586\n",
      "Iteration 19960: loss = 0.002474484994067584\n",
      "Iteration 19961: loss = 0.002474474165161642\n",
      "Iteration 19962: loss = 0.002474463336373731\n",
      "Iteration 19963: loss = 0.00247445250770385\n",
      "Iteration 19964: loss = 0.002474441679151997\n",
      "Iteration 19965: loss = 0.002474430850718171\n",
      "Iteration 19966: loss = 0.0024744200224023696\n",
      "Iteration 19967: loss = 0.0024744091942045924\n",
      "Iteration 19968: loss = 0.002474398366124839\n",
      "Iteration 19969: loss = 0.0024743875381631054\n",
      "Iteration 19970: loss = 0.002474376710319392\n",
      "Iteration 19971: loss = 0.0024743658825936974\n",
      "Iteration 19972: loss = 0.002474355054986019\n",
      "Iteration 19973: loss = 0.0024743442274963562\n",
      "Iteration 19974: loss = 0.002474333400124708\n",
      "Iteration 19975: loss = 0.0024743225728710723\n",
      "Iteration 19976: loss = 0.0024743117457354472\n",
      "Iteration 19977: loss = 0.002474300918717833\n",
      "Iteration 19978: loss = 0.0024742900918182263\n",
      "Iteration 19979: loss = 0.0024742792650366274\n",
      "Iteration 19980: loss = 0.0024742684383730336\n",
      "Iteration 19981: loss = 0.002474257611827444\n",
      "Iteration 19982: loss = 0.0024742467853998574\n",
      "Iteration 19983: loss = 0.002474235959090272\n",
      "Iteration 19984: loss = 0.0024742251328986864\n",
      "Iteration 19985: loss = 0.0024742143068251002\n",
      "Iteration 19986: loss = 0.0024742034808695103\n",
      "Iteration 19987: loss = 0.0024741926550319164\n",
      "Iteration 19988: loss = 0.0024741818293123165\n",
      "Iteration 19989: loss = 0.0024741710037107095\n",
      "Iteration 19990: loss = 0.0024741601782270945\n",
      "Iteration 19991: loss = 0.0024741493528614694\n",
      "Iteration 19992: loss = 0.0024741385276138327\n",
      "Iteration 19993: loss = 0.0024741277024841832\n",
      "Iteration 19994: loss = 0.0024741168774725193\n",
      "Iteration 19995: loss = 0.0024741060525788404\n",
      "Iteration 19996: loss = 0.0024740952278031443\n",
      "Iteration 19997: loss = 0.00247408440314543\n",
      "Iteration 19998: loss = 0.0024740735786056957\n",
      "Iteration 19999: loss = 0.00247406275418394\n",
      "Iteration 20000: loss = 0.002474051929880161\n",
      "Iteration 20001: loss = 0.002474041105694359\n",
      "Iteration 20002: loss = 0.002474030281626531\n",
      "Iteration 20003: loss = 0.0024740194576766757\n",
      "Iteration 20004: loss = 0.0024740086338447933\n",
      "Iteration 20005: loss = 0.0024739978101308807\n",
      "Iteration 20006: loss = 0.0024739869865349367\n",
      "Iteration 20007: loss = 0.00247397616305696\n",
      "Iteration 20008: loss = 0.0024739653396969497\n",
      "Iteration 20009: loss = 0.002473954516454904\n",
      "Iteration 20010: loss = 0.0024739436933308213\n",
      "Iteration 20011: loss = 0.0024739328703247\n",
      "Iteration 20012: loss = 0.0024739220474365403\n",
      "Iteration 20013: loss = 0.0024739112246663386\n",
      "Iteration 20014: loss = 0.002473900402014094\n",
      "Iteration 20015: loss = 0.002473889579479807\n",
      "Iteration 20016: loss = 0.0024738787570634736\n",
      "Iteration 20017: loss = 0.002473867934765094\n",
      "Iteration 20018: loss = 0.0024738571125846663\n",
      "Iteration 20019: loss = 0.0024738462905221892\n",
      "Iteration 20020: loss = 0.002473835468577661\n",
      "Iteration 20021: loss = 0.00247382464675108\n",
      "Iteration 20022: loss = 0.002473813825042446\n",
      "Iteration 20023: loss = 0.002473803003451757\n",
      "Iteration 20024: loss = 0.0024737921819790107\n",
      "Iteration 20025: loss = 0.0024737813606242067\n",
      "Iteration 20026: loss = 0.002473770539387343\n",
      "Iteration 20027: loss = 0.0024737597182684194\n",
      "Iteration 20028: loss = 0.0024737488972674327\n",
      "Iteration 20029: loss = 0.002473738076384383\n",
      "Iteration 20030: loss = 0.0024737272556192677\n",
      "Iteration 20031: loss = 0.002473716434972086\n",
      "Iteration 20032: loss = 0.0024737056144428365\n",
      "Iteration 20033: loss = 0.0024736947940315174\n",
      "Iteration 20034: loss = 0.0024736839737381283\n",
      "Iteration 20035: loss = 0.0024736731535626665\n",
      "Iteration 20036: loss = 0.0024736623335051317\n",
      "Iteration 20037: loss = 0.0024736515135655216\n",
      "Iteration 20038: loss = 0.0024736406937438354\n",
      "Iteration 20039: loss = 0.0024736298740400714\n",
      "Iteration 20040: loss = 0.0024736190544542283\n",
      "Iteration 20041: loss = 0.0024736082349863043\n",
      "Iteration 20042: loss = 0.0024735974156362985\n",
      "Iteration 20043: loss = 0.0024735865964042093\n",
      "Iteration 20044: loss = 0.002473575777290035\n",
      "Iteration 20045: loss = 0.0024735649582937748\n",
      "Iteration 20046: loss = 0.002473554139415427\n",
      "Iteration 20047: loss = 0.0024735433206549895\n",
      "Iteration 20048: loss = 0.0024735325020124616\n",
      "Iteration 20049: loss = 0.002473521683487843\n",
      "Iteration 20050: loss = 0.00247351086508113\n",
      "Iteration 20051: loss = 0.0024735000467923226\n",
      "Iteration 20052: loss = 0.002473489228621419\n",
      "Iteration 20053: loss = 0.0024734784105684177\n",
      "Iteration 20054: loss = 0.002473467592633318\n",
      "Iteration 20055: loss = 0.0024734567748161173\n",
      "Iteration 20056: loss = 0.0024734459571168154\n",
      "Iteration 20057: loss = 0.00247343513953541\n",
      "Iteration 20058: loss = 0.0024734243220718996\n",
      "Iteration 20059: loss = 0.0024734135047262836\n",
      "Iteration 20060: loss = 0.0024734026874985602\n",
      "Iteration 20061: loss = 0.0024733918703887282\n",
      "Iteration 20062: loss = 0.0024733810533967854\n",
      "Iteration 20063: loss = 0.0024733702365227314\n",
      "Iteration 20064: loss = 0.002473359419766565\n",
      "Iteration 20065: loss = 0.002473348603128283\n",
      "Iteration 20066: loss = 0.002473337786607885\n",
      "Iteration 20067: loss = 0.0024733269702053706\n",
      "Iteration 20068: loss = 0.0024733161539207365\n",
      "Iteration 20069: loss = 0.002473305337753983\n",
      "Iteration 20070: loss = 0.002473294521705107\n",
      "Iteration 20071: loss = 0.002473283705774109\n",
      "Iteration 20072: loss = 0.002473272889960986\n",
      "Iteration 20073: loss = 0.0024732620742657376\n",
      "Iteration 20074: loss = 0.0024732512586883625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20075: loss = 0.002473240443228858\n",
      "Iteration 20076: loss = 0.002473229627887224\n",
      "Iteration 20077: loss = 0.002473218812663458\n",
      "Iteration 20078: loss = 0.0024732079975575594\n",
      "Iteration 20079: loss = 0.0024731971825695262\n",
      "Iteration 20080: loss = 0.002473186367699358\n",
      "Iteration 20081: loss = 0.0024731755529470525\n",
      "Iteration 20082: loss = 0.002473164738312609\n",
      "Iteration 20083: loss = 0.0024731539237960246\n",
      "Iteration 20084: loss = 0.002473143109397299\n",
      "Iteration 20085: loss = 0.0024731322951164307\n",
      "Iteration 20086: loss = 0.0024731214809534186\n",
      "Iteration 20087: loss = 0.002473110666908261\n",
      "Iteration 20088: loss = 0.0024730998529809566\n",
      "Iteration 20089: loss = 0.002473089039171503\n",
      "Iteration 20090: loss = 0.002473078225479901\n",
      "Iteration 20091: loss = 0.002473067411906146\n",
      "Iteration 20092: loss = 0.0024730565984502394\n",
      "Iteration 20093: loss = 0.0024730457851121785\n",
      "Iteration 20094: loss = 0.002473034971891962\n",
      "Iteration 20095: loss = 0.0024730241587895894\n",
      "Iteration 20096: loss = 0.0024730133458050587\n",
      "Iteration 20097: loss = 0.0024730025329383672\n",
      "Iteration 20098: loss = 0.002472991720189515\n",
      "Iteration 20099: loss = 0.002472980907558501\n",
      "Iteration 20100: loss = 0.002472970095045322\n",
      "Iteration 20101: loss = 0.0024729592826499783\n",
      "Iteration 20102: loss = 0.0024729484703724675\n",
      "Iteration 20103: loss = 0.0024729376582127887\n",
      "Iteration 20104: loss = 0.0024729268461709405\n",
      "Iteration 20105: loss = 0.0024729160342469216\n",
      "Iteration 20106: loss = 0.00247290522244073\n",
      "Iteration 20107: loss = 0.0024728944107523646\n",
      "Iteration 20108: loss = 0.002472883599181824\n",
      "Iteration 20109: loss = 0.0024728727877291064\n",
      "Iteration 20110: loss = 0.0024728619763942114\n",
      "Iteration 20111: loss = 0.0024728511651771366\n",
      "Iteration 20112: loss = 0.0024728403540778808\n",
      "Iteration 20113: loss = 0.002472829543096443\n",
      "Iteration 20114: loss = 0.0024728187322328216\n",
      "Iteration 20115: loss = 0.0024728079214870148\n",
      "Iteration 20116: loss = 0.0024727971108590217\n",
      "Iteration 20117: loss = 0.0024727863003488407\n",
      "Iteration 20118: loss = 0.0024727754899564708\n",
      "Iteration 20119: loss = 0.0024727646796819094\n",
      "Iteration 20120: loss = 0.002472753869525156\n",
      "Iteration 20121: loss = 0.0024727430594862097\n",
      "Iteration 20122: loss = 0.002472732249565068\n",
      "Iteration 20123: loss = 0.00247272143976173\n",
      "Iteration 20124: loss = 0.002472710630076194\n",
      "Iteration 20125: loss = 0.002472699820508458\n",
      "Iteration 20126: loss = 0.0024726890110585223\n",
      "Iteration 20127: loss = 0.0024726782017263846\n",
      "Iteration 20128: loss = 0.0024726673925120437\n",
      "Iteration 20129: loss = 0.002472656583415497\n",
      "Iteration 20130: loss = 0.002472645774436745\n",
      "Iteration 20131: loss = 0.002472634965575785\n",
      "Iteration 20132: loss = 0.002472624156832616\n",
      "Iteration 20133: loss = 0.0024726133482072367\n",
      "Iteration 20134: loss = 0.002472602539699645\n",
      "Iteration 20135: loss = 0.00247259173130984\n",
      "Iteration 20136: loss = 0.0024725809230378205\n",
      "Iteration 20137: loss = 0.0024725701148835843\n",
      "Iteration 20138: loss = 0.002472559306847131\n",
      "Iteration 20139: loss = 0.002472548498928459\n",
      "Iteration 20140: loss = 0.002472537691127566\n",
      "Iteration 20141: loss = 0.002472526883444452\n",
      "Iteration 20142: loss = 0.0024725160758791143\n",
      "Iteration 20143: loss = 0.002472505268431552\n",
      "Iteration 20144: loss = 0.002472494461101764\n",
      "Iteration 20145: loss = 0.002472483653889748\n",
      "Iteration 20146: loss = 0.0024724728467955034\n",
      "Iteration 20147: loss = 0.0024724620398190286\n",
      "Iteration 20148: loss = 0.0024724512329603217\n",
      "Iteration 20149: loss = 0.0024724404262193825\n",
      "Iteration 20150: loss = 0.002472429619596209\n",
      "Iteration 20151: loss = 0.0024724188130907983\n",
      "Iteration 20152: loss = 0.002472408006703151\n",
      "Iteration 20153: loss = 0.002472397200433265\n",
      "Iteration 20154: loss = 0.002472386394281139\n",
      "Iteration 20155: loss = 0.002472375588246771\n",
      "Iteration 20156: loss = 0.0024723647823301606\n",
      "Iteration 20157: loss = 0.002472353976531305\n",
      "Iteration 20158: loss = 0.0024723431708502048\n",
      "Iteration 20159: loss = 0.0024723323652868566\n",
      "Iteration 20160: loss = 0.00247232155984126\n",
      "Iteration 20161: loss = 0.002472310754513413\n",
      "Iteration 20162: loss = 0.002472299949303315\n",
      "Iteration 20163: loss = 0.0024722891442109643\n",
      "Iteration 20164: loss = 0.002472278339236359\n",
      "Iteration 20165: loss = 0.0024722675343794975\n",
      "Iteration 20166: loss = 0.00247225672964038\n",
      "Iteration 20167: loss = 0.0024722459250190038\n",
      "Iteration 20168: loss = 0.0024722351205153674\n",
      "Iteration 20169: loss = 0.0024722243161294697\n",
      "Iteration 20170: loss = 0.0024722135118613094\n",
      "Iteration 20171: loss = 0.002472202707710885\n",
      "Iteration 20172: loss = 0.0024721919036781947\n",
      "Iteration 20173: loss = 0.002472181099763238\n",
      "Iteration 20174: loss = 0.0024721702959660126\n",
      "Iteration 20175: loss = 0.002472159492286518\n",
      "Iteration 20176: loss = 0.0024721486887247512\n",
      "Iteration 20177: loss = 0.0024721378852807126\n",
      "Iteration 20178: loss = 0.0024721270819543996\n",
      "Iteration 20179: loss = 0.0024721162787458115\n",
      "Iteration 20180: loss = 0.002472105475654946\n",
      "Iteration 20181: loss = 0.002472094672681803\n",
      "Iteration 20182: loss = 0.0024720838698263797\n",
      "Iteration 20183: loss = 0.0024720730670886757\n",
      "Iteration 20184: loss = 0.0024720622644686893\n",
      "Iteration 20185: loss = 0.002472051461966419\n",
      "Iteration 20186: loss = 0.0024720406595818627\n",
      "Iteration 20187: loss = 0.0024720298573150203\n",
      "Iteration 20188: loss = 0.0024720190551658903\n",
      "Iteration 20189: loss = 0.00247200825313447\n",
      "Iteration 20190: loss = 0.0024719974512207588\n",
      "Iteration 20191: loss = 0.0024719866494247555\n",
      "Iteration 20192: loss = 0.0024719758477464585\n",
      "Iteration 20193: loss = 0.002471965046185866\n",
      "Iteration 20194: loss = 0.002471954244742977\n",
      "Iteration 20195: loss = 0.00247194344341779\n",
      "Iteration 20196: loss = 0.0024719326422103036\n",
      "Iteration 20197: loss = 0.0024719218411205167\n",
      "Iteration 20198: loss = 0.002471911040148428\n",
      "Iteration 20199: loss = 0.0024719002392940346\n",
      "Iteration 20200: loss = 0.002471889438557337\n",
      "Iteration 20201: loss = 0.002471878637938332\n",
      "Iteration 20202: loss = 0.0024718678374370195\n",
      "Iteration 20203: loss = 0.0024718570370533986\n",
      "Iteration 20204: loss = 0.002471846236787466\n",
      "Iteration 20205: loss = 0.0024718354366392217\n",
      "Iteration 20206: loss = 0.0024718246366086634\n",
      "Iteration 20207: loss = 0.0024718138366957906\n",
      "Iteration 20208: loss = 0.002471803036900602\n",
      "Iteration 20209: loss = 0.002471792237223095\n",
      "Iteration 20210: loss = 0.002471781437663269\n",
      "Iteration 20211: loss = 0.002471770638221122\n",
      "Iteration 20212: loss = 0.0024717598388966536\n",
      "Iteration 20213: loss = 0.0024717490396898617\n",
      "Iteration 20214: loss = 0.002471738240600745\n",
      "Iteration 20215: loss = 0.002471727441629302\n",
      "Iteration 20216: loss = 0.0024717166427755317\n",
      "Iteration 20217: loss = 0.002471705844039432\n",
      "Iteration 20218: loss = 0.002471695045421002\n",
      "Iteration 20219: loss = 0.0024716842469202404\n",
      "Iteration 20220: loss = 0.0024716734485371447\n",
      "Iteration 20221: loss = 0.0024716626502717153\n",
      "Iteration 20222: loss = 0.0024716518521239496\n",
      "Iteration 20223: loss = 0.002471641054093846\n",
      "Iteration 20224: loss = 0.002471630256181404\n",
      "Iteration 20225: loss = 0.002471619458386621\n",
      "Iteration 20226: loss = 0.0024716086607094964\n",
      "Iteration 20227: loss = 0.0024715978631500296\n",
      "Iteration 20228: loss = 0.0024715870657082174\n",
      "Iteration 20229: loss = 0.00247157626838406\n",
      "Iteration 20230: loss = 0.0024715654711775547\n",
      "Iteration 20231: loss = 0.0024715546740886997\n",
      "Iteration 20232: loss = 0.0024715438771174955\n",
      "Iteration 20233: loss = 0.0024715330802639397\n",
      "Iteration 20234: loss = 0.0024715222835280317\n",
      "Iteration 20235: loss = 0.0024715114869097677\n",
      "Iteration 20236: loss = 0.002471500690409149\n",
      "Iteration 20237: loss = 0.002471489894026173\n",
      "Iteration 20238: loss = 0.0024714790977608376\n",
      "Iteration 20239: loss = 0.002471468301613143\n",
      "Iteration 20240: loss = 0.0024714575055830865\n",
      "Iteration 20241: loss = 0.002471446709670667\n",
      "Iteration 20242: loss = 0.002471435913875883\n",
      "Iteration 20243: loss = 0.002471425118198734\n",
      "Iteration 20244: loss = 0.0024714143226392173\n",
      "Iteration 20245: loss = 0.0024714035271973327\n",
      "Iteration 20246: loss = 0.0024713927318730775\n",
      "Iteration 20247: loss = 0.002471381936666452\n",
      "Iteration 20248: loss = 0.002471371141577453\n",
      "Iteration 20249: loss = 0.0024713603466060795\n",
      "Iteration 20250: loss = 0.0024713495517523307\n",
      "Iteration 20251: loss = 0.0024713387570162054\n",
      "Iteration 20252: loss = 0.002471327962397701\n",
      "Iteration 20253: loss = 0.0024713171678968173\n",
      "Iteration 20254: loss = 0.0024713063735135525\n",
      "Iteration 20255: loss = 0.0024712955792479045\n",
      "Iteration 20256: loss = 0.0024712847850998734\n",
      "Iteration 20257: loss = 0.002471273991069456\n",
      "Iteration 20258: loss = 0.002471263197156652\n",
      "Iteration 20259: loss = 0.00247125240336146\n",
      "Iteration 20260: loss = 0.002471241609683878\n",
      "Iteration 20261: loss = 0.002471230816123905\n",
      "Iteration 20262: loss = 0.0024712200226815395\n",
      "Iteration 20263: loss = 0.00247120922935678\n",
      "Iteration 20264: loss = 0.002471198436149625\n",
      "Iteration 20265: loss = 0.0024711876430600745\n",
      "Iteration 20266: loss = 0.0024711768500881247\n",
      "Iteration 20267: loss = 0.0024711660572337758\n",
      "Iteration 20268: loss = 0.0024711552644970255\n",
      "Iteration 20269: loss = 0.0024711444718778734\n",
      "Iteration 20270: loss = 0.0024711336793763173\n",
      "Iteration 20271: loss = 0.0024711228869923555\n",
      "Iteration 20272: loss = 0.0024711120947259876\n",
      "Iteration 20273: loss = 0.0024711013025772123\n",
      "Iteration 20274: loss = 0.0024710905105460273\n",
      "Iteration 20275: loss = 0.002471079718632431\n",
      "Iteration 20276: loss = 0.0024710689268364226\n",
      "Iteration 20277: loss = 0.0024710581351580006\n",
      "Iteration 20278: loss = 0.0024710473435971634\n",
      "Iteration 20279: loss = 0.00247103655215391\n",
      "Iteration 20280: loss = 0.0024710257608282385\n",
      "Iteration 20281: loss = 0.002471014969620148\n",
      "Iteration 20282: loss = 0.002471004178529637\n",
      "Iteration 20283: loss = 0.0024709933875567036\n",
      "Iteration 20284: loss = 0.002470982596701347\n",
      "Iteration 20285: loss = 0.002470971805963565\n",
      "Iteration 20286: loss = 0.002470961015343357\n",
      "Iteration 20287: loss = 0.002470950224840721\n",
      "Iteration 20288: loss = 0.002470939434455656\n",
      "Iteration 20289: loss = 0.0024709286441881606\n",
      "Iteration 20290: loss = 0.002470917854038233\n",
      "Iteration 20291: loss = 0.0024709070640058727\n",
      "Iteration 20292: loss = 0.0024708962740910773\n",
      "Iteration 20293: loss = 0.002470885484293845\n",
      "Iteration 20294: loss = 0.002470874694614175\n",
      "Iteration 20295: loss = 0.0024708639050520674\n",
      "Iteration 20296: loss = 0.002470853115607518\n",
      "Iteration 20297: loss = 0.0024708423262805273\n",
      "Iteration 20298: loss = 0.0024708315370710935\n",
      "Iteration 20299: loss = 0.002470820747979215\n",
      "Iteration 20300: loss = 0.0024708099590048907\n",
      "Iteration 20301: loss = 0.0024707991701481183\n",
      "Iteration 20302: loss = 0.0024707883814088978\n",
      "Iteration 20303: loss = 0.002470777592787227\n",
      "Iteration 20304: loss = 0.0024707668042831033\n",
      "Iteration 20305: loss = 0.0024707560158965273\n",
      "Iteration 20306: loss = 0.0024707452276274966\n",
      "Iteration 20307: loss = 0.0024707344394760107\n",
      "Iteration 20308: loss = 0.0024707236514420667\n",
      "Iteration 20309: loss = 0.002470712863525664\n",
      "Iteration 20310: loss = 0.002470702075726801\n",
      "Iteration 20311: loss = 0.002470691288045477\n",
      "Iteration 20312: loss = 0.0024706805004816893\n",
      "Iteration 20313: loss = 0.0024706697130354384\n",
      "Iteration 20314: loss = 0.0024706589257067206\n",
      "Iteration 20315: loss = 0.0024706481384955364\n",
      "Iteration 20316: loss = 0.0024706373514018828\n",
      "Iteration 20317: loss = 0.0024706265644257597\n",
      "Iteration 20318: loss = 0.002470615777567165\n",
      "Iteration 20319: loss = 0.0024706049908260978\n",
      "Iteration 20320: loss = 0.002470594204202556\n",
      "Iteration 20321: loss = 0.0024705834176965385\n",
      "Iteration 20322: loss = 0.0024705726313080443\n",
      "Iteration 20323: loss = 0.0024705618450370716\n",
      "Iteration 20324: loss = 0.0024705510588836186\n",
      "Iteration 20325: loss = 0.002470540272847684\n",
      "Iteration 20326: loss = 0.0024705294869292677\n",
      "Iteration 20327: loss = 0.002470518701128367\n",
      "Iteration 20328: loss = 0.0024705079154449805\n",
      "Iteration 20329: loss = 0.002470497129879107\n",
      "Iteration 20330: loss = 0.0024704863444307452\n",
      "Iteration 20331: loss = 0.0024704755590998937\n",
      "Iteration 20332: loss = 0.0024704647738865515\n",
      "Iteration 20333: loss = 0.002470453988790716\n",
      "Iteration 20334: loss = 0.002470443203812387\n",
      "Iteration 20335: loss = 0.0024704324189515625\n",
      "Iteration 20336: loss = 0.002470421634208241\n",
      "Iteration 20337: loss = 0.002470410849582422\n",
      "Iteration 20338: loss = 0.0024704000650741026\n",
      "Iteration 20339: loss = 0.0024703892806832825\n",
      "Iteration 20340: loss = 0.0024703784964099596\n",
      "Iteration 20341: loss = 0.0024703677122541335\n",
      "Iteration 20342: loss = 0.002470356928215802\n",
      "Iteration 20343: loss = 0.0024703461442949636\n",
      "Iteration 20344: loss = 0.002470335360491617\n",
      "Iteration 20345: loss = 0.0024703245768057614\n",
      "Iteration 20346: loss = 0.0024703137932373946\n",
      "Iteration 20347: loss = 0.002470303009786516\n",
      "Iteration 20348: loss = 0.0024702922264531234\n",
      "Iteration 20349: loss = 0.0024702814432372155\n",
      "Iteration 20350: loss = 0.002470270660138791\n",
      "Iteration 20351: loss = 0.0024702598771578488\n",
      "Iteration 20352: loss = 0.0024702490942943873\n",
      "Iteration 20353: loss = 0.002470238311548405\n",
      "Iteration 20354: loss = 0.0024702275289199007\n",
      "Iteration 20355: loss = 0.002470216746408873\n",
      "Iteration 20356: loss = 0.00247020596401532\n",
      "Iteration 20357: loss = 0.00247019518173924\n",
      "Iteration 20358: loss = 0.0024701843995806327\n",
      "Iteration 20359: loss = 0.0024701736175394965\n",
      "Iteration 20360: loss = 0.0024701628356158297\n",
      "Iteration 20361: loss = 0.0024701520538096306\n",
      "Iteration 20362: loss = 0.002470141272120898\n",
      "Iteration 20363: loss = 0.002470130490549631\n",
      "Iteration 20364: loss = 0.0024701197090958274\n",
      "Iteration 20365: loss = 0.0024701089277594864\n",
      "Iteration 20366: loss = 0.002470098146540606\n",
      "Iteration 20367: loss = 0.002470087365439185\n",
      "Iteration 20368: loss = 0.002470076584455223\n",
      "Iteration 20369: loss = 0.0024700658035887173\n",
      "Iteration 20370: loss = 0.0024700550228396664\n",
      "Iteration 20371: loss = 0.00247004424220807\n",
      "Iteration 20372: loss = 0.002470033461693926\n",
      "Iteration 20373: loss = 0.0024700226812972327\n",
      "Iteration 20374: loss = 0.0024700119010179896\n",
      "Iteration 20375: loss = 0.0024700011208561947\n",
      "Iteration 20376: loss = 0.0024699903408118462\n",
      "Iteration 20377: loss = 0.0024699795608849437\n",
      "Iteration 20378: loss = 0.0024699687810754846\n",
      "Iteration 20379: loss = 0.002469958001383469\n",
      "Iteration 20380: loss = 0.0024699472218088934\n",
      "Iteration 20381: loss = 0.0024699364423517584\n",
      "Iteration 20382: loss = 0.0024699256630120615\n",
      "Iteration 20383: loss = 0.002469914883789802\n",
      "Iteration 20384: loss = 0.002469904104684978\n",
      "Iteration 20385: loss = 0.002469893325697588\n",
      "Iteration 20386: loss = 0.0024698825468276306\n",
      "Iteration 20387: loss = 0.002469871768075105\n",
      "Iteration 20388: loss = 0.0024698609894400095\n",
      "Iteration 20389: loss = 0.0024698502109223417\n",
      "Iteration 20390: loss = 0.002469839432522102\n",
      "Iteration 20391: loss = 0.0024698286542392867\n",
      "Iteration 20392: loss = 0.002469817876073897\n",
      "Iteration 20393: loss = 0.0024698070980259295\n",
      "Iteration 20394: loss = 0.002469796320095384\n",
      "Iteration 20395: loss = 0.0024697855422822585\n",
      "Iteration 20396: loss = 0.0024697747645865513\n",
      "Iteration 20397: loss = 0.0024697639870082614\n",
      "Iteration 20398: loss = 0.002469753209547388\n",
      "Iteration 20399: loss = 0.002469742432203929\n",
      "Iteration 20400: loss = 0.002469731654977883\n",
      "Iteration 20401: loss = 0.0024697208778692477\n",
      "Iteration 20402: loss = 0.0024697101008780238\n",
      "Iteration 20403: loss = 0.0024696993240042077\n",
      "Iteration 20404: loss = 0.0024696885472478\n",
      "Iteration 20405: loss = 0.0024696777706087972\n",
      "Iteration 20406: loss = 0.0024696669940872\n",
      "Iteration 20407: loss = 0.002469656217683006\n",
      "Iteration 20408: loss = 0.002469645441396213\n",
      "Iteration 20409: loss = 0.002469634665226821\n",
      "Iteration 20410: loss = 0.0024696238891748273\n",
      "Iteration 20411: loss = 0.002469613113240232\n",
      "Iteration 20412: loss = 0.0024696023374230327\n",
      "Iteration 20413: loss = 0.0024695915617232275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20414: loss = 0.0024695807861408164\n",
      "Iteration 20415: loss = 0.0024695700106757974\n",
      "Iteration 20416: loss = 0.002469559235328168\n",
      "Iteration 20417: loss = 0.0024695484600979283\n",
      "Iteration 20418: loss = 0.002469537684985076\n",
      "Iteration 20419: loss = 0.0024695269099896102\n",
      "Iteration 20420: loss = 0.0024695161351115293\n",
      "Iteration 20421: loss = 0.002469505360350832\n",
      "Iteration 20422: loss = 0.002469494585707516\n",
      "Iteration 20423: loss = 0.0024694838111815817\n",
      "Iteration 20424: loss = 0.002469473036773026\n",
      "Iteration 20425: loss = 0.0024694622624818483\n",
      "Iteration 20426: loss = 0.002469451488308047\n",
      "Iteration 20427: loss = 0.002469440714251621\n",
      "Iteration 20428: loss = 0.0024694299403125682\n",
      "Iteration 20429: loss = 0.0024694191664908884\n",
      "Iteration 20430: loss = 0.0024694083927865786\n",
      "Iteration 20431: loss = 0.0024693976191996385\n",
      "Iteration 20432: loss = 0.0024693868457300663\n",
      "Iteration 20433: loss = 0.0024693760723778608\n",
      "Iteration 20434: loss = 0.00246936529914302\n",
      "Iteration 20435: loss = 0.0024693545260255434\n",
      "Iteration 20436: loss = 0.002469343753025429\n",
      "Iteration 20437: loss = 0.002469332980142676\n",
      "Iteration 20438: loss = 0.0024693222073772824\n",
      "Iteration 20439: loss = 0.0024693114347292466\n",
      "Iteration 20440: loss = 0.002469300662198568\n",
      "Iteration 20441: loss = 0.0024692898897852446\n",
      "Iteration 20442: loss = 0.0024692791174892744\n",
      "Iteration 20443: loss = 0.0024692683453106574\n",
      "Iteration 20444: loss = 0.002469257573249391\n",
      "Iteration 20445: loss = 0.0024692468013054747\n",
      "Iteration 20446: loss = 0.002469236029478907\n",
      "Iteration 20447: loss = 0.002469225257769686\n",
      "Iteration 20448: loss = 0.0024692144861778106\n",
      "Iteration 20449: loss = 0.0024692037147032784\n",
      "Iteration 20450: loss = 0.00246919294334609\n",
      "Iteration 20451: loss = 0.0024691821721062415\n",
      "Iteration 20452: loss = 0.0024691714009837337\n",
      "Iteration 20453: loss = 0.0024691606299785643\n",
      "Iteration 20454: loss = 0.0024691498590907316\n",
      "Iteration 20455: loss = 0.0024691390883202348\n",
      "Iteration 20456: loss = 0.002469128317667072\n",
      "Iteration 20457: loss = 0.0024691175471312425\n",
      "Iteration 20458: loss = 0.002469106776712744\n",
      "Iteration 20459: loss = 0.0024690960064115753\n",
      "Iteration 20460: loss = 0.0024690852362277355\n",
      "Iteration 20461: loss = 0.0024690744661612224\n",
      "Iteration 20462: loss = 0.002469063696212035\n",
      "Iteration 20463: loss = 0.002469052926380173\n",
      "Iteration 20464: loss = 0.002469042156665633\n",
      "Iteration 20465: loss = 0.002469031387068415\n",
      "Iteration 20466: loss = 0.0024690206175885174\n",
      "Iteration 20467: loss = 0.0024690098482259373\n",
      "Iteration 20468: loss = 0.002468999078980676\n",
      "Iteration 20469: loss = 0.00246898830985273\n",
      "Iteration 20470: loss = 0.002468977540842098\n",
      "Iteration 20471: loss = 0.002468966771948779\n",
      "Iteration 20472: loss = 0.0024689560031727724\n",
      "Iteration 20473: loss = 0.0024689452345140763\n",
      "Iteration 20474: loss = 0.0024689344659726887\n",
      "Iteration 20475: loss = 0.002468923697548608\n",
      "Iteration 20476: loss = 0.0024689129292418343\n",
      "Iteration 20477: loss = 0.002468902161052364\n",
      "Iteration 20478: loss = 0.0024688913929801983\n",
      "Iteration 20479: loss = 0.0024688806250253337\n",
      "Iteration 20480: loss = 0.0024688698571877694\n",
      "Iteration 20481: loss = 0.002468859089467505\n",
      "Iteration 20482: loss = 0.002468848321864537\n",
      "Iteration 20483: loss = 0.002468837554378866\n",
      "Iteration 20484: loss = 0.0024688267870104897\n",
      "Iteration 20485: loss = 0.0024688160197594068\n",
      "Iteration 20486: loss = 0.0024688052526256154\n",
      "Iteration 20487: loss = 0.002468794485609115\n",
      "Iteration 20488: loss = 0.002468783718709904\n",
      "Iteration 20489: loss = 0.00246877295192798\n",
      "Iteration 20490: loss = 0.002468762185263343\n",
      "Iteration 20491: loss = 0.0024687514187159906\n",
      "Iteration 20492: loss = 0.002468740652285922\n",
      "Iteration 20493: loss = 0.0024687298859731354\n",
      "Iteration 20494: loss = 0.0024687191197776295\n",
      "Iteration 20495: loss = 0.0024687083536994027\n",
      "Iteration 20496: loss = 0.0024686975877384544\n",
      "Iteration 20497: loss = 0.002468686821894782\n",
      "Iteration 20498: loss = 0.0024686760561683854\n",
      "Iteration 20499: loss = 0.002468665290559262\n",
      "Iteration 20500: loss = 0.002468654525067411\n",
      "Iteration 20501: loss = 0.0024686437596928303\n",
      "Iteration 20502: loss = 0.00246863299443552\n",
      "Iteration 20503: loss = 0.002468622229295477\n",
      "Iteration 20504: loss = 0.0024686114642727013\n",
      "Iteration 20505: loss = 0.0024686006993671905\n",
      "Iteration 20506: loss = 0.002468589934578943\n",
      "Iteration 20507: loss = 0.002468579169907959\n",
      "Iteration 20508: loss = 0.002468568405354236\n",
      "Iteration 20509: loss = 0.002468557640917772\n",
      "Iteration 20510: loss = 0.002468546876598566\n",
      "Iteration 20511: loss = 0.002468536112396617\n",
      "Iteration 20512: loss = 0.0024685253483119236\n",
      "Iteration 20513: loss = 0.002468514584344484\n",
      "Iteration 20514: loss = 0.0024685038204942974\n",
      "Iteration 20515: loss = 0.0024684930567613617\n",
      "Iteration 20516: loss = 0.0024684822931456754\n",
      "Iteration 20517: loss = 0.0024684715296472383\n",
      "Iteration 20518: loss = 0.002468460766266047\n",
      "Iteration 20519: loss = 0.0024684500030021025\n",
      "Iteration 20520: loss = 0.0024684392398554014\n",
      "Iteration 20521: loss = 0.002468428476825943\n",
      "Iteration 20522: loss = 0.002468417713913726\n",
      "Iteration 20523: loss = 0.0024684069511187487\n",
      "Iteration 20524: loss = 0.0024683961884410105\n",
      "Iteration 20525: loss = 0.0024683854258805093\n",
      "Iteration 20526: loss = 0.0024683746634372433\n",
      "Iteration 20527: loss = 0.0024683639011112116\n",
      "Iteration 20528: loss = 0.002468353138902414\n",
      "Iteration 20529: loss = 0.0024683423768108465\n",
      "Iteration 20530: loss = 0.0024683316148365097\n",
      "Iteration 20531: loss = 0.0024683208529794015\n",
      "Iteration 20532: loss = 0.0024683100912395203\n",
      "Iteration 20533: loss = 0.0024682993296168652\n",
      "Iteration 20534: loss = 0.0024682885681114345\n",
      "Iteration 20535: loss = 0.0024682778067232265\n",
      "Iteration 20536: loss = 0.0024682670454522406\n",
      "Iteration 20537: loss = 0.0024682562842984752\n",
      "Iteration 20538: loss = 0.002468245523261928\n",
      "Iteration 20539: loss = 0.002468234762342598\n",
      "Iteration 20540: loss = 0.0024682240015404845\n",
      "Iteration 20541: loss = 0.0024682132408555854\n",
      "Iteration 20542: loss = 0.0024682024802879002\n",
      "Iteration 20543: loss = 0.002468191719837426\n",
      "Iteration 20544: loss = 0.0024681809595041623\n",
      "Iteration 20545: loss = 0.0024681701992881073\n",
      "Iteration 20546: loss = 0.00246815943918926\n",
      "Iteration 20547: loss = 0.0024681486792076197\n",
      "Iteration 20548: loss = 0.0024681379193431828\n",
      "Iteration 20549: loss = 0.0024681271595959507\n",
      "Iteration 20550: loss = 0.0024681163999659196\n",
      "Iteration 20551: loss = 0.002468105640453089\n",
      "Iteration 20552: loss = 0.0024680948810574576\n",
      "Iteration 20553: loss = 0.0024680841217790245\n",
      "Iteration 20554: loss = 0.002468073362617787\n",
      "Iteration 20555: loss = 0.002468062603573745\n",
      "Iteration 20556: loss = 0.0024680518446468956\n",
      "Iteration 20557: loss = 0.002468041085837239\n",
      "Iteration 20558: loss = 0.002468030327144773\n",
      "Iteration 20559: loss = 0.0024680195685694963\n",
      "Iteration 20560: loss = 0.002468008810111407\n",
      "Iteration 20561: loss = 0.002467998051770505\n",
      "Iteration 20562: loss = 0.002467987293546787\n",
      "Iteration 20563: loss = 0.0024679765354402535\n",
      "Iteration 20564: loss = 0.002467965777450902\n",
      "Iteration 20565: loss = 0.002467955019578732\n",
      "Iteration 20566: loss = 0.00246794426182374\n",
      "Iteration 20567: loss = 0.0024679335041859277\n",
      "Iteration 20568: loss = 0.002467922746665291\n",
      "Iteration 20569: loss = 0.0024679119892618293\n",
      "Iteration 20570: loss = 0.0024679012319755414\n",
      "Iteration 20571: loss = 0.0024678904748064263\n",
      "Iteration 20572: loss = 0.0024678797177544827\n",
      "Iteration 20573: loss = 0.002467868960819708\n",
      "Iteration 20574: loss = 0.0024678582040021015\n",
      "Iteration 20575: loss = 0.002467847447301662\n",
      "Iteration 20576: loss = 0.0024678366907183876\n",
      "Iteration 20577: loss = 0.002467825934252277\n",
      "Iteration 20578: loss = 0.0024678151779033296\n",
      "Iteration 20579: loss = 0.002467804421671543\n",
      "Iteration 20580: loss = 0.0024677936655569162\n",
      "Iteration 20581: loss = 0.0024677829095594477\n",
      "Iteration 20582: loss = 0.0024677721536791363\n",
      "Iteration 20583: loss = 0.0024677613979159803\n",
      "Iteration 20584: loss = 0.0024677506422699784\n",
      "Iteration 20585: loss = 0.002467739886741129\n",
      "Iteration 20586: loss = 0.0024677291313294314\n",
      "Iteration 20587: loss = 0.002467718376034883\n",
      "Iteration 20588: loss = 0.002467707620857484\n",
      "Iteration 20589: loss = 0.0024676968657972313\n",
      "Iteration 20590: loss = 0.002467686110854125\n",
      "Iteration 20591: loss = 0.0024676753560281626\n",
      "Iteration 20592: loss = 0.0024676646013193433\n",
      "Iteration 20593: loss = 0.0024676538467276648\n",
      "Iteration 20594: loss = 0.002467643092253127\n",
      "Iteration 20595: loss = 0.0024676323378957274\n",
      "Iteration 20596: loss = 0.0024676215836554657\n",
      "Iteration 20597: loss = 0.0024676108295323394\n",
      "Iteration 20598: loss = 0.0024676000755263477\n",
      "Iteration 20599: loss = 0.0024675893216374893\n",
      "Iteration 20600: loss = 0.0024675785678657616\n",
      "Iteration 20601: loss = 0.0024675678142111646\n",
      "Iteration 20602: loss = 0.0024675570606736965\n",
      "Iteration 20603: loss = 0.002467546307253356\n",
      "Iteration 20604: loss = 0.0024675355539501413\n",
      "Iteration 20605: loss = 0.0024675248007640506\n",
      "Iteration 20606: loss = 0.002467514047695084\n",
      "Iteration 20607: loss = 0.0024675032947432387\n",
      "Iteration 20608: loss = 0.002467492541908514\n",
      "Iteration 20609: loss = 0.0024674817891909085\n",
      "Iteration 20610: loss = 0.0024674710365904205\n",
      "Iteration 20611: loss = 0.002467460284107048\n",
      "Iteration 20612: loss = 0.0024674495317407908\n",
      "Iteration 20613: loss = 0.0024674387794916468\n",
      "Iteration 20614: loss = 0.0024674280273596144\n",
      "Iteration 20615: loss = 0.002467417275344693\n",
      "Iteration 20616: loss = 0.0024674065234468814\n",
      "Iteration 20617: loss = 0.0024673957716661757\n",
      "Iteration 20618: loss = 0.002467385020002578\n",
      "Iteration 20619: loss = 0.0024673742684560847\n",
      "Iteration 20620: loss = 0.0024673635170266942\n",
      "Iteration 20621: loss = 0.002467352765714407\n",
      "Iteration 20622: loss = 0.0024673420145192196\n",
      "Iteration 20623: loss = 0.002467331263441132\n",
      "Iteration 20624: loss = 0.002467320512480142\n",
      "Iteration 20625: loss = 0.0024673097616362484\n",
      "Iteration 20626: loss = 0.0024672990109094505\n",
      "Iteration 20627: loss = 0.0024672882602997455\n",
      "Iteration 20628: loss = 0.0024672775098071335\n",
      "Iteration 20629: loss = 0.0024672667594316114\n",
      "Iteration 20630: loss = 0.0024672560091731793\n",
      "Iteration 20631: loss = 0.0024672452590318353\n",
      "Iteration 20632: loss = 0.002467234509007578\n",
      "Iteration 20633: loss = 0.002467223759100406\n",
      "Iteration 20634: loss = 0.0024672130093103175\n",
      "Iteration 20635: loss = 0.0024672022596373116\n",
      "Iteration 20636: loss = 0.0024671915100813865\n",
      "Iteration 20637: loss = 0.0024671807606425414\n",
      "Iteration 20638: loss = 0.002467170011320774\n",
      "Iteration 20639: loss = 0.0024671592621160836\n",
      "Iteration 20640: loss = 0.0024671485130284684\n",
      "Iteration 20641: loss = 0.002467137764057928\n",
      "Iteration 20642: loss = 0.0024671270152044596\n",
      "Iteration 20643: loss = 0.002467116266468062\n",
      "Iteration 20644: loss = 0.002467105517848735\n",
      "Iteration 20645: loss = 0.002467094769346476\n",
      "Iteration 20646: loss = 0.0024670840209612835\n",
      "Iteration 20647: loss = 0.0024670732726931573\n",
      "Iteration 20648: loss = 0.0024670625245420946\n",
      "Iteration 20649: loss = 0.002467051776508095\n",
      "Iteration 20650: loss = 0.0024670410285911573\n",
      "Iteration 20651: loss = 0.0024670302807912785\n",
      "Iteration 20652: loss = 0.002467019533108459\n",
      "Iteration 20653: loss = 0.002467008785542696\n",
      "Iteration 20654: loss = 0.002466998038093989\n",
      "Iteration 20655: loss = 0.0024669872907623364\n",
      "Iteration 20656: loss = 0.002466976543547737\n",
      "Iteration 20657: loss = 0.0024669657964501886\n",
      "Iteration 20658: loss = 0.0024669550494696908\n",
      "Iteration 20659: loss = 0.0024669443026062413\n",
      "Iteration 20660: loss = 0.0024669335558598388\n",
      "Iteration 20661: loss = 0.002466922809230483\n",
      "Iteration 20662: loss = 0.002466912062718171\n",
      "Iteration 20663: loss = 0.0024669013163229024\n",
      "Iteration 20664: loss = 0.002466890570044675\n",
      "Iteration 20665: loss = 0.0024668798238834887\n",
      "Iteration 20666: loss = 0.0024668690778393404\n",
      "Iteration 20667: loss = 0.00246685833191223\n",
      "Iteration 20668: loss = 0.0024668475861021554\n",
      "Iteration 20669: loss = 0.0024668368404091156\n",
      "Iteration 20670: loss = 0.0024668260948331093\n",
      "Iteration 20671: loss = 0.002466815349374135\n",
      "Iteration 20672: loss = 0.00246680460403219\n",
      "Iteration 20673: loss = 0.002466793858807275\n",
      "Iteration 20674: loss = 0.002466783113699387\n",
      "Iteration 20675: loss = 0.002466772368708526\n",
      "Iteration 20676: loss = 0.0024667616238346892\n",
      "Iteration 20677: loss = 0.002466750879077876\n",
      "Iteration 20678: loss = 0.0024667401344380846\n",
      "Iteration 20679: loss = 0.002466729389915314\n",
      "Iteration 20680: loss = 0.002466718645509563\n",
      "Iteration 20681: loss = 0.0024667079012208287\n",
      "Iteration 20682: loss = 0.0024666971570491115\n",
      "Iteration 20683: loss = 0.002466686412994409\n",
      "Iteration 20684: loss = 0.0024666756690567204\n",
      "Iteration 20685: loss = 0.0024666649252360435\n",
      "Iteration 20686: loss = 0.0024666541815323777\n",
      "Iteration 20687: loss = 0.002466643437945721\n",
      "Iteration 20688: loss = 0.002466632694476073\n",
      "Iteration 20689: loss = 0.002466621951123431\n",
      "Iteration 20690: loss = 0.002466611207887794\n",
      "Iteration 20691: loss = 0.002466600464769161\n",
      "Iteration 20692: loss = 0.00246658972176753\n",
      "Iteration 20693: loss = 0.0024665789788828997\n",
      "Iteration 20694: loss = 0.002466568236115269\n",
      "Iteration 20695: loss = 0.0024665574934646375\n",
      "Iteration 20696: loss = 0.002466546750931002\n",
      "Iteration 20697: loss = 0.002466536008514361\n",
      "Iteration 20698: loss = 0.002466525266214715\n",
      "Iteration 20699: loss = 0.0024665145240320607\n",
      "Iteration 20700: loss = 0.002466503781966398\n",
      "Iteration 20701: loss = 0.002466493040017725\n",
      "Iteration 20702: loss = 0.00246648229818604\n",
      "Iteration 20703: loss = 0.002466471556471342\n",
      "Iteration 20704: loss = 0.0024664608148736297\n",
      "Iteration 20705: loss = 0.0024664500733929014\n",
      "Iteration 20706: loss = 0.0024664393320291554\n",
      "Iteration 20707: loss = 0.0024664285907823914\n",
      "Iteration 20708: loss = 0.0024664178496526063\n",
      "Iteration 20709: loss = 0.0024664071086398\n",
      "Iteration 20710: loss = 0.0024663963677439707\n",
      "Iteration 20711: loss = 0.002466385626965117\n",
      "Iteration 20712: loss = 0.0024663748863032377\n",
      "Iteration 20713: loss = 0.0024663641457583316\n",
      "Iteration 20714: loss = 0.002466353405330396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20715: loss = 0.0024663426650194313\n",
      "Iteration 20716: loss = 0.0024663319248254345\n",
      "Iteration 20717: loss = 0.0024663211847484054\n",
      "Iteration 20718: loss = 0.0024663104447883418\n",
      "Iteration 20719: loss = 0.002466299704945243\n",
      "Iteration 20720: loss = 0.002466288965219107\n",
      "Iteration 20721: loss = 0.0024662782256099328\n",
      "Iteration 20722: loss = 0.002466267486117718\n",
      "Iteration 20723: loss = 0.0024662567467424626\n",
      "Iteration 20724: loss = 0.002466246007484165\n",
      "Iteration 20725: loss = 0.0024662352683428226\n",
      "Iteration 20726: loss = 0.0024662245293184345\n",
      "Iteration 20727: loss = 0.0024662137904110006\n",
      "Iteration 20728: loss = 0.0024662030516205175\n",
      "Iteration 20729: loss = 0.0024661923129469855\n",
      "Iteration 20730: loss = 0.002466181574390402\n",
      "Iteration 20731: loss = 0.0024661708359507662\n",
      "Iteration 20732: loss = 0.002466160097628076\n",
      "Iteration 20733: loss = 0.0024661493594223313\n",
      "Iteration 20734: loss = 0.00246613862133353\n",
      "Iteration 20735: loss = 0.0024661278833616697\n",
      "Iteration 20736: loss = 0.002466117145506751\n",
      "Iteration 20737: loss = 0.0024661064077687706\n",
      "Iteration 20738: loss = 0.0024660956701477287\n",
      "Iteration 20739: loss = 0.0024660849326436224\n",
      "Iteration 20740: loss = 0.002466074195256451\n",
      "Iteration 20741: loss = 0.0024660634579862133\n",
      "Iteration 20742: loss = 0.002466052720832908\n",
      "Iteration 20743: loss = 0.002466041983796533\n",
      "Iteration 20744: loss = 0.0024660312468770874\n",
      "Iteration 20745: loss = 0.00246602051007457\n",
      "Iteration 20746: loss = 0.0024660097733889786\n",
      "Iteration 20747: loss = 0.002465999036820313\n",
      "Iteration 20748: loss = 0.0024659883003685704\n",
      "Iteration 20749: loss = 0.00246597756403375\n",
      "Iteration 20750: loss = 0.0024659668278158506\n",
      "Iteration 20751: loss = 0.0024659560917148706\n",
      "Iteration 20752: loss = 0.0024659453557308087\n",
      "Iteration 20753: loss = 0.0024659346198636634\n",
      "Iteration 20754: loss = 0.002465923884113434\n",
      "Iteration 20755: loss = 0.0024659131484801177\n",
      "Iteration 20756: loss = 0.002465902412963714\n",
      "Iteration 20757: loss = 0.002465891677564221\n",
      "Iteration 20758: loss = 0.0024658809422816383\n",
      "Iteration 20759: loss = 0.002465870207115964\n",
      "Iteration 20760: loss = 0.0024658594720671956\n",
      "Iteration 20761: loss = 0.0024658487371353325\n",
      "Iteration 20762: loss = 0.002465838002320375\n",
      "Iteration 20763: loss = 0.0024658272676223183\n",
      "Iteration 20764: loss = 0.0024658165330411637\n",
      "Iteration 20765: loss = 0.0024658057985769088\n",
      "Iteration 20766: loss = 0.002465795064229553\n",
      "Iteration 20767: loss = 0.0024657843299990935\n",
      "Iteration 20768: loss = 0.002465773595885529\n",
      "Iteration 20769: loss = 0.002465762861888859\n",
      "Iteration 20770: loss = 0.0024657521280090824\n",
      "Iteration 20771: loss = 0.0024657413942461965\n",
      "Iteration 20772: loss = 0.002465730660600201\n",
      "Iteration 20773: loss = 0.0024657199270710934\n",
      "Iteration 20774: loss = 0.002465709193658874\n",
      "Iteration 20775: loss = 0.0024656984603635395\n",
      "Iteration 20776: loss = 0.0024656877271850897\n",
      "Iteration 20777: loss = 0.002465676994123523\n",
      "Iteration 20778: loss = 0.002465666261178838\n",
      "Iteration 20779: loss = 0.002465655528351033\n",
      "Iteration 20780: loss = 0.0024656447956401062\n",
      "Iteration 20781: loss = 0.002465634063046057\n",
      "Iteration 20782: loss = 0.0024656233305688836\n",
      "Iteration 20783: loss = 0.002465612598208585\n",
      "Iteration 20784: loss = 0.0024656018659651602\n",
      "Iteration 20785: loss = 0.002465591133838606\n",
      "Iteration 20786: loss = 0.0024655804018289223\n",
      "Iteration 20787: loss = 0.0024655696699361082\n",
      "Iteration 20788: loss = 0.002465558938160161\n",
      "Iteration 20789: loss = 0.0024655482065010803\n",
      "Iteration 20790: loss = 0.0024655374749588638\n",
      "Iteration 20791: loss = 0.0024655267435335112\n",
      "Iteration 20792: loss = 0.0024655160122250197\n",
      "Iteration 20793: loss = 0.0024655052810333895\n",
      "Iteration 20794: loss = 0.002465494549958618\n",
      "Iteration 20795: loss = 0.0024654838190007046\n",
      "Iteration 20796: loss = 0.002465473088159647\n",
      "Iteration 20797: loss = 0.0024654623574354444\n",
      "Iteration 20798: loss = 0.002465451626828095\n",
      "Iteration 20799: loss = 0.0024654408963375976\n",
      "Iteration 20800: loss = 0.0024654301659639516\n",
      "Iteration 20801: loss = 0.0024654194357071543\n",
      "Iteration 20802: loss = 0.002465408705567205\n",
      "Iteration 20803: loss = 0.0024653979755441023\n",
      "Iteration 20804: loss = 0.002465387245637845\n",
      "Iteration 20805: loss = 0.0024653765158484304\n",
      "Iteration 20806: loss = 0.002465365786175859\n",
      "Iteration 20807: loss = 0.0024653550566201277\n",
      "Iteration 20808: loss = 0.0024653443271812356\n",
      "Iteration 20809: loss = 0.0024653335978591824\n",
      "Iteration 20810: loss = 0.0024653228686539654\n",
      "Iteration 20811: loss = 0.002465312139565584\n",
      "Iteration 20812: loss = 0.0024653014105940354\n",
      "Iteration 20813: loss = 0.00246529068173932\n",
      "Iteration 20814: loss = 0.0024652799530014355\n",
      "Iteration 20815: loss = 0.0024652692243803806\n",
      "Iteration 20816: loss = 0.002465258495876154\n",
      "Iteration 20817: loss = 0.0024652477674887542\n",
      "Iteration 20818: loss = 0.0024652370392181794\n",
      "Iteration 20819: loss = 0.0024652263110644295\n",
      "Iteration 20820: loss = 0.0024652155830275015\n",
      "Iteration 20821: loss = 0.002465204855107395\n",
      "Iteration 20822: loss = 0.002465194127304108\n",
      "Iteration 20823: loss = 0.002465183399617639\n",
      "Iteration 20824: loss = 0.002465172672047988\n",
      "Iteration 20825: loss = 0.002465161944595152\n",
      "Iteration 20826: loss = 0.0024651512172591305\n",
      "Iteration 20827: loss = 0.002465140490039921\n",
      "Iteration 20828: loss = 0.0024651297629375234\n",
      "Iteration 20829: loss = 0.002465119035951936\n",
      "Iteration 20830: loss = 0.002465108309083157\n",
      "Iteration 20831: loss = 0.002465097582331185\n",
      "Iteration 20832: loss = 0.0024650868556960184\n",
      "Iteration 20833: loss = 0.002465076129177657\n",
      "Iteration 20834: loss = 0.0024650654027760983\n",
      "Iteration 20835: loss = 0.0024650546764913408\n",
      "Iteration 20836: loss = 0.002465043950323383\n",
      "Iteration 20837: loss = 0.002465033224272225\n",
      "Iteration 20838: loss = 0.0024650224983378635\n",
      "Iteration 20839: loss = 0.002465011772520298\n",
      "Iteration 20840: loss = 0.002465001046819528\n",
      "Iteration 20841: loss = 0.0024649903212355505\n",
      "Iteration 20842: loss = 0.0024649795957683643\n",
      "Iteration 20843: loss = 0.0024649688704179687\n",
      "Iteration 20844: loss = 0.0024649581451843625\n",
      "Iteration 20845: loss = 0.002464947420067543\n",
      "Iteration 20846: loss = 0.00246493669506751\n",
      "Iteration 20847: loss = 0.0024649259701842617\n",
      "Iteration 20848: loss = 0.002464915245417797\n",
      "Iteration 20849: loss = 0.002464904520768114\n",
      "Iteration 20850: loss = 0.002464893796235211\n",
      "Iteration 20851: loss = 0.002464883071819088\n",
      "Iteration 20852: loss = 0.002464872347519742\n",
      "Iteration 20853: loss = 0.0024648616233371723\n",
      "Iteration 20854: loss = 0.002464850899271378\n",
      "Iteration 20855: loss = 0.0024648401753223567\n",
      "Iteration 20856: loss = 0.002464829451490108\n",
      "Iteration 20857: loss = 0.0024648187277746298\n",
      "Iteration 20858: loss = 0.0024648080041759198\n",
      "Iteration 20859: loss = 0.002464797280693979\n",
      "Iteration 20860: loss = 0.0024647865573288047\n",
      "Iteration 20861: loss = 0.002464775834080395\n",
      "Iteration 20862: loss = 0.0024647651109487483\n",
      "Iteration 20863: loss = 0.0024647543879338647\n",
      "Iteration 20864: loss = 0.002464743665035742\n",
      "Iteration 20865: loss = 0.002464732942254378\n",
      "Iteration 20866: loss = 0.002464722219589773\n",
      "Iteration 20867: loss = 0.0024647114970419238\n",
      "Iteration 20868: loss = 0.0024647007746108306\n",
      "Iteration 20869: loss = 0.0024646900522964908\n",
      "Iteration 20870: loss = 0.002464679330098904\n",
      "Iteration 20871: loss = 0.002464668608018067\n",
      "Iteration 20872: loss = 0.002464657886053981\n",
      "Iteration 20873: loss = 0.002464647164206642\n",
      "Iteration 20874: loss = 0.0024646364424760507\n",
      "Iteration 20875: loss = 0.0024646257208622052\n",
      "Iteration 20876: loss = 0.0024646149993651023\n",
      "Iteration 20877: loss = 0.0024646042779847433\n",
      "Iteration 20878: loss = 0.0024645935567211247\n",
      "Iteration 20879: loss = 0.002464582835574247\n",
      "Iteration 20880: loss = 0.0024645721145441063\n",
      "Iteration 20881: loss = 0.0024645613936307036\n",
      "Iteration 20882: loss = 0.002464550672834036\n",
      "Iteration 20883: loss = 0.002464539952154103\n",
      "Iteration 20884: loss = 0.002464529231590902\n",
      "Iteration 20885: loss = 0.0024645185111444334\n",
      "Iteration 20886: loss = 0.002464507790814694\n",
      "Iteration 20887: loss = 0.0024644970706016837\n",
      "Iteration 20888: loss = 0.002464486350505401\n",
      "Iteration 20889: loss = 0.0024644756305258436\n",
      "Iteration 20890: loss = 0.00246446491066301\n",
      "Iteration 20891: loss = 0.0024644541909169\n",
      "Iteration 20892: loss = 0.0024644434712875114\n",
      "Iteration 20893: loss = 0.002464432751774843\n",
      "Iteration 20894: loss = 0.0024644220323788934\n",
      "Iteration 20895: loss = 0.002464411313099661\n",
      "Iteration 20896: loss = 0.0024644005939371454\n",
      "Iteration 20897: loss = 0.002464389874891343\n",
      "Iteration 20898: loss = 0.002464379155962255\n",
      "Iteration 20899: loss = 0.0024643684371498786\n",
      "Iteration 20900: loss = 0.002464357718454212\n",
      "Iteration 20901: loss = 0.0024643469998752543\n",
      "Iteration 20902: loss = 0.002464336281413004\n",
      "Iteration 20903: loss = 0.0024643255630674606\n",
      "Iteration 20904: loss = 0.0024643148448386214\n",
      "Iteration 20905: loss = 0.0024643041267264857\n",
      "Iteration 20906: loss = 0.0024642934087310523\n",
      "Iteration 20907: loss = 0.0024642826908523193\n",
      "Iteration 20908: loss = 0.0024642719730902855\n",
      "Iteration 20909: loss = 0.002464261255444949\n",
      "Iteration 20910: loss = 0.0024642505379163086\n",
      "Iteration 20911: loss = 0.002464239820504364\n",
      "Iteration 20912: loss = 0.0024642291032091123\n",
      "Iteration 20913: loss = 0.0024642183860305532\n",
      "Iteration 20914: loss = 0.002464207668968684\n",
      "Iteration 20915: loss = 0.002464196952023505\n",
      "Iteration 20916: loss = 0.002464186235195014\n",
      "Iteration 20917: loss = 0.0024641755184832085\n",
      "Iteration 20918: loss = 0.0024641648018880885\n",
      "Iteration 20919: loss = 0.002464154085409653\n",
      "Iteration 20920: loss = 0.0024641433690478992\n",
      "Iteration 20921: loss = 0.002464132652802826\n",
      "Iteration 20922: loss = 0.0024641219366744326\n",
      "Iteration 20923: loss = 0.002464111220662718\n",
      "Iteration 20924: loss = 0.00246410050476768\n",
      "Iteration 20925: loss = 0.002464089788989316\n",
      "Iteration 20926: loss = 0.0024640790733276266\n",
      "Iteration 20927: loss = 0.0024640683577826098\n",
      "Iteration 20928: loss = 0.002464057642354264\n",
      "Iteration 20929: loss = 0.0024640469270425878\n",
      "Iteration 20930: loss = 0.00246403621184758\n",
      "Iteration 20931: loss = 0.00246402549676924\n",
      "Iteration 20932: loss = 0.002464014781807564\n",
      "Iteration 20933: loss = 0.002464004066962553\n",
      "Iteration 20934: loss = 0.0024639933522342043\n",
      "Iteration 20935: loss = 0.002463982637622517\n",
      "Iteration 20936: loss = 0.002463971923127489\n",
      "Iteration 20937: loss = 0.00246396120874912\n",
      "Iteration 20938: loss = 0.0024639504944874084\n",
      "Iteration 20939: loss = 0.002463939780342352\n",
      "Iteration 20940: loss = 0.0024639290663139497\n",
      "Iteration 20941: loss = 0.002463918352402201\n",
      "Iteration 20942: loss = 0.0024639076386071034\n",
      "Iteration 20943: loss = 0.0024638969249286555\n",
      "Iteration 20944: loss = 0.0024638862113668565\n",
      "Iteration 20945: loss = 0.0024638754979217047\n",
      "Iteration 20946: loss = 0.0024638647845931995\n",
      "Iteration 20947: loss = 0.0024638540713813376\n",
      "Iteration 20948: loss = 0.0024638433582861194\n",
      "Iteration 20949: loss = 0.0024638326453075426\n",
      "Iteration 20950: loss = 0.0024638219324456057\n",
      "Iteration 20951: loss = 0.0024638112197003085\n",
      "Iteration 20952: loss = 0.002463800507071649\n",
      "Iteration 20953: loss = 0.0024637897945596243\n",
      "Iteration 20954: loss = 0.002463779082164235\n",
      "Iteration 20955: loss = 0.0024637683698854784\n",
      "Iteration 20956: loss = 0.0024637576577233545\n",
      "Iteration 20957: loss = 0.002463746945677861\n",
      "Iteration 20958: loss = 0.002463736233748995\n",
      "Iteration 20959: loss = 0.002463725521936758\n",
      "Iteration 20960: loss = 0.002463714810241147\n",
      "Iteration 20961: loss = 0.0024637040986621606\n",
      "Iteration 20962: loss = 0.002463693387199798\n",
      "Iteration 20963: loss = 0.0024636826758540568\n",
      "Iteration 20964: loss = 0.002463671964624937\n",
      "Iteration 20965: loss = 0.002463661253512436\n",
      "Iteration 20966: loss = 0.002463650542516553\n",
      "Iteration 20967: loss = 0.002463639831637286\n",
      "Iteration 20968: loss = 0.0024636291208746343\n",
      "Iteration 20969: loss = 0.0024636184102285966\n",
      "Iteration 20970: loss = 0.0024636076996991706\n",
      "Iteration 20971: loss = 0.0024635969892863553\n",
      "Iteration 20972: loss = 0.00246358627899015\n",
      "Iteration 20973: loss = 0.002463575568810552\n",
      "Iteration 20974: loss = 0.002463564858747561\n",
      "Iteration 20975: loss = 0.002463554148801175\n",
      "Iteration 20976: loss = 0.0024635434389713926\n",
      "Iteration 20977: loss = 0.0024635327292582138\n",
      "Iteration 20978: loss = 0.002463522019661635\n",
      "Iteration 20979: loss = 0.002463511310181656\n",
      "Iteration 20980: loss = 0.002463500600818275\n",
      "Iteration 20981: loss = 0.002463489891571491\n",
      "Iteration 20982: loss = 0.0024634791824413027\n",
      "Iteration 20983: loss = 0.0024634684734277075\n",
      "Iteration 20984: loss = 0.0024634577645307053\n",
      "Iteration 20985: loss = 0.0024634470557502947\n",
      "Iteration 20986: loss = 0.002463436347086473\n",
      "Iteration 20987: loss = 0.0024634256385392406\n",
      "Iteration 20988: loss = 0.002463414930108595\n",
      "Iteration 20989: loss = 0.002463404221794535\n",
      "Iteration 20990: loss = 0.0024633935135970585\n",
      "Iteration 20991: loss = 0.0024633828055161656\n",
      "Iteration 20992: loss = 0.002463372097551854\n",
      "Iteration 20993: loss = 0.0024633613897041217\n",
      "Iteration 20994: loss = 0.0024633506819729685\n",
      "Iteration 20995: loss = 0.0024633399743583923\n",
      "Iteration 20996: loss = 0.002463329266860392\n",
      "Iteration 20997: loss = 0.0024633185594789657\n",
      "Iteration 20998: loss = 0.002463307852214113\n",
      "Iteration 20999: loss = 0.0024632971450658315\n",
      "Iteration 21000: loss = 0.0024632864380341206\n",
      "Iteration 21001: loss = 0.002463275731118978\n",
      "Iteration 21002: loss = 0.0024632650243204025\n",
      "Iteration 21003: loss = 0.0024632543176383932\n",
      "Iteration 21004: loss = 0.0024632436110729487\n",
      "Iteration 21005: loss = 0.0024632329046240676\n",
      "Iteration 21006: loss = 0.0024632221982917478\n",
      "Iteration 21007: loss = 0.0024632114920759883\n",
      "Iteration 21008: loss = 0.002463200785976788\n",
      "Iteration 21009: loss = 0.0024631900799941446\n",
      "Iteration 21010: loss = 0.002463179374128058\n",
      "Iteration 21011: loss = 0.002463168668378526\n",
      "Iteration 21012: loss = 0.002463157962745548\n",
      "Iteration 21013: loss = 0.0024631472572291205\n",
      "Iteration 21014: loss = 0.002463136551829245\n",
      "Iteration 21015: loss = 0.0024631258465459176\n",
      "Iteration 21016: loss = 0.0024631151413791385\n",
      "Iteration 21017: loss = 0.002463104436328906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21018: loss = 0.002463093731395218\n",
      "Iteration 21019: loss = 0.0024630830265780735\n",
      "Iteration 21020: loss = 0.0024630723218774715\n",
      "Iteration 21021: loss = 0.0024630616172934103\n",
      "Iteration 21022: loss = 0.0024630509128258874\n",
      "Iteration 21023: loss = 0.0024630402084749037\n",
      "Iteration 21024: loss = 0.0024630295042404556\n",
      "Iteration 21025: loss = 0.0024630188001225433\n",
      "Iteration 21026: loss = 0.002463008096121164\n",
      "Iteration 21027: loss = 0.002462997392236318\n",
      "Iteration 21028: loss = 0.002462986688468002\n",
      "Iteration 21029: loss = 0.0024629759848162165\n",
      "Iteration 21030: loss = 0.0024629652812809583\n",
      "Iteration 21031: loss = 0.002462954577862227\n",
      "Iteration 21032: loss = 0.002462943874560021\n",
      "Iteration 21033: loss = 0.0024629331713743397\n",
      "Iteration 21034: loss = 0.00246292246830518\n",
      "Iteration 21035: loss = 0.0024629117653525417\n",
      "Iteration 21036: loss = 0.002462901062516423\n",
      "Iteration 21037: loss = 0.0024628903597968227\n",
      "Iteration 21038: loss = 0.00246287965719374\n",
      "Iteration 21039: loss = 0.002462868954707172\n",
      "Iteration 21040: loss = 0.0024628582523371183\n",
      "Iteration 21041: loss = 0.0024628475500835774\n",
      "Iteration 21042: loss = 0.0024628368479465475\n",
      "Iteration 21043: loss = 0.002462826145926028\n",
      "Iteration 21044: loss = 0.0024628154440220168\n",
      "Iteration 21045: loss = 0.002462804742234513\n",
      "Iteration 21046: loss = 0.002462794040563515\n",
      "Iteration 21047: loss = 0.00246278333900902\n",
      "Iteration 21048: loss = 0.0024627726375710296\n",
      "Iteration 21049: loss = 0.0024627619362495396\n",
      "Iteration 21050: loss = 0.0024627512350445502\n",
      "Iteration 21051: loss = 0.002462740533956059\n",
      "Iteration 21052: loss = 0.0024627298329840657\n",
      "Iteration 21053: loss = 0.002462719132128569\n",
      "Iteration 21054: loss = 0.002462708431389565\n",
      "Iteration 21055: loss = 0.002462697730767055\n",
      "Iteration 21056: loss = 0.002462687030261037\n",
      "Iteration 21057: loss = 0.0024626763298715095\n",
      "Iteration 21058: loss = 0.0024626656295984703\n",
      "Iteration 21059: loss = 0.0024626549294419187\n",
      "Iteration 21060: loss = 0.0024626442294018534\n",
      "Iteration 21061: loss = 0.0024626335294782725\n",
      "Iteration 21062: loss = 0.002462622829671175\n",
      "Iteration 21063: loss = 0.00246261212998056\n",
      "Iteration 21064: loss = 0.002462601430406425\n",
      "Iteration 21065: loss = 0.002462590730948769\n",
      "Iteration 21066: loss = 0.002462580031607591\n",
      "Iteration 21067: loss = 0.0024625693323828885\n",
      "Iteration 21068: loss = 0.0024625586332746616\n",
      "Iteration 21069: loss = 0.0024625479342829082\n",
      "Iteration 21070: loss = 0.002462537235407627\n",
      "Iteration 21071: loss = 0.0024625265366488166\n",
      "Iteration 21072: loss = 0.0024625158380064743\n",
      "Iteration 21073: loss = 0.0024625051394806014\n",
      "Iteration 21074: loss = 0.002462494441071194\n",
      "Iteration 21075: loss = 0.0024624837427782522\n",
      "Iteration 21076: loss = 0.002462473044601774\n",
      "Iteration 21077: loss = 0.002462462346541758\n",
      "Iteration 21078: loss = 0.002462451648598203\n",
      "Iteration 21079: loss = 0.0024624409507711074\n",
      "Iteration 21080: loss = 0.0024624302530604704\n",
      "Iteration 21081: loss = 0.0024624195554662897\n",
      "Iteration 21082: loss = 0.002462408857988564\n",
      "Iteration 21083: loss = 0.0024623981606272925\n",
      "Iteration 21084: loss = 0.002462387463382473\n",
      "Iteration 21085: loss = 0.0024623767662541054\n",
      "Iteration 21086: loss = 0.0024623660692421868\n",
      "Iteration 21087: loss = 0.002462355372346717\n",
      "Iteration 21088: loss = 0.0024623446755676934\n",
      "Iteration 21089: loss = 0.0024623339789051164\n",
      "Iteration 21090: loss = 0.0024623232823589828\n",
      "Iteration 21091: loss = 0.002462312585929292\n",
      "Iteration 21092: loss = 0.0024623018896160424\n",
      "Iteration 21093: loss = 0.0024622911934192326\n",
      "Iteration 21094: loss = 0.0024622804973388614\n",
      "Iteration 21095: loss = 0.0024622698013749274\n",
      "Iteration 21096: loss = 0.002462259105527429\n",
      "Iteration 21097: loss = 0.0024622484097963646\n",
      "Iteration 21098: loss = 0.002462237714181733\n",
      "Iteration 21099: loss = 0.0024622270186835334\n",
      "Iteration 21100: loss = 0.002462216323301764\n",
      "Iteration 21101: loss = 0.0024622056280364235\n",
      "Iteration 21102: loss = 0.0024621949328875094\n",
      "Iteration 21103: loss = 0.0024621842378550213\n",
      "Iteration 21104: loss = 0.002462173542938958\n",
      "Iteration 21105: loss = 0.0024621628481393187\n",
      "Iteration 21106: loss = 0.0024621521534560994\n",
      "Iteration 21107: loss = 0.0024621414588893017\n",
      "Iteration 21108: loss = 0.002462130764438922\n",
      "Iteration 21109: loss = 0.0024621200701049604\n",
      "Iteration 21110: loss = 0.0024621093758874146\n",
      "Iteration 21111: loss = 0.002462098681786283\n",
      "Iteration 21112: loss = 0.0024620879878015653\n",
      "Iteration 21113: loss = 0.0024620772939332597\n",
      "Iteration 21114: loss = 0.002462066600181364\n",
      "Iteration 21115: loss = 0.0024620559065458777\n",
      "Iteration 21116: loss = 0.002462045213026799\n",
      "Iteration 21117: loss = 0.002462034519624127\n",
      "Iteration 21118: loss = 0.0024620238263378595\n",
      "Iteration 21119: loss = 0.002462013133167995\n",
      "Iteration 21120: loss = 0.0024620024401145334\n",
      "Iteration 21121: loss = 0.002461991747177472\n",
      "Iteration 21122: loss = 0.0024619810543568097\n",
      "Iteration 21123: loss = 0.0024619703616525455\n",
      "Iteration 21124: loss = 0.0024619596690646783\n",
      "Iteration 21125: loss = 0.0024619489765932054\n",
      "Iteration 21126: loss = 0.002461938284238127\n",
      "Iteration 21127: loss = 0.002461927591999441\n",
      "Iteration 21128: loss = 0.0024619168998771443\n",
      "Iteration 21129: loss = 0.0024619062078712387\n",
      "Iteration 21130: loss = 0.002461895515981721\n",
      "Iteration 21131: loss = 0.0024618848242085887\n",
      "Iteration 21132: loss = 0.002461874132551843\n",
      "Iteration 21133: loss = 0.0024618634410114803\n",
      "Iteration 21134: loss = 0.002461852749587501\n",
      "Iteration 21135: loss = 0.002461842058279902\n",
      "Iteration 21136: loss = 0.002461831367088683\n",
      "Iteration 21137: loss = 0.0024618206760138425\n",
      "Iteration 21138: loss = 0.0024618099850553785\n",
      "Iteration 21139: loss = 0.0024617992942132906\n",
      "Iteration 21140: loss = 0.002461788603487576\n",
      "Iteration 21141: loss = 0.002461777912878235\n",
      "Iteration 21142: loss = 0.002461767222385264\n",
      "Iteration 21143: loss = 0.002461756532008664\n",
      "Iteration 21144: loss = 0.0024617458417484322\n",
      "Iteration 21145: loss = 0.002461735151604567\n",
      "Iteration 21146: loss = 0.0024617244615770684\n",
      "Iteration 21147: loss = 0.0024617137716659338\n",
      "Iteration 21148: loss = 0.0024617030818711613\n",
      "Iteration 21149: loss = 0.0024616923921927515\n",
      "Iteration 21150: loss = 0.002461681702630701\n",
      "Iteration 21151: loss = 0.0024616710131850094\n",
      "Iteration 21152: loss = 0.002461660323855675\n",
      "Iteration 21153: loss = 0.0024616496346426963\n",
      "Iteration 21154: loss = 0.0024616389455460724\n",
      "Iteration 21155: loss = 0.0024616282565658016\n",
      "Iteration 21156: loss = 0.0024616175677018826\n",
      "Iteration 21157: loss = 0.0024616068789543137\n",
      "Iteration 21158: loss = 0.002461596190323093\n",
      "Iteration 21159: loss = 0.002461585501808221\n",
      "Iteration 21160: loss = 0.0024615748134096944\n",
      "Iteration 21161: loss = 0.0024615641251275132\n",
      "Iteration 21162: loss = 0.0024615534369616743\n",
      "Iteration 21163: loss = 0.0024615427489121777\n",
      "Iteration 21164: loss = 0.0024615320609790215\n",
      "Iteration 21165: loss = 0.002461521373162205\n",
      "Iteration 21166: loss = 0.0024615106854617257\n",
      "Iteration 21167: loss = 0.0024614999978775825\n",
      "Iteration 21168: loss = 0.0024614893104097746\n",
      "Iteration 21169: loss = 0.0024614786230583\n",
      "Iteration 21170: loss = 0.0024614679358231574\n",
      "Iteration 21171: loss = 0.002461457248704346\n",
      "Iteration 21172: loss = 0.0024614465617018634\n",
      "Iteration 21173: loss = 0.002461435874815709\n",
      "Iteration 21174: loss = 0.0024614251880458807\n",
      "Iteration 21175: loss = 0.002461414501392378\n",
      "Iteration 21176: loss = 0.0024614038148551985\n",
      "Iteration 21177: loss = 0.002461393128434342\n",
      "Iteration 21178: loss = 0.0024613824421298057\n",
      "Iteration 21179: loss = 0.0024613717559415892\n",
      "Iteration 21180: loss = 0.0024613610698696903\n",
      "Iteration 21181: loss = 0.002461350383914109\n",
      "Iteration 21182: loss = 0.002461339698074843\n",
      "Iteration 21183: loss = 0.0024613290123518907\n",
      "Iteration 21184: loss = 0.0024613183267452503\n",
      "Iteration 21185: loss = 0.002461307641254922\n",
      "Iteration 21186: loss = 0.002461296955880902\n",
      "Iteration 21187: loss = 0.0024612862706231914\n",
      "Iteration 21188: loss = 0.002461275585481787\n",
      "Iteration 21189: loss = 0.0024612649004566887\n",
      "Iteration 21190: loss = 0.0024612542155478946\n",
      "Iteration 21191: loss = 0.002461243530755403\n",
      "Iteration 21192: loss = 0.002461232846079213\n",
      "Iteration 21193: loss = 0.002461222161519322\n",
      "Iteration 21194: loss = 0.0024612114770757305\n",
      "Iteration 21195: loss = 0.0024612007927484357\n",
      "Iteration 21196: loss = 0.0024611901085374367\n",
      "Iteration 21197: loss = 0.0024611794244427322\n",
      "Iteration 21198: loss = 0.00246116874046432\n",
      "Iteration 21199: loss = 0.0024611580566022\n",
      "Iteration 21200: loss = 0.0024611473728563702\n",
      "Iteration 21201: loss = 0.0024611366892268284\n",
      "Iteration 21202: loss = 0.0024611260057135743\n",
      "Iteration 21203: loss = 0.002461115322316606\n",
      "Iteration 21204: loss = 0.0024611046390359227\n",
      "Iteration 21205: loss = 0.002461093955871522\n",
      "Iteration 21206: loss = 0.0024610832728234034\n",
      "Iteration 21207: loss = 0.002461072589891565\n",
      "Iteration 21208: loss = 0.002461061907076005\n",
      "Iteration 21209: loss = 0.0024610512243767235\n",
      "Iteration 21210: loss = 0.002461040541793718\n",
      "Iteration 21211: loss = 0.0024610298593269863\n",
      "Iteration 21212: loss = 0.0024610191769765285\n",
      "Iteration 21213: loss = 0.002461008494742343\n",
      "Iteration 21214: loss = 0.0024609978126244273\n",
      "Iteration 21215: loss = 0.0024609871306227812\n",
      "Iteration 21216: loss = 0.002460976448737403\n",
      "Iteration 21217: loss = 0.0024609657669682904\n",
      "Iteration 21218: loss = 0.0024609550853154434\n",
      "Iteration 21219: loss = 0.00246094440377886\n",
      "Iteration 21220: loss = 0.0024609337223585382\n",
      "Iteration 21221: loss = 0.002460923041054478\n",
      "Iteration 21222: loss = 0.002460912359866676\n",
      "Iteration 21223: loss = 0.0024609016787951327\n",
      "Iteration 21224: loss = 0.0024608909978398458\n",
      "Iteration 21225: loss = 0.0024608803170008144\n",
      "Iteration 21226: loss = 0.002460869636278036\n",
      "Iteration 21227: loss = 0.0024608589556715105\n",
      "Iteration 21228: loss = 0.0024608482751812353\n",
      "Iteration 21229: loss = 0.0024608375948072104\n",
      "Iteration 21230: loss = 0.0024608269145494337\n",
      "Iteration 21231: loss = 0.002460816234407903\n",
      "Iteration 21232: loss = 0.0024608055543826183\n",
      "Iteration 21233: loss = 0.0024607948744735774\n",
      "Iteration 21234: loss = 0.002460784194680779\n",
      "Iteration 21235: loss = 0.002460773515004222\n",
      "Iteration 21236: loss = 0.002460762835443904\n",
      "Iteration 21237: loss = 0.0024607521559998255\n",
      "Iteration 21238: loss = 0.0024607414766719834\n",
      "Iteration 21239: loss = 0.0024607307974603763\n",
      "Iteration 21240: loss = 0.0024607201183650044\n",
      "Iteration 21241: loss = 0.002460709439385865\n",
      "Iteration 21242: loss = 0.002460698760522956\n",
      "Iteration 21243: loss = 0.0024606880817762784\n",
      "Iteration 21244: loss = 0.0024606774031458284\n",
      "Iteration 21245: loss = 0.002460666724631606\n",
      "Iteration 21246: loss = 0.0024606560462336093\n",
      "Iteration 21247: loss = 0.0024606453679518366\n",
      "Iteration 21248: loss = 0.0024606346897862873\n",
      "Iteration 21249: loss = 0.0024606240117369597\n",
      "Iteration 21250: loss = 0.002460613333803852\n",
      "Iteration 21251: loss = 0.002460602655986963\n",
      "Iteration 21252: loss = 0.0024605919782862916\n",
      "Iteration 21253: loss = 0.002460581300701836\n",
      "Iteration 21254: loss = 0.0024605706232335954\n",
      "Iteration 21255: loss = 0.0024605599458815677\n",
      "Iteration 21256: loss = 0.0024605492686457517\n",
      "Iteration 21257: loss = 0.0024605385915261457\n",
      "Iteration 21258: loss = 0.0024605279145227497\n",
      "Iteration 21259: loss = 0.0024605172376355607\n",
      "Iteration 21260: loss = 0.002460506560864578\n",
      "Iteration 21261: loss = 0.0024604958842098\n",
      "Iteration 21262: loss = 0.002460485207671226\n",
      "Iteration 21263: loss = 0.0024604745312488525\n",
      "Iteration 21264: loss = 0.002460463854942681\n",
      "Iteration 21265: loss = 0.0024604531787527084\n",
      "Iteration 21266: loss = 0.0024604425026789334\n",
      "Iteration 21267: loss = 0.0024604318267213553\n",
      "Iteration 21268: loss = 0.0024604211508799712\n",
      "Iteration 21269: loss = 0.002460410475154781\n",
      "Iteration 21270: loss = 0.0024603997995457836\n",
      "Iteration 21271: loss = 0.002460389124052977\n",
      "Iteration 21272: loss = 0.00246037844867636\n",
      "Iteration 21273: loss = 0.0024603677734159305\n",
      "Iteration 21274: loss = 0.002460357098271687\n",
      "Iteration 21275: loss = 0.0024603464232436294\n",
      "Iteration 21276: loss = 0.002460335748331756\n",
      "Iteration 21277: loss = 0.002460325073536065\n",
      "Iteration 21278: loss = 0.002460314398856554\n",
      "Iteration 21279: loss = 0.002460303724293224\n",
      "Iteration 21280: loss = 0.0024602930498460714\n",
      "Iteration 21281: loss = 0.0024602823755150954\n",
      "Iteration 21282: loss = 0.0024602717013002956\n",
      "Iteration 21283: loss = 0.0024602610272016698\n",
      "Iteration 21284: loss = 0.002460250353219216\n",
      "Iteration 21285: loss = 0.0024602396793529336\n",
      "Iteration 21286: loss = 0.002460229005602821\n",
      "Iteration 21287: loss = 0.002460218331968877\n",
      "Iteration 21288: loss = 0.0024602076584511008\n",
      "Iteration 21289: loss = 0.0024601969850494895\n",
      "Iteration 21290: loss = 0.002460186311764042\n",
      "Iteration 21291: loss = 0.002460175638594758\n",
      "Iteration 21292: loss = 0.0024601649655416357\n",
      "Iteration 21293: loss = 0.0024601542926046722\n",
      "Iteration 21294: loss = 0.0024601436197838685\n",
      "Iteration 21295: loss = 0.002460132947079222\n",
      "Iteration 21296: loss = 0.0024601222744907313\n",
      "Iteration 21297: loss = 0.0024601116020183944\n",
      "Iteration 21298: loss = 0.002460100929662211\n",
      "Iteration 21299: loss = 0.0024600902574221798\n",
      "Iteration 21300: loss = 0.002460079585298298\n",
      "Iteration 21301: loss = 0.0024600689132905653\n",
      "Iteration 21302: loss = 0.002460058241398981\n",
      "Iteration 21303: loss = 0.0024600475696235414\n",
      "Iteration 21304: loss = 0.0024600368979642473\n",
      "Iteration 21305: loss = 0.0024600262264210964\n",
      "Iteration 21306: loss = 0.0024600155549940874\n",
      "Iteration 21307: loss = 0.002460004883683218\n",
      "Iteration 21308: loss = 0.0024599942124884887\n",
      "Iteration 21309: loss = 0.002459983541409897\n",
      "Iteration 21310: loss = 0.002459972870447441\n",
      "Iteration 21311: loss = 0.002459962199601121\n",
      "Iteration 21312: loss = 0.0024599515288709337\n",
      "Iteration 21313: loss = 0.002459940858256878\n",
      "Iteration 21314: loss = 0.0024599301877589537\n",
      "Iteration 21315: loss = 0.002459919517377159\n",
      "Iteration 21316: loss = 0.002459908847111492\n",
      "Iteration 21317: loss = 0.0024598981769619506\n",
      "Iteration 21318: loss = 0.002459887506928535\n",
      "Iteration 21319: loss = 0.0024598768370112435\n",
      "Iteration 21320: loss = 0.0024598661672100737\n",
      "Iteration 21321: loss = 0.0024598554975250255\n",
      "Iteration 21322: loss = 0.002459844827956096\n",
      "Iteration 21323: loss = 0.002459834158503285\n",
      "Iteration 21324: loss = 0.002459823489166591\n",
      "Iteration 21325: loss = 0.0024598128199460117\n",
      "Iteration 21326: loss = 0.002459802150841547\n",
      "Iteration 21327: loss = 0.0024597914818531943\n",
      "Iteration 21328: loss = 0.0024597808129809533\n",
      "Iteration 21329: loss = 0.0024597701442248213\n",
      "Iteration 21330: loss = 0.0024597594755847983\n",
      "Iteration 21331: loss = 0.0024597488070608815\n",
      "Iteration 21332: loss = 0.002459738138653071\n",
      "Iteration 21333: loss = 0.0024597274703613645\n",
      "Iteration 21334: loss = 0.00245971680218576\n",
      "Iteration 21335: loss = 0.0024597061341262573\n",
      "Iteration 21336: loss = 0.002459695466182855\n",
      "Iteration 21337: loss = 0.0024596847983555507\n",
      "Iteration 21338: loss = 0.0024596741306443437\n",
      "Iteration 21339: loss = 0.0024596634630492326\n",
      "Iteration 21340: loss = 0.002459652795570216\n",
      "Iteration 21341: loss = 0.0024596421282072917\n",
      "Iteration 21342: loss = 0.00245963146096046\n",
      "Iteration 21343: loss = 0.0024596207938297177\n",
      "Iteration 21344: loss = 0.0024596101268150646\n",
      "Iteration 21345: loss = 0.002459599459916499\n",
      "Iteration 21346: loss = 0.002459588793134018\n",
      "Iteration 21347: loss = 0.0024595781264676224\n",
      "Iteration 21348: loss = 0.0024595674599173106\n",
      "Iteration 21349: loss = 0.0024595567934830804\n",
      "Iteration 21350: loss = 0.0024595461271649304\n",
      "Iteration 21351: loss = 0.002459535460962859\n",
      "Iteration 21352: loss = 0.0024595247948768655\n",
      "Iteration 21353: loss = 0.002459514128906948\n",
      "Iteration 21354: loss = 0.0024595034630531063\n",
      "Iteration 21355: loss = 0.002459492797315337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21356: loss = 0.0024594821316936397\n",
      "Iteration 21357: loss = 0.002459471466188013\n",
      "Iteration 21358: loss = 0.002459460800798456\n",
      "Iteration 21359: loss = 0.0024594501355249664\n",
      "Iteration 21360: loss = 0.0024594394703675434\n",
      "Iteration 21361: loss = 0.0024594288053261856\n",
      "Iteration 21362: loss = 0.002459418140400891\n",
      "Iteration 21363: loss = 0.0024594074755916588\n",
      "Iteration 21364: loss = 0.0024593968108984872\n",
      "Iteration 21365: loss = 0.0024593861463213756\n",
      "Iteration 21366: loss = 0.0024593754818603214\n",
      "Iteration 21367: loss = 0.002459364817515324\n",
      "Iteration 21368: loss = 0.002459354153286382\n",
      "Iteration 21369: loss = 0.0024593434891734935\n",
      "Iteration 21370: loss = 0.0024593328251766577\n",
      "Iteration 21371: loss = 0.0024593221612958733\n",
      "Iteration 21372: loss = 0.0024593114975311383\n",
      "Iteration 21373: loss = 0.0024593008338824516\n",
      "Iteration 21374: loss = 0.0024592901703498113\n",
      "Iteration 21375: loss = 0.0024592795069332167\n",
      "Iteration 21376: loss = 0.0024592688436326665\n",
      "Iteration 21377: loss = 0.0024592581804481588\n",
      "Iteration 21378: loss = 0.002459247517379692\n",
      "Iteration 21379: loss = 0.002459236854427265\n",
      "Iteration 21380: loss = 0.0024592261915908774\n",
      "Iteration 21381: loss = 0.0024592155288705258\n",
      "Iteration 21382: loss = 0.0024592048662662107\n",
      "Iteration 21383: loss = 0.0024591942037779295\n",
      "Iteration 21384: loss = 0.002459183541405681\n",
      "Iteration 21385: loss = 0.0024591728791494645\n",
      "Iteration 21386: loss = 0.0024591622170092777\n",
      "Iteration 21387: loss = 0.0024591515549851196\n",
      "Iteration 21388: loss = 0.0024591408930769885\n",
      "Iteration 21389: loss = 0.002459130231284884\n",
      "Iteration 21390: loss = 0.0024591195696088038\n",
      "Iteration 21391: loss = 0.0024591089080487462\n",
      "Iteration 21392: loss = 0.002459098246604711\n",
      "Iteration 21393: loss = 0.002459087585276695\n",
      "Iteration 21394: loss = 0.0024590769240646995\n",
      "Iteration 21395: loss = 0.00245906626296872\n",
      "Iteration 21396: loss = 0.0024590556019887574\n",
      "Iteration 21397: loss = 0.0024590449411248097\n",
      "Iteration 21398: loss = 0.0024590342803768746\n",
      "Iteration 21399: loss = 0.0024590236197449527\n",
      "Iteration 21400: loss = 0.00245901295922904\n",
      "Iteration 21401: loss = 0.002459002298829137\n",
      "Iteration 21402: loss = 0.002458991638545242\n",
      "Iteration 21403: loss = 0.0024589809783773526\n",
      "Iteration 21404: loss = 0.0024589703183254684\n",
      "Iteration 21405: loss = 0.0024589596583895882\n",
      "Iteration 21406: loss = 0.00245894899856971\n",
      "Iteration 21407: loss = 0.002458938338865832\n",
      "Iteration 21408: loss = 0.002458927679277954\n",
      "Iteration 21409: loss = 0.002458917019806074\n",
      "Iteration 21410: loss = 0.00245890636045019\n",
      "Iteration 21411: loss = 0.002458895701210301\n",
      "Iteration 21412: loss = 0.0024588850420864065\n",
      "Iteration 21413: loss = 0.0024588743830785045\n",
      "Iteration 21414: loss = 0.0024588637241865934\n",
      "Iteration 21415: loss = 0.0024588530654106712\n",
      "Iteration 21416: loss = 0.0024588424067507374\n",
      "Iteration 21417: loss = 0.0024588317482067902\n",
      "Iteration 21418: loss = 0.002458821089778829\n",
      "Iteration 21419: loss = 0.0024588104314668515\n",
      "Iteration 21420: loss = 0.0024587997732708564\n",
      "Iteration 21421: loss = 0.0024587891151908433\n",
      "Iteration 21422: loss = 0.002458778457226809\n",
      "Iteration 21423: loss = 0.0024587677993787535\n",
      "Iteration 21424: loss = 0.002458757141646675\n",
      "Iteration 21425: loss = 0.002458746484030572\n",
      "Iteration 21426: loss = 0.0024587358265304436\n",
      "Iteration 21427: loss = 0.0024587251691462874\n",
      "Iteration 21428: loss = 0.002458714511878103\n",
      "Iteration 21429: loss = 0.002458703854725889\n",
      "Iteration 21430: loss = 0.0024586931976896434\n",
      "Iteration 21431: loss = 0.002458682540769365\n",
      "Iteration 21432: loss = 0.002458671883965052\n",
      "Iteration 21433: loss = 0.0024586612272767035\n",
      "Iteration 21434: loss = 0.002458650570704318\n",
      "Iteration 21435: loss = 0.002458639914247895\n",
      "Iteration 21436: loss = 0.002458629257907432\n",
      "Iteration 21437: loss = 0.002458618601682927\n",
      "Iteration 21438: loss = 0.0024586079455743803\n",
      "Iteration 21439: loss = 0.0024585972895817894\n",
      "Iteration 21440: loss = 0.002458586633705153\n",
      "Iteration 21441: loss = 0.002458575977944471\n",
      "Iteration 21442: loss = 0.0024585653222997388\n",
      "Iteration 21443: loss = 0.002458554666770958\n",
      "Iteration 21444: loss = 0.002458544011358127\n",
      "Iteration 21445: loss = 0.002458533356061243\n",
      "Iteration 21446: loss = 0.0024585227008803055\n",
      "Iteration 21447: loss = 0.002458512045815313\n",
      "Iteration 21448: loss = 0.0024585013908662635\n",
      "Iteration 21449: loss = 0.0024584907360331565\n",
      "Iteration 21450: loss = 0.0024584800813159903\n",
      "Iteration 21451: loss = 0.002458469426714763\n",
      "Iteration 21452: loss = 0.002458458772229474\n",
      "Iteration 21453: loss = 0.0024584481178601216\n",
      "Iteration 21454: loss = 0.0024584374636067038\n",
      "Iteration 21455: loss = 0.00245842680946922\n",
      "Iteration 21456: loss = 0.002458416155447668\n",
      "Iteration 21457: loss = 0.002458405501542048\n",
      "Iteration 21458: loss = 0.0024583948477523566\n",
      "Iteration 21459: loss = 0.002458384194078594\n",
      "Iteration 21460: loss = 0.0024583735405207577\n",
      "Iteration 21461: loss = 0.002458362887078847\n",
      "Iteration 21462: loss = 0.0024583522337528604\n",
      "Iteration 21463: loss = 0.002458341580542796\n",
      "Iteration 21464: loss = 0.0024583309274486525\n",
      "Iteration 21465: loss = 0.0024583202744704294\n",
      "Iteration 21466: loss = 0.0024583096216081245\n",
      "Iteration 21467: loss = 0.002458298968861736\n",
      "Iteration 21468: loss = 0.002458288316231264\n",
      "Iteration 21469: loss = 0.0024582776637167058\n",
      "Iteration 21470: loss = 0.00245826701131806\n",
      "Iteration 21471: loss = 0.0024582563590353256\n",
      "Iteration 21472: loss = 0.0024582457068685016\n",
      "Iteration 21473: loss = 0.0024582350548175863\n",
      "Iteration 21474: loss = 0.0024582244028825777\n",
      "Iteration 21475: loss = 0.002458213751063475\n",
      "Iteration 21476: loss = 0.0024582030993602765\n",
      "Iteration 21477: loss = 0.002458192447772982\n",
      "Iteration 21478: loss = 0.0024581817963015887\n",
      "Iteration 21479: loss = 0.002458171144946095\n",
      "Iteration 21480: loss = 0.0024581604937065006\n",
      "Iteration 21481: loss = 0.0024581498425828036\n",
      "Iteration 21482: loss = 0.002458139191575002\n",
      "Iteration 21483: loss = 0.0024581285406830963\n",
      "Iteration 21484: loss = 0.002458117889907082\n",
      "Iteration 21485: loss = 0.0024581072392469614\n",
      "Iteration 21486: loss = 0.0024580965887027306\n",
      "Iteration 21487: loss = 0.0024580859382743884\n",
      "Iteration 21488: loss = 0.0024580752879619343\n",
      "Iteration 21489: loss = 0.002458064637765366\n",
      "Iteration 21490: loss = 0.0024580539876846836\n",
      "Iteration 21491: loss = 0.0024580433377198838\n",
      "Iteration 21492: loss = 0.002458032687870966\n",
      "Iteration 21493: loss = 0.0024580220381379293\n",
      "Iteration 21494: loss = 0.0024580113885207715\n",
      "Iteration 21495: loss = 0.002458000739019492\n",
      "Iteration 21496: loss = 0.0024579900896340885\n",
      "Iteration 21497: loss = 0.0024579794403645607\n",
      "Iteration 21498: loss = 0.0024579687912109063\n",
      "Iteration 21499: loss = 0.002457958142173124\n",
      "Iteration 21500: loss = 0.0024579474932512127\n",
      "Iteration 21501: loss = 0.002457936844445171\n",
      "Iteration 21502: loss = 0.0024579261957549973\n",
      "Iteration 21503: loss = 0.0024579155471806903\n",
      "Iteration 21504: loss = 0.002457904898722249\n",
      "Iteration 21505: loss = 0.0024578942503796714\n",
      "Iteration 21506: loss = 0.002457883602152956\n",
      "Iteration 21507: loss = 0.002457872954042102\n",
      "Iteration 21508: loss = 0.0024578623060471077\n",
      "Iteration 21509: loss = 0.002457851658167972\n",
      "Iteration 21510: loss = 0.0024578410104046933\n",
      "Iteration 21511: loss = 0.00245783036275727\n",
      "Iteration 21512: loss = 0.0024578197152257007\n",
      "Iteration 21513: loss = 0.0024578090678099842\n",
      "Iteration 21514: loss = 0.002457798420510119\n",
      "Iteration 21515: loss = 0.0024577877733261036\n",
      "Iteration 21516: loss = 0.0024577771262579372\n",
      "Iteration 21517: loss = 0.0024577664793056174\n",
      "Iteration 21518: loss = 0.002457755832469144\n",
      "Iteration 21519: loss = 0.002457745185748515\n",
      "Iteration 21520: loss = 0.0024577345391437284\n",
      "Iteration 21521: loss = 0.002457723892654784\n",
      "Iteration 21522: loss = 0.0024577132462816793\n",
      "Iteration 21523: loss = 0.0024577026000244134\n",
      "Iteration 21524: loss = 0.0024576919538829853\n",
      "Iteration 21525: loss = 0.0024576813078573934\n",
      "Iteration 21526: loss = 0.0024576706619476355\n",
      "Iteration 21527: loss = 0.0024576600161537116\n",
      "Iteration 21528: loss = 0.0024576493704756186\n",
      "Iteration 21529: loss = 0.0024576387249133566\n",
      "Iteration 21530: loss = 0.002457628079466924\n",
      "Iteration 21531: loss = 0.002457617434136318\n",
      "Iteration 21532: loss = 0.002457606788921539\n",
      "Iteration 21533: loss = 0.0024575961438225843\n",
      "Iteration 21534: loss = 0.0024575854988394532\n",
      "Iteration 21535: loss = 0.002457574853972145\n",
      "Iteration 21536: loss = 0.0024575642092206566\n",
      "Iteration 21537: loss = 0.0024575535645849876\n",
      "Iteration 21538: loss = 0.0024575429200651366\n",
      "Iteration 21539: loss = 0.002457532275661102\n",
      "Iteration 21540: loss = 0.0024575216313728826\n",
      "Iteration 21541: loss = 0.0024575109872004767\n",
      "Iteration 21542: loss = 0.0024575003431438836\n",
      "Iteration 21543: loss = 0.002457489699203101\n",
      "Iteration 21544: loss = 0.0024574790553781273\n",
      "Iteration 21545: loss = 0.002457468411668963\n",
      "Iteration 21546: loss = 0.002457457768075605\n",
      "Iteration 21547: loss = 0.0024574471245980513\n",
      "Iteration 21548: loss = 0.0024574364812363025\n",
      "Iteration 21549: loss = 0.002457425837990356\n",
      "Iteration 21550: loss = 0.002457415194860211\n",
      "Iteration 21551: loss = 0.0024574045518458653\n",
      "Iteration 21552: loss = 0.0024573939089473184\n",
      "Iteration 21553: loss = 0.002457383266164568\n",
      "Iteration 21554: loss = 0.002457372623497613\n",
      "Iteration 21555: loss = 0.002457361980946452\n",
      "Iteration 21556: loss = 0.0024573513385110846\n",
      "Iteration 21557: loss = 0.002457340696191508\n",
      "Iteration 21558: loss = 0.0024573300539877216\n",
      "Iteration 21559: loss = 0.0024573194118997234\n",
      "Iteration 21560: loss = 0.0024573087699275124\n",
      "Iteration 21561: loss = 0.002457298128071088\n",
      "Iteration 21562: loss = 0.002457287486330447\n",
      "Iteration 21563: loss = 0.0024572768447055893\n",
      "Iteration 21564: loss = 0.0024572662031965135\n",
      "Iteration 21565: loss = 0.002457255561803218\n",
      "Iteration 21566: loss = 0.0024572449205257007\n",
      "Iteration 21567: loss = 0.002457234279363961\n",
      "Iteration 21568: loss = 0.002457223638317998\n",
      "Iteration 21569: loss = 0.002457212997387809\n",
      "Iteration 21570: loss = 0.002457202356573393\n",
      "Iteration 21571: loss = 0.0024571917158747493\n",
      "Iteration 21572: loss = 0.0024571810752918757\n",
      "Iteration 21573: loss = 0.002457170434824771\n",
      "Iteration 21574: loss = 0.0024571597944734345\n",
      "Iteration 21575: loss = 0.0024571491542378643\n",
      "Iteration 21576: loss = 0.0024571385141180587\n",
      "Iteration 21577: loss = 0.0024571278741140168\n",
      "Iteration 21578: loss = 0.002457117234225737\n",
      "Iteration 21579: loss = 0.0024571065944532173\n",
      "Iteration 21580: loss = 0.002457095954796457\n",
      "Iteration 21581: loss = 0.002457085315255455\n",
      "Iteration 21582: loss = 0.0024570746758302096\n",
      "Iteration 21583: loss = 0.0024570640365207184\n",
      "Iteration 21584: loss = 0.002457053397326982\n",
      "Iteration 21585: loss = 0.0024570427582489974\n",
      "Iteration 21586: loss = 0.0024570321192867637\n",
      "Iteration 21587: loss = 0.0024570214804402793\n",
      "Iteration 21588: loss = 0.0024570108417095436\n",
      "Iteration 21589: loss = 0.002457000203094554\n",
      "Iteration 21590: loss = 0.00245698956459531\n",
      "Iteration 21591: loss = 0.0024569789262118102\n",
      "Iteration 21592: loss = 0.0024569682879440526\n",
      "Iteration 21593: loss = 0.002456957649792036\n",
      "Iteration 21594: loss = 0.0024569470117557597\n",
      "Iteration 21595: loss = 0.0024569363738352217\n",
      "Iteration 21596: loss = 0.00245692573603042\n",
      "Iteration 21597: loss = 0.0024569150983413546\n",
      "Iteration 21598: loss = 0.002456904460768023\n",
      "Iteration 21599: loss = 0.0024568938233104245\n",
      "Iteration 21600: loss = 0.002456883185968557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21601: loss = 0.0024568725487424197\n",
      "Iteration 21602: loss = 0.0024568619116320106\n",
      "Iteration 21603: loss = 0.0024568512746373293\n",
      "Iteration 21604: loss = 0.002456840637758374\n",
      "Iteration 21605: loss = 0.0024568300009951425\n",
      "Iteration 21606: loss = 0.0024568193643476344\n",
      "Iteration 21607: loss = 0.0024568087278158475\n",
      "Iteration 21608: loss = 0.002456798091399781\n",
      "Iteration 21609: loss = 0.002456787455099434\n",
      "Iteration 21610: loss = 0.002456776818914803\n",
      "Iteration 21611: loss = 0.0024567661828458895\n",
      "Iteration 21612: loss = 0.00245675554689269\n",
      "Iteration 21613: loss = 0.0024567449110552034\n",
      "Iteration 21614: loss = 0.0024567342753334296\n",
      "Iteration 21615: loss = 0.0024567236397273653\n",
      "Iteration 21616: loss = 0.0024567130042370106\n",
      "Iteration 21617: loss = 0.0024567023688623632\n",
      "Iteration 21618: loss = 0.002456691733603423\n",
      "Iteration 21619: loss = 0.0024566810984601867\n",
      "Iteration 21620: loss = 0.0024566704634326544\n",
      "Iteration 21621: loss = 0.0024566598285208244\n",
      "Iteration 21622: loss = 0.0024566491937246947\n",
      "Iteration 21623: loss = 0.0024566385590442646\n",
      "Iteration 21624: loss = 0.002456627924479532\n",
      "Iteration 21625: loss = 0.002456617290030496\n",
      "Iteration 21626: loss = 0.0024566066556971555\n",
      "Iteration 21627: loss = 0.0024565960214795084\n",
      "Iteration 21628: loss = 0.0024565853873775544\n",
      "Iteration 21629: loss = 0.00245657475339129\n",
      "Iteration 21630: loss = 0.002456564119520716\n",
      "Iteration 21631: loss = 0.00245655348576583\n",
      "Iteration 21632: loss = 0.002456542852126631\n",
      "Iteration 21633: loss = 0.0024565322186031174\n",
      "Iteration 21634: loss = 0.0024565215851952873\n",
      "Iteration 21635: loss = 0.0024565109519031404\n",
      "Iteration 21636: loss = 0.0024565003187266743\n",
      "Iteration 21637: loss = 0.0024564896856658883\n",
      "Iteration 21638: loss = 0.0024564790527207797\n",
      "Iteration 21639: loss = 0.0024564684198913494\n",
      "Iteration 21640: loss = 0.002456457787177594\n",
      "Iteration 21641: loss = 0.002456447154579513\n",
      "Iteration 21642: loss = 0.0024564365220971045\n",
      "Iteration 21643: loss = 0.0024564258897303683\n",
      "Iteration 21644: loss = 0.0024564152574793013\n",
      "Iteration 21645: loss = 0.002456404625343903\n",
      "Iteration 21646: loss = 0.0024563939933241728\n",
      "Iteration 21647: loss = 0.002456383361420107\n",
      "Iteration 21648: loss = 0.0024563727296317067\n",
      "Iteration 21649: loss = 0.0024563620979589696\n",
      "Iteration 21650: loss = 0.002456351466401894\n",
      "Iteration 21651: loss = 0.0024563408349604784\n",
      "Iteration 21652: loss = 0.002456330203634721\n",
      "Iteration 21653: loss = 0.0024563195724246227\n",
      "Iteration 21654: loss = 0.0024563089413301795\n",
      "Iteration 21655: loss = 0.0024562983103513913\n",
      "Iteration 21656: loss = 0.0024562876794882566\n",
      "Iteration 21657: loss = 0.002456277048740773\n",
      "Iteration 21658: loss = 0.00245626641810894\n",
      "Iteration 21659: loss = 0.0024562557875927576\n",
      "Iteration 21660: loss = 0.0024562451571922216\n",
      "Iteration 21661: loss = 0.0024562345269073323\n",
      "Iteration 21662: loss = 0.0024562238967380874\n",
      "Iteration 21663: loss = 0.002456213266684487\n",
      "Iteration 21664: loss = 0.002456202636746528\n",
      "Iteration 21665: loss = 0.00245619200692421\n",
      "Iteration 21666: loss = 0.0024561813772175315\n",
      "Iteration 21667: loss = 0.002456170747626491\n",
      "Iteration 21668: loss = 0.0024561601181510868\n",
      "Iteration 21669: loss = 0.002456149488791318\n",
      "Iteration 21670: loss = 0.002456138859547183\n",
      "Iteration 21671: loss = 0.0024561282304186804\n",
      "Iteration 21672: loss = 0.002456117601405809\n",
      "Iteration 21673: loss = 0.0024561069725085667\n",
      "Iteration 21674: loss = 0.002456096343726953\n",
      "Iteration 21675: loss = 0.002456085715060966\n",
      "Iteration 21676: loss = 0.002456075086510605\n",
      "Iteration 21677: loss = 0.002456064458075867\n",
      "Iteration 21678: loss = 0.002456053829756752\n",
      "Iteration 21679: loss = 0.0024560432015532587\n",
      "Iteration 21680: loss = 0.002456032573465385\n",
      "Iteration 21681: loss = 0.0024560219454931296\n",
      "Iteration 21682: loss = 0.002456011317636492\n",
      "Iteration 21683: loss = 0.0024560006898954687\n",
      "Iteration 21684: loss = 0.002455990062270061\n",
      "Iteration 21685: loss = 0.002455979434760265\n",
      "Iteration 21686: loss = 0.0024559688073660818\n",
      "Iteration 21687: loss = 0.0024559581800875077\n",
      "Iteration 21688: loss = 0.0024559475529245425\n",
      "Iteration 21689: loss = 0.002455936925877185\n",
      "Iteration 21690: loss = 0.002455926298945433\n",
      "Iteration 21691: loss = 0.0024559156721292857\n",
      "Iteration 21692: loss = 0.0024559050454287415\n",
      "Iteration 21693: loss = 0.002455894418843799\n",
      "Iteration 21694: loss = 0.0024558837923744567\n",
      "Iteration 21695: loss = 0.002455873166020714\n",
      "Iteration 21696: loss = 0.0024558625397825685\n",
      "Iteration 21697: loss = 0.0024558519136600185\n",
      "Iteration 21698: loss = 0.0024558412876530644\n",
      "Iteration 21699: loss = 0.002455830661761703\n",
      "Iteration 21700: loss = 0.0024558200359859335\n",
      "Iteration 21701: loss = 0.0024558094103257553\n",
      "Iteration 21702: loss = 0.0024557987847811658\n",
      "Iteration 21703: loss = 0.002455788159352164\n",
      "Iteration 21704: loss = 0.0024557775340387487\n",
      "Iteration 21705: loss = 0.0024557669088409186\n",
      "Iteration 21706: loss = 0.002455756283758672\n",
      "Iteration 21707: loss = 0.0024557456587920076\n",
      "Iteration 21708: loss = 0.0024557350339409244\n",
      "Iteration 21709: loss = 0.00245572440920542\n",
      "Iteration 21710: loss = 0.002455713784585494\n",
      "Iteration 21711: loss = 0.002455703160081144\n",
      "Iteration 21712: loss = 0.0024556925356923704\n",
      "Iteration 21713: loss = 0.00245568191141917\n",
      "Iteration 21714: loss = 0.0024556712872615422\n",
      "Iteration 21715: loss = 0.0024556606632194855\n",
      "Iteration 21716: loss = 0.0024556500392929987\n",
      "Iteration 21717: loss = 0.0024556394154820796\n",
      "Iteration 21718: loss = 0.0024556287917867283\n",
      "Iteration 21719: loss = 0.002455618168206942\n",
      "Iteration 21720: loss = 0.0024556075447427195\n",
      "Iteration 21721: loss = 0.00245559692139406\n",
      "Iteration 21722: loss = 0.002455586298160962\n",
      "Iteration 21723: loss = 0.0024555756750434237\n",
      "Iteration 21724: loss = 0.0024555650520414443\n",
      "Iteration 21725: loss = 0.0024555544291550214\n",
      "Iteration 21726: loss = 0.002455543806384155\n",
      "Iteration 21727: loss = 0.0024555331837288422\n",
      "Iteration 21728: loss = 0.0024555225611890834\n",
      "Iteration 21729: loss = 0.002455511938764876\n",
      "Iteration 21730: loss = 0.0024555013164562177\n",
      "Iteration 21731: loss = 0.002455490694263109\n",
      "Iteration 21732: loss = 0.0024554800721855474\n",
      "Iteration 21733: loss = 0.0024554694502235317\n",
      "Iteration 21734: loss = 0.0024554588283770607\n",
      "Iteration 21735: loss = 0.002455448206646134\n",
      "Iteration 21736: loss = 0.0024554375850307482\n",
      "Iteration 21737: loss = 0.0024554269635309023\n",
      "Iteration 21738: loss = 0.0024554163421465964\n",
      "Iteration 21739: loss = 0.0024554057208778283\n",
      "Iteration 21740: loss = 0.0024553950997245953\n",
      "Iteration 21741: loss = 0.002455384478686898\n",
      "Iteration 21742: loss = 0.002455373857764734\n",
      "Iteration 21743: loss = 0.0024553632369581023\n",
      "Iteration 21744: loss = 0.0024553526162670006\n",
      "Iteration 21745: loss = 0.002455341995691429\n",
      "Iteration 21746: loss = 0.0024553313752313844\n",
      "Iteration 21747: loss = 0.002455320754886867\n",
      "Iteration 21748: loss = 0.0024553101346578747\n",
      "Iteration 21749: loss = 0.0024552995145444055\n",
      "Iteration 21750: loss = 0.00245528889454646\n",
      "Iteration 21751: loss = 0.0024552782746640337\n",
      "Iteration 21752: loss = 0.002455267654897128\n",
      "Iteration 21753: loss = 0.00245525703524574\n",
      "Iteration 21754: loss = 0.0024552464157098688\n",
      "Iteration 21755: loss = 0.002455235796289513\n",
      "Iteration 21756: loss = 0.0024552251769846714\n",
      "Iteration 21757: loss = 0.0024552145577953427\n",
      "Iteration 21758: loss = 0.0024552039387215245\n",
      "Iteration 21759: loss = 0.002455193319763217\n",
      "Iteration 21760: loss = 0.0024551827009204165\n",
      "Iteration 21761: loss = 0.0024551720821931237\n",
      "Iteration 21762: loss = 0.0024551614635813365\n",
      "Iteration 21763: loss = 0.0024551508450850537\n",
      "Iteration 21764: loss = 0.002455140226704273\n",
      "Iteration 21765: loss = 0.0024551296084389944\n",
      "Iteration 21766: loss = 0.002455118990289216\n",
      "Iteration 21767: loss = 0.002455108372254936\n",
      "Iteration 21768: loss = 0.0024550977543361533\n",
      "Iteration 21769: loss = 0.0024550871365328665\n",
      "Iteration 21770: loss = 0.0024550765188450737\n",
      "Iteration 21771: loss = 0.002455065901272775\n",
      "Iteration 21772: loss = 0.002455055283815967\n",
      "Iteration 21773: loss = 0.00245504466647465\n",
      "Iteration 21774: loss = 0.0024550340492488216\n",
      "Iteration 21775: loss = 0.00245502343213848\n",
      "Iteration 21776: loss = 0.0024550128151436257\n",
      "Iteration 21777: loss = 0.0024550021982642552\n",
      "Iteration 21778: loss = 0.0024549915815003684\n",
      "Iteration 21779: loss = 0.002454980964851964\n",
      "Iteration 21780: loss = 0.0024549703483190393\n",
      "Iteration 21781: loss = 0.0024549597319015945\n",
      "Iteration 21782: loss = 0.0024549491155996268\n",
      "Iteration 21783: loss = 0.0024549384994131352\n",
      "Iteration 21784: loss = 0.00245492788334212\n",
      "Iteration 21785: loss = 0.002454917267386577\n",
      "Iteration 21786: loss = 0.0024549066515465066\n",
      "Iteration 21787: loss = 0.0024548960358219073\n",
      "Iteration 21788: loss = 0.0024548854202127773\n",
      "Iteration 21789: loss = 0.002454874804719115\n",
      "Iteration 21790: loss = 0.0024548641893409194\n",
      "Iteration 21791: loss = 0.00245485357407819\n",
      "Iteration 21792: loss = 0.002454842958930923\n",
      "Iteration 21793: loss = 0.002454832343899119\n",
      "Iteration 21794: loss = 0.002454821728982776\n",
      "Iteration 21795: loss = 0.002454811114181893\n",
      "Iteration 21796: loss = 0.0024548004994964677\n",
      "Iteration 21797: loss = 0.0024547898849264995\n",
      "Iteration 21798: loss = 0.002454779270471987\n",
      "Iteration 21799: loss = 0.0024547686561329283\n",
      "Iteration 21800: loss = 0.0024547580419093224\n",
      "Iteration 21801: loss = 0.002454747427801168\n",
      "Iteration 21802: loss = 0.0024547368138084632\n",
      "Iteration 21803: loss = 0.002454726199931207\n",
      "Iteration 21804: loss = 0.0024547155861693983\n",
      "Iteration 21805: loss = 0.002454704972523035\n",
      "Iteration 21806: loss = 0.002454694358992116\n",
      "Iteration 21807: loss = 0.00245468374557664\n",
      "Iteration 21808: loss = 0.002454673132276605\n",
      "Iteration 21809: loss = 0.0024546625190920114\n",
      "Iteration 21810: loss = 0.0024546519060228552\n",
      "Iteration 21811: loss = 0.002454641293069138\n",
      "Iteration 21812: loss = 0.0024546306802308558\n",
      "Iteration 21813: loss = 0.002454620067508008\n",
      "Iteration 21814: loss = 0.0024546094549005937\n",
      "Iteration 21815: loss = 0.002454598842408611\n",
      "Iteration 21816: loss = 0.0024545882300320587\n",
      "Iteration 21817: loss = 0.002454577617770936\n",
      "Iteration 21818: loss = 0.0024545670056252402\n",
      "Iteration 21819: loss = 0.002454556393594971\n",
      "Iteration 21820: loss = 0.0024545457816801267\n",
      "Iteration 21821: loss = 0.0024545351698807057\n",
      "Iteration 21822: loss = 0.002454524558196707\n",
      "Iteration 21823: loss = 0.0024545139466281285\n",
      "Iteration 21824: loss = 0.00245450333517497\n",
      "Iteration 21825: loss = 0.0024544927238372284\n",
      "Iteration 21826: loss = 0.002454482112614904\n",
      "Iteration 21827: loss = 0.0024544715015079955\n",
      "Iteration 21828: loss = 0.002454460890516499\n",
      "Iteration 21829: loss = 0.0024544502796404154\n",
      "Iteration 21830: loss = 0.0024544396688797434\n",
      "Iteration 21831: loss = 0.0024544290582344808\n",
      "Iteration 21832: loss = 0.002454418447704626\n",
      "Iteration 21833: loss = 0.0024544078372901778\n",
      "Iteration 21834: loss = 0.002454397226991135\n",
      "Iteration 21835: loss = 0.0024543866168074964\n",
      "Iteration 21836: loss = 0.0024543760067392605\n",
      "Iteration 21837: loss = 0.002454365396786425\n",
      "Iteration 21838: loss = 0.00245435478694899\n",
      "Iteration 21839: loss = 0.0024543441772269534\n",
      "Iteration 21840: loss = 0.0024543335676203135\n",
      "Iteration 21841: loss = 0.0024543229581290692\n",
      "Iteration 21842: loss = 0.0024543123487532196\n",
      "Iteration 21843: loss = 0.0024543017394927625\n",
      "Iteration 21844: loss = 0.002454291130347697\n",
      "Iteration 21845: loss = 0.0024542805213180214\n",
      "Iteration 21846: loss = 0.0024542699124037344\n",
      "Iteration 21847: loss = 0.002454259303604835\n",
      "Iteration 21848: loss = 0.002454248694921321\n",
      "Iteration 21849: loss = 0.0024542380863531916\n",
      "Iteration 21850: loss = 0.0024542274779004456\n",
      "Iteration 21851: loss = 0.0024542168695630808\n",
      "Iteration 21852: loss = 0.0024542062613410968\n",
      "Iteration 21853: loss = 0.0024541956532344914\n",
      "Iteration 21854: loss = 0.0024541850452432642\n",
      "Iteration 21855: loss = 0.002454174437367412\n",
      "Iteration 21856: loss = 0.0024541638296069353\n",
      "Iteration 21857: loss = 0.0024541532219618314\n",
      "Iteration 21858: loss = 0.0024541426144321006\n",
      "Iteration 21859: loss = 0.002454132007017739\n",
      "Iteration 21860: loss = 0.0024541213997187473\n",
      "Iteration 21861: loss = 0.0024541107925351233\n",
      "Iteration 21862: loss = 0.0024541001854668652\n",
      "Iteration 21863: loss = 0.002454089578513973\n",
      "Iteration 21864: loss = 0.002454078971676444\n",
      "Iteration 21865: loss = 0.002454068364954277\n",
      "Iteration 21866: loss = 0.0024540577583474707\n",
      "Iteration 21867: loss = 0.0024540471518560247\n",
      "Iteration 21868: loss = 0.002454036545479936\n",
      "Iteration 21869: loss = 0.002454025939219204\n",
      "Iteration 21870: loss = 0.0024540153330738276\n",
      "Iteration 21871: loss = 0.0024540047270438046\n",
      "Iteration 21872: loss = 0.002453994121129134\n",
      "Iteration 21873: loss = 0.0024539835153298147\n",
      "Iteration 21874: loss = 0.002453972909645845\n",
      "Iteration 21875: loss = 0.0024539623040772235\n",
      "Iteration 21876: loss = 0.0024539516986239497\n",
      "Iteration 21877: loss = 0.0024539410932860207\n",
      "Iteration 21878: loss = 0.002453930488063436\n",
      "Iteration 21879: loss = 0.0024539198829561944\n",
      "Iteration 21880: loss = 0.0024539092779642933\n",
      "Iteration 21881: loss = 0.002453898673087733\n",
      "Iteration 21882: loss = 0.0024538880683265107\n",
      "Iteration 21883: loss = 0.002453877463680626\n",
      "Iteration 21884: loss = 0.0024538668591500765\n",
      "Iteration 21885: loss = 0.0024538562547348616\n",
      "Iteration 21886: loss = 0.00245384565043498\n",
      "Iteration 21887: loss = 0.0024538350462504294\n",
      "Iteration 21888: loss = 0.00245382444218121\n",
      "Iteration 21889: loss = 0.0024538138382273187\n",
      "Iteration 21890: loss = 0.002453803234388755\n",
      "Iteration 21891: loss = 0.002453792630665517\n",
      "Iteration 21892: loss = 0.002453782027057604\n",
      "Iteration 21893: loss = 0.002453771423565014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21894: loss = 0.0024537608201877464\n",
      "Iteration 21895: loss = 0.0024537502169257987\n",
      "Iteration 21896: loss = 0.00245373961377917\n",
      "Iteration 21897: loss = 0.0024537290107478596\n",
      "Iteration 21898: loss = 0.002453718407831865\n",
      "Iteration 21899: loss = 0.0024537078050311857\n",
      "Iteration 21900: loss = 0.0024536972023458197\n",
      "Iteration 21901: loss = 0.002453686599775766\n",
      "Iteration 21902: loss = 0.0024536759973210224\n",
      "Iteration 21903: loss = 0.002453665394981589\n",
      "Iteration 21904: loss = 0.002453654792757463\n",
      "Iteration 21905: loss = 0.0024536441906486436\n",
      "Iteration 21906: loss = 0.0024536335886551296\n",
      "Iteration 21907: loss = 0.002453622986776919\n",
      "Iteration 21908: loss = 0.0024536123850140107\n",
      "Iteration 21909: loss = 0.0024536017833664044\n",
      "Iteration 21910: loss = 0.0024535911818340966\n",
      "Iteration 21911: loss = 0.002453580580417087\n",
      "Iteration 21912: loss = 0.002453569979115375\n",
      "Iteration 21913: loss = 0.002453559377928958\n",
      "Iteration 21914: loss = 0.0024535487768578353\n",
      "Iteration 21915: loss = 0.002453538175902005\n",
      "Iteration 21916: loss = 0.0024535275750614664\n",
      "Iteration 21917: loss = 0.002453516974336217\n",
      "Iteration 21918: loss = 0.0024535063737262564\n",
      "Iteration 21919: loss = 0.0024534957732315825\n",
      "Iteration 21920: loss = 0.0024534851728521944\n",
      "Iteration 21921: loss = 0.0024534745725880917\n",
      "Iteration 21922: loss = 0.0024534639724392705\n",
      "Iteration 21923: loss = 0.0024534533724057312\n",
      "Iteration 21924: loss = 0.002453442772487473\n",
      "Iteration 21925: loss = 0.0024534321726844923\n",
      "Iteration 21926: loss = 0.002453421572996789\n",
      "Iteration 21927: loss = 0.0024534109734243624\n",
      "Iteration 21928: loss = 0.0024534003739672097\n",
      "Iteration 21929: loss = 0.0024533897746253307\n",
      "Iteration 21930: loss = 0.002453379175398723\n",
      "Iteration 21931: loss = 0.0024533685762873863\n",
      "Iteration 21932: loss = 0.002453357977291318\n",
      "Iteration 21933: loss = 0.002453347378410517\n",
      "Iteration 21934: loss = 0.0024533367796449827\n",
      "Iteration 21935: loss = 0.0024533261809947136\n",
      "Iteration 21936: loss = 0.0024533155824597083\n",
      "Iteration 21937: loss = 0.002453304984039964\n",
      "Iteration 21938: loss = 0.0024532943857354804\n",
      "Iteration 21939: loss = 0.0024532837875462566\n",
      "Iteration 21940: loss = 0.0024532731894722905\n",
      "Iteration 21941: loss = 0.002453262591513581\n",
      "Iteration 21942: loss = 0.0024532519936701263\n",
      "Iteration 21943: loss = 0.002453241395941925\n",
      "Iteration 21944: loss = 0.002453230798328977\n",
      "Iteration 21945: loss = 0.0024532202008312794\n",
      "Iteration 21946: loss = 0.0024532096034488304\n",
      "Iteration 21947: loss = 0.0024531990061816313\n",
      "Iteration 21948: loss = 0.0024531884090296776\n",
      "Iteration 21949: loss = 0.0024531778119929702\n",
      "Iteration 21950: loss = 0.002453167215071506\n",
      "Iteration 21951: loss = 0.0024531566182652846\n",
      "Iteration 21952: loss = 0.0024531460215743046\n",
      "Iteration 21953: loss = 0.002453135424998564\n",
      "Iteration 21954: loss = 0.0024531248285380616\n",
      "Iteration 21955: loss = 0.0024531142321927973\n",
      "Iteration 21956: loss = 0.002453103635962768\n",
      "Iteration 21957: loss = 0.002453093039847973\n",
      "Iteration 21958: loss = 0.002453082443848411\n",
      "Iteration 21959: loss = 0.0024530718479640794\n",
      "Iteration 21960: loss = 0.002453061252194979\n",
      "Iteration 21961: loss = 0.002453050656541107\n",
      "Iteration 21962: loss = 0.002453040061002462\n",
      "Iteration 21963: loss = 0.002453029465579043\n",
      "Iteration 21964: loss = 0.002453018870270849\n",
      "Iteration 21965: loss = 0.002453008275077878\n",
      "Iteration 21966: loss = 0.002452997680000128\n",
      "Iteration 21967: loss = 0.002452987085037599\n",
      "Iteration 21968: loss = 0.002452976490190289\n",
      "Iteration 21969: loss = 0.002452965895458196\n",
      "Iteration 21970: loss = 0.002452955300841319\n",
      "Iteration 21971: loss = 0.002452944706339657\n",
      "Iteration 21972: loss = 0.002452934111953209\n",
      "Iteration 21973: loss = 0.002452923517681972\n",
      "Iteration 21974: loss = 0.002452912923525946\n",
      "Iteration 21975: loss = 0.0024529023294851296\n",
      "Iteration 21976: loss = 0.0024528917355595204\n",
      "Iteration 21977: loss = 0.0024528811417491186\n",
      "Iteration 21978: loss = 0.002452870548053921\n",
      "Iteration 21979: loss = 0.002452859954473927\n",
      "Iteration 21980: loss = 0.0024528493610091358\n",
      "Iteration 21981: loss = 0.002452838767659544\n",
      "Iteration 21982: loss = 0.0024528281744251534\n",
      "Iteration 21983: loss = 0.0024528175813059603\n",
      "Iteration 21984: loss = 0.0024528069883019642\n",
      "Iteration 21985: loss = 0.0024527963954131624\n",
      "Iteration 21986: loss = 0.002452785802639556\n",
      "Iteration 21987: loss = 0.002452775209981141\n",
      "Iteration 21988: loss = 0.002452764617437917\n",
      "Iteration 21989: loss = 0.002452754025009883\n",
      "Iteration 21990: loss = 0.0024527434326970378\n",
      "Iteration 21991: loss = 0.0024527328404993793\n",
      "Iteration 21992: loss = 0.002452722248416906\n",
      "Iteration 21993: loss = 0.0024527116564496175\n",
      "Iteration 21994: loss = 0.0024527010645975116\n",
      "Iteration 21995: loss = 0.0024526904728605866\n",
      "Iteration 21996: loss = 0.002452679881238842\n",
      "Iteration 21997: loss = 0.002452669289732276\n",
      "Iteration 21998: loss = 0.002452658698340887\n",
      "Iteration 21999: loss = 0.0024526481070646744\n",
      "Iteration 22000: loss = 0.0024526375159036355\n",
      "Iteration 22001: loss = 0.0024526269248577705\n",
      "Iteration 22002: loss = 0.002452616333927077\n",
      "Iteration 22003: loss = 0.002452605743111553\n",
      "Iteration 22004: loss = 0.002452595152411199\n",
      "Iteration 22005: loss = 0.0024525845618260113\n",
      "Iteration 22006: loss = 0.00245257397135599\n",
      "Iteration 22007: loss = 0.0024525633810011335\n",
      "Iteration 22008: loss = 0.002452552790761441\n",
      "Iteration 22009: loss = 0.00245254220063691\n",
      "Iteration 22010: loss = 0.0024525316106275394\n",
      "Iteration 22011: loss = 0.0024525210207333283\n",
      "Iteration 22012: loss = 0.0024525104309542746\n",
      "Iteration 22013: loss = 0.002452499841290377\n",
      "Iteration 22014: loss = 0.0024524892517416353\n",
      "Iteration 22015: loss = 0.002452478662308046\n",
      "Iteration 22016: loss = 0.0024524680729896098\n",
      "Iteration 22017: loss = 0.0024524574837863243\n",
      "Iteration 22018: loss = 0.0024524468946981876\n",
      "Iteration 22019: loss = 0.0024524363057251996\n",
      "Iteration 22020: loss = 0.0024524257168673583\n",
      "Iteration 22021: loss = 0.0024524151281246622\n",
      "Iteration 22022: loss = 0.0024524045394971094\n",
      "Iteration 22023: loss = 0.0024523939509846996\n",
      "Iteration 22024: loss = 0.002452383362587431\n",
      "Iteration 22025: loss = 0.0024523727743053017\n",
      "Iteration 22026: loss = 0.002452362186138311\n",
      "Iteration 22027: loss = 0.0024523515980864573\n",
      "Iteration 22028: loss = 0.0024523410101497385\n",
      "Iteration 22029: loss = 0.0024523304223281546\n",
      "Iteration 22030: loss = 0.0024523198346217026\n",
      "Iteration 22031: loss = 0.002452309247030383\n",
      "Iteration 22032: loss = 0.0024522986595541924\n",
      "Iteration 22033: loss = 0.0024522880721931308\n",
      "Iteration 22034: loss = 0.0024522774849471967\n",
      "Iteration 22035: loss = 0.002452266897816388\n",
      "Iteration 22036: loss = 0.0024522563108007036\n",
      "Iteration 22037: loss = 0.0024522457239001426\n",
      "Iteration 22038: loss = 0.002452235137114703\n",
      "Iteration 22039: loss = 0.0024522245504443835\n",
      "Iteration 22040: loss = 0.002452213963889183\n",
      "Iteration 22041: loss = 0.0024522033774491\n",
      "Iteration 22042: loss = 0.0024521927911241334\n",
      "Iteration 22043: loss = 0.0024521822049142807\n",
      "Iteration 22044: loss = 0.0024521716188195416\n",
      "Iteration 22045: loss = 0.0024521610328399144\n",
      "Iteration 22046: loss = 0.002452150446975398\n",
      "Iteration 22047: loss = 0.002452139861225991\n",
      "Iteration 22048: loss = 0.002452129275591691\n",
      "Iteration 22049: loss = 0.0024521186900724975\n",
      "Iteration 22050: loss = 0.002452108104668409\n",
      "Iteration 22051: loss = 0.0024520975193794243\n",
      "Iteration 22052: loss = 0.0024520869342055415\n",
      "Iteration 22053: loss = 0.0024520763491467598\n",
      "Iteration 22054: loss = 0.002452065764203077\n",
      "Iteration 22055: loss = 0.002452055179374493\n",
      "Iteration 22056: loss = 0.0024520445946610045\n",
      "Iteration 22057: loss = 0.0024520340100626127\n",
      "Iteration 22058: loss = 0.0024520234255793133\n",
      "Iteration 22059: loss = 0.002452012841211107\n",
      "Iteration 22060: loss = 0.0024520022569579918\n",
      "Iteration 22061: loss = 0.002451991672819966\n",
      "Iteration 22062: loss = 0.0024519810887970285\n",
      "Iteration 22063: loss = 0.002451970504889178\n",
      "Iteration 22064: loss = 0.0024519599210964128\n",
      "Iteration 22065: loss = 0.0024519493374187324\n",
      "Iteration 22066: loss = 0.002451938753856134\n",
      "Iteration 22067: loss = 0.0024519281704086173\n",
      "Iteration 22068: loss = 0.0024519175870761807\n",
      "Iteration 22069: loss = 0.0024519070038588222\n",
      "Iteration 22070: loss = 0.002451896420756541\n",
      "Iteration 22071: loss = 0.0024518858377693352\n",
      "Iteration 22072: loss = 0.002451875254897204\n",
      "Iteration 22073: loss = 0.0024518646721401467\n",
      "Iteration 22074: loss = 0.00245185408949816\n",
      "Iteration 22075: loss = 0.0024518435069712436\n",
      "Iteration 22076: loss = 0.0024518329245593966\n",
      "Iteration 22077: loss = 0.0024518223422626165\n",
      "Iteration 22078: loss = 0.0024518117600809027\n",
      "Iteration 22079: loss = 0.0024518011780142535\n",
      "Iteration 22080: loss = 0.0024517905960626677\n",
      "Iteration 22081: loss = 0.0024517800142261435\n",
      "Iteration 22082: loss = 0.00245176943250468\n",
      "Iteration 22083: loss = 0.0024517588508982755\n",
      "Iteration 22084: loss = 0.0024517482694069287\n",
      "Iteration 22085: loss = 0.002451737688030638\n",
      "Iteration 22086: loss = 0.0024517271067694027\n",
      "Iteration 22087: loss = 0.0024517165256232208\n",
      "Iteration 22088: loss = 0.0024517059445920914\n",
      "Iteration 22089: loss = 0.002451695363676012\n",
      "Iteration 22090: loss = 0.0024516847828749823\n",
      "Iteration 22091: loss = 0.0024516742021890004\n",
      "Iteration 22092: loss = 0.002451663621618066\n",
      "Iteration 22093: loss = 0.0024516530411621764\n",
      "Iteration 22094: loss = 0.00245164246082133\n",
      "Iteration 22095: loss = 0.0024516318805955263\n",
      "Iteration 22096: loss = 0.002451621300484764\n",
      "Iteration 22097: loss = 0.002451610720489041\n",
      "Iteration 22098: loss = 0.002451600140608356\n",
      "Iteration 22099: loss = 0.0024515895608427088\n",
      "Iteration 22100: loss = 0.0024515789811920966\n",
      "Iteration 22101: loss = 0.0024515684016565183\n",
      "Iteration 22102: loss = 0.002451557822235973\n",
      "Iteration 22103: loss = 0.002451547242930458\n",
      "Iteration 22104: loss = 0.0024515366637399743\n",
      "Iteration 22105: loss = 0.0024515260846645183\n",
      "Iteration 22106: loss = 0.00245151550570409\n",
      "Iteration 22107: loss = 0.002451504926858688\n",
      "Iteration 22108: loss = 0.002451494348128309\n",
      "Iteration 22109: loss = 0.002451483769512954\n",
      "Iteration 22110: loss = 0.00245147319101262\n",
      "Iteration 22111: loss = 0.0024514626126273063\n",
      "Iteration 22112: loss = 0.002451452034357012\n",
      "Iteration 22113: loss = 0.0024514414562017343\n",
      "Iteration 22114: loss = 0.002451430878161473\n",
      "Iteration 22115: loss = 0.0024514203002362262\n",
      "Iteration 22116: loss = 0.002451409722425993\n",
      "Iteration 22117: loss = 0.002451399144730772\n",
      "Iteration 22118: loss = 0.002451388567150561\n",
      "Iteration 22119: loss = 0.002451377989685359\n",
      "Iteration 22120: loss = 0.0024513674123351646\n",
      "Iteration 22121: loss = 0.002451356835099977\n",
      "Iteration 22122: loss = 0.0024513462579797943\n",
      "Iteration 22123: loss = 0.0024513356809746146\n",
      "Iteration 22124: loss = 0.0024513251040844375\n",
      "Iteration 22125: loss = 0.0024513145273092614\n",
      "Iteration 22126: loss = 0.0024513039506490835\n",
      "Iteration 22127: loss = 0.0024512933741039048\n",
      "Iteration 22128: loss = 0.002451282797673722\n",
      "Iteration 22129: loss = 0.002451272221358535\n",
      "Iteration 22130: loss = 0.0024512616451583414\n",
      "Iteration 22131: loss = 0.00245125106907314\n",
      "Iteration 22132: loss = 0.0024512404931029304\n",
      "Iteration 22133: loss = 0.00245122991724771\n",
      "Iteration 22134: loss = 0.0024512193415074783\n",
      "Iteration 22135: loss = 0.0024512087658822334\n",
      "Iteration 22136: loss = 0.0024511981903719733\n",
      "Iteration 22137: loss = 0.0024511876149766977\n",
      "Iteration 22138: loss = 0.0024511770396964047\n",
      "Iteration 22139: loss = 0.0024511664645310936\n",
      "Iteration 22140: loss = 0.002451155889480762\n",
      "Iteration 22141: loss = 0.0024511453145454084\n",
      "Iteration 22142: loss = 0.002451134739725033\n",
      "Iteration 22143: loss = 0.0024511241650196328\n",
      "Iteration 22144: loss = 0.002451113590429207\n",
      "Iteration 22145: loss = 0.002451103015953754\n",
      "Iteration 22146: loss = 0.0024510924415932734\n",
      "Iteration 22147: loss = 0.002451081867347762\n",
      "Iteration 22148: loss = 0.0024510712932172196\n",
      "Iteration 22149: loss = 0.0024510607192016454\n",
      "Iteration 22150: loss = 0.002451050145301037\n",
      "Iteration 22151: loss = 0.002451039571515393\n",
      "Iteration 22152: loss = 0.002451028997844712\n",
      "Iteration 22153: loss = 0.0024510184242889934\n",
      "Iteration 22154: loss = 0.0024510078508482355\n",
      "Iteration 22155: loss = 0.002450997277522436\n",
      "Iteration 22156: loss = 0.0024509867043115943\n",
      "Iteration 22157: loss = 0.0024509761312157094\n",
      "Iteration 22158: loss = 0.0024509655582347794\n",
      "Iteration 22159: loss = 0.0024509549853688026\n",
      "Iteration 22160: loss = 0.002450944412617778\n",
      "Iteration 22161: loss = 0.002450933839981704\n",
      "Iteration 22162: loss = 0.00245092326746058\n",
      "Iteration 22163: loss = 0.0024509126950544038\n",
      "Iteration 22164: loss = 0.0024509021227631742\n",
      "Iteration 22165: loss = 0.0024508915505868896\n",
      "Iteration 22166: loss = 0.002450880978525549\n",
      "Iteration 22167: loss = 0.002450870406579151\n",
      "Iteration 22168: loss = 0.002450859834747694\n",
      "Iteration 22169: loss = 0.0024508492630311764\n",
      "Iteration 22170: loss = 0.002450838691429597\n",
      "Iteration 22171: loss = 0.002450828119942955\n",
      "Iteration 22172: loss = 0.002450817548571248\n",
      "Iteration 22173: loss = 0.0024508069773144757\n",
      "Iteration 22174: loss = 0.0024507964061726358\n",
      "Iteration 22175: loss = 0.0024507858351457273\n",
      "Iteration 22176: loss = 0.0024507752642337487\n",
      "Iteration 22177: loss = 0.0024507646934366985\n",
      "Iteration 22178: loss = 0.002450754122754576\n",
      "Iteration 22179: loss = 0.0024507435521873787\n",
      "Iteration 22180: loss = 0.002450732981735106\n",
      "Iteration 22181: loss = 0.0024507224113977564\n",
      "Iteration 22182: loss = 0.0024507118411753286\n",
      "Iteration 22183: loss = 0.002450701271067821\n",
      "Iteration 22184: loss = 0.0024506907010752314\n",
      "Iteration 22185: loss = 0.0024506801311975604\n",
      "Iteration 22186: loss = 0.002450669561434805\n",
      "Iteration 22187: loss = 0.002450658991786964\n",
      "Iteration 22188: loss = 0.002450648422254037\n",
      "Iteration 22189: loss = 0.0024506378528360213\n",
      "Iteration 22190: loss = 0.0024506272835329163\n",
      "Iteration 22191: loss = 0.00245061671434472\n",
      "Iteration 22192: loss = 0.0024506061452714326\n",
      "Iteration 22193: loss = 0.0024505955763130505\n",
      "Iteration 22194: loss = 0.0024505850074695735\n",
      "Iteration 22195: loss = 0.002450574438741\n",
      "Iteration 22196: loss = 0.0024505638701273294\n",
      "Iteration 22197: loss = 0.0024505533016285596\n",
      "Iteration 22198: loss = 0.002450542733244689\n",
      "Iteration 22199: loss = 0.002450532164975716\n",
      "Iteration 22200: loss = 0.0024505215968216396\n",
      "Iteration 22201: loss = 0.002450511028782459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22202: loss = 0.002450500460858172\n",
      "Iteration 22203: loss = 0.002450489893048777\n",
      "Iteration 22204: loss = 0.002450479325354274\n",
      "Iteration 22205: loss = 0.0024504687577746598\n",
      "Iteration 22206: loss = 0.0024504581903099345\n",
      "Iteration 22207: loss = 0.0024504476229600964\n",
      "Iteration 22208: loss = 0.0024504370557251434\n",
      "Iteration 22209: loss = 0.002450426488605075\n",
      "Iteration 22210: loss = 0.002450415921599889\n",
      "Iteration 22211: loss = 0.0024504053547095843\n",
      "Iteration 22212: loss = 0.0024503947879341596\n",
      "Iteration 22213: loss = 0.002450384221273613\n",
      "Iteration 22214: loss = 0.0024503736547279447\n",
      "Iteration 22215: loss = 0.002450363088297152\n",
      "Iteration 22216: loss = 0.002450352521981233\n",
      "Iteration 22217: loss = 0.002450341955780187\n",
      "Iteration 22218: loss = 0.0024503313896940136\n",
      "Iteration 22219: loss = 0.00245032082372271\n",
      "Iteration 22220: loss = 0.0024503102578662755\n",
      "Iteration 22221: loss = 0.0024502996921247078\n",
      "Iteration 22222: loss = 0.002450289126498007\n",
      "Iteration 22223: loss = 0.002450278560986171\n",
      "Iteration 22224: loss = 0.002450267995589198\n",
      "Iteration 22225: loss = 0.0024502574303070864\n",
      "Iteration 22226: loss = 0.002450246865139836\n",
      "Iteration 22227: loss = 0.0024502363000874446\n",
      "Iteration 22228: loss = 0.002450225735149911\n",
      "Iteration 22229: loss = 0.002450215170327234\n",
      "Iteration 22230: loss = 0.002450204605619412\n",
      "Iteration 22231: loss = 0.0024501940410264433\n",
      "Iteration 22232: loss = 0.0024501834765483267\n",
      "Iteration 22233: loss = 0.0024501729121850615\n",
      "Iteration 22234: loss = 0.0024501623479366453\n",
      "Iteration 22235: loss = 0.002450151783803078\n",
      "Iteration 22236: loss = 0.0024501412197843564\n",
      "Iteration 22237: loss = 0.0024501306558804806\n",
      "Iteration 22238: loss = 0.0024501200920914483\n",
      "Iteration 22239: loss = 0.002450109528417259\n",
      "Iteration 22240: loss = 0.0024500989648579105\n",
      "Iteration 22241: loss = 0.0024500884014134017\n",
      "Iteration 22242: loss = 0.0024500778380837315\n",
      "Iteration 22243: loss = 0.0024500672748688987\n",
      "Iteration 22244: loss = 0.0024500567117689003\n",
      "Iteration 22245: loss = 0.002450046148783737\n",
      "Iteration 22246: loss = 0.0024500355859134065\n",
      "Iteration 22247: loss = 0.0024500250231579072\n",
      "Iteration 22248: loss = 0.002450014460517238\n",
      "Iteration 22249: loss = 0.0024500038979913975\n",
      "Iteration 22250: loss = 0.002449993335580384\n",
      "Iteration 22251: loss = 0.0024499827732841965\n",
      "Iteration 22252: loss = 0.0024499722111028335\n",
      "Iteration 22253: loss = 0.002449961649036294\n",
      "Iteration 22254: loss = 0.002449951087084575\n",
      "Iteration 22255: loss = 0.002449940525247678\n",
      "Iteration 22256: loss = 0.0024499299635255987\n",
      "Iteration 22257: loss = 0.0024499194019183376\n",
      "Iteration 22258: loss = 0.0024499088404258923\n",
      "Iteration 22259: loss = 0.002449898279048262\n",
      "Iteration 22260: loss = 0.0024498877177854448\n",
      "Iteration 22261: loss = 0.00244987715663744\n",
      "Iteration 22262: loss = 0.0024498665956042454\n",
      "Iteration 22263: loss = 0.00244985603468586\n",
      "Iteration 22264: loss = 0.0024498454738822828\n",
      "Iteration 22265: loss = 0.002449834913193512\n",
      "Iteration 22266: loss = 0.0024498243526195457\n",
      "Iteration 22267: loss = 0.0024498137921603836\n",
      "Iteration 22268: loss = 0.002449803231816024\n",
      "Iteration 22269: loss = 0.002449792671586465\n",
      "Iteration 22270: loss = 0.0024497821114717053\n",
      "Iteration 22271: loss = 0.002449771551471744\n",
      "Iteration 22272: loss = 0.00244976099158658\n",
      "Iteration 22273: loss = 0.00244975043181621\n",
      "Iteration 22274: loss = 0.0024497398721606348\n",
      "Iteration 22275: loss = 0.0024497293126198526\n",
      "Iteration 22276: loss = 0.0024497187531938606\n",
      "Iteration 22277: loss = 0.002449708193882659\n",
      "Iteration 22278: loss = 0.002449697634686246\n",
      "Iteration 22279: loss = 0.0024496870756046197\n",
      "Iteration 22280: loss = 0.0024496765166377795\n",
      "Iteration 22281: loss = 0.002449665957785723\n",
      "Iteration 22282: loss = 0.00244965539904845\n",
      "Iteration 22283: loss = 0.0024496448404259575\n",
      "Iteration 22284: loss = 0.0024496342819182457\n",
      "Iteration 22285: loss = 0.0024496237235253127\n",
      "Iteration 22286: loss = 0.002449613165247157\n",
      "Iteration 22287: loss = 0.002449602607083777\n",
      "Iteration 22288: loss = 0.0024495920490351713\n",
      "Iteration 22289: loss = 0.002449581491101339\n",
      "Iteration 22290: loss = 0.002449570933282279\n",
      "Iteration 22291: loss = 0.002449560375577989\n",
      "Iteration 22292: loss = 0.002449549817988468\n",
      "Iteration 22293: loss = 0.0024495392605137145\n",
      "Iteration 22294: loss = 0.002449528703153727\n",
      "Iteration 22295: loss = 0.0024495181459085053\n",
      "Iteration 22296: loss = 0.0024495075887780464\n",
      "Iteration 22297: loss = 0.0024494970317623494\n",
      "Iteration 22298: loss = 0.0024494864748614126\n",
      "Iteration 22299: loss = 0.0024494759180752365\n",
      "Iteration 22300: loss = 0.0024494653614038175\n",
      "Iteration 22301: loss = 0.0024494548048471553\n",
      "Iteration 22302: loss = 0.0024494442484052476\n",
      "Iteration 22303: loss = 0.002449433692078094\n",
      "Iteration 22304: loss = 0.0024494231358656933\n",
      "Iteration 22305: loss = 0.002449412579768043\n",
      "Iteration 22306: loss = 0.002449402023785142\n",
      "Iteration 22307: loss = 0.0024493914679169894\n",
      "Iteration 22308: loss = 0.0024493809121635844\n",
      "Iteration 22309: loss = 0.002449370356524924\n",
      "Iteration 22310: loss = 0.0024493598010010076\n",
      "Iteration 22311: loss = 0.002449349245591834\n",
      "Iteration 22312: loss = 0.0024493386902974023\n",
      "Iteration 22313: loss = 0.0024493281351177097\n",
      "Iteration 22314: loss = 0.002449317580052756\n",
      "Iteration 22315: loss = 0.002449307025102539\n",
      "Iteration 22316: loss = 0.002449296470267058\n",
      "Iteration 22317: loss = 0.0024492859155463118\n",
      "Iteration 22318: loss = 0.002449275360940298\n",
      "Iteration 22319: loss = 0.002449264806449016\n",
      "Iteration 22320: loss = 0.0024492542520724637\n",
      "Iteration 22321: loss = 0.0024492436978106403\n",
      "Iteration 22322: loss = 0.002449233143663545\n",
      "Iteration 22323: loss = 0.002449222589631175\n",
      "Iteration 22324: loss = 0.0024492120357135294\n",
      "Iteration 22325: loss = 0.0024492014819106076\n",
      "Iteration 22326: loss = 0.0024491909282224078\n",
      "Iteration 22327: loss = 0.0024491803746489282\n",
      "Iteration 22328: loss = 0.0024491698211901677\n",
      "Iteration 22329: loss = 0.002449159267846125\n",
      "Iteration 22330: loss = 0.0024491487146167984\n",
      "Iteration 22331: loss = 0.0024491381615021866\n",
      "Iteration 22332: loss = 0.002449127608502289\n",
      "Iteration 22333: loss = 0.0024491170556171027\n",
      "Iteration 22334: loss = 0.002449106502846627\n",
      "Iteration 22335: loss = 0.002449095950190862\n",
      "Iteration 22336: loss = 0.002449085397649804\n",
      "Iteration 22337: loss = 0.0024490748452234527\n",
      "Iteration 22338: loss = 0.002449064292911807\n",
      "Iteration 22339: loss = 0.0024490537407148645\n",
      "Iteration 22340: loss = 0.0024490431886326253\n",
      "Iteration 22341: loss = 0.002449032636665087\n",
      "Iteration 22342: loss = 0.0024490220848122474\n",
      "Iteration 22343: loss = 0.0024490115330741066\n",
      "Iteration 22344: loss = 0.002449000981450663\n",
      "Iteration 22345: loss = 0.0024489904299419153\n",
      "Iteration 22346: loss = 0.0024489798785478608\n",
      "Iteration 22347: loss = 0.0024489693272684993\n",
      "Iteration 22348: loss = 0.0024489587761038294\n",
      "Iteration 22349: loss = 0.0024489482250538495\n",
      "Iteration 22350: loss = 0.002448937674118558\n",
      "Iteration 22351: loss = 0.002448927123297954\n",
      "Iteration 22352: loss = 0.002448916572592035\n",
      "Iteration 22353: loss = 0.002448906022000801\n",
      "Iteration 22354: loss = 0.00244889547152425\n",
      "Iteration 22355: loss = 0.002448884921162381\n",
      "Iteration 22356: loss = 0.002448874370915192\n",
      "Iteration 22357: loss = 0.0024488638207826817\n",
      "Iteration 22358: loss = 0.002448853270764849\n",
      "Iteration 22359: loss = 0.002448842720861692\n",
      "Iteration 22360: loss = 0.00244883217107321\n",
      "Iteration 22361: loss = 0.0024488216213994014\n",
      "Iteration 22362: loss = 0.002448811071840265\n",
      "Iteration 22363: loss = 0.0024488005223957986\n",
      "Iteration 22364: loss = 0.002448789973066002\n",
      "Iteration 22365: loss = 0.0024487794238508723\n",
      "Iteration 22366: loss = 0.00244876887475041\n",
      "Iteration 22367: loss = 0.0024487583257646124\n",
      "Iteration 22368: loss = 0.002448747776893478\n",
      "Iteration 22369: loss = 0.002448737228137006\n",
      "Iteration 22370: loss = 0.002448726679495196\n",
      "Iteration 22371: loss = 0.002448716130968044\n",
      "Iteration 22372: loss = 0.002448705582555551\n",
      "Iteration 22373: loss = 0.002448695034257714\n",
      "Iteration 22374: loss = 0.0024486844860745326\n",
      "Iteration 22375: loss = 0.002448673938006005\n",
      "Iteration 22376: loss = 0.00244866339005213\n",
      "Iteration 22377: loss = 0.0024486528422129066\n",
      "Iteration 22378: loss = 0.0024486422944883326\n",
      "Iteration 22379: loss = 0.0024486317468784074\n",
      "Iteration 22380: loss = 0.002448621199383129\n",
      "Iteration 22381: loss = 0.0024486106520024958\n",
      "Iteration 22382: loss = 0.0024486001047365064\n",
      "Iteration 22383: loss = 0.0024485895575851613\n",
      "Iteration 22384: loss = 0.002448579010548457\n",
      "Iteration 22385: loss = 0.0024485684636263924\n",
      "Iteration 22386: loss = 0.0024485579168189663\n",
      "Iteration 22387: loss = 0.002448547370126178\n",
      "Iteration 22388: loss = 0.0024485368235480256\n",
      "Iteration 22389: loss = 0.002448526277084508\n",
      "Iteration 22390: loss = 0.0024485157307356227\n",
      "Iteration 22391: loss = 0.00244850518450137\n",
      "Iteration 22392: loss = 0.002448494638381747\n",
      "Iteration 22393: loss = 0.0024484840923767533\n",
      "Iteration 22394: loss = 0.0024484735464863874\n",
      "Iteration 22395: loss = 0.0024484630007106474\n",
      "Iteration 22396: loss = 0.0024484524550495326\n",
      "Iteration 22397: loss = 0.0024484419095030406\n",
      "Iteration 22398: loss = 0.0024484313640711713\n",
      "Iteration 22399: loss = 0.0024484208187539225\n",
      "Iteration 22400: loss = 0.0024484102735512926\n",
      "Iteration 22401: loss = 0.002448399728463281\n",
      "Iteration 22402: loss = 0.002448389183489886\n",
      "Iteration 22403: loss = 0.002448378638631105\n",
      "Iteration 22404: loss = 0.0024483680938869393\n",
      "Iteration 22405: loss = 0.002448357549257385\n",
      "Iteration 22406: loss = 0.002448347004742442\n",
      "Iteration 22407: loss = 0.002448336460342109\n",
      "Iteration 22408: loss = 0.002448325916056384\n",
      "Iteration 22409: loss = 0.0024483153718852656\n",
      "Iteration 22410: loss = 0.0024483048278287523\n",
      "Iteration 22411: loss = 0.0024482942838868435\n",
      "Iteration 22412: loss = 0.002448283740059537\n",
      "Iteration 22413: loss = 0.0024482731963468324\n",
      "Iteration 22414: loss = 0.0024482626527487267\n",
      "Iteration 22415: loss = 0.0024482521092652203\n",
      "Iteration 22416: loss = 0.0024482415658963106\n",
      "Iteration 22417: loss = 0.002448231022641997\n",
      "Iteration 22418: loss = 0.002448220479502278\n",
      "Iteration 22419: loss = 0.002448209936477151\n",
      "Iteration 22420: loss = 0.002448199393566616\n",
      "Iteration 22421: loss = 0.0024481888507706714\n",
      "Iteration 22422: loss = 0.002448178308089316\n",
      "Iteration 22423: loss = 0.0024481677655225475\n",
      "Iteration 22424: loss = 0.0024481572230703645\n",
      "Iteration 22425: loss = 0.0024481466807327666\n",
      "Iteration 22426: loss = 0.002448136138509752\n",
      "Iteration 22427: loss = 0.002448125596401319\n",
      "Iteration 22428: loss = 0.0024481150544074675\n",
      "Iteration 22429: loss = 0.002448104512528194\n",
      "Iteration 22430: loss = 0.002448093970763499\n",
      "Iteration 22431: loss = 0.0024480834291133796\n",
      "Iteration 22432: loss = 0.0024480728875778354\n",
      "Iteration 22433: loss = 0.002448062346156865\n",
      "Iteration 22434: loss = 0.002448051804850467\n",
      "Iteration 22435: loss = 0.002448041263658639\n",
      "Iteration 22436: loss = 0.002448030722581381\n",
      "Iteration 22437: loss = 0.002448020181618691\n",
      "Iteration 22438: loss = 0.0024480096407705674\n",
      "Iteration 22439: loss = 0.002447999100037009\n",
      "Iteration 22440: loss = 0.0024479885594180144\n",
      "Iteration 22441: loss = 0.0024479780189135825\n",
      "Iteration 22442: loss = 0.0024479674785237117\n",
      "Iteration 22443: loss = 0.002447956938248401\n",
      "Iteration 22444: loss = 0.002447946398087648\n",
      "Iteration 22445: loss = 0.0024479358580414526\n",
      "Iteration 22446: loss = 0.0024479253181098116\n",
      "Iteration 22447: loss = 0.0024479147782927257\n",
      "Iteration 22448: loss = 0.002447904238590192\n",
      "Iteration 22449: loss = 0.0024478936990022103\n",
      "Iteration 22450: loss = 0.0024478831595287787\n",
      "Iteration 22451: loss = 0.002447872620169895\n",
      "Iteration 22452: loss = 0.0024478620809255595\n",
      "Iteration 22453: loss = 0.002447851541795769\n",
      "Iteration 22454: loss = 0.0024478410027805233\n",
      "Iteration 22455: loss = 0.002447830463879821\n",
      "Iteration 22456: loss = 0.0024478199250936596\n",
      "Iteration 22457: loss = 0.002447809386422039\n",
      "Iteration 22458: loss = 0.0024477988478649577\n",
      "Iteration 22459: loss = 0.0024477883094224133\n",
      "Iteration 22460: loss = 0.002447777771094405\n",
      "Iteration 22461: loss = 0.002447767232880932\n",
      "Iteration 22462: loss = 0.0024477566947819925\n",
      "Iteration 22463: loss = 0.0024477461567975847\n",
      "Iteration 22464: loss = 0.002447735618927707\n",
      "Iteration 22465: loss = 0.0024477250811723596\n",
      "Iteration 22466: loss = 0.0024477145435315394\n",
      "Iteration 22467: loss = 0.002447704006005246\n",
      "Iteration 22468: loss = 0.0024476934685934773\n",
      "Iteration 22469: loss = 0.0024476829312962325\n",
      "Iteration 22470: loss = 0.0024476723941135098\n",
      "Iteration 22471: loss = 0.0024476618570453083\n",
      "Iteration 22472: loss = 0.0024476513200916263\n",
      "Iteration 22473: loss = 0.0024476407832524625\n",
      "Iteration 22474: loss = 0.0024476302465278156\n",
      "Iteration 22475: loss = 0.002447619709917684\n",
      "Iteration 22476: loss = 0.0024476091734220657\n",
      "Iteration 22477: loss = 0.0024475986370409613\n",
      "Iteration 22478: loss = 0.002447588100774367\n",
      "Iteration 22479: loss = 0.002447577564622283\n",
      "Iteration 22480: loss = 0.0024475670285847074\n",
      "Iteration 22481: loss = 0.002447556492661639\n",
      "Iteration 22482: loss = 0.002447545956853076\n",
      "Iteration 22483: loss = 0.0024475354211590176\n",
      "Iteration 22484: loss = 0.002447524885579462\n",
      "Iteration 22485: loss = 0.0024475143501144084\n",
      "Iteration 22486: loss = 0.0024475038147638547\n",
      "Iteration 22487: loss = 0.0024474932795277996\n",
      "Iteration 22488: loss = 0.002447482744406242\n",
      "Iteration 22489: loss = 0.00244747220939918\n",
      "Iteration 22490: loss = 0.002447461674506613\n",
      "Iteration 22491: loss = 0.0024474511397285393\n",
      "Iteration 22492: loss = 0.0024474406050649573\n",
      "Iteration 22493: loss = 0.0024474300705158653\n",
      "Iteration 22494: loss = 0.002447419536081263\n",
      "Iteration 22495: loss = 0.0024474090017611483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22496: loss = 0.0024473984675555203\n",
      "Iteration 22497: loss = 0.002447387933464377\n",
      "Iteration 22498: loss = 0.002447377399487717\n",
      "Iteration 22499: loss = 0.0024473668656255386\n",
      "Iteration 22500: loss = 0.002447356331877842\n",
      "Iteration 22501: loss = 0.002447345798244624\n",
      "Iteration 22502: loss = 0.002447335264725885\n",
      "Iteration 22503: loss = 0.0024473247313216216\n",
      "Iteration 22504: loss = 0.002447314198031834\n",
      "Iteration 22505: loss = 0.00244730366485652\n",
      "Iteration 22506: loss = 0.0024472931317956782\n",
      "Iteration 22507: loss = 0.0024472825988493077\n",
      "Iteration 22508: loss = 0.002447272066017407\n",
      "Iteration 22509: loss = 0.0024472615332999745\n",
      "Iteration 22510: loss = 0.0024472510006970094\n",
      "Iteration 22511: loss = 0.0024472404682085096\n",
      "Iteration 22512: loss = 0.0024472299358344737\n",
      "Iteration 22513: loss = 0.0024472194035749005\n",
      "Iteration 22514: loss = 0.002447208871429789\n",
      "Iteration 22515: loss = 0.002447198339399138\n",
      "Iteration 22516: loss = 0.0024471878074829446\n",
      "Iteration 22517: loss = 0.0024471772756812084\n",
      "Iteration 22518: loss = 0.0024471667439939287\n",
      "Iteration 22519: loss = 0.002447156212421103\n",
      "Iteration 22520: loss = 0.002447145680962731\n",
      "Iteration 22521: loss = 0.002447135149618811\n",
      "Iteration 22522: loss = 0.00244712461838934\n",
      "Iteration 22523: loss = 0.002447114087274319\n",
      "Iteration 22524: loss = 0.002447103556273745\n",
      "Iteration 22525: loss = 0.0024470930253876174\n",
      "Iteration 22526: loss = 0.002447082494615934\n",
      "Iteration 22527: loss = 0.002447071963958695\n",
      "Iteration 22528: loss = 0.0024470614334158973\n",
      "Iteration 22529: loss = 0.0024470509029875407\n",
      "Iteration 22530: loss = 0.002447040372673623\n",
      "Iteration 22531: loss = 0.002447029842474143\n",
      "Iteration 22532: loss = 0.0024470193123890993\n",
      "Iteration 22533: loss = 0.002447008782418491\n",
      "Iteration 22534: loss = 0.002446998252562317\n",
      "Iteration 22535: loss = 0.002446987722820575\n",
      "Iteration 22536: loss = 0.002446977193193263\n",
      "Iteration 22537: loss = 0.0024469666636803816\n",
      "Iteration 22538: loss = 0.002446956134281928\n",
      "Iteration 22539: loss = 0.0024469456049979015\n",
      "Iteration 22540: loss = 0.0024469350758282997\n",
      "Iteration 22541: loss = 0.0024469245467731225\n",
      "Iteration 22542: loss = 0.0024469140178323676\n",
      "Iteration 22543: loss = 0.0024469034890060343\n",
      "Iteration 22544: loss = 0.0024468929602941203\n",
      "Iteration 22545: loss = 0.002446882431696625\n",
      "Iteration 22546: loss = 0.0024468719032135476\n",
      "Iteration 22547: loss = 0.002446861374844885\n",
      "Iteration 22548: loss = 0.002446850846590637\n",
      "Iteration 22549: loss = 0.002446840318450802\n",
      "Iteration 22550: loss = 0.0024468297904253786\n",
      "Iteration 22551: loss = 0.002446819262514365\n",
      "Iteration 22552: loss = 0.00244680873471776\n",
      "Iteration 22553: loss = 0.0024467982070355636\n",
      "Iteration 22554: loss = 0.0024467876794677717\n",
      "Iteration 22555: loss = 0.0024467771520143853\n",
      "Iteration 22556: loss = 0.0024467666246754025\n",
      "Iteration 22557: loss = 0.0024467560974508213\n",
      "Iteration 22558: loss = 0.0024467455703406403\n",
      "Iteration 22559: loss = 0.0024467350433448583\n",
      "Iteration 22560: loss = 0.0024467245164634744\n",
      "Iteration 22561: loss = 0.0024467139896964863\n",
      "Iteration 22562: loss = 0.0024467034630438938\n",
      "Iteration 22563: loss = 0.0024466929365056945\n",
      "Iteration 22564: loss = 0.0024466824100818877\n",
      "Iteration 22565: loss = 0.0024466718837724715\n",
      "Iteration 22566: loss = 0.0024466613575774444\n",
      "Iteration 22567: loss = 0.0024466508314968053\n",
      "Iteration 22568: loss = 0.0024466403055305535\n",
      "Iteration 22569: loss = 0.0024466297796786863\n",
      "Iteration 22570: loss = 0.0024466192539412037\n",
      "Iteration 22571: loss = 0.002446608728318103\n",
      "Iteration 22572: loss = 0.002446598202809384\n",
      "Iteration 22573: loss = 0.002446587677415044\n",
      "Iteration 22574: loss = 0.002446577152135082\n",
      "Iteration 22575: loss = 0.0024465666269694983\n",
      "Iteration 22576: loss = 0.0024465561019182897\n",
      "Iteration 22577: loss = 0.002446545576981455\n",
      "Iteration 22578: loss = 0.002446535052158993\n",
      "Iteration 22579: loss = 0.0024465245274509027\n",
      "Iteration 22580: loss = 0.0024465140028571823\n",
      "Iteration 22581: loss = 0.002446503478377831\n",
      "Iteration 22582: loss = 0.002446492954012846\n",
      "Iteration 22583: loss = 0.002446482429762228\n",
      "Iteration 22584: loss = 0.002446471905625974\n",
      "Iteration 22585: loss = 0.002446461381604083\n",
      "Iteration 22586: loss = 0.0024464508576965533\n",
      "Iteration 22587: loss = 0.002446440333903385\n",
      "Iteration 22588: loss = 0.0024464298102245753\n",
      "Iteration 22589: loss = 0.002446419286660123\n",
      "Iteration 22590: loss = 0.0024464087632100266\n",
      "Iteration 22591: loss = 0.0024463982398742856\n",
      "Iteration 22592: loss = 0.002446387716652898\n",
      "Iteration 22593: loss = 0.002446377193545862\n",
      "Iteration 22594: loss = 0.0024463666705531765\n",
      "Iteration 22595: loss = 0.002446356147674841\n",
      "Iteration 22596: loss = 0.0024463456249108535\n",
      "Iteration 22597: loss = 0.0024463351022612125\n",
      "Iteration 22598: loss = 0.0024463245797259163\n",
      "Iteration 22599: loss = 0.0024463140573049635\n",
      "Iteration 22600: loss = 0.0024463035349983537\n",
      "Iteration 22601: loss = 0.0024462930128060843\n",
      "Iteration 22602: loss = 0.0024462824907281553\n",
      "Iteration 22603: loss = 0.002446271968764564\n",
      "Iteration 22604: loss = 0.0024462614469153094\n",
      "Iteration 22605: loss = 0.0024462509251803907\n",
      "Iteration 22606: loss = 0.0024462404035598055\n",
      "Iteration 22607: loss = 0.0024462298820535533\n",
      "Iteration 22608: loss = 0.002446219360661633\n",
      "Iteration 22609: loss = 0.0024462088393840415\n",
      "Iteration 22610: loss = 0.0024461983182207793\n",
      "Iteration 22611: loss = 0.0024461877971718444\n",
      "Iteration 22612: loss = 0.0024461772762372344\n",
      "Iteration 22613: loss = 0.0024461667554169495\n",
      "Iteration 22614: loss = 0.0024461562347109873\n",
      "Iteration 22615: loss = 0.002446145714119347\n",
      "Iteration 22616: loss = 0.0024461351936420263\n",
      "Iteration 22617: loss = 0.0024461246732790246\n",
      "Iteration 22618: loss = 0.0024461141530303415\n",
      "Iteration 22619: loss = 0.0024461036328959733\n",
      "Iteration 22620: loss = 0.00244609311287592\n",
      "Iteration 22621: loss = 0.002446082592970181\n",
      "Iteration 22622: loss = 0.0024460720731787526\n",
      "Iteration 22623: loss = 0.0024460615535016354\n",
      "Iteration 22624: loss = 0.002446051033938827\n",
      "Iteration 22625: loss = 0.0024460405144903267\n",
      "Iteration 22626: loss = 0.0024460299951561326\n",
      "Iteration 22627: loss = 0.0024460194759362438\n",
      "Iteration 22628: loss = 0.002446008956830659\n",
      "Iteration 22629: loss = 0.0024459984378393754\n",
      "Iteration 22630: loss = 0.0024459879189623933\n",
      "Iteration 22631: loss = 0.002445977400199711\n",
      "Iteration 22632: loss = 0.002445966881551326\n",
      "Iteration 22633: loss = 0.002445956363017238\n",
      "Iteration 22634: loss = 0.0024459458445974455\n",
      "Iteration 22635: loss = 0.002445935326291947\n",
      "Iteration 22636: loss = 0.0024459248081007408\n",
      "Iteration 22637: loss = 0.002445914290023826\n",
      "Iteration 22638: loss = 0.002445903772061201\n",
      "Iteration 22639: loss = 0.0024458932542128645\n",
      "Iteration 22640: loss = 0.0024458827364788154\n",
      "Iteration 22641: loss = 0.002445872218859051\n",
      "Iteration 22642: loss = 0.0024458617013535717\n",
      "Iteration 22643: loss = 0.0024458511839623746\n",
      "Iteration 22644: loss = 0.0024458406666854593\n",
      "Iteration 22645: loss = 0.002445830149522825\n",
      "Iteration 22646: loss = 0.0024458196324744683\n",
      "Iteration 22647: loss = 0.0024458091155403895\n",
      "Iteration 22648: loss = 0.0024457985987205865\n",
      "Iteration 22649: loss = 0.002445788082015058\n",
      "Iteration 22650: loss = 0.0024457775654238025\n",
      "Iteration 22651: loss = 0.0024457670489468194\n",
      "Iteration 22652: loss = 0.002445756532584106\n",
      "Iteration 22653: loss = 0.002445746016335663\n",
      "Iteration 22654: loss = 0.0024457355002014862\n",
      "Iteration 22655: loss = 0.0024457249841815768\n",
      "Iteration 22656: loss = 0.002445714468275932\n",
      "Iteration 22657: loss = 0.0024457039524845505\n",
      "Iteration 22658: loss = 0.002445693436807431\n",
      "Iteration 22659: loss = 0.0024456829212445726\n",
      "Iteration 22660: loss = 0.0024456724057959735\n",
      "Iteration 22661: loss = 0.002445661890461632\n",
      "Iteration 22662: loss = 0.0024456513752415475\n",
      "Iteration 22663: loss = 0.0024456408601357185\n",
      "Iteration 22664: loss = 0.0024456303451441427\n",
      "Iteration 22665: loss = 0.00244561983026682\n",
      "Iteration 22666: loss = 0.0024456093155037486\n",
      "Iteration 22667: loss = 0.002445598800854926\n",
      "Iteration 22668: loss = 0.0024455882863203523\n",
      "Iteration 22669: loss = 0.0024455777719000252\n",
      "Iteration 22670: loss = 0.002445567257593944\n",
      "Iteration 22671: loss = 0.0024455567434021066\n",
      "Iteration 22672: loss = 0.0024455462293245128\n",
      "Iteration 22673: loss = 0.0024455357153611596\n",
      "Iteration 22674: loss = 0.0024455252015120462\n",
      "Iteration 22675: loss = 0.0024455146877771726\n",
      "Iteration 22676: loss = 0.0024455041741565354\n",
      "Iteration 22677: loss = 0.002445493660650134\n",
      "Iteration 22678: loss = 0.0024454831472579676\n",
      "Iteration 22679: loss = 0.0024454726339800336\n",
      "Iteration 22680: loss = 0.002445462120816332\n",
      "Iteration 22681: loss = 0.0024454516077668607\n",
      "Iteration 22682: loss = 0.0024454410948316183\n",
      "Iteration 22683: loss = 0.0024454305820106035\n",
      "Iteration 22684: loss = 0.0024454200693038147\n",
      "Iteration 22685: loss = 0.0024454095567112504\n",
      "Iteration 22686: loss = 0.00244539904423291\n",
      "Iteration 22687: loss = 0.002445388531868792\n",
      "Iteration 22688: loss = 0.0024453780196188944\n",
      "Iteration 22689: loss = 0.002445367507483215\n",
      "Iteration 22690: loss = 0.0024453569954617552\n",
      "Iteration 22691: loss = 0.002445346483554511\n",
      "Iteration 22692: loss = 0.002445335971761482\n",
      "Iteration 22693: loss = 0.0024453254600826668\n",
      "Iteration 22694: loss = 0.002445314948518064\n",
      "Iteration 22695: loss = 0.0024453044370676726\n",
      "Iteration 22696: loss = 0.00244529392573149\n",
      "Iteration 22697: loss = 0.0024452834145095165\n",
      "Iteration 22698: loss = 0.0024452729034017493\n",
      "Iteration 22699: loss = 0.0024452623924081877\n",
      "Iteration 22700: loss = 0.0024452518815288302\n",
      "Iteration 22701: loss = 0.0024452413707636753\n",
      "Iteration 22702: loss = 0.0024452308601127216\n",
      "Iteration 22703: loss = 0.0024452203495759677\n",
      "Iteration 22704: loss = 0.002445209839153413\n",
      "Iteration 22705: loss = 0.002445199328845055\n",
      "Iteration 22706: loss = 0.0024451888186508932\n",
      "Iteration 22707: loss = 0.0024451783085709254\n",
      "Iteration 22708: loss = 0.0024451677986051506\n",
      "Iteration 22709: loss = 0.002445157288753568\n",
      "Iteration 22710: loss = 0.002445146779016175\n",
      "Iteration 22711: loss = 0.0024451362693929716\n",
      "Iteration 22712: loss = 0.0024451257598839546\n",
      "Iteration 22713: loss = 0.0024451152504891245\n",
      "Iteration 22714: loss = 0.0024451047412084787\n",
      "Iteration 22715: loss = 0.0024450942320420168\n",
      "Iteration 22716: loss = 0.0024450837229897365\n",
      "Iteration 22717: loss = 0.002445073214051637\n",
      "Iteration 22718: loss = 0.0024450627052277165\n",
      "Iteration 22719: loss = 0.002445052196517974\n",
      "Iteration 22720: loss = 0.0024450416879224076\n",
      "Iteration 22721: loss = 0.0024450311794410166\n",
      "Iteration 22722: loss = 0.002445020671073799\n",
      "Iteration 22723: loss = 0.0024450101628207535\n",
      "Iteration 22724: loss = 0.0024449996546818797\n",
      "Iteration 22725: loss = 0.0024449891466571745\n",
      "Iteration 22726: loss = 0.0024449786387466385\n",
      "Iteration 22727: loss = 0.0024449681309502685\n",
      "Iteration 22728: loss = 0.002444957623268064\n",
      "Iteration 22729: loss = 0.0024449471157000236\n",
      "Iteration 22730: loss = 0.0024449366082461457\n",
      "Iteration 22731: loss = 0.002444926100906429\n",
      "Iteration 22732: loss = 0.002444915593680872\n",
      "Iteration 22733: loss = 0.0024449050865694745\n",
      "Iteration 22734: loss = 0.002444894579572233\n",
      "Iteration 22735: loss = 0.002444884072689147\n",
      "Iteration 22736: loss = 0.002444873565920216\n",
      "Iteration 22737: loss = 0.0024448630592654383\n",
      "Iteration 22738: loss = 0.0024448525527248116\n",
      "Iteration 22739: loss = 0.0024448420462983345\n",
      "Iteration 22740: loss = 0.002444831539986007\n",
      "Iteration 22741: loss = 0.002444821033787827\n",
      "Iteration 22742: loss = 0.002444810527703793\n",
      "Iteration 22743: loss = 0.002444800021733903\n",
      "Iteration 22744: loss = 0.0024447895158781568\n",
      "Iteration 22745: loss = 0.0024447790101365524\n",
      "Iteration 22746: loss = 0.0024447685045090885\n",
      "Iteration 22747: loss = 0.002444757998995764\n",
      "Iteration 22748: loss = 0.0024447474935965765\n",
      "Iteration 22749: loss = 0.002444736988311526\n",
      "Iteration 22750: loss = 0.00244472648314061\n",
      "Iteration 22751: loss = 0.002444715978083828\n",
      "Iteration 22752: loss = 0.0024447054731411783\n",
      "Iteration 22753: loss = 0.002444694968312659\n",
      "Iteration 22754: loss = 0.002444684463598269\n",
      "Iteration 22755: loss = 0.0024446739589980074\n",
      "Iteration 22756: loss = 0.0024446634545118727\n",
      "Iteration 22757: loss = 0.0024446529501398633\n",
      "Iteration 22758: loss = 0.002444642445881978\n",
      "Iteration 22759: loss = 0.002444631941738214\n",
      "Iteration 22760: loss = 0.0024446214377085726\n",
      "Iteration 22761: loss = 0.00244461093379305\n",
      "Iteration 22762: loss = 0.0024446004299916464\n",
      "Iteration 22763: loss = 0.0024445899263043592\n",
      "Iteration 22764: loss = 0.0024445794227311887\n",
      "Iteration 22765: loss = 0.002444568919272132\n",
      "Iteration 22766: loss = 0.002444558415927188\n",
      "Iteration 22767: loss = 0.0024445479126963547\n",
      "Iteration 22768: loss = 0.0024445374095796333\n",
      "Iteration 22769: loss = 0.002444526906577019\n",
      "Iteration 22770: loss = 0.0024445164036885126\n",
      "Iteration 22771: loss = 0.0024445059009141124\n",
      "Iteration 22772: loss = 0.0024444953982538167\n",
      "Iteration 22773: loss = 0.002444484895707624\n",
      "Iteration 22774: loss = 0.0024444743932755335\n",
      "Iteration 22775: loss = 0.002444463890957543\n",
      "Iteration 22776: loss = 0.0024444533887536517\n",
      "Iteration 22777: loss = 0.0024444428866638584\n",
      "Iteration 22778: loss = 0.002444432384688161\n",
      "Iteration 22779: loss = 0.0024444218828265584\n",
      "Iteration 22780: loss = 0.00244441138107905\n",
      "Iteration 22781: loss = 0.0024444008794456333\n",
      "Iteration 22782: loss = 0.002444390377926307\n",
      "Iteration 22783: loss = 0.00244437987652107\n",
      "Iteration 22784: loss = 0.0024443693752299214\n",
      "Iteration 22785: loss = 0.00244435887405286\n",
      "Iteration 22786: loss = 0.0024443483729898836\n",
      "Iteration 22787: loss = 0.00244433787204099\n",
      "Iteration 22788: loss = 0.0024443273712061804\n",
      "Iteration 22789: loss = 0.002444316870485451\n",
      "Iteration 22790: loss = 0.0024443063698788015\n",
      "Iteration 22791: loss = 0.00244429586938623\n",
      "Iteration 22792: loss = 0.0024442853690077354\n",
      "Iteration 22793: loss = 0.002444274868743317\n",
      "Iteration 22794: loss = 0.002444264368592973\n",
      "Iteration 22795: loss = 0.0024442538685567005\n",
      "Iteration 22796: loss = 0.0024442433686345006\n",
      "Iteration 22797: loss = 0.002444232868826371\n",
      "Iteration 22798: loss = 0.0024442223691323087\n",
      "Iteration 22799: loss = 0.0024442118695523147\n",
      "Iteration 22800: loss = 0.002444201370086386\n",
      "Iteration 22801: loss = 0.0024441908707345224\n",
      "Iteration 22802: loss = 0.002444180371496722\n",
      "Iteration 22803: loss = 0.002444169872372983\n",
      "Iteration 22804: loss = 0.0024441593733633043\n",
      "Iteration 22805: loss = 0.0024441488744676846\n",
      "Iteration 22806: loss = 0.0024441383756861226\n",
      "Iteration 22807: loss = 0.0024441278770186165\n",
      "Iteration 22808: loss = 0.002444117378465166\n",
      "Iteration 22809: loss = 0.002444106880025769\n",
      "Iteration 22810: loss = 0.0024440963817004236\n",
      "Iteration 22811: loss = 0.0024440858834891286\n",
      "Iteration 22812: loss = 0.0024440753853918833\n",
      "Iteration 22813: loss = 0.0024440648874086866\n",
      "Iteration 22814: loss = 0.0024440543895395354\n",
      "Iteration 22815: loss = 0.00244404389178443\n",
      "Iteration 22816: loss = 0.002444033394143368\n",
      "Iteration 22817: loss = 0.002444022896616349\n",
      "Iteration 22818: loss = 0.0024440123992033706\n",
      "Iteration 22819: loss = 0.0024440019019044317\n",
      "Iteration 22820: loss = 0.0024439914047195313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22821: loss = 0.002443980907648668\n",
      "Iteration 22822: loss = 0.00244397041069184\n",
      "Iteration 22823: loss = 0.0024439599138490467\n",
      "Iteration 22824: loss = 0.0024439494171202857\n",
      "Iteration 22825: loss = 0.002443938920505556\n",
      "Iteration 22826: loss = 0.002443928424004856\n",
      "Iteration 22827: loss = 0.0024439179276181853\n",
      "Iteration 22828: loss = 0.0024439074313455417\n",
      "Iteration 22829: loss = 0.002443896935186923\n",
      "Iteration 22830: loss = 0.0024438864391423294\n",
      "Iteration 22831: loss = 0.0024438759432117594\n",
      "Iteration 22832: loss = 0.0024438654473952106\n",
      "Iteration 22833: loss = 0.002443854951692682\n",
      "Iteration 22834: loss = 0.002443844456104173\n",
      "Iteration 22835: loss = 0.002443833960629681\n",
      "Iteration 22836: loss = 0.002443823465269206\n",
      "Iteration 22837: loss = 0.0024438129700227445\n",
      "Iteration 22838: loss = 0.002443802474890297\n",
      "Iteration 22839: loss = 0.002443791979871862\n",
      "Iteration 22840: loss = 0.002443781484967437\n",
      "Iteration 22841: loss = 0.002443770990177022\n",
      "Iteration 22842: loss = 0.002443760495500614\n",
      "Iteration 22843: loss = 0.0024437500009382132\n",
      "Iteration 22844: loss = 0.002443739506489817\n",
      "Iteration 22845: loss = 0.0024437290121554246\n",
      "Iteration 22846: loss = 0.0024437185179350352\n",
      "Iteration 22847: loss = 0.0024437080238286463\n",
      "Iteration 22848: loss = 0.0024436975298362577\n",
      "Iteration 22849: loss = 0.0024436870359578664\n",
      "Iteration 22850: loss = 0.0024436765421934725\n",
      "Iteration 22851: loss = 0.0024436660485430742\n",
      "Iteration 22852: loss = 0.0024436555550066694\n",
      "Iteration 22853: loss = 0.002443645061584258\n",
      "Iteration 22854: loss = 0.002443634568275837\n",
      "Iteration 22855: loss = 0.002443624075081407\n",
      "Iteration 22856: loss = 0.002443613582000965\n",
      "Iteration 22857: loss = 0.00244360308903451\n",
      "Iteration 22858: loss = 0.0024435925961820413\n",
      "Iteration 22859: loss = 0.002443582103443557\n",
      "Iteration 22860: loss = 0.0024435716108190553\n",
      "Iteration 22861: loss = 0.0024435611183085356\n",
      "Iteration 22862: loss = 0.0024435506259119963\n",
      "Iteration 22863: loss = 0.0024435401336294358\n",
      "Iteration 22864: loss = 0.0024435296414608526\n",
      "Iteration 22865: loss = 0.0024435191494062455\n",
      "Iteration 22866: loss = 0.0024435086574656137\n",
      "Iteration 22867: loss = 0.002443498165638955\n",
      "Iteration 22868: loss = 0.002443487673926268\n",
      "Iteration 22869: loss = 0.0024434771823275523\n",
      "Iteration 22870: loss = 0.002443466690842805\n",
      "Iteration 22871: loss = 0.0024434561994720262\n",
      "Iteration 22872: loss = 0.0024434457082152137\n",
      "Iteration 22873: loss = 0.0024434352170723665\n",
      "Iteration 22874: loss = 0.0024434247260434827\n",
      "Iteration 22875: loss = 0.0024434142351285612\n",
      "Iteration 22876: loss = 0.0024434037443276007\n",
      "Iteration 22877: loss = 0.0024433932536406\n",
      "Iteration 22878: loss = 0.0024433827630675575\n",
      "Iteration 22879: loss = 0.002443372272608472\n",
      "Iteration 22880: loss = 0.002443361782263341\n",
      "Iteration 22881: loss = 0.002443351292032165\n",
      "Iteration 22882: loss = 0.002443340801914942\n",
      "Iteration 22883: loss = 0.0024433303119116693\n",
      "Iteration 22884: loss = 0.002443319822022347\n",
      "Iteration 22885: loss = 0.0024433093322469733\n",
      "Iteration 22886: loss = 0.0024432988425855465\n",
      "Iteration 22887: loss = 0.0024432883530380654\n",
      "Iteration 22888: loss = 0.0024432778636045292\n",
      "Iteration 22889: loss = 0.0024432673742849353\n",
      "Iteration 22890: loss = 0.0024432568850792837\n",
      "Iteration 22891: loss = 0.002443246395987572\n",
      "Iteration 22892: loss = 0.0024432359070097995\n",
      "Iteration 22893: loss = 0.0024432254181459643\n",
      "Iteration 22894: loss = 0.0024432149293960653\n",
      "Iteration 22895: loss = 0.0024432044407601008\n",
      "Iteration 22896: loss = 0.00244319395223807\n",
      "Iteration 22897: loss = 0.0024431834638299708\n",
      "Iteration 22898: loss = 0.0024431729755358027\n",
      "Iteration 22899: loss = 0.002443162487355563\n",
      "Iteration 22900: loss = 0.002443151999289252\n",
      "Iteration 22901: loss = 0.0024431415113368667\n",
      "Iteration 22902: loss = 0.002443131023498407\n",
      "Iteration 22903: loss = 0.002443120535773871\n",
      "Iteration 22904: loss = 0.002443110048163257\n",
      "Iteration 22905: loss = 0.002443099560666564\n",
      "Iteration 22906: loss = 0.002443089073283791\n",
      "Iteration 22907: loss = 0.002443078586014936\n",
      "Iteration 22908: loss = 0.002443068098859997\n",
      "Iteration 22909: loss = 0.002443057611818974\n",
      "Iteration 22910: loss = 0.002443047124891866\n",
      "Iteration 22911: loss = 0.0024430366380786693\n",
      "Iteration 22912: loss = 0.0024430261513793843\n",
      "Iteration 22913: loss = 0.0024430156647940087\n",
      "Iteration 22914: loss = 0.0024430051783225423\n",
      "Iteration 22915: loss = 0.002442994691964983\n",
      "Iteration 22916: loss = 0.0024429842057213293\n",
      "Iteration 22917: loss = 0.00244297371959158\n",
      "Iteration 22918: loss = 0.0024429632335757337\n",
      "Iteration 22919: loss = 0.002442952747673789\n",
      "Iteration 22920: loss = 0.0024429422618857445\n",
      "Iteration 22921: loss = 0.002442931776211599\n",
      "Iteration 22922: loss = 0.002442921290651351\n",
      "Iteration 22923: loss = 0.0024429108052049986\n",
      "Iteration 22924: loss = 0.0024429003198725417\n",
      "Iteration 22925: loss = 0.0024428898346539776\n",
      "Iteration 22926: loss = 0.0024428793495493055\n",
      "Iteration 22927: loss = 0.002442868864558524\n",
      "Iteration 22928: loss = 0.0024428583796816316\n",
      "Iteration 22929: loss = 0.002442847894918627\n",
      "Iteration 22930: loss = 0.00244283741026951\n",
      "Iteration 22931: loss = 0.002442826925734277\n",
      "Iteration 22932: loss = 0.0024428164413129276\n",
      "Iteration 22933: loss = 0.0024428059570054607\n",
      "Iteration 22934: loss = 0.0024427954728118745\n",
      "Iteration 22935: loss = 0.0024427849887321677\n",
      "Iteration 22936: loss = 0.0024427745047663394\n",
      "Iteration 22937: loss = 0.002442764020914388\n",
      "Iteration 22938: loss = 0.002442753537176312\n",
      "Iteration 22939: loss = 0.00244274305355211\n",
      "Iteration 22940: loss = 0.0024427325700417802\n",
      "Iteration 22941: loss = 0.002442722086645322\n",
      "Iteration 22942: loss = 0.0024427116033627337\n",
      "Iteration 22943: loss = 0.0024427011201940134\n",
      "Iteration 22944: loss = 0.002442690637139161\n",
      "Iteration 22945: loss = 0.0024426801541981742\n",
      "Iteration 22946: loss = 0.0024426696713710514\n",
      "Iteration 22947: loss = 0.002442659188657792\n",
      "Iteration 22948: loss = 0.0024426487060583944\n",
      "Iteration 22949: loss = 0.0024426382235728564\n",
      "Iteration 22950: loss = 0.0024426277412011774\n",
      "Iteration 22951: loss = 0.0024426172589433557\n",
      "Iteration 22952: loss = 0.0024426067767993904\n",
      "Iteration 22953: loss = 0.0024425962947692793\n",
      "Iteration 22954: loss = 0.0024425858128530216\n",
      "Iteration 22955: loss = 0.0024425753310506173\n",
      "Iteration 22956: loss = 0.002442564849362062\n",
      "Iteration 22957: loss = 0.0024425543677873566\n",
      "Iteration 22958: loss = 0.0024425438863264985\n",
      "Iteration 22959: loss = 0.002442533404979487\n",
      "Iteration 22960: loss = 0.0024425229237463207\n",
      "Iteration 22961: loss = 0.002442512442626998\n",
      "Iteration 22962: loss = 0.002442501961621518\n",
      "Iteration 22963: loss = 0.002442491480729878\n",
      "Iteration 22964: loss = 0.002442480999952078\n",
      "Iteration 22965: loss = 0.0024424705192881155\n",
      "Iteration 22966: loss = 0.002442460038737991\n",
      "Iteration 22967: loss = 0.0024424495583017015\n",
      "Iteration 22968: loss = 0.0024424390779792457\n",
      "Iteration 22969: loss = 0.002442428597770623\n",
      "Iteration 22970: loss = 0.002442418117675831\n",
      "Iteration 22971: loss = 0.0024424076376948693\n",
      "Iteration 22972: loss = 0.0024423971578277355\n",
      "Iteration 22973: loss = 0.00244238667807443\n",
      "Iteration 22974: loss = 0.002442376198434949\n",
      "Iteration 22975: loss = 0.002442365718909293\n",
      "Iteration 22976: loss = 0.0024423552394974594\n",
      "Iteration 22977: loss = 0.0024423447601994485\n",
      "Iteration 22978: loss = 0.002442334281015256\n",
      "Iteration 22979: loss = 0.002442323801944884\n",
      "Iteration 22980: loss = 0.002442313322988329\n",
      "Iteration 22981: loss = 0.0024423028441455897\n",
      "Iteration 22982: loss = 0.002442292365416666\n",
      "Iteration 22983: loss = 0.002442281886801555\n",
      "Iteration 22984: loss = 0.002442271408300256\n",
      "Iteration 22985: loss = 0.002442260929912768\n",
      "Iteration 22986: loss = 0.002442250451639088\n",
      "Iteration 22987: loss = 0.0024422399734792166\n",
      "Iteration 22988: loss = 0.0024422294954331513\n",
      "Iteration 22989: loss = 0.0024422190175008916\n",
      "Iteration 22990: loss = 0.002442208539682436\n",
      "Iteration 22991: loss = 0.0024421980619777814\n",
      "Iteration 22992: loss = 0.0024421875843869288\n",
      "Iteration 22993: loss = 0.002442177106909875\n",
      "Iteration 22994: loss = 0.0024421666295466197\n",
      "Iteration 22995: loss = 0.002442156152297161\n",
      "Iteration 22996: loss = 0.0024421456751614976\n",
      "Iteration 22997: loss = 0.0024421351981396286\n",
      "Iteration 22998: loss = 0.0024421247212315523\n",
      "Iteration 22999: loss = 0.002442114244437267\n",
      "Iteration 23000: loss = 0.0024421037677567715\n",
      "Iteration 23001: loss = 0.002442093291190065\n",
      "Iteration 23002: loss = 0.0024420828147371444\n",
      "Iteration 23003: loss = 0.00244207233839801\n",
      "Iteration 23004: loss = 0.002442061862172661\n",
      "Iteration 23005: loss = 0.002442051386061094\n",
      "Iteration 23006: loss = 0.002442040910063309\n",
      "Iteration 23007: loss = 0.002442030434179304\n",
      "Iteration 23008: loss = 0.0024420199584090784\n",
      "Iteration 23009: loss = 0.0024420094827526297\n",
      "Iteration 23010: loss = 0.0024419990072099573\n",
      "Iteration 23011: loss = 0.0024419885317810593\n",
      "Iteration 23012: loss = 0.0024419780564659353\n",
      "Iteration 23013: loss = 0.002441967581264583\n",
      "Iteration 23014: loss = 0.002441957106177001\n",
      "Iteration 23015: loss = 0.002441946631203188\n",
      "Iteration 23016: loss = 0.0024419361563431437\n",
      "Iteration 23017: loss = 0.002441925681596865\n",
      "Iteration 23018: loss = 0.0024419152069643517\n",
      "Iteration 23019: loss = 0.002441904732445602\n",
      "Iteration 23020: loss = 0.002441894258040615\n",
      "Iteration 23021: loss = 0.002441883783749388\n",
      "Iteration 23022: loss = 0.0024418733095719213\n",
      "Iteration 23023: loss = 0.0024418628355082125\n",
      "Iteration 23024: loss = 0.0024418523615582608\n",
      "Iteration 23025: loss = 0.002441841887722064\n",
      "Iteration 23026: loss = 0.0024418314139996217\n",
      "Iteration 23027: loss = 0.0024418209403909317\n",
      "Iteration 23028: loss = 0.0024418104668959936\n",
      "Iteration 23029: loss = 0.002441799993514805\n",
      "Iteration 23030: loss = 0.002441789520247365\n",
      "Iteration 23031: loss = 0.0024417790470936712\n",
      "Iteration 23032: loss = 0.002441768574053724\n",
      "Iteration 23033: loss = 0.0024417581011275213\n",
      "Iteration 23034: loss = 0.0024417476283150616\n",
      "Iteration 23035: loss = 0.0024417371556163434\n",
      "Iteration 23036: loss = 0.0024417266830313654\n",
      "Iteration 23037: loss = 0.002441716210560126\n",
      "Iteration 23038: loss = 0.002441705738202624\n",
      "Iteration 23039: loss = 0.002441695265958859\n",
      "Iteration 23040: loss = 0.002441684793828828\n",
      "Iteration 23041: loss = 0.0024416743218125306\n",
      "Iteration 23042: loss = 0.002441663849909965\n",
      "Iteration 23043: loss = 0.0024416533781211297\n",
      "Iteration 23044: loss = 0.002441642906446024\n",
      "Iteration 23045: loss = 0.0024416324348846465\n",
      "Iteration 23046: loss = 0.0024416219634369953\n",
      "Iteration 23047: loss = 0.002441611492103069\n",
      "Iteration 23048: loss = 0.0024416010208828665\n",
      "Iteration 23049: loss = 0.002441590549776386\n",
      "Iteration 23050: loss = 0.002441580078783627\n",
      "Iteration 23051: loss = 0.0024415696079045867\n",
      "Iteration 23052: loss = 0.0024415591371392655\n",
      "Iteration 23053: loss = 0.0024415486664876606\n",
      "Iteration 23054: loss = 0.002441538195949771\n",
      "Iteration 23055: loss = 0.002441527725525596\n",
      "Iteration 23056: loss = 0.0024415172552151336\n",
      "Iteration 23057: loss = 0.0024415067850183823\n",
      "Iteration 23058: loss = 0.002441496314935341\n",
      "Iteration 23059: loss = 0.002441485844966008\n",
      "Iteration 23060: loss = 0.002441475375110382\n",
      "Iteration 23061: loss = 0.0024414649053684625\n",
      "Iteration 23062: loss = 0.002441454435740247\n",
      "Iteration 23063: loss = 0.0024414439662257346\n",
      "Iteration 23064: loss = 0.002441433496824924\n",
      "Iteration 23065: loss = 0.0024414230275378126\n",
      "Iteration 23066: loss = 0.0024414125583644016\n",
      "Iteration 23067: loss = 0.002441402089304687\n",
      "Iteration 23068: loss = 0.0024413916203586687\n",
      "Iteration 23069: loss = 0.002441381151526346\n",
      "Iteration 23070: loss = 0.0024413706828077157\n",
      "Iteration 23071: loss = 0.002441360214202778\n",
      "Iteration 23072: loss = 0.002441349745711531\n",
      "Iteration 23073: loss = 0.002441339277333973\n",
      "Iteration 23074: loss = 0.0024413288090701026\n",
      "Iteration 23075: loss = 0.002441318340919919\n",
      "Iteration 23076: loss = 0.0024413078728834203\n",
      "Iteration 23077: loss = 0.0024412974049606057\n",
      "Iteration 23078: loss = 0.002441286937151473\n",
      "Iteration 23079: loss = 0.002441276469456022\n",
      "Iteration 23080: loss = 0.0024412660018742495\n",
      "Iteration 23081: loss = 0.002441255534406156\n",
      "Iteration 23082: loss = 0.0024412450670517395\n",
      "Iteration 23083: loss = 0.002441234599810998\n",
      "Iteration 23084: loss = 0.002441224132683931\n",
      "Iteration 23085: loss = 0.0024412136656705363\n",
      "Iteration 23086: loss = 0.002441203198770813\n",
      "Iteration 23087: loss = 0.0024411927319847593\n",
      "Iteration 23088: loss = 0.002441182265312375\n",
      "Iteration 23089: loss = 0.0024411717987536576\n",
      "Iteration 23090: loss = 0.002441161332308606\n",
      "Iteration 23091: loss = 0.0024411508659772188\n",
      "Iteration 23092: loss = 0.0024411403997594947\n",
      "Iteration 23093: loss = 0.002441129933655432\n",
      "Iteration 23094: loss = 0.0024411194676650294\n",
      "Iteration 23095: loss = 0.0024411090017882864\n",
      "Iteration 23096: loss = 0.0024410985360252004\n",
      "Iteration 23097: loss = 0.002441088070375771\n",
      "Iteration 23098: loss = 0.0024410776048399965\n",
      "Iteration 23099: loss = 0.002441067139417875\n",
      "Iteration 23100: loss = 0.0024410566741094055\n",
      "Iteration 23101: loss = 0.0024410462089145867\n",
      "Iteration 23102: loss = 0.0024410357438334174\n",
      "Iteration 23103: loss = 0.002441025278865896\n",
      "Iteration 23104: loss = 0.002441014814012021\n",
      "Iteration 23105: loss = 0.0024410043492717913\n",
      "Iteration 23106: loss = 0.002440993884645205\n",
      "Iteration 23107: loss = 0.0024409834201322617\n",
      "Iteration 23108: loss = 0.0024409729557329594\n",
      "Iteration 23109: loss = 0.002440962491447296\n",
      "Iteration 23110: loss = 0.002440952027275271\n",
      "Iteration 23111: loss = 0.0024409415632168836\n",
      "Iteration 23112: loss = 0.0024409310992721312\n",
      "Iteration 23113: loss = 0.0024409206354410132\n",
      "Iteration 23114: loss = 0.0024409101717235275\n",
      "Iteration 23115: loss = 0.0024408997081196734\n",
      "Iteration 23116: loss = 0.002440889244629449\n",
      "Iteration 23117: loss = 0.0024408787812528537\n",
      "Iteration 23118: loss = 0.002440868317989885\n",
      "Iteration 23119: loss = 0.0024408578548405432\n",
      "Iteration 23120: loss = 0.0024408473918048255\n",
      "Iteration 23121: loss = 0.0024408369288827303\n",
      "Iteration 23122: loss = 0.002440826466074258\n",
      "Iteration 23123: loss = 0.0024408160033794046\n",
      "Iteration 23124: loss = 0.002440805540798171\n",
      "Iteration 23125: loss = 0.0024407950783305547\n",
      "Iteration 23126: loss = 0.002440784615976555\n",
      "Iteration 23127: loss = 0.00244077415373617\n",
      "Iteration 23128: loss = 0.0024407636916093985\n",
      "Iteration 23129: loss = 0.0024407532295962387\n",
      "Iteration 23130: loss = 0.0024407427676966897\n",
      "Iteration 23131: loss = 0.00244073230591075\n",
      "Iteration 23132: loss = 0.0024407218442384186\n",
      "Iteration 23133: loss = 0.0024407113826796934\n",
      "Iteration 23134: loss = 0.002440700921234574\n",
      "Iteration 23135: loss = 0.0024406904599030576\n",
      "Iteration 23136: loss = 0.002440679998685144\n",
      "Iteration 23137: loss = 0.002440669537580832\n",
      "Iteration 23138: loss = 0.0024406590765901188\n",
      "Iteration 23139: loss = 0.002440648615713004\n",
      "Iteration 23140: loss = 0.002440638154949487\n",
      "Iteration 23141: loss = 0.0024406276942995652\n",
      "Iteration 23142: loss = 0.0024406172337632375\n",
      "Iteration 23143: loss = 0.0024406067733405024\n",
      "Iteration 23144: loss = 0.002440596313031359\n",
      "Iteration 23145: loss = 0.0024405858528358053\n",
      "Iteration 23146: loss = 0.0024405753927538403\n",
      "Iteration 23147: loss = 0.0024405649327854627\n",
      "Iteration 23148: loss = 0.0024405544729306713\n",
      "Iteration 23149: loss = 0.002440544013189464\n",
      "Iteration 23150: loss = 0.0024405335535618395\n",
      "Iteration 23151: loss = 0.0024405230940477978\n",
      "Iteration 23152: loss = 0.0024405126346473357\n",
      "Iteration 23153: loss = 0.0024405021753604534\n",
      "Iteration 23154: loss = 0.002440491716187148\n",
      "Iteration 23155: loss = 0.002440481257127419\n",
      "Iteration 23156: loss = 0.002440470798181265\n",
      "Iteration 23157: loss = 0.0024404603393486848\n",
      "Iteration 23158: loss = 0.0024404498806296763\n",
      "Iteration 23159: loss = 0.002440439422024239\n",
      "Iteration 23160: loss = 0.0024404289635323703\n",
      "Iteration 23161: loss = 0.0024404185051540707\n",
      "Iteration 23162: loss = 0.002440408046889337\n",
      "Iteration 23163: loss = 0.002440397588738169\n",
      "Iteration 23164: loss = 0.002440387130700565\n",
      "Iteration 23165: loss = 0.0024403766727765226\n",
      "Iteration 23166: loss = 0.002440366214966042\n",
      "Iteration 23167: loss = 0.002440355757269121\n",
      "Iteration 23168: loss = 0.0024403452996857584\n",
      "Iteration 23169: loss = 0.002440334842215953\n",
      "Iteration 23170: loss = 0.0024403243848597025\n",
      "Iteration 23171: loss = 0.002440313927617006\n",
      "Iteration 23172: loss = 0.002440303470487864\n",
      "Iteration 23173: loss = 0.0024402930134722724\n",
      "Iteration 23174: loss = 0.002440282556570231\n",
      "Iteration 23175: loss = 0.0024402720997817383\n",
      "Iteration 23176: loss = 0.002440261643106793\n",
      "Iteration 23177: loss = 0.0024402511865453938\n",
      "Iteration 23178: loss = 0.0024402407300975386\n",
      "Iteration 23179: loss = 0.0024402302737632276\n",
      "Iteration 23180: loss = 0.0024402198175424578\n",
      "Iteration 23181: loss = 0.0024402093614352286\n",
      "Iteration 23182: loss = 0.0024401989054415388\n",
      "Iteration 23183: loss = 0.002440188449561386\n",
      "Iteration 23184: loss = 0.0024401779937947704\n",
      "Iteration 23185: loss = 0.0024401675381416892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23186: loss = 0.002440157082602142\n",
      "Iteration 23187: loss = 0.0024401466271761264\n",
      "Iteration 23188: loss = 0.0024401361718636417\n",
      "Iteration 23189: loss = 0.002440125716664687\n",
      "Iteration 23190: loss = 0.0024401152615792605\n",
      "Iteration 23191: loss = 0.0024401048066073597\n",
      "Iteration 23192: loss = 0.0024400943517489844\n",
      "Iteration 23193: loss = 0.0024400838970041337\n",
      "Iteration 23194: loss = 0.0024400734423728056\n",
      "Iteration 23195: loss = 0.002440062987854998\n",
      "Iteration 23196: loss = 0.002440052533450711\n",
      "Iteration 23197: loss = 0.002440042079159942\n",
      "Iteration 23198: loss = 0.0024400316249826902\n",
      "Iteration 23199: loss = 0.002440021170918954\n",
      "Iteration 23200: loss = 0.002440010716968732\n",
      "Iteration 23201: loss = 0.0024400002631320226\n",
      "Iteration 23202: loss = 0.002439989809408825\n",
      "Iteration 23203: loss = 0.002439979355799138\n",
      "Iteration 23204: loss = 0.0024399689023029596\n",
      "Iteration 23205: loss = 0.002439958448920289\n",
      "Iteration 23206: loss = 0.0024399479956511236\n",
      "Iteration 23207: loss = 0.0024399375424954636\n",
      "Iteration 23208: loss = 0.002439927089453306\n",
      "Iteration 23209: loss = 0.002439916636524651\n",
      "Iteration 23210: loss = 0.0024399061837094968\n",
      "Iteration 23211: loss = 0.002439895731007841\n",
      "Iteration 23212: loss = 0.0024398852784196833\n",
      "Iteration 23213: loss = 0.0024398748259450227\n",
      "Iteration 23214: loss = 0.002439864373583856\n",
      "Iteration 23215: loss = 0.0024398539213361836\n",
      "Iteration 23216: loss = 0.0024398434692020033\n",
      "Iteration 23217: loss = 0.002439833017181314\n",
      "Iteration 23218: loss = 0.002439822565274114\n",
      "Iteration 23219: loss = 0.0024398121134804026\n",
      "Iteration 23220: loss = 0.0024398016618001773\n",
      "Iteration 23221: loss = 0.0024397912102334385\n",
      "Iteration 23222: loss = 0.0024397807587801827\n",
      "Iteration 23223: loss = 0.0024397703074404095\n",
      "Iteration 23224: loss = 0.002439759856214118\n",
      "Iteration 23225: loss = 0.0024397494051013064\n",
      "Iteration 23226: loss = 0.002439738954101973\n",
      "Iteration 23227: loss = 0.0024397285032161176\n",
      "Iteration 23228: loss = 0.0024397180524437373\n",
      "Iteration 23229: loss = 0.002439707601784831\n",
      "Iteration 23230: loss = 0.0024396971512393984\n",
      "Iteration 23231: loss = 0.0024396867008074368\n",
      "Iteration 23232: loss = 0.0024396762504889464\n",
      "Iteration 23233: loss = 0.0024396658002839243\n",
      "Iteration 23234: loss = 0.0024396553501923697\n",
      "Iteration 23235: loss = 0.0024396449002142807\n",
      "Iteration 23236: loss = 0.002439634450349657\n",
      "Iteration 23237: loss = 0.002439624000598497\n",
      "Iteration 23238: loss = 0.0024396135509607984\n",
      "Iteration 23239: loss = 0.002439603101436561\n",
      "Iteration 23240: loss = 0.0024395926520257823\n",
      "Iteration 23241: loss = 0.0024395822027284617\n",
      "Iteration 23242: loss = 0.0024395717535445976\n",
      "Iteration 23243: loss = 0.0024395613044741883\n",
      "Iteration 23244: loss = 0.002439550855517233\n",
      "Iteration 23245: loss = 0.00243954040667373\n",
      "Iteration 23246: loss = 0.0024395299579436786\n",
      "Iteration 23247: loss = 0.002439519509327076\n",
      "Iteration 23248: loss = 0.002439509060823922\n",
      "Iteration 23249: loss = 0.0024394986124342153\n",
      "Iteration 23250: loss = 0.002439488164157953\n",
      "Iteration 23251: loss = 0.002439477715995136\n",
      "Iteration 23252: loss = 0.0024394672679457605\n",
      "Iteration 23253: loss = 0.0024394568200098274\n",
      "Iteration 23254: loss = 0.0024394463721873335\n",
      "Iteration 23255: loss = 0.0024394359244782784\n",
      "Iteration 23256: loss = 0.002439425476882661\n",
      "Iteration 23257: loss = 0.0024394150294004785\n",
      "Iteration 23258: loss = 0.002439404582031731\n",
      "Iteration 23259: loss = 0.002439394134776417\n",
      "Iteration 23260: loss = 0.0024393836876345346\n",
      "Iteration 23261: loss = 0.0024393732406060827\n",
      "Iteration 23262: loss = 0.002439362793691059\n",
      "Iteration 23263: loss = 0.0024393523468894634\n",
      "Iteration 23264: loss = 0.002439341900201294\n",
      "Iteration 23265: loss = 0.0024393314536265494\n",
      "Iteration 23266: loss = 0.002439321007165228\n",
      "Iteration 23267: loss = 0.002439310560817329\n",
      "Iteration 23268: loss = 0.0024393001145828507\n",
      "Iteration 23269: loss = 0.0024392896684617914\n",
      "Iteration 23270: loss = 0.0024392792224541505\n",
      "Iteration 23271: loss = 0.0024392687765599263\n",
      "Iteration 23272: loss = 0.002439258330779117\n",
      "Iteration 23273: loss = 0.002439247885111721\n",
      "Iteration 23274: loss = 0.002439237439557738\n",
      "Iteration 23275: loss = 0.0024392269941171663\n",
      "Iteration 23276: loss = 0.002439216548790004\n",
      "Iteration 23277: loss = 0.00243920610357625\n",
      "Iteration 23278: loss = 0.0024391956584759035\n",
      "Iteration 23279: loss = 0.002439185213488962\n",
      "Iteration 23280: loss = 0.002439174768615425\n",
      "Iteration 23281: loss = 0.002439164323855291\n",
      "Iteration 23282: loss = 0.002439153879208558\n",
      "Iteration 23283: loss = 0.0024391434346752246\n",
      "Iteration 23284: loss = 0.002439132990255291\n",
      "Iteration 23285: loss = 0.0024391225459487543\n",
      "Iteration 23286: loss = 0.0024391121017556133\n",
      "Iteration 23287: loss = 0.002439101657675867\n",
      "Iteration 23288: loss = 0.0024390912137095145\n",
      "Iteration 23289: loss = 0.002439080769856553\n",
      "Iteration 23290: loss = 0.002439070326116982\n",
      "Iteration 23291: loss = 0.0024390598824908004\n",
      "Iteration 23292: loss = 0.0024390494389780063\n",
      "Iteration 23293: loss = 0.0024390389955785984\n",
      "Iteration 23294: loss = 0.002439028552292576\n",
      "Iteration 23295: loss = 0.002439018109119937\n",
      "Iteration 23296: loss = 0.0024390076660606797\n",
      "Iteration 23297: loss = 0.0024389972231148036\n",
      "Iteration 23298: loss = 0.0024389867802823064\n",
      "Iteration 23299: loss = 0.0024389763375631877\n",
      "Iteration 23300: loss = 0.002438965894957446\n",
      "Iteration 23301: loss = 0.0024389554524650793\n",
      "Iteration 23302: loss = 0.0024389450100860866\n",
      "Iteration 23303: loss = 0.0024389345678204667\n",
      "Iteration 23304: loss = 0.0024389241256682166\n",
      "Iteration 23305: loss = 0.0024389136836293376\n",
      "Iteration 23306: loss = 0.0024389032417038267\n",
      "Iteration 23307: loss = 0.0024388927998916835\n",
      "Iteration 23308: loss = 0.002438882358192905\n",
      "Iteration 23309: loss = 0.0024388719166074913\n",
      "Iteration 23310: loss = 0.0024388614751354406\n",
      "Iteration 23311: loss = 0.0024388510337767515\n",
      "Iteration 23312: loss = 0.002438840592531422\n",
      "Iteration 23313: loss = 0.002438830151399452\n",
      "Iteration 23314: loss = 0.0024388197103808393\n",
      "Iteration 23315: loss = 0.002438809269475582\n",
      "Iteration 23316: loss = 0.0024387988286836805\n",
      "Iteration 23317: loss = 0.002438788388005132\n",
      "Iteration 23318: loss = 0.0024387779474399347\n",
      "Iteration 23319: loss = 0.002438767506988088\n",
      "Iteration 23320: loss = 0.0024387570666495914\n",
      "Iteration 23321: loss = 0.0024387466264244417\n",
      "Iteration 23322: loss = 0.002438736186312639\n",
      "Iteration 23323: loss = 0.0024387257463141807\n",
      "Iteration 23324: loss = 0.0024387153064290667\n",
      "Iteration 23325: loss = 0.002438704866657295\n",
      "Iteration 23326: loss = 0.0024386944269988637\n",
      "Iteration 23327: loss = 0.0024386839874537725\n",
      "Iteration 23328: loss = 0.0024386735480220194\n",
      "Iteration 23329: loss = 0.002438663108703603\n",
      "Iteration 23330: loss = 0.0024386526694985216\n",
      "Iteration 23331: loss = 0.002438642230406775\n",
      "Iteration 23332: loss = 0.0024386317914283605\n",
      "Iteration 23333: loss = 0.0024386213525632774\n",
      "Iteration 23334: loss = 0.0024386109138115246\n",
      "Iteration 23335: loss = 0.0024386004751730996\n",
      "Iteration 23336: loss = 0.002438590036648002\n",
      "Iteration 23337: loss = 0.0024385795982362307\n",
      "Iteration 23338: loss = 0.0024385691599377833\n",
      "Iteration 23339: loss = 0.0024385587217526593\n",
      "Iteration 23340: loss = 0.0024385482836808575\n",
      "Iteration 23341: loss = 0.0024385378457223747\n",
      "Iteration 23342: loss = 0.0024385274078772114\n",
      "Iteration 23343: loss = 0.0024385169701453655\n",
      "Iteration 23344: loss = 0.002438506532526836\n",
      "Iteration 23345: loss = 0.0024384960950216217\n",
      "Iteration 23346: loss = 0.0024384856576297204\n",
      "Iteration 23347: loss = 0.002438475220351131\n",
      "Iteration 23348: loss = 0.002438464783185852\n",
      "Iteration 23349: loss = 0.0024384543461338833\n",
      "Iteration 23350: loss = 0.002438443909195222\n",
      "Iteration 23351: loss = 0.0024384334723698666\n",
      "Iteration 23352: loss = 0.0024384230356578173\n",
      "Iteration 23353: loss = 0.002438412599059071\n",
      "Iteration 23354: loss = 0.002438402162573628\n",
      "Iteration 23355: loss = 0.0024383917262014855\n",
      "Iteration 23356: loss = 0.0024383812899426427\n",
      "Iteration 23357: loss = 0.0024383708537970977\n",
      "Iteration 23358: loss = 0.0024383604177648506\n",
      "Iteration 23359: loss = 0.0024383499818458987\n",
      "Iteration 23360: loss = 0.002438339546040241\n",
      "Iteration 23361: loss = 0.0024383291103478753\n",
      "Iteration 23362: loss = 0.0024383186747688025\n",
      "Iteration 23363: loss = 0.0024383082393030185\n",
      "Iteration 23364: loss = 0.0024382978039505236\n",
      "Iteration 23365: loss = 0.002438287368711316\n",
      "Iteration 23366: loss = 0.0024382769335853944\n",
      "Iteration 23367: loss = 0.0024382664985727574\n",
      "Iteration 23368: loss = 0.002438256063673403\n",
      "Iteration 23369: loss = 0.002438245628887331\n",
      "Iteration 23370: loss = 0.0024382351942145394\n",
      "Iteration 23371: loss = 0.0024382247596550265\n",
      "Iteration 23372: loss = 0.002438214325208792\n",
      "Iteration 23373: loss = 0.002438203890875833\n",
      "Iteration 23374: loss = 0.0024381934566561495\n",
      "Iteration 23375: loss = 0.002438183022549739\n",
      "Iteration 23376: loss = 0.0024381725885566008\n",
      "Iteration 23377: loss = 0.002438162154676733\n",
      "Iteration 23378: loss = 0.002438151720910136\n",
      "Iteration 23379: loss = 0.0024381412872568056\n",
      "Iteration 23380: loss = 0.002438130853716742\n",
      "Iteration 23381: loss = 0.002438120420289944\n",
      "Iteration 23382: loss = 0.0024381099869764107\n",
      "Iteration 23383: loss = 0.002438099553776139\n",
      "Iteration 23384: loss = 0.0024380891206891293\n",
      "Iteration 23385: loss = 0.0024380786877153783\n",
      "Iteration 23386: loss = 0.002438068254854887\n",
      "Iteration 23387: loss = 0.002438057822107651\n",
      "Iteration 23388: loss = 0.002438047389473672\n",
      "Iteration 23389: loss = 0.002438036956952947\n",
      "Iteration 23390: loss = 0.0024380265245454755\n",
      "Iteration 23391: loss = 0.0024380160922512547\n",
      "Iteration 23392: loss = 0.0024380056600702845\n",
      "Iteration 23393: loss = 0.0024379952280025627\n",
      "Iteration 23394: loss = 0.002437984796048089\n",
      "Iteration 23395: loss = 0.0024379743642068603\n",
      "Iteration 23396: loss = 0.0024379639324788767\n",
      "Iteration 23397: loss = 0.002437953500864137\n",
      "Iteration 23398: loss = 0.002437943069362639\n",
      "Iteration 23399: loss = 0.002437932637974381\n",
      "Iteration 23400: loss = 0.002437922206699363\n",
      "Iteration 23401: loss = 0.002437911775537582\n",
      "Iteration 23402: loss = 0.002437901344489038\n",
      "Iteration 23403: loss = 0.0024378909135537286\n",
      "Iteration 23404: loss = 0.002437880482731653\n",
      "Iteration 23405: loss = 0.0024378700520228105\n",
      "Iteration 23406: loss = 0.002437859621427198\n",
      "Iteration 23407: loss = 0.0024378491909448157\n",
      "Iteration 23408: loss = 0.002437838760575661\n",
      "Iteration 23409: loss = 0.002437828330319733\n",
      "Iteration 23410: loss = 0.002437817900177031\n",
      "Iteration 23411: loss = 0.002437807470147553\n",
      "Iteration 23412: loss = 0.0024377970402312973\n",
      "Iteration 23413: loss = 0.0024377866104282634\n",
      "Iteration 23414: loss = 0.002437776180738449\n",
      "Iteration 23415: loss = 0.002437765751161853\n",
      "Iteration 23416: loss = 0.0024377553216984745\n",
      "Iteration 23417: loss = 0.002437744892348312\n",
      "Iteration 23418: loss = 0.0024377344631113644\n",
      "Iteration 23419: loss = 0.0024377240339876283\n",
      "Iteration 23420: loss = 0.002437713604977105\n",
      "Iteration 23421: loss = 0.002437703176079792\n",
      "Iteration 23422: loss = 0.002437692747295688\n",
      "Iteration 23423: loss = 0.002437682318624791\n",
      "Iteration 23424: loss = 0.0024376718900671005\n",
      "Iteration 23425: loss = 0.002437661461622615\n",
      "Iteration 23426: loss = 0.002437651033291333\n",
      "Iteration 23427: loss = 0.0024376406050732524\n",
      "Iteration 23428: loss = 0.002437630176968373\n",
      "Iteration 23429: loss = 0.002437619748976693\n",
      "Iteration 23430: loss = 0.0024376093210982113\n",
      "Iteration 23431: loss = 0.0024375988933329255\n",
      "Iteration 23432: loss = 0.0024375884656808347\n",
      "Iteration 23433: loss = 0.0024375780381419385\n",
      "Iteration 23434: loss = 0.0024375676107162344\n",
      "Iteration 23435: loss = 0.0024375571834037214\n",
      "Iteration 23436: loss = 0.002437546756204398\n",
      "Iteration 23437: loss = 0.002437536329118263\n",
      "Iteration 23438: loss = 0.0024375259021453153\n",
      "Iteration 23439: loss = 0.002437515475285552\n",
      "Iteration 23440: loss = 0.002437505048538974\n",
      "Iteration 23441: loss = 0.002437494621905579\n",
      "Iteration 23442: loss = 0.0024374841953853647\n",
      "Iteration 23443: loss = 0.002437473768978331\n",
      "Iteration 23444: loss = 0.002437463342684476\n",
      "Iteration 23445: loss = 0.002437452916503798\n",
      "Iteration 23446: loss = 0.0024374424904362963\n",
      "Iteration 23447: loss = 0.002437432064481969\n",
      "Iteration 23448: loss = 0.0024374216386408147\n",
      "Iteration 23449: loss = 0.0024374112129128328\n",
      "Iteration 23450: loss = 0.002437400787298021\n",
      "Iteration 23451: loss = 0.0024373903617963787\n",
      "Iteration 23452: loss = 0.002437379936407904\n",
      "Iteration 23453: loss = 0.002437369511132595\n",
      "Iteration 23454: loss = 0.0024373590859704513\n",
      "Iteration 23455: loss = 0.002437348660921472\n",
      "Iteration 23456: loss = 0.002437338235985653\n",
      "Iteration 23457: loss = 0.0024373278111629966\n",
      "Iteration 23458: loss = 0.002437317386453499\n",
      "Iteration 23459: loss = 0.0024373069618571597\n",
      "Iteration 23460: loss = 0.0024372965373739767\n",
      "Iteration 23461: loss = 0.00243728611300395\n",
      "Iteration 23462: loss = 0.0024372756887470763\n",
      "Iteration 23463: loss = 0.002437265264603356\n",
      "Iteration 23464: loss = 0.002437254840572786\n",
      "Iteration 23465: loss = 0.0024372444166553663\n",
      "Iteration 23466: loss = 0.002437233992851095\n",
      "Iteration 23467: loss = 0.0024372235691599714\n",
      "Iteration 23468: loss = 0.002437213145581993\n",
      "Iteration 23469: loss = 0.0024372027221171593\n",
      "Iteration 23470: loss = 0.0024371922987654684\n",
      "Iteration 23471: loss = 0.002437181875526919\n",
      "Iteration 23472: loss = 0.00243717145240151\n",
      "Iteration 23473: loss = 0.00243716102938924\n",
      "Iteration 23474: loss = 0.002437150606490107\n",
      "Iteration 23475: loss = 0.002437140183704111\n",
      "Iteration 23476: loss = 0.002437129761031249\n",
      "Iteration 23477: loss = 0.002437119338471521\n",
      "Iteration 23478: loss = 0.002437108916024924\n",
      "Iteration 23479: loss = 0.0024370984936914583\n",
      "Iteration 23480: loss = 0.002437088071471122\n",
      "Iteration 23481: loss = 0.002437077649363913\n",
      "Iteration 23482: loss = 0.002437067227369831\n",
      "Iteration 23483: loss = 0.0024370568054888743\n",
      "Iteration 23484: loss = 0.00243704638372104\n",
      "Iteration 23485: loss = 0.00243703596206633\n",
      "Iteration 23486: loss = 0.0024370255405247398\n",
      "Iteration 23487: loss = 0.0024370151190962696\n",
      "Iteration 23488: loss = 0.0024370046977809177\n",
      "Iteration 23489: loss = 0.0024369942765786823\n",
      "Iteration 23490: loss = 0.002436983855489563\n",
      "Iteration 23491: loss = 0.0024369734345135578\n",
      "Iteration 23492: loss = 0.002436963013650665\n",
      "Iteration 23493: loss = 0.0024369525929008834\n",
      "Iteration 23494: loss = 0.002436942172264212\n",
      "Iteration 23495: loss = 0.0024369317517406496\n",
      "Iteration 23496: loss = 0.002436921331330194\n",
      "Iteration 23497: loss = 0.002436910911032845\n",
      "Iteration 23498: loss = 0.0024369004908485997\n",
      "Iteration 23499: loss = 0.0024368900707774587\n",
      "Iteration 23500: loss = 0.0024368796508194186\n",
      "Iteration 23501: loss = 0.002436869230974479\n",
      "Iteration 23502: loss = 0.0024368588112426387\n",
      "Iteration 23503: loss = 0.002436848391623896\n",
      "Iteration 23504: loss = 0.002436837972118249\n",
      "Iteration 23505: loss = 0.0024368275527256976\n",
      "Iteration 23506: loss = 0.0024368171334462397\n",
      "Iteration 23507: loss = 0.002436806714279874\n",
      "Iteration 23508: loss = 0.0024367962952265986\n",
      "Iteration 23509: loss = 0.0024367858762864127\n",
      "Iteration 23510: loss = 0.002436775457459315\n",
      "Iteration 23511: loss = 0.0024367650387453043\n",
      "Iteration 23512: loss = 0.0024367546201443783\n",
      "Iteration 23513: loss = 0.0024367442016565368\n",
      "Iteration 23514: loss = 0.002436733783281778\n",
      "Iteration 23515: loss = 0.0024367233650201\n",
      "Iteration 23516: loss = 0.0024367129468715015\n",
      "Iteration 23517: loss = 0.002436702528835982\n",
      "Iteration 23518: loss = 0.002436692110913539\n",
      "Iteration 23519: loss = 0.0024366816931041726\n",
      "Iteration 23520: loss = 0.0024366712754078797\n",
      "Iteration 23521: loss = 0.00243666085782466\n",
      "Iteration 23522: loss = 0.002436650440354512\n",
      "Iteration 23523: loss = 0.0024366400229974337\n",
      "Iteration 23524: loss = 0.002436629605753425\n",
      "Iteration 23525: loss = 0.002436619188622483\n",
      "Iteration 23526: loss = 0.0024366087716046077\n",
      "Iteration 23527: loss = 0.0024365983546997968\n",
      "Iteration 23528: loss = 0.002436587937908049\n",
      "Iteration 23529: loss = 0.0024365775212293636\n",
      "Iteration 23530: loss = 0.0024365671046637388\n",
      "Iteration 23531: loss = 0.002436556688211173\n",
      "Iteration 23532: loss = 0.002436546271871665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23533: loss = 0.002436535855645213\n",
      "Iteration 23534: loss = 0.002436525439531817\n",
      "Iteration 23535: loss = 0.0024365150235314747\n",
      "Iteration 23536: loss = 0.0024365046076441836\n",
      "Iteration 23537: loss = 0.002436494191869945\n",
      "Iteration 23538: loss = 0.0024364837762087554\n",
      "Iteration 23539: loss = 0.002436473360660613\n",
      "Iteration 23540: loss = 0.0024364629452255193\n",
      "Iteration 23541: loss = 0.00243645252990347\n",
      "Iteration 23542: loss = 0.002436442114694465\n",
      "Iteration 23543: loss = 0.0024364316995985017\n",
      "Iteration 23544: loss = 0.0024364212846155813\n",
      "Iteration 23545: loss = 0.0024364108697457003\n",
      "Iteration 23546: loss = 0.002436400454988858\n",
      "Iteration 23547: loss = 0.0024363900403450533\n",
      "Iteration 23548: loss = 0.002436379625814284\n",
      "Iteration 23549: loss = 0.002436369211396549\n",
      "Iteration 23550: loss = 0.002436358797091848\n",
      "Iteration 23551: loss = 0.0024363483829001778\n",
      "Iteration 23552: loss = 0.0024363379688215384\n",
      "Iteration 23553: loss = 0.0024363275548559275\n",
      "Iteration 23554: loss = 0.002436317141003345\n",
      "Iteration 23555: loss = 0.0024363067272637883\n",
      "Iteration 23556: loss = 0.002436296313637257\n",
      "Iteration 23557: loss = 0.0024362859001237492\n",
      "Iteration 23558: loss = 0.002436275486723263\n",
      "Iteration 23559: loss = 0.0024362650734357978\n",
      "Iteration 23560: loss = 0.0024362546602613518\n",
      "Iteration 23561: loss = 0.0024362442471999243\n",
      "Iteration 23562: loss = 0.0024362338342515132\n",
      "Iteration 23563: loss = 0.0024362234214161172\n",
      "Iteration 23564: loss = 0.002436213008693736\n",
      "Iteration 23565: loss = 0.002436202596084366\n",
      "Iteration 23566: loss = 0.0024361921835880074\n",
      "Iteration 23567: loss = 0.002436181771204659\n",
      "Iteration 23568: loss = 0.0024361713589343193\n",
      "Iteration 23569: loss = 0.0024361609467769864\n",
      "Iteration 23570: loss = 0.002436150534732659\n",
      "Iteration 23571: loss = 0.0024361401228013358\n",
      "Iteration 23572: loss = 0.002436129710983016\n",
      "Iteration 23573: loss = 0.0024361192992776977\n",
      "Iteration 23574: loss = 0.002436108887685379\n",
      "Iteration 23575: loss = 0.0024360984762060597\n",
      "Iteration 23576: loss = 0.0024360880648397376\n",
      "Iteration 23577: loss = 0.002436077653586411\n",
      "Iteration 23578: loss = 0.0024360672424460802\n",
      "Iteration 23579: loss = 0.0024360568314187423\n",
      "Iteration 23580: loss = 0.0024360464205043964\n",
      "Iteration 23581: loss = 0.002436036009703041\n",
      "Iteration 23582: loss = 0.0024360255990146743\n",
      "Iteration 23583: loss = 0.002436015188439296\n",
      "Iteration 23584: loss = 0.002436004777976904\n",
      "Iteration 23585: loss = 0.0024359943676274974\n",
      "Iteration 23586: loss = 0.002435983957391074\n",
      "Iteration 23587: loss = 0.0024359735472676333\n",
      "Iteration 23588: loss = 0.0024359631372571733\n",
      "Iteration 23589: loss = 0.002435952727359693\n",
      "Iteration 23590: loss = 0.002435942317575191\n",
      "Iteration 23591: loss = 0.0024359319079036656\n",
      "Iteration 23592: loss = 0.0024359214983451162\n",
      "Iteration 23593: loss = 0.0024359110888995403\n",
      "Iteration 23594: loss = 0.002435900679566938\n",
      "Iteration 23595: loss = 0.002435890270347306\n",
      "Iteration 23596: loss = 0.0024358798612406445\n",
      "Iteration 23597: loss = 0.0024358694522469515\n",
      "Iteration 23598: loss = 0.002435859043366226\n",
      "Iteration 23599: loss = 0.002435848634598466\n",
      "Iteration 23600: loss = 0.0024358382259436705\n",
      "Iteration 23601: loss = 0.0024358278174018387\n",
      "Iteration 23602: loss = 0.002435817408972968\n",
      "Iteration 23603: loss = 0.0024358070006570585\n",
      "Iteration 23604: loss = 0.0024357965924541075\n",
      "Iteration 23605: loss = 0.002435786184364114\n",
      "Iteration 23606: loss = 0.0024357757763870773\n",
      "Iteration 23607: loss = 0.0024357653685229946\n",
      "Iteration 23608: loss = 0.002435754960771866\n",
      "Iteration 23609: loss = 0.002435744553133689\n",
      "Iteration 23610: loss = 0.002435734145608464\n",
      "Iteration 23611: loss = 0.0024357237381961877\n",
      "Iteration 23612: loss = 0.002435713330896859\n",
      "Iteration 23613: loss = 0.002435702923710477\n",
      "Iteration 23614: loss = 0.0024356925166370406\n",
      "Iteration 23615: loss = 0.0024356821096765484\n",
      "Iteration 23616: loss = 0.0024356717028289985\n",
      "Iteration 23617: loss = 0.0024356612960943894\n",
      "Iteration 23618: loss = 0.0024356508894727213\n",
      "Iteration 23619: loss = 0.002435640482963991\n",
      "Iteration 23620: loss = 0.002435630076568197\n",
      "Iteration 23621: loss = 0.0024356196702853394\n",
      "Iteration 23622: loss = 0.002435609264115416\n",
      "Iteration 23623: loss = 0.0024355988580584253\n",
      "Iteration 23624: loss = 0.0024355884521143666\n",
      "Iteration 23625: loss = 0.002435578046283238\n",
      "Iteration 23626: loss = 0.0024355676405650377\n",
      "Iteration 23627: loss = 0.0024355572349597653\n",
      "Iteration 23628: loss = 0.002435546829467419\n",
      "Iteration 23629: loss = 0.0024355364240879972\n",
      "Iteration 23630: loss = 0.002435526018821499\n",
      "Iteration 23631: loss = 0.002435515613667923\n",
      "Iteration 23632: loss = 0.002435505208627267\n",
      "Iteration 23633: loss = 0.0024354948036995304\n",
      "Iteration 23634: loss = 0.002435484398884712\n",
      "Iteration 23635: loss = 0.0024354739941828095\n",
      "Iteration 23636: loss = 0.0024354635895938227\n",
      "Iteration 23637: loss = 0.002435453185117749\n",
      "Iteration 23638: loss = 0.0024354427807545876\n",
      "Iteration 23639: loss = 0.002435432376504338\n",
      "Iteration 23640: loss = 0.002435421972366998\n",
      "Iteration 23641: loss = 0.002435411568342566\n",
      "Iteration 23642: loss = 0.00243540116443104\n",
      "Iteration 23643: loss = 0.0024353907606324207\n",
      "Iteration 23644: loss = 0.002435380356946705\n",
      "Iteration 23645: loss = 0.002435369953373892\n",
      "Iteration 23646: loss = 0.0024353595499139805\n",
      "Iteration 23647: loss = 0.002435349146566969\n",
      "Iteration 23648: loss = 0.002435338743332856\n",
      "Iteration 23649: loss = 0.0024353283402116406\n",
      "Iteration 23650: loss = 0.0024353179372033212\n",
      "Iteration 23651: loss = 0.002435307534307896\n",
      "Iteration 23652: loss = 0.0024352971315253643\n",
      "Iteration 23653: loss = 0.0024352867288557237\n",
      "Iteration 23654: loss = 0.002435276326298974\n",
      "Iteration 23655: loss = 0.0024352659238551133\n",
      "Iteration 23656: loss = 0.0024352555215241406\n",
      "Iteration 23657: loss = 0.0024352451193060533\n",
      "Iteration 23658: loss = 0.0024352347172008518\n",
      "Iteration 23659: loss = 0.0024352243152085335\n",
      "Iteration 23660: loss = 0.0024352139133290976\n",
      "Iteration 23661: loss = 0.002435203511562542\n",
      "Iteration 23662: loss = 0.0024351931099088667\n",
      "Iteration 23663: loss = 0.0024351827083680688\n",
      "Iteration 23664: loss = 0.0024351723069401475\n",
      "Iteration 23665: loss = 0.0024351619056251013\n",
      "Iteration 23666: loss = 0.0024351515044229296\n",
      "Iteration 23667: loss = 0.0024351411033336308\n",
      "Iteration 23668: loss = 0.0024351307023572025\n",
      "Iteration 23669: loss = 0.0024351203014936445\n",
      "Iteration 23670: loss = 0.0024351099007429546\n",
      "Iteration 23671: loss = 0.0024350995001051323\n",
      "Iteration 23672: loss = 0.002435089099580176\n",
      "Iteration 23673: loss = 0.002435078699168083\n",
      "Iteration 23674: loss = 0.0024350682988688536\n",
      "Iteration 23675: loss = 0.0024350578986824857\n",
      "Iteration 23676: loss = 0.002435047498608978\n",
      "Iteration 23677: loss = 0.0024350370986483292\n",
      "Iteration 23678: loss = 0.002435026698800538\n",
      "Iteration 23679: loss = 0.0024350162990656025\n",
      "Iteration 23680: loss = 0.0024350058994435228\n",
      "Iteration 23681: loss = 0.002434995499934295\n",
      "Iteration 23682: loss = 0.00243498510053792\n",
      "Iteration 23683: loss = 0.002434974701254396\n",
      "Iteration 23684: loss = 0.0024349643020837213\n",
      "Iteration 23685: loss = 0.002434953903025894\n",
      "Iteration 23686: loss = 0.0024349435040809135\n",
      "Iteration 23687: loss = 0.002434933105248777\n",
      "Iteration 23688: loss = 0.002434922706529486\n",
      "Iteration 23689: loss = 0.002434912307923037\n",
      "Iteration 23690: loss = 0.002434901909429428\n",
      "Iteration 23691: loss = 0.0024348915110486593\n",
      "Iteration 23692: loss = 0.0024348811127807295\n",
      "Iteration 23693: loss = 0.002434870714625636\n",
      "Iteration 23694: loss = 0.0024348603165833785\n",
      "Iteration 23695: loss = 0.0024348499186539543\n",
      "Iteration 23696: loss = 0.002434839520837364\n",
      "Iteration 23697: loss = 0.002434829123133604\n",
      "Iteration 23698: loss = 0.002434818725542675\n",
      "Iteration 23699: loss = 0.0024348083280645747\n",
      "Iteration 23700: loss = 0.0024347979306993013\n",
      "Iteration 23701: loss = 0.0024347875334468534\n",
      "Iteration 23702: loss = 0.002434777136307231\n",
      "Iteration 23703: loss = 0.0024347667392804312\n",
      "Iteration 23704: loss = 0.002434756342366454\n",
      "Iteration 23705: loss = 0.0024347459455652965\n",
      "Iteration 23706: loss = 0.0024347355488769594\n",
      "Iteration 23707: loss = 0.0024347251523014384\n",
      "Iteration 23708: loss = 0.0024347147558387346\n",
      "Iteration 23709: loss = 0.0024347043594888456\n",
      "Iteration 23710: loss = 0.0024346939632517704\n",
      "Iteration 23711: loss = 0.0024346835671275073\n",
      "Iteration 23712: loss = 0.002434673171116055\n",
      "Iteration 23713: loss = 0.002434662775217412\n",
      "Iteration 23714: loss = 0.002434652379431577\n",
      "Iteration 23715: loss = 0.0024346419837585494\n",
      "Iteration 23716: loss = 0.0024346315881983273\n",
      "Iteration 23717: loss = 0.0024346211927509086\n",
      "Iteration 23718: loss = 0.002434610797416293\n",
      "Iteration 23719: loss = 0.0024346004021944785\n",
      "Iteration 23720: loss = 0.0024345900070854637\n",
      "Iteration 23721: loss = 0.002434579612089248\n",
      "Iteration 23722: loss = 0.002434569217205829\n",
      "Iteration 23723: loss = 0.002434558822435206\n",
      "Iteration 23724: loss = 0.0024345484277773777\n",
      "Iteration 23725: loss = 0.0024345380332323418\n",
      "Iteration 23726: loss = 0.0024345276388000976\n",
      "Iteration 23727: loss = 0.002434517244480644\n",
      "Iteration 23728: loss = 0.0024345068502739796\n",
      "Iteration 23729: loss = 0.0024344964561801023\n",
      "Iteration 23730: loss = 0.002434486062199011\n",
      "Iteration 23731: loss = 0.002434475668330705\n",
      "Iteration 23732: loss = 0.002434465274575183\n",
      "Iteration 23733: loss = 0.002434454880932442\n",
      "Iteration 23734: loss = 0.002434444487402482\n",
      "Iteration 23735: loss = 0.002434434093985302\n",
      "Iteration 23736: loss = 0.0024344237006808994\n",
      "Iteration 23737: loss = 0.002434413307489274\n",
      "Iteration 23738: loss = 0.002434402914410423\n",
      "Iteration 23739: loss = 0.002434392521444346\n",
      "Iteration 23740: loss = 0.002434382128591042\n",
      "Iteration 23741: loss = 0.002434371735850509\n",
      "Iteration 23742: loss = 0.0024343613432227457\n",
      "Iteration 23743: loss = 0.0024343509507077504\n",
      "Iteration 23744: loss = 0.002434340558305522\n",
      "Iteration 23745: loss = 0.00243433016601606\n",
      "Iteration 23746: loss = 0.002434319773839362\n",
      "Iteration 23747: loss = 0.002434309381775427\n",
      "Iteration 23748: loss = 0.002434298989824253\n",
      "Iteration 23749: loss = 0.0024342885979858394\n",
      "Iteration 23750: loss = 0.002434278206260185\n",
      "Iteration 23751: loss = 0.002434267814647288\n",
      "Iteration 23752: loss = 0.0024342574231471467\n",
      "Iteration 23753: loss = 0.0024342470317597602\n",
      "Iteration 23754: loss = 0.002434236640485127\n",
      "Iteration 23755: loss = 0.002434226249323245\n",
      "Iteration 23756: loss = 0.0024342158582741145\n",
      "Iteration 23757: loss = 0.002434205467337733\n",
      "Iteration 23758: loss = 0.002434195076514099\n",
      "Iteration 23759: loss = 0.002434184685803212\n",
      "Iteration 23760: loss = 0.0024341742952050695\n",
      "Iteration 23761: loss = 0.002434163904719671\n",
      "Iteration 23762: loss = 0.002434153514347015\n",
      "Iteration 23763: loss = 0.0024341431240870997\n",
      "Iteration 23764: loss = 0.0024341327339399243\n",
      "Iteration 23765: loss = 0.0024341223439054865\n",
      "Iteration 23766: loss = 0.002434111953983786\n",
      "Iteration 23767: loss = 0.002434101564174821\n",
      "Iteration 23768: loss = 0.0024340911744785898\n",
      "Iteration 23769: loss = 0.0024340807848950924\n",
      "Iteration 23770: loss = 0.002434070395424325\n",
      "Iteration 23771: loss = 0.0024340600060662886\n",
      "Iteration 23772: loss = 0.0024340496168209803\n",
      "Iteration 23773: loss = 0.002434039227688399\n",
      "Iteration 23774: loss = 0.0024340288386685447\n",
      "Iteration 23775: loss = 0.0024340184497614143\n",
      "Iteration 23776: loss = 0.0024340080609670067\n",
      "Iteration 23777: loss = 0.002433997672285321\n",
      "Iteration 23778: loss = 0.0024339872837163563\n",
      "Iteration 23779: loss = 0.0024339768952601096\n",
      "Iteration 23780: loss = 0.0024339665069165813\n",
      "Iteration 23781: loss = 0.00243395611868577\n",
      "Iteration 23782: loss = 0.002433945730567672\n",
      "Iteration 23783: loss = 0.0024339353425622892\n",
      "Iteration 23784: loss = 0.0024339249546696177\n",
      "Iteration 23785: loss = 0.002433914566889657\n",
      "Iteration 23786: loss = 0.0024339041792224058\n",
      "Iteration 23787: loss = 0.002433893791667863\n",
      "Iteration 23788: loss = 0.0024338834042260267\n",
      "Iteration 23789: loss = 0.0024338730168968954\n",
      "Iteration 23790: loss = 0.0024338626296804685\n",
      "Iteration 23791: loss = 0.0024338522425767445\n",
      "Iteration 23792: loss = 0.002433841855585721\n",
      "Iteration 23793: loss = 0.002433831468707398\n",
      "Iteration 23794: loss = 0.002433821081941773\n",
      "Iteration 23795: loss = 0.002433810695288846\n",
      "Iteration 23796: loss = 0.0024338003087486133\n",
      "Iteration 23797: loss = 0.0024337899223210763\n",
      "Iteration 23798: loss = 0.0024337795360062312\n",
      "Iteration 23799: loss = 0.0024337691498040786\n",
      "Iteration 23800: loss = 0.002433758763714616\n",
      "Iteration 23801: loss = 0.002433748377737842\n",
      "Iteration 23802: loss = 0.0024337379918737564\n",
      "Iteration 23803: loss = 0.002433727606122356\n",
      "Iteration 23804: loss = 0.0024337172204836406\n",
      "Iteration 23805: loss = 0.002433706834957609\n",
      "Iteration 23806: loss = 0.0024336964495442594\n",
      "Iteration 23807: loss = 0.00243368606424359\n",
      "Iteration 23808: loss = 0.0024336756790556\n",
      "Iteration 23809: loss = 0.0024336652939802883\n",
      "Iteration 23810: loss = 0.002433654909017653\n",
      "Iteration 23811: loss = 0.0024336445241676936\n",
      "Iteration 23812: loss = 0.002433634139430407\n",
      "Iteration 23813: loss = 0.002433623754805793\n",
      "Iteration 23814: loss = 0.002433613370293851\n",
      "Iteration 23815: loss = 0.0024336029858945778\n",
      "Iteration 23816: loss = 0.002433592601607973\n",
      "Iteration 23817: loss = 0.002433582217434036\n",
      "Iteration 23818: loss = 0.002433571833372764\n",
      "Iteration 23819: loss = 0.0024335614494241558\n",
      "Iteration 23820: loss = 0.0024335510655882115\n",
      "Iteration 23821: loss = 0.002433540681864928\n",
      "Iteration 23822: loss = 0.0024335302982543047\n",
      "Iteration 23823: loss = 0.00243351991475634\n",
      "Iteration 23824: loss = 0.002433509531371033\n",
      "Iteration 23825: loss = 0.002433499148098382\n",
      "Iteration 23826: loss = 0.0024334887649383858\n",
      "Iteration 23827: loss = 0.002433478381891043\n",
      "Iteration 23828: loss = 0.0024334679989563516\n",
      "Iteration 23829: loss = 0.0024334576161343102\n",
      "Iteration 23830: loss = 0.002433447233424919\n",
      "Iteration 23831: loss = 0.0024334368508281755\n",
      "Iteration 23832: loss = 0.0024334264683440786\n",
      "Iteration 23833: loss = 0.002433416085972626\n",
      "Iteration 23834: loss = 0.0024334057037138177\n",
      "Iteration 23835: loss = 0.0024333953215676515\n",
      "Iteration 23836: loss = 0.002433384939534127\n",
      "Iteration 23837: loss = 0.002433374557613241\n",
      "Iteration 23838: loss = 0.0024333641758049938\n",
      "Iteration 23839: loss = 0.0024333537941093834\n",
      "Iteration 23840: loss = 0.002433343412526408\n",
      "Iteration 23841: loss = 0.0024333330310560676\n",
      "Iteration 23842: loss = 0.0024333226496983593\n",
      "Iteration 23843: loss = 0.0024333122684532826\n",
      "Iteration 23844: loss = 0.0024333018873208354\n",
      "Iteration 23845: loss = 0.002433291506301018\n",
      "Iteration 23846: loss = 0.0024332811253938266\n",
      "Iteration 23847: loss = 0.002433270744599262\n",
      "Iteration 23848: loss = 0.0024332603639173216\n",
      "Iteration 23849: loss = 0.002433249983348004\n",
      "Iteration 23850: loss = 0.0024332396028913087\n",
      "Iteration 23851: loss = 0.0024332292225472335\n",
      "Iteration 23852: loss = 0.002433218842315777\n",
      "Iteration 23853: loss = 0.002433208462196939\n",
      "Iteration 23854: loss = 0.002433198082190717\n",
      "Iteration 23855: loss = 0.0024331877022971096\n",
      "Iteration 23856: loss = 0.002433177322516116\n",
      "Iteration 23857: loss = 0.002433166942847735\n",
      "Iteration 23858: loss = 0.0024331565632919644\n",
      "Iteration 23859: loss = 0.002433146183848803\n",
      "Iteration 23860: loss = 0.00243313580451825\n",
      "Iteration 23861: loss = 0.002433125425300304\n",
      "Iteration 23862: loss = 0.002433115046194963\n",
      "Iteration 23863: loss = 0.0024331046672022264\n",
      "Iteration 23864: loss = 0.0024330942883220917\n",
      "Iteration 23865: loss = 0.002433083909554559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23866: loss = 0.002433073530899626\n",
      "Iteration 23867: loss = 0.002433063152357291\n",
      "Iteration 23868: loss = 0.0024330527739275533\n",
      "Iteration 23869: loss = 0.002433042395610412\n",
      "Iteration 23870: loss = 0.0024330320174058644\n",
      "Iteration 23871: loss = 0.0024330216393139096\n",
      "Iteration 23872: loss = 0.002433011261334547\n",
      "Iteration 23873: loss = 0.0024330008834677753\n",
      "Iteration 23874: loss = 0.002432990505713591\n",
      "Iteration 23875: loss = 0.0024329801280719957\n",
      "Iteration 23876: loss = 0.0024329697505429855\n",
      "Iteration 23877: loss = 0.0024329593731265606\n",
      "Iteration 23878: loss = 0.0024329489958227193\n",
      "Iteration 23879: loss = 0.00243293861863146\n",
      "Iteration 23880: loss = 0.0024329282415527817\n",
      "Iteration 23881: loss = 0.0024329178645866823\n",
      "Iteration 23882: loss = 0.0024329074877331605\n",
      "Iteration 23883: loss = 0.002432897110992216\n",
      "Iteration 23884: loss = 0.0024328867343638458\n",
      "Iteration 23885: loss = 0.0024328763578480507\n",
      "Iteration 23886: loss = 0.0024328659814448267\n",
      "Iteration 23887: loss = 0.0024328556051541753\n",
      "Iteration 23888: loss = 0.0024328452289760936\n",
      "Iteration 23889: loss = 0.0024328348529105786\n",
      "Iteration 23890: loss = 0.0024328244769576323\n",
      "Iteration 23891: loss = 0.002432814101117251\n",
      "Iteration 23892: loss = 0.002432803725389434\n",
      "Iteration 23893: loss = 0.0024327933497741796\n",
      "Iteration 23894: loss = 0.0024327829742714874\n",
      "Iteration 23895: loss = 0.0024327725988813552\n",
      "Iteration 23896: loss = 0.0024327622236037815\n",
      "Iteration 23897: loss = 0.002432751848438765\n",
      "Iteration 23898: loss = 0.0024327414733863054\n",
      "Iteration 23899: loss = 0.0024327310984464\n",
      "Iteration 23900: loss = 0.0024327207236190473\n",
      "Iteration 23901: loss = 0.0024327103489042478\n",
      "Iteration 23902: loss = 0.002432699974301998\n",
      "Iteration 23903: loss = 0.0024326895998122975\n",
      "Iteration 23904: loss = 0.0024326792254351455\n",
      "Iteration 23905: loss = 0.0024326688511705397\n",
      "Iteration 23906: loss = 0.002432658477018479\n",
      "Iteration 23907: loss = 0.0024326481029789617\n",
      "Iteration 23908: loss = 0.0024326377290519864\n",
      "Iteration 23909: loss = 0.002432627355237553\n",
      "Iteration 23910: loss = 0.002432616981535659\n",
      "Iteration 23911: loss = 0.002432606607946303\n",
      "Iteration 23912: loss = 0.0024325962344694846\n",
      "Iteration 23913: loss = 0.002432585861105201\n",
      "Iteration 23914: loss = 0.0024325754878534523\n",
      "Iteration 23915: loss = 0.0024325651147142358\n",
      "Iteration 23916: loss = 0.0024325547416875508\n",
      "Iteration 23917: loss = 0.0024325443687733955\n",
      "Iteration 23918: loss = 0.002432533995971769\n",
      "Iteration 23919: loss = 0.0024325236232826705\n",
      "Iteration 23920: loss = 0.0024325132507060977\n",
      "Iteration 23921: loss = 0.0024325028782420494\n",
      "Iteration 23922: loss = 0.0024324925058905236\n",
      "Iteration 23923: loss = 0.00243248213365152\n",
      "Iteration 23924: loss = 0.0024324717615250378\n",
      "Iteration 23925: loss = 0.002432461389511074\n",
      "Iteration 23926: loss = 0.0024324510176096286\n",
      "Iteration 23927: loss = 0.0024324406458206987\n",
      "Iteration 23928: loss = 0.0024324302741442843\n",
      "Iteration 23929: loss = 0.0024324199025803827\n",
      "Iteration 23930: loss = 0.002432409531128994\n",
      "Iteration 23931: loss = 0.0024323991597901165\n",
      "Iteration 23932: loss = 0.0024323887885637483\n",
      "Iteration 23933: loss = 0.0024323784174498883\n",
      "Iteration 23934: loss = 0.002432368046448535\n",
      "Iteration 23935: loss = 0.002432357675559687\n",
      "Iteration 23936: loss = 0.002432347304783343\n",
      "Iteration 23937: loss = 0.0024323369341195016\n",
      "Iteration 23938: loss = 0.002432326563568162\n",
      "Iteration 23939: loss = 0.0024323161931293223\n",
      "Iteration 23940: loss = 0.002432305822802981\n",
      "Iteration 23941: loss = 0.0024322954525891364\n",
      "Iteration 23942: loss = 0.0024322850824877885\n",
      "Iteration 23943: loss = 0.0024322747124989347\n",
      "Iteration 23944: loss = 0.002432264342622574\n",
      "Iteration 23945: loss = 0.002432253972858705\n",
      "Iteration 23946: loss = 0.0024322436032073267\n",
      "Iteration 23947: loss = 0.0024322332336684373\n",
      "Iteration 23948: loss = 0.0024322228642420347\n",
      "Iteration 23949: loss = 0.0024322124949281193\n",
      "Iteration 23950: loss = 0.0024322021257266887\n",
      "Iteration 23951: loss = 0.0024321917566377414\n",
      "Iteration 23952: loss = 0.002432181387661276\n",
      "Iteration 23953: loss = 0.002432171018797292\n",
      "Iteration 23954: loss = 0.002432160650045787\n",
      "Iteration 23955: loss = 0.0024321502814067604\n",
      "Iteration 23956: loss = 0.00243213991288021\n",
      "Iteration 23957: loss = 0.0024321295444661357\n",
      "Iteration 23958: loss = 0.0024321191761645347\n",
      "Iteration 23959: loss = 0.0024321088079754063\n",
      "Iteration 23960: loss = 0.0024320984398987495\n",
      "Iteration 23961: loss = 0.0024320880719345623\n",
      "Iteration 23962: loss = 0.0024320777040828436\n",
      "Iteration 23963: loss = 0.002432067336343592\n",
      "Iteration 23964: loss = 0.0024320569687168057\n",
      "Iteration 23965: loss = 0.0024320466012024847\n",
      "Iteration 23966: loss = 0.0024320362338006263\n",
      "Iteration 23967: loss = 0.002432025866511229\n",
      "Iteration 23968: loss = 0.0024320154993342927\n",
      "Iteration 23969: loss = 0.002432005132269815\n",
      "Iteration 23970: loss = 0.0024319947653177946\n",
      "Iteration 23971: loss = 0.002431984398478231\n",
      "Iteration 23972: loss = 0.0024319740317511216\n",
      "Iteration 23973: loss = 0.002431963665136466\n",
      "Iteration 23974: loss = 0.002431953298634262\n",
      "Iteration 23975: loss = 0.002431942932244509\n",
      "Iteration 23976: loss = 0.002431932565967205\n",
      "Iteration 23977: loss = 0.00243192219980235\n",
      "Iteration 23978: loss = 0.0024319118337499403\n",
      "Iteration 23979: loss = 0.0024319014678099767\n",
      "Iteration 23980: loss = 0.002431891101982456\n",
      "Iteration 23981: loss = 0.002431880736267379\n",
      "Iteration 23982: loss = 0.0024318703706647424\n",
      "Iteration 23983: loss = 0.002431860005174546\n",
      "Iteration 23984: loss = 0.0024318496397967874\n",
      "Iteration 23985: loss = 0.002431839274531466\n",
      "Iteration 23986: loss = 0.0024318289093785803\n",
      "Iteration 23987: loss = 0.0024318185443381293\n",
      "Iteration 23988: loss = 0.0024318081794101106\n",
      "Iteration 23989: loss = 0.002431797814594523\n",
      "Iteration 23990: loss = 0.0024317874498913667\n",
      "Iteration 23991: loss = 0.002431777085300639\n",
      "Iteration 23992: loss = 0.002431766720822338\n",
      "Iteration 23993: loss = 0.002431756356456464\n",
      "Iteration 23994: loss = 0.002431745992203014\n",
      "Iteration 23995: loss = 0.0024317356280619874\n",
      "Iteration 23996: loss = 0.0024317252640333833\n",
      "Iteration 23997: loss = 0.0024317149001171993\n",
      "Iteration 23998: loss = 0.0024317045363134344\n",
      "Iteration 23999: loss = 0.002431694172622088\n",
      "Iteration 24000: loss = 0.0024316838090431576\n",
      "Iteration 24001: loss = 0.0024316734455766425\n",
      "Iteration 24002: loss = 0.002431663082222541\n",
      "Iteration 24003: loss = 0.002431652718980852\n",
      "Iteration 24004: loss = 0.0024316423558515743\n",
      "Iteration 24005: loss = 0.002431631992834706\n",
      "Iteration 24006: loss = 0.002431621629930246\n",
      "Iteration 24007: loss = 0.0024316112671381933\n",
      "Iteration 24008: loss = 0.0024316009044585457\n",
      "Iteration 24009: loss = 0.0024315905418913018\n",
      "Iteration 24010: loss = 0.002431580179436461\n",
      "Iteration 24011: loss = 0.0024315698170940224\n",
      "Iteration 24012: loss = 0.0024315594548639835\n",
      "Iteration 24013: loss = 0.002431549092746343\n",
      "Iteration 24014: loss = 0.0024315387307411\n",
      "Iteration 24015: loss = 0.002431528368848253\n",
      "Iteration 24016: loss = 0.002431518007067801\n",
      "Iteration 24017: loss = 0.0024315076453997417\n",
      "Iteration 24018: loss = 0.0024314972838440745\n",
      "Iteration 24019: loss = 0.0024314869224007977\n",
      "Iteration 24020: loss = 0.0024314765610699103\n",
      "Iteration 24021: loss = 0.0024314661998514103\n",
      "Iteration 24022: loss = 0.0024314558387452966\n",
      "Iteration 24023: loss = 0.0024314454777515687\n",
      "Iteration 24024: loss = 0.0024314351168702236\n",
      "Iteration 24025: loss = 0.0024314247561012616\n",
      "Iteration 24026: loss = 0.0024314143954446803\n",
      "Iteration 24027: loss = 0.002431404034900478\n",
      "Iteration 24028: loss = 0.0024313936744686545\n",
      "Iteration 24029: loss = 0.0024313833141492076\n",
      "Iteration 24030: loss = 0.0024313729539421364\n",
      "Iteration 24031: loss = 0.0024313625938474385\n",
      "Iteration 24032: loss = 0.0024313522338651142\n",
      "Iteration 24033: loss = 0.002431341873995161\n",
      "Iteration 24034: loss = 0.002431331514237578\n",
      "Iteration 24035: loss = 0.002431321154592364\n",
      "Iteration 24036: loss = 0.0024313107950595164\n",
      "Iteration 24037: loss = 0.0024313004356390348\n",
      "Iteration 24038: loss = 0.002431290076330918\n",
      "Iteration 24039: loss = 0.002431279717135165\n",
      "Iteration 24040: loss = 0.0024312693580517725\n",
      "Iteration 24041: loss = 0.002431258999080741\n",
      "Iteration 24042: loss = 0.002431248640222068\n",
      "Iteration 24043: loss = 0.002431238281475754\n",
      "Iteration 24044: loss = 0.0024312279228417954\n",
      "Iteration 24045: loss = 0.002431217564320192\n",
      "Iteration 24046: loss = 0.0024312072059109423\n",
      "Iteration 24047: loss = 0.002431196847614044\n",
      "Iteration 24048: loss = 0.0024311864894294983\n",
      "Iteration 24049: loss = 0.0024311761313573\n",
      "Iteration 24050: loss = 0.0024311657733974514\n",
      "Iteration 24051: loss = 0.0024311554155499486\n",
      "Iteration 24052: loss = 0.002431145057814792\n",
      "Iteration 24053: loss = 0.0024311347001919788\n",
      "Iteration 24054: loss = 0.0024311243426815084\n",
      "Iteration 24055: loss = 0.0024311139852833796\n",
      "Iteration 24056: loss = 0.0024311036279975906\n",
      "Iteration 24057: loss = 0.0024310932708241396\n",
      "Iteration 24058: loss = 0.002431082913763026\n",
      "Iteration 24059: loss = 0.0024310725568142486\n",
      "Iteration 24060: loss = 0.0024310621999778055\n",
      "Iteration 24061: loss = 0.002431051843253695\n",
      "Iteration 24062: loss = 0.0024310414866419174\n",
      "Iteration 24063: loss = 0.002431031130142469\n",
      "Iteration 24064: loss = 0.00243102077375535\n",
      "Iteration 24065: loss = 0.0024310104174805587\n",
      "Iteration 24066: loss = 0.002431000061318094\n",
      "Iteration 24067: loss = 0.0024309897052679536\n",
      "Iteration 24068: loss = 0.0024309793493301366\n",
      "Iteration 24069: loss = 0.002430968993504642\n",
      "Iteration 24070: loss = 0.0024309586377914685\n",
      "Iteration 24071: loss = 0.0024309482821906142\n",
      "Iteration 24072: loss = 0.0024309379267020773\n",
      "Iteration 24073: loss = 0.0024309275713258575\n",
      "Iteration 24074: loss = 0.0024309172160619534\n",
      "Iteration 24075: loss = 0.002430906860910363\n",
      "Iteration 24076: loss = 0.002430896505871085\n",
      "Iteration 24077: loss = 0.002430886150944118\n",
      "Iteration 24078: loss = 0.0024308757961294617\n",
      "Iteration 24079: loss = 0.0024308654414271135\n",
      "Iteration 24080: loss = 0.0024308550868370727\n",
      "Iteration 24081: loss = 0.002430844732359337\n",
      "Iteration 24082: loss = 0.002430834377993906\n",
      "Iteration 24083: loss = 0.0024308240237407783\n",
      "Iteration 24084: loss = 0.002430813669599951\n",
      "Iteration 24085: loss = 0.002430803315571425\n",
      "Iteration 24086: loss = 0.002430792961655198\n",
      "Iteration 24087: loss = 0.0024307826078512683\n",
      "Iteration 24088: loss = 0.002430772254159635\n",
      "Iteration 24089: loss = 0.002430761900580296\n",
      "Iteration 24090: loss = 0.0024307515471132506\n",
      "Iteration 24091: loss = 0.0024307411937584977\n",
      "Iteration 24092: loss = 0.0024307308405160347\n",
      "Iteration 24093: loss = 0.002430720487385862\n",
      "Iteration 24094: loss = 0.002430710134367977\n",
      "Iteration 24095: loss = 0.002430699781462378\n",
      "Iteration 24096: loss = 0.0024306894286690653\n",
      "Iteration 24097: loss = 0.002430679075988035\n",
      "Iteration 24098: loss = 0.002430668723419288\n",
      "Iteration 24099: loss = 0.002430658370962823\n",
      "Iteration 24100: loss = 0.0024306480186186367\n",
      "Iteration 24101: loss = 0.002430637666386729\n",
      "Iteration 24102: loss = 0.0024306273142670988\n",
      "Iteration 24103: loss = 0.0024306169622597437\n",
      "Iteration 24104: loss = 0.0024306066103646634\n",
      "Iteration 24105: loss = 0.0024305962585818557\n",
      "Iteration 24106: loss = 0.0024305859069113193\n",
      "Iteration 24107: loss = 0.002430575555353054\n",
      "Iteration 24108: loss = 0.002430565203907057\n",
      "Iteration 24109: loss = 0.002430554852573327\n",
      "Iteration 24110: loss = 0.002430544501351864\n",
      "Iteration 24111: loss = 0.002430534150242665\n",
      "Iteration 24112: loss = 0.00243052379924573\n",
      "Iteration 24113: loss = 0.0024305134483610566\n",
      "Iteration 24114: loss = 0.002430503097588644\n",
      "Iteration 24115: loss = 0.0024304927469284902\n",
      "Iteration 24116: loss = 0.0024304823963805953\n",
      "Iteration 24117: loss = 0.0024304720459449565\n",
      "Iteration 24118: loss = 0.0024304616956215726\n",
      "Iteration 24119: loss = 0.0024304513454104426\n",
      "Iteration 24120: loss = 0.0024304409953115652\n",
      "Iteration 24121: loss = 0.002430430645324939\n",
      "Iteration 24122: loss = 0.002430420295450562\n",
      "Iteration 24123: loss = 0.002430409945688434\n",
      "Iteration 24124: loss = 0.0024303995960385525\n",
      "Iteration 24125: loss = 0.0024303892465009167\n",
      "Iteration 24126: loss = 0.0024303788970755254\n",
      "Iteration 24127: loss = 0.0024303685477623767\n",
      "Iteration 24128: loss = 0.0024303581985614703\n",
      "Iteration 24129: loss = 0.0024303478494728027\n",
      "Iteration 24130: loss = 0.0024303375004963747\n",
      "Iteration 24131: loss = 0.0024303271516321837\n",
      "Iteration 24132: loss = 0.0024303168028802294\n",
      "Iteration 24133: loss = 0.0024303064542405095\n",
      "Iteration 24134: loss = 0.0024302961057130223\n",
      "Iteration 24135: loss = 0.0024302857572977682\n",
      "Iteration 24136: loss = 0.002430275408994744\n",
      "Iteration 24137: loss = 0.0024302650608039487\n",
      "Iteration 24138: loss = 0.002430254712725382\n",
      "Iteration 24139: loss = 0.0024302443647590414\n",
      "Iteration 24140: loss = 0.0024302340169049257\n",
      "Iteration 24141: loss = 0.002430223669163034\n",
      "Iteration 24142: loss = 0.0024302133215333647\n",
      "Iteration 24143: loss = 0.0024302029740159163\n",
      "Iteration 24144: loss = 0.002430192626610688\n",
      "Iteration 24145: loss = 0.0024301822793176774\n",
      "Iteration 24146: loss = 0.0024301719321368842\n",
      "Iteration 24147: loss = 0.0024301615850683064\n",
      "Iteration 24148: loss = 0.0024301512381119427\n",
      "Iteration 24149: loss = 0.0024301408912677917\n",
      "Iteration 24150: loss = 0.002430130544535852\n",
      "Iteration 24151: loss = 0.002430120197916123\n",
      "Iteration 24152: loss = 0.0024301098514086026\n",
      "Iteration 24153: loss = 0.0024300995050132892\n",
      "Iteration 24154: loss = 0.002430089158730182\n",
      "Iteration 24155: loss = 0.002430078812559279\n",
      "Iteration 24156: loss = 0.00243006846650058\n",
      "Iteration 24157: loss = 0.002430058120554083\n",
      "Iteration 24158: loss = 0.0024300477747197856\n",
      "Iteration 24159: loss = 0.002430037428997688\n",
      "Iteration 24160: loss = 0.002430027083387788\n",
      "Iteration 24161: loss = 0.0024300167378900847\n",
      "Iteration 24162: loss = 0.002430006392504576\n",
      "Iteration 24163: loss = 0.0024299960472312612\n",
      "Iteration 24164: loss = 0.002429985702070139\n",
      "Iteration 24165: loss = 0.0024299753570212073\n",
      "Iteration 24166: loss = 0.002429965012084466\n",
      "Iteration 24167: loss = 0.002429954667259912\n",
      "Iteration 24168: loss = 0.002429944322547545\n",
      "Iteration 24169: loss = 0.0024299339779473646\n",
      "Iteration 24170: loss = 0.0024299236334593675\n",
      "Iteration 24171: loss = 0.0024299132890835532\n",
      "Iteration 24172: loss = 0.0024299029448199205\n",
      "Iteration 24173: loss = 0.002429892600668467\n",
      "Iteration 24174: loss = 0.002429882256629193\n",
      "Iteration 24175: loss = 0.0024298719127020963\n",
      "Iteration 24176: loss = 0.002429861568887176\n",
      "Iteration 24177: loss = 0.002429851225184429\n",
      "Iteration 24178: loss = 0.002429840881593856\n",
      "Iteration 24179: loss = 0.0024298305381154546\n",
      "Iteration 24180: loss = 0.0024298201947492238\n",
      "Iteration 24181: loss = 0.002429809851495163\n",
      "Iteration 24182: loss = 0.002429799508353268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24183: loss = 0.0024297891653235412\n",
      "Iteration 24184: loss = 0.002429778822405978\n",
      "Iteration 24185: loss = 0.0024297684796005794\n",
      "Iteration 24186: loss = 0.002429758136907343\n",
      "Iteration 24187: loss = 0.002429747794326268\n",
      "Iteration 24188: loss = 0.0024297374518573514\n",
      "Iteration 24189: loss = 0.0024297271095005935\n",
      "Iteration 24190: loss = 0.002429716767255992\n",
      "Iteration 24191: loss = 0.002429706425123547\n",
      "Iteration 24192: loss = 0.0024296960831032555\n",
      "Iteration 24193: loss = 0.002429685741195116\n",
      "Iteration 24194: loss = 0.002429675399399129\n",
      "Iteration 24195: loss = 0.002429665057715292\n",
      "Iteration 24196: loss = 0.0024296547161436026\n",
      "Iteration 24197: loss = 0.0024296443746840615\n",
      "Iteration 24198: loss = 0.0024296340333366654\n",
      "Iteration 24199: loss = 0.002429623692101415\n",
      "Iteration 24200: loss = 0.002429613350978307\n",
      "Iteration 24201: loss = 0.002429603009967341\n",
      "Iteration 24202: loss = 0.002429592669068515\n",
      "Iteration 24203: loss = 0.0024295823282818284\n",
      "Iteration 24204: loss = 0.0024295719876072795\n",
      "Iteration 24205: loss = 0.002429561647044867\n",
      "Iteration 24206: loss = 0.00242955130659459\n",
      "Iteration 24207: loss = 0.0024295409662564455\n",
      "Iteration 24208: loss = 0.0024295306260304337\n",
      "Iteration 24209: loss = 0.002429520285916553\n",
      "Iteration 24210: loss = 0.002429509945914802\n",
      "Iteration 24211: loss = 0.0024294996060251783\n",
      "Iteration 24212: loss = 0.0024294892662476823\n",
      "Iteration 24213: loss = 0.0024294789265823114\n",
      "Iteration 24214: loss = 0.002429468587029065\n",
      "Iteration 24215: loss = 0.00242945824758794\n",
      "Iteration 24216: loss = 0.002429447908258937\n",
      "Iteration 24217: loss = 0.0024294375690420544\n",
      "Iteration 24218: loss = 0.00242942722993729\n",
      "Iteration 24219: loss = 0.002429416890944643\n",
      "Iteration 24220: loss = 0.0024294065520641117\n",
      "Iteration 24221: loss = 0.002429396213295695\n",
      "Iteration 24222: loss = 0.0024293858746393913\n",
      "Iteration 24223: loss = 0.0024293755360951996\n",
      "Iteration 24224: loss = 0.0024293651976631182\n",
      "Iteration 24225: loss = 0.0024293548593431456\n",
      "Iteration 24226: loss = 0.002429344521135281\n",
      "Iteration 24227: loss = 0.0024293341830395228\n",
      "Iteration 24228: loss = 0.002429323845055869\n",
      "Iteration 24229: loss = 0.0024293135071843195\n",
      "Iteration 24230: loss = 0.0024293031694248715\n",
      "Iteration 24231: loss = 0.0024292928317775245\n",
      "Iteration 24232: loss = 0.002429282494242277\n",
      "Iteration 24233: loss = 0.002429272156819128\n",
      "Iteration 24234: loss = 0.002429261819508076\n",
      "Iteration 24235: loss = 0.0024292514823091188\n",
      "Iteration 24236: loss = 0.002429241145222255\n",
      "Iteration 24237: loss = 0.0024292308082474855\n",
      "Iteration 24238: loss = 0.0024292204713848064\n",
      "Iteration 24239: loss = 0.0024292101346342163\n",
      "Iteration 24240: loss = 0.0024291997979957163\n",
      "Iteration 24241: loss = 0.002429189461469303\n",
      "Iteration 24242: loss = 0.002429179125054975\n",
      "Iteration 24243: loss = 0.0024291687887527317\n",
      "Iteration 24244: loss = 0.002429158452562572\n",
      "Iteration 24245: loss = 0.002429148116484493\n",
      "Iteration 24246: loss = 0.002429137780518496\n",
      "Iteration 24247: loss = 0.002429127444664576\n",
      "Iteration 24248: loss = 0.0024291171089227347\n",
      "Iteration 24249: loss = 0.0024291067732929703\n",
      "Iteration 24250: loss = 0.0024290964377752804\n",
      "Iteration 24251: loss = 0.0024290861023696635\n",
      "Iteration 24252: loss = 0.002429075767076119\n",
      "Iteration 24253: loss = 0.0024290654318946453\n",
      "Iteration 24254: loss = 0.0024290550968252417\n",
      "Iteration 24255: loss = 0.0024290447618679056\n",
      "Iteration 24256: loss = 0.002429034427022636\n",
      "Iteration 24257: loss = 0.002429024092289432\n",
      "Iteration 24258: loss = 0.002429013757668292\n",
      "Iteration 24259: loss = 0.0024290034231592146\n",
      "Iteration 24260: loss = 0.0024289930887621985\n",
      "Iteration 24261: loss = 0.002428982754477243\n",
      "Iteration 24262: loss = 0.0024289724203043444\n",
      "Iteration 24263: loss = 0.0024289620862435043\n",
      "Iteration 24264: loss = 0.0024289517522947195\n",
      "Iteration 24265: loss = 0.0024289414184579896\n",
      "Iteration 24266: loss = 0.0024289310847333124\n",
      "Iteration 24267: loss = 0.0024289207511206863\n",
      "Iteration 24268: loss = 0.0024289104176201115\n",
      "Iteration 24269: loss = 0.002428900084231585\n",
      "Iteration 24270: loss = 0.0024288897509551067\n",
      "Iteration 24271: loss = 0.0024288794177906746\n",
      "Iteration 24272: loss = 0.0024288690847382873\n",
      "Iteration 24273: loss = 0.0024288587517979432\n",
      "Iteration 24274: loss = 0.0024288484189696415\n",
      "Iteration 24275: loss = 0.0024288380862533807\n",
      "Iteration 24276: loss = 0.0024288277536491593\n",
      "Iteration 24277: loss = 0.002428817421156976\n",
      "Iteration 24278: loss = 0.002428807088776829\n",
      "Iteration 24279: loss = 0.0024287967565087176\n",
      "Iteration 24280: loss = 0.00242878642435264\n",
      "Iteration 24281: loss = 0.002428776092308595\n",
      "Iteration 24282: loss = 0.002428765760376582\n",
      "Iteration 24283: loss = 0.002428755428556598\n",
      "Iteration 24284: loss = 0.0024287450968486428\n",
      "Iteration 24285: loss = 0.0024287347652527146\n",
      "Iteration 24286: loss = 0.0024287244337688123\n",
      "Iteration 24287: loss = 0.002428714102396934\n",
      "Iteration 24288: loss = 0.0024287037711370795\n",
      "Iteration 24289: loss = 0.0024286934399892464\n",
      "Iteration 24290: loss = 0.0024286831089534336\n",
      "Iteration 24291: loss = 0.00242867277802964\n",
      "Iteration 24292: loss = 0.0024286624472178633\n",
      "Iteration 24293: loss = 0.0024286521165181037\n",
      "Iteration 24294: loss = 0.0024286417859303577\n",
      "Iteration 24295: loss = 0.002428631455454627\n",
      "Iteration 24296: loss = 0.0024286211250909074\n",
      "Iteration 24297: loss = 0.002428610794839198\n",
      "Iteration 24298: loss = 0.002428600464699499\n",
      "Iteration 24299: loss = 0.002428590134671808\n",
      "Iteration 24300: loss = 0.002428579804756123\n",
      "Iteration 24301: loss = 0.0024285694749524435\n",
      "Iteration 24302: loss = 0.002428559145260768\n",
      "Iteration 24303: loss = 0.002428548815681095\n",
      "Iteration 24304: loss = 0.0024285384862134237\n",
      "Iteration 24305: loss = 0.002428528156857752\n",
      "Iteration 24306: loss = 0.002428517827614079\n",
      "Iteration 24307: loss = 0.002428507498482403\n",
      "Iteration 24308: loss = 0.0024284971694627224\n",
      "Iteration 24309: loss = 0.0024284868405550367\n",
      "Iteration 24310: loss = 0.002428476511759344\n",
      "Iteration 24311: loss = 0.0024284661830756425\n",
      "Iteration 24312: loss = 0.002428455854503932\n",
      "Iteration 24313: loss = 0.0024284455260442095\n",
      "Iteration 24314: loss = 0.002428435197696475\n",
      "Iteration 24315: loss = 0.0024284248694607267\n",
      "Iteration 24316: loss = 0.002428414541336964\n",
      "Iteration 24317: loss = 0.002428404213325184\n",
      "Iteration 24318: loss = 0.0024283938854253863\n",
      "Iteration 24319: loss = 0.0024283835576375695\n",
      "Iteration 24320: loss = 0.0024283732299617317\n",
      "Iteration 24321: loss = 0.002428362902397872\n",
      "Iteration 24322: loss = 0.002428352574945989\n",
      "Iteration 24323: loss = 0.0024283422476060815\n",
      "Iteration 24324: loss = 0.002428331920378148\n",
      "Iteration 24325: loss = 0.0024283215932621874\n",
      "Iteration 24326: loss = 0.002428311266258197\n",
      "Iteration 24327: loss = 0.002428300939366177\n",
      "Iteration 24328: loss = 0.0024282906125861255\n",
      "Iteration 24329: loss = 0.002428280285918041\n",
      "Iteration 24330: loss = 0.0024282699593619226\n",
      "Iteration 24331: loss = 0.002428259632917768\n",
      "Iteration 24332: loss = 0.002428249306585577\n",
      "Iteration 24333: loss = 0.0024282389803653476\n",
      "Iteration 24334: loss = 0.002428228654257078\n",
      "Iteration 24335: loss = 0.0024282183282607677\n",
      "Iteration 24336: loss = 0.002428208002376415\n",
      "Iteration 24337: loss = 0.002428197676604019\n",
      "Iteration 24338: loss = 0.0024281873509435768\n",
      "Iteration 24339: loss = 0.002428177025395089\n",
      "Iteration 24340: loss = 0.0024281666999585523\n",
      "Iteration 24341: loss = 0.0024281563746339665\n",
      "Iteration 24342: loss = 0.002428146049421331\n",
      "Iteration 24343: loss = 0.002428135724320643\n",
      "Iteration 24344: loss = 0.0024281253993319015\n",
      "Iteration 24345: loss = 0.002428115074455106\n",
      "Iteration 24346: loss = 0.0024281047496902534\n",
      "Iteration 24347: loss = 0.002428094425037344\n",
      "Iteration 24348: loss = 0.0024280841004963756\n",
      "Iteration 24349: loss = 0.002428073776067347\n",
      "Iteration 24350: loss = 0.002428063451750257\n",
      "Iteration 24351: loss = 0.002428053127545104\n",
      "Iteration 24352: loss = 0.002428042803451887\n",
      "Iteration 24353: loss = 0.0024280324794706044\n",
      "Iteration 24354: loss = 0.0024280221556012547\n",
      "Iteration 24355: loss = 0.0024280118318438368\n",
      "Iteration 24356: loss = 0.002428001508198349\n",
      "Iteration 24357: loss = 0.0024279911846647895\n",
      "Iteration 24358: loss = 0.002427980861243158\n",
      "Iteration 24359: loss = 0.002427970537933453\n",
      "Iteration 24360: loss = 0.0024279602147356735\n",
      "Iteration 24361: loss = 0.0024279498916498165\n",
      "Iteration 24362: loss = 0.002427939568675881\n",
      "Iteration 24363: loss = 0.0024279292458138676\n",
      "Iteration 24364: loss = 0.0024279189230637733\n",
      "Iteration 24365: loss = 0.0024279086004255963\n",
      "Iteration 24366: loss = 0.0024278982778993367\n",
      "Iteration 24367: loss = 0.002427887955484992\n",
      "Iteration 24368: loss = 0.0024278776331825612\n",
      "Iteration 24369: loss = 0.0024278673109920435\n",
      "Iteration 24370: loss = 0.0024278569889134363\n",
      "Iteration 24371: loss = 0.002427846666946739\n",
      "Iteration 24372: loss = 0.0024278363450919507\n",
      "Iteration 24373: loss = 0.0024278260233490693\n",
      "Iteration 24374: loss = 0.0024278157017180936\n",
      "Iteration 24375: loss = 0.0024278053801990223\n",
      "Iteration 24376: loss = 0.002427795058791854\n",
      "Iteration 24377: loss = 0.002427784737496587\n",
      "Iteration 24378: loss = 0.002427774416313221\n",
      "Iteration 24379: loss = 0.002427764095241753\n",
      "Iteration 24380: loss = 0.0024277537742821835\n",
      "Iteration 24381: loss = 0.00242774345343451\n",
      "Iteration 24382: loss = 0.002427733132698731\n",
      "Iteration 24383: loss = 0.0024277228120748455\n",
      "Iteration 24384: loss = 0.0024277124915628528\n",
      "Iteration 24385: loss = 0.0024277021711627497\n",
      "Iteration 24386: loss = 0.0024276918508745367\n",
      "Iteration 24387: loss = 0.002427681530698211\n",
      "Iteration 24388: loss = 0.002427671210633773\n",
      "Iteration 24389: loss = 0.0024276608906812205\n",
      "Iteration 24390: loss = 0.002427650570840551\n",
      "Iteration 24391: loss = 0.0024276402511117646\n",
      "Iteration 24392: loss = 0.002427629931494859\n",
      "Iteration 24393: loss = 0.0024276196119898336\n",
      "Iteration 24394: loss = 0.0024276092925966863\n",
      "Iteration 24395: loss = 0.0024275989733154165\n",
      "Iteration 24396: loss = 0.002427588654146022\n",
      "Iteration 24397: loss = 0.002427578335088502\n",
      "Iteration 24398: loss = 0.0024275680161428555\n",
      "Iteration 24399: loss = 0.00242755769730908\n",
      "Iteration 24400: loss = 0.002427547378587175\n",
      "Iteration 24401: loss = 0.0024275370599771396\n",
      "Iteration 24402: loss = 0.0024275267414789713\n",
      "Iteration 24403: loss = 0.002427516423092669\n",
      "Iteration 24404: loss = 0.0024275061048182316\n",
      "Iteration 24405: loss = 0.002427495786655658\n",
      "Iteration 24406: loss = 0.002427485468604946\n",
      "Iteration 24407: loss = 0.0024274751506660956\n",
      "Iteration 24408: loss = 0.0024274648328391035\n",
      "Iteration 24409: loss = 0.0024274545151239703\n",
      "Iteration 24410: loss = 0.0024274441975206933\n",
      "Iteration 24411: loss = 0.002427433880029272\n",
      "Iteration 24412: loss = 0.0024274235626497048\n",
      "Iteration 24413: loss = 0.002427413245381989\n",
      "Iteration 24414: loss = 0.0024274029282261252\n",
      "Iteration 24415: loss = 0.002427392611182112\n",
      "Iteration 24416: loss = 0.002427382294249946\n",
      "Iteration 24417: loss = 0.0024273719774296275\n",
      "Iteration 24418: loss = 0.0024273616607211556\n",
      "Iteration 24419: loss = 0.002427351344124527\n",
      "Iteration 24420: loss = 0.002427341027639742\n",
      "Iteration 24421: loss = 0.0024273307112667986\n",
      "Iteration 24422: loss = 0.0024273203950056954\n",
      "Iteration 24423: loss = 0.0024273100788564316\n",
      "Iteration 24424: loss = 0.002427299762819005\n",
      "Iteration 24425: loss = 0.002427289446893414\n",
      "Iteration 24426: loss = 0.0024272791310796585\n",
      "Iteration 24427: loss = 0.0024272688153777365\n",
      "Iteration 24428: loss = 0.002427258499787647\n",
      "Iteration 24429: loss = 0.0024272481843093884\n",
      "Iteration 24430: loss = 0.0024272378689429584\n",
      "Iteration 24431: loss = 0.002427227553688357\n",
      "Iteration 24432: loss = 0.0024272172385455815\n",
      "Iteration 24433: loss = 0.002427206923514632\n",
      "Iteration 24434: loss = 0.0024271966085955064\n",
      "Iteration 24435: loss = 0.002427186293788203\n",
      "Iteration 24436: loss = 0.002427175979092721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24437: loss = 0.0024271656645090595\n",
      "Iteration 24438: loss = 0.002427155350037216\n",
      "Iteration 24439: loss = 0.0024271450356771894\n",
      "Iteration 24440: loss = 0.0024271347214289787\n",
      "Iteration 24441: loss = 0.0024271244072925826\n",
      "Iteration 24442: loss = 0.0024271140932679994\n",
      "Iteration 24443: loss = 0.002427103779355228\n",
      "Iteration 24444: loss = 0.002427093465554267\n",
      "Iteration 24445: loss = 0.002427083151865115\n",
      "Iteration 24446: loss = 0.0024270728382877705\n",
      "Iteration 24447: loss = 0.0024270625248222323\n",
      "Iteration 24448: loss = 0.0024270522114684993\n",
      "Iteration 24449: loss = 0.002427041898226569\n",
      "Iteration 24450: loss = 0.0024270315850964414\n",
      "Iteration 24451: loss = 0.0024270212720781145\n",
      "Iteration 24452: loss = 0.0024270109591715866\n",
      "Iteration 24453: loss = 0.0024270006463768578\n",
      "Iteration 24454: loss = 0.002426990333693925\n",
      "Iteration 24455: loss = 0.0024269800211227876\n",
      "Iteration 24456: loss = 0.0024269697086634445\n",
      "Iteration 24457: loss = 0.002426959396315893\n",
      "Iteration 24458: loss = 0.0024269490840801338\n",
      "Iteration 24459: loss = 0.002426938771956164\n",
      "Iteration 24460: loss = 0.002426928459943983\n",
      "Iteration 24461: loss = 0.002426918148043589\n",
      "Iteration 24462: loss = 0.002426907836254981\n",
      "Iteration 24463: loss = 0.0024268975245781573\n",
      "Iteration 24464: loss = 0.0024268872130131167\n",
      "Iteration 24465: loss = 0.0024268769015598573\n",
      "Iteration 24466: loss = 0.002426866590218379\n",
      "Iteration 24467: loss = 0.0024268562789886796\n",
      "Iteration 24468: loss = 0.0024268459678707575\n",
      "Iteration 24469: loss = 0.0024268356568646118\n",
      "Iteration 24470: loss = 0.002426825345970241\n",
      "Iteration 24471: loss = 0.0024268150351876437\n",
      "Iteration 24472: loss = 0.0024268047245168192\n",
      "Iteration 24473: loss = 0.0024267944139577642\n",
      "Iteration 24474: loss = 0.0024267841035104797\n",
      "Iteration 24475: loss = 0.002426773793174963\n",
      "Iteration 24476: loss = 0.002426763482951213\n",
      "Iteration 24477: loss = 0.0024267531728392284\n",
      "Iteration 24478: loss = 0.002426742862839008\n",
      "Iteration 24479: loss = 0.0024267325529505495\n",
      "Iteration 24480: loss = 0.002426722243173853\n",
      "Iteration 24481: loss = 0.0024267119335089162\n",
      "Iteration 24482: loss = 0.002426701623955738\n",
      "Iteration 24483: loss = 0.002426691314514317\n",
      "Iteration 24484: loss = 0.0024266810051846517\n",
      "Iteration 24485: loss = 0.0024266706959667414\n",
      "Iteration 24486: loss = 0.0024266603868605837\n",
      "Iteration 24487: loss = 0.0024266500778661777\n",
      "Iteration 24488: loss = 0.0024266397689835227\n",
      "Iteration 24489: loss = 0.002426629460212616\n",
      "Iteration 24490: loss = 0.002426619151553457\n",
      "Iteration 24491: loss = 0.002426608843006045\n",
      "Iteration 24492: loss = 0.0024265985345703765\n",
      "Iteration 24493: loss = 0.002426588226246453\n",
      "Iteration 24494: loss = 0.002426577918034271\n",
      "Iteration 24495: loss = 0.00242656760993383\n",
      "Iteration 24496: loss = 0.0024265573019451284\n",
      "Iteration 24497: loss = 0.0024265469940681654\n",
      "Iteration 24498: loss = 0.002426536686302939\n",
      "Iteration 24499: loss = 0.0024265263786494473\n",
      "Iteration 24500: loss = 0.00242651607110769\n",
      "Iteration 24501: loss = 0.0024265057636776656\n",
      "Iteration 24502: loss = 0.002426495456359373\n",
      "Iteration 24503: loss = 0.0024264851491528094\n",
      "Iteration 24504: loss = 0.002426474842057974\n",
      "Iteration 24505: loss = 0.002426464535074867\n",
      "Iteration 24506: loss = 0.0024264542282034854\n",
      "Iteration 24507: loss = 0.0024264439214438283\n",
      "Iteration 24508: loss = 0.0024264336147958947\n",
      "Iteration 24509: loss = 0.0024264233082596826\n",
      "Iteration 24510: loss = 0.002426413001835191\n",
      "Iteration 24511: loss = 0.002426402695522418\n",
      "Iteration 24512: loss = 0.002426392389321363\n",
      "Iteration 24513: loss = 0.002426382083232024\n",
      "Iteration 24514: loss = 0.0024263717772544007\n",
      "Iteration 24515: loss = 0.0024263614713884905\n",
      "Iteration 24516: loss = 0.0024263511656342934\n",
      "Iteration 24517: loss = 0.002426340859991806\n",
      "Iteration 24518: loss = 0.0024263305544610287\n",
      "Iteration 24519: loss = 0.0024263202490419594\n",
      "Iteration 24520: loss = 0.002426309943734597\n",
      "Iteration 24521: loss = 0.00242629963853894\n",
      "Iteration 24522: loss = 0.0024262893334549866\n",
      "Iteration 24523: loss = 0.002426279028482736\n",
      "Iteration 24524: loss = 0.002426268723622188\n",
      "Iteration 24525: loss = 0.0024262584188733385\n",
      "Iteration 24526: loss = 0.002426248114236189\n",
      "Iteration 24527: loss = 0.0024262378097107355\n",
      "Iteration 24528: loss = 0.0024262275052969783\n",
      "Iteration 24529: loss = 0.0024262172009949156\n",
      "Iteration 24530: loss = 0.0024262068968045458\n",
      "Iteration 24531: loss = 0.0024261965927258686\n",
      "Iteration 24532: loss = 0.0024261862887588813\n",
      "Iteration 24533: loss = 0.0024261759849035836\n",
      "Iteration 24534: loss = 0.0024261656811599727\n",
      "Iteration 24535: loss = 0.002426155377528049\n",
      "Iteration 24536: loss = 0.00242614507400781\n",
      "Iteration 24537: loss = 0.002426134770599255\n",
      "Iteration 24538: loss = 0.0024261244673023817\n",
      "Iteration 24539: loss = 0.002426114164117189\n",
      "Iteration 24540: loss = 0.002426103861043677\n",
      "Iteration 24541: loss = 0.002426093558081843\n",
      "Iteration 24542: loss = 0.0024260832552316855\n",
      "Iteration 24543: loss = 0.0024260729524932033\n",
      "Iteration 24544: loss = 0.002426062649866395\n",
      "Iteration 24545: loss = 0.0024260523473512608\n",
      "Iteration 24546: loss = 0.0024260420449477966\n",
      "Iteration 24547: loss = 0.002426031742656003\n",
      "Iteration 24548: loss = 0.0024260214404758776\n",
      "Iteration 24549: loss = 0.00242601113840742\n",
      "Iteration 24550: loss = 0.002426000836450628\n",
      "Iteration 24551: loss = 0.0024259905346055008\n",
      "Iteration 24552: loss = 0.002425980232872037\n",
      "Iteration 24553: loss = 0.0024259699312502345\n",
      "Iteration 24554: loss = 0.002425959629740093\n",
      "Iteration 24555: loss = 0.0024259493283416106\n",
      "Iteration 24556: loss = 0.0024259390270547857\n",
      "Iteration 24557: loss = 0.002425928725879617\n",
      "Iteration 24558: loss = 0.0024259184248161037\n",
      "Iteration 24559: loss = 0.002425908123864245\n",
      "Iteration 24560: loss = 0.0024258978230240376\n",
      "Iteration 24561: loss = 0.0024258875222954814\n",
      "Iteration 24562: loss = 0.0024258772216785747\n",
      "Iteration 24563: loss = 0.002425866921173316\n",
      "Iteration 24564: loss = 0.0024258566207797047\n",
      "Iteration 24565: loss = 0.0024258463204977388\n",
      "Iteration 24566: loss = 0.0024258360203274166\n",
      "Iteration 24567: loss = 0.002425825720268738\n",
      "Iteration 24568: loss = 0.0024258154203217\n",
      "Iteration 24569: loss = 0.0024258051204863024\n",
      "Iteration 24570: loss = 0.002425794820762544\n",
      "Iteration 24571: loss = 0.0024257845211504226\n",
      "Iteration 24572: loss = 0.002425774221649937\n",
      "Iteration 24573: loss = 0.002425763922261086\n",
      "Iteration 24574: loss = 0.002425753622983868\n",
      "Iteration 24575: loss = 0.002425743323818283\n",
      "Iteration 24576: loss = 0.002425733024764328\n",
      "Iteration 24577: loss = 0.0024257227258220023\n",
      "Iteration 24578: loss = 0.0024257124269913043\n",
      "Iteration 24579: loss = 0.0024257021282722323\n",
      "Iteration 24580: loss = 0.0024256918296647863\n",
      "Iteration 24581: loss = 0.0024256815311689633\n",
      "Iteration 24582: loss = 0.0024256712327847627\n",
      "Iteration 24583: loss = 0.0024256609345121834\n",
      "Iteration 24584: loss = 0.0024256506363512236\n",
      "Iteration 24585: loss = 0.0024256403383018828\n",
      "Iteration 24586: loss = 0.002425630040364158\n",
      "Iteration 24587: loss = 0.002425619742538049\n",
      "Iteration 24588: loss = 0.0024256094448235547\n",
      "Iteration 24589: loss = 0.002425599147220673\n",
      "Iteration 24590: loss = 0.0024255888497294025\n",
      "Iteration 24591: loss = 0.0024255785523497426\n",
      "Iteration 24592: loss = 0.002425568255081691\n",
      "Iteration 24593: loss = 0.002425557957925247\n",
      "Iteration 24594: loss = 0.0024255476608804087\n",
      "Iteration 24595: loss = 0.002425537363947176\n",
      "Iteration 24596: loss = 0.0024255270671255462\n",
      "Iteration 24597: loss = 0.0024255167704155182\n",
      "Iteration 24598: loss = 0.0024255064738170906\n",
      "Iteration 24599: loss = 0.002425496177330262\n",
      "Iteration 24600: loss = 0.002425485880955032\n",
      "Iteration 24601: loss = 0.0024254755846913993\n",
      "Iteration 24602: loss = 0.00242546528853936\n",
      "Iteration 24603: loss = 0.002425454992498915\n",
      "Iteration 24604: loss = 0.0024254446965700635\n",
      "Iteration 24605: loss = 0.0024254344007528022\n",
      "Iteration 24606: loss = 0.0024254241050471305\n",
      "Iteration 24607: loss = 0.0024254138094530475\n",
      "Iteration 24608: loss = 0.002425403513970551\n",
      "Iteration 24609: loss = 0.002425393218599641\n",
      "Iteration 24610: loss = 0.002425382923340315\n",
      "Iteration 24611: loss = 0.0024253726281925716\n",
      "Iteration 24612: loss = 0.0024253623331564095\n",
      "Iteration 24613: loss = 0.002425352038231828\n",
      "Iteration 24614: loss = 0.0024253417434188253\n",
      "Iteration 24615: loss = 0.0024253314487174\n",
      "Iteration 24616: loss = 0.0024253211541275512\n",
      "Iteration 24617: loss = 0.0024253108596492766\n",
      "Iteration 24618: loss = 0.0024253005652825755\n",
      "Iteration 24619: loss = 0.0024252902710274467\n",
      "Iteration 24620: loss = 0.0024252799768838887\n",
      "Iteration 24621: loss = 0.0024252696828518995\n",
      "Iteration 24622: loss = 0.0024252593889314785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24623: loss = 0.0024252490951226237\n",
      "Iteration 24624: loss = 0.0024252388014253346\n",
      "Iteration 24625: loss = 0.0024252285078396095\n",
      "Iteration 24626: loss = 0.0024252182143654467\n",
      "Iteration 24627: loss = 0.0024252079210028447\n",
      "Iteration 24628: loss = 0.0024251976277518024\n",
      "Iteration 24629: loss = 0.0024251873346123193\n",
      "Iteration 24630: loss = 0.002425177041584393\n",
      "Iteration 24631: loss = 0.0024251667486680224\n",
      "Iteration 24632: loss = 0.0024251564558632056\n",
      "Iteration 24633: loss = 0.002425146163169942\n",
      "Iteration 24634: loss = 0.002425135870588231\n",
      "Iteration 24635: loss = 0.002425125578118069\n",
      "Iteration 24636: loss = 0.0024251152857594563\n",
      "Iteration 24637: loss = 0.0024251049935123913\n",
      "Iteration 24638: loss = 0.0024250947013768724\n",
      "Iteration 24639: loss = 0.002425084409352898\n",
      "Iteration 24640: loss = 0.0024250741174404677\n",
      "Iteration 24641: loss = 0.002425063825639579\n",
      "Iteration 24642: loss = 0.0024250535339502316\n",
      "Iteration 24643: loss = 0.0024250432423724225\n",
      "Iteration 24644: loss = 0.002425032950906152\n",
      "Iteration 24645: loss = 0.0024250226595514185\n",
      "Iteration 24646: loss = 0.00242501236830822\n",
      "Iteration 24647: loss = 0.0024250020771765554\n",
      "Iteration 24648: loss = 0.0024249917861564234\n",
      "Iteration 24649: loss = 0.0024249814952478225\n",
      "Iteration 24650: loss = 0.002424971204450752\n",
      "Iteration 24651: loss = 0.002424960913765209\n",
      "Iteration 24652: loss = 0.0024249506231911943\n",
      "Iteration 24653: loss = 0.0024249403327287044\n",
      "Iteration 24654: loss = 0.0024249300423777394\n",
      "Iteration 24655: loss = 0.002424919752138297\n",
      "Iteration 24656: loss = 0.0024249094620103773\n",
      "Iteration 24657: loss = 0.0024248991719939767\n",
      "Iteration 24658: loss = 0.0024248888820890963\n",
      "Iteration 24659: loss = 0.002424878592295732\n",
      "Iteration 24660: loss = 0.0024248683026138855\n",
      "Iteration 24661: loss = 0.002424858013043553\n",
      "Iteration 24662: loss = 0.0024248477235847337\n",
      "Iteration 24663: loss = 0.0024248374342374273\n",
      "Iteration 24664: loss = 0.0024248271450016314\n",
      "Iteration 24665: loss = 0.0024248168558773453\n",
      "Iteration 24666: loss = 0.002424806566864567\n",
      "Iteration 24667: loss = 0.0024247962779632955\n",
      "Iteration 24668: loss = 0.002424785989173529\n",
      "Iteration 24669: loss = 0.002424775700495267\n",
      "Iteration 24670: loss = 0.002424765411928507\n",
      "Iteration 24671: loss = 0.002424755123473249\n",
      "Iteration 24672: loss = 0.002424744835129491\n",
      "Iteration 24673: loss = 0.0024247345468972306\n",
      "Iteration 24674: loss = 0.0024247242587764686\n",
      "Iteration 24675: loss = 0.0024247139707672016\n",
      "Iteration 24676: loss = 0.0024247036828694292\n",
      "Iteration 24677: loss = 0.0024246933950831506\n",
      "Iteration 24678: loss = 0.0024246831074083634\n",
      "Iteration 24679: loss = 0.002424672819845066\n",
      "Iteration 24680: loss = 0.002424662532393258\n",
      "Iteration 24681: loss = 0.0024246522450529382\n",
      "Iteration 24682: loss = 0.0024246419578241047\n",
      "Iteration 24683: loss = 0.0024246316707067558\n",
      "Iteration 24684: loss = 0.00242462138370089\n",
      "Iteration 24685: loss = 0.0024246110968065074\n",
      "Iteration 24686: loss = 0.0024246008100236053\n",
      "Iteration 24687: loss = 0.0024245905233521827\n",
      "Iteration 24688: loss = 0.002424580236792238\n",
      "Iteration 24689: loss = 0.0024245699503437704\n",
      "Iteration 24690: loss = 0.0024245596640067777\n",
      "Iteration 24691: loss = 0.0024245493777812596\n",
      "Iteration 24692: loss = 0.0024245390916672145\n",
      "Iteration 24693: loss = 0.0024245288056646405\n",
      "Iteration 24694: loss = 0.002424518519773537\n",
      "Iteration 24695: loss = 0.0024245082339939008\n",
      "Iteration 24696: loss = 0.0024244979483257333\n",
      "Iteration 24697: loss = 0.002424487662769031\n",
      "Iteration 24698: loss = 0.002424477377323793\n",
      "Iteration 24699: loss = 0.0024244670919900185\n",
      "Iteration 24700: loss = 0.002424456806767706\n",
      "Iteration 24701: loss = 0.0024244465216568542\n",
      "Iteration 24702: loss = 0.0024244362366574617\n",
      "Iteration 24703: loss = 0.002424425951769526\n",
      "Iteration 24704: loss = 0.0024244156669930472\n",
      "Iteration 24705: loss = 0.0024244053823280235\n",
      "Iteration 24706: loss = 0.0024243950977744537\n",
      "Iteration 24707: loss = 0.002424384813332336\n",
      "Iteration 24708: loss = 0.0024243745290016693\n",
      "Iteration 24709: loss = 0.002424364244782452\n",
      "Iteration 24710: loss = 0.0024243539606746832\n",
      "Iteration 24711: loss = 0.0024243436766783616\n",
      "Iteration 24712: loss = 0.002424333392793485\n",
      "Iteration 24713: loss = 0.002424323109020053\n",
      "Iteration 24714: loss = 0.0024243128253580637\n",
      "Iteration 24715: loss = 0.0024243025418075152\n",
      "Iteration 24716: loss = 0.0024242922583684076\n",
      "Iteration 24717: loss = 0.002424281975040738\n",
      "Iteration 24718: loss = 0.0024242716918245065\n",
      "Iteration 24719: loss = 0.002424261408719711\n",
      "Iteration 24720: loss = 0.00242425112572635\n",
      "Iteration 24721: loss = 0.002424240842844422\n",
      "Iteration 24722: loss = 0.002424230560073926\n",
      "Iteration 24723: loss = 0.002424220277414861\n",
      "Iteration 24724: loss = 0.0024242099948672247\n",
      "Iteration 24725: loss = 0.0024241997124310162\n",
      "Iteration 24726: loss = 0.0024241894301062343\n",
      "Iteration 24727: loss = 0.002424179147892878\n",
      "Iteration 24728: loss = 0.0024241688657909454\n",
      "Iteration 24729: loss = 0.0024241585838004347\n",
      "Iteration 24730: loss = 0.0024241483019213452\n",
      "Iteration 24731: loss = 0.0024241380201536758\n",
      "Iteration 24732: loss = 0.002424127738497424\n",
      "Iteration 24733: loss = 0.0024241174569525903\n",
      "Iteration 24734: loss = 0.0024241071755191712\n",
      "Iteration 24735: loss = 0.002424096894197167\n",
      "Iteration 24736: loss = 0.0024240866129865758\n",
      "Iteration 24737: loss = 0.002424076331887396\n",
      "Iteration 24738: loss = 0.002424066050899626\n",
      "Iteration 24739: loss = 0.0024240557700232644\n",
      "Iteration 24740: loss = 0.002424045489258311\n",
      "Iteration 24741: loss = 0.0024240352086047634\n",
      "Iteration 24742: loss = 0.0024240249280626214\n",
      "Iteration 24743: loss = 0.0024240146476318817\n",
      "Iteration 24744: loss = 0.0024240043673125445\n",
      "Iteration 24745: loss = 0.002423994087104608\n",
      "Iteration 24746: loss = 0.002423983807008071\n",
      "Iteration 24747: loss = 0.002423973527022931\n",
      "Iteration 24748: loss = 0.002423963247149189\n",
      "Iteration 24749: loss = 0.002423952967386841\n",
      "Iteration 24750: loss = 0.0024239426877358874\n",
      "Iteration 24751: loss = 0.0024239324081963263\n",
      "Iteration 24752: loss = 0.0024239221287681565\n",
      "Iteration 24753: loss = 0.002423911849451376\n",
      "Iteration 24754: loss = 0.002423901570245984\n",
      "Iteration 24755: loss = 0.0024238912911519793\n",
      "Iteration 24756: loss = 0.0024238810121693607\n",
      "Iteration 24757: loss = 0.002423870733298126\n",
      "Iteration 24758: loss = 0.0024238604545382745\n",
      "Iteration 24759: loss = 0.002423850175889805\n",
      "Iteration 24760: loss = 0.002423839897352715\n",
      "Iteration 24761: loss = 0.0024238296189270044\n",
      "Iteration 24762: loss = 0.0024238193406126707\n",
      "Iteration 24763: loss = 0.0024238090624097144\n",
      "Iteration 24764: loss = 0.0024237987843181326\n",
      "Iteration 24765: loss = 0.0024237885063379235\n",
      "Iteration 24766: loss = 0.002423778228469087\n",
      "Iteration 24767: loss = 0.0024237679507116217\n",
      "Iteration 24768: loss = 0.0024237576730655247\n",
      "Iteration 24769: loss = 0.002423747395530797\n",
      "Iteration 24770: loss = 0.002423737118107435\n",
      "Iteration 24771: loss = 0.0024237268407954387\n",
      "Iteration 24772: loss = 0.002423716563594807\n",
      "Iteration 24773: loss = 0.002423706286505537\n",
      "Iteration 24774: loss = 0.002423696009527628\n",
      "Iteration 24775: loss = 0.00242368573266108\n",
      "Iteration 24776: loss = 0.00242367545590589\n",
      "Iteration 24777: loss = 0.002423665179262057\n",
      "Iteration 24778: loss = 0.0024236549027295807\n",
      "Iteration 24779: loss = 0.0024236446263084578\n",
      "Iteration 24780: loss = 0.002423634349998688\n",
      "Iteration 24781: loss = 0.0024236240738002707\n",
      "Iteration 24782: loss = 0.0024236137977132035\n",
      "Iteration 24783: loss = 0.002423603521737485\n",
      "Iteration 24784: loss = 0.0024235932458731143\n",
      "Iteration 24785: loss = 0.00242358297012009\n",
      "Iteration 24786: loss = 0.0024235726944784102\n",
      "Iteration 24787: loss = 0.0024235624189480748\n",
      "Iteration 24788: loss = 0.0024235521435290812\n",
      "Iteration 24789: loss = 0.0024235418682214288\n",
      "Iteration 24790: loss = 0.0024235315930251156\n",
      "Iteration 24791: loss = 0.0024235213179401405\n",
      "Iteration 24792: loss = 0.0024235110429665025\n",
      "Iteration 24793: loss = 0.0024235007681041995\n",
      "Iteration 24794: loss = 0.0024234904933532306\n",
      "Iteration 24795: loss = 0.0024234802187135954\n",
      "Iteration 24796: loss = 0.0024234699441852904\n",
      "Iteration 24797: loss = 0.0024234596697683157\n",
      "Iteration 24798: loss = 0.0024234493954626694\n",
      "Iteration 24799: loss = 0.002423439121268351\n",
      "Iteration 24800: loss = 0.0024234288471853584\n",
      "Iteration 24801: loss = 0.00242341857321369\n",
      "Iteration 24802: loss = 0.0024234082993533452\n",
      "Iteration 24803: loss = 0.0024233980256043223\n",
      "Iteration 24804: loss = 0.002423387751966619\n",
      "Iteration 24805: loss = 0.002423377478440236\n",
      "Iteration 24806: loss = 0.00242336720502517\n",
      "Iteration 24807: loss = 0.0024233569317214207\n",
      "Iteration 24808: loss = 0.0024233466585289864\n",
      "Iteration 24809: loss = 0.0024233363854478662\n",
      "Iteration 24810: loss = 0.0024233261124780576\n",
      "Iteration 24811: loss = 0.0024233158396195606\n",
      "Iteration 24812: loss = 0.002423305566872373\n",
      "Iteration 24813: loss = 0.002423295294236494\n",
      "Iteration 24814: loss = 0.002423285021711921\n",
      "Iteration 24815: loss = 0.0024232747492986544\n",
      "Iteration 24816: loss = 0.0024232644769966918\n",
      "Iteration 24817: loss = 0.0024232542048060316\n",
      "Iteration 24818: loss = 0.0024232439327266735\n",
      "Iteration 24819: loss = 0.002423233660758615\n",
      "Iteration 24820: loss = 0.0024232233889018556\n",
      "Iteration 24821: loss = 0.0024232131171563927\n",
      "Iteration 24822: loss = 0.002423202845522227\n",
      "Iteration 24823: loss = 0.002423192573999356\n",
      "Iteration 24824: loss = 0.002423182302587778\n",
      "Iteration 24825: loss = 0.0024231720312874916\n",
      "Iteration 24826: loss = 0.0024231617600984964\n",
      "Iteration 24827: loss = 0.0024231514890207893\n",
      "Iteration 24828: loss = 0.002423141218054371\n",
      "Iteration 24829: loss = 0.0024231309471992395\n",
      "Iteration 24830: loss = 0.002423120676455393\n",
      "Iteration 24831: loss = 0.00242311040582283\n",
      "Iteration 24832: loss = 0.002423100135301549\n",
      "Iteration 24833: loss = 0.0024230898648915505\n",
      "Iteration 24834: loss = 0.0024230795945928306\n",
      "Iteration 24835: loss = 0.002423069324405389\n",
      "Iteration 24836: loss = 0.002423059054329225\n",
      "Iteration 24837: loss = 0.0024230487843643366\n",
      "Iteration 24838: loss = 0.0024230385145107224\n",
      "Iteration 24839: loss = 0.002423028244768381\n",
      "Iteration 24840: loss = 0.0024230179751373116\n",
      "Iteration 24841: loss = 0.002423007705617512\n",
      "Iteration 24842: loss = 0.0024229974362089813\n",
      "Iteration 24843: loss = 0.0024229871669117185\n",
      "Iteration 24844: loss = 0.0024229768977257213\n",
      "Iteration 24845: loss = 0.002422966628650989\n",
      "Iteration 24846: loss = 0.0024229563596875206\n",
      "Iteration 24847: loss = 0.002422946090835314\n",
      "Iteration 24848: loss = 0.0024229358220943685\n",
      "Iteration 24849: loss = 0.0024229255534646815\n",
      "Iteration 24850: loss = 0.002422915284946253\n",
      "Iteration 24851: loss = 0.0024229050165390812\n",
      "Iteration 24852: loss = 0.0024228947482431646\n",
      "Iteration 24853: loss = 0.0024228844800585018\n",
      "Iteration 24854: loss = 0.002422874211985092\n",
      "Iteration 24855: loss = 0.0024228639440229335\n",
      "Iteration 24856: loss = 0.002422853676172024\n",
      "Iteration 24857: loss = 0.002422843408432364\n",
      "Iteration 24858: loss = 0.0024228331408039506\n",
      "Iteration 24859: loss = 0.002422822873286783\n",
      "Iteration 24860: loss = 0.0024228126058808597\n",
      "Iteration 24861: loss = 0.00242280233858618\n",
      "Iteration 24862: loss = 0.0024227920714027414\n",
      "Iteration 24863: loss = 0.0024227818043305433\n",
      "Iteration 24864: loss = 0.002422771537369585\n",
      "Iteration 24865: loss = 0.0024227612705198633\n",
      "Iteration 24866: loss = 0.0024227510037813783\n",
      "Iteration 24867: loss = 0.0024227407371541277\n",
      "Iteration 24868: loss = 0.0024227304706381115\n",
      "Iteration 24869: loss = 0.002422720204233327\n",
      "Iteration 24870: loss = 0.0024227099379397735\n",
      "Iteration 24871: loss = 0.00242269967175745\n",
      "Iteration 24872: loss = 0.002422689405686354\n",
      "Iteration 24873: loss = 0.002422679139726485\n",
      "Iteration 24874: loss = 0.0024226688738778405\n",
      "Iteration 24875: loss = 0.002422658608140421\n",
      "Iteration 24876: loss = 0.0024226483425142244\n",
      "Iteration 24877: loss = 0.0024226380769992485\n",
      "Iteration 24878: loss = 0.0024226278115954927\n",
      "Iteration 24879: loss = 0.002422617546302956\n",
      "Iteration 24880: loss = 0.0024226072811216366\n",
      "Iteration 24881: loss = 0.002422597016051533\n",
      "Iteration 24882: loss = 0.002422586751092643\n",
      "Iteration 24883: loss = 0.0024225764862449675\n",
      "Iteration 24884: loss = 0.002422566221508503\n",
      "Iteration 24885: loss = 0.0024225559568832487\n",
      "Iteration 24886: loss = 0.0024225456923692044\n",
      "Iteration 24887: loss = 0.002422535427966368\n",
      "Iteration 24888: loss = 0.0024225251636747374\n",
      "Iteration 24889: loss = 0.0024225148994943125\n",
      "Iteration 24890: loss = 0.0024225046354250904\n",
      "Iteration 24891: loss = 0.002422494371467071\n",
      "Iteration 24892: loss = 0.002422484107620253\n",
      "Iteration 24893: loss = 0.0024224738438846337\n",
      "Iteration 24894: loss = 0.0024224635802602133\n",
      "Iteration 24895: loss = 0.00242245331674699\n",
      "Iteration 24896: loss = 0.002422443053344962\n",
      "Iteration 24897: loss = 0.002422432790054128\n",
      "Iteration 24898: loss = 0.0024224225268744868\n",
      "Iteration 24899: loss = 0.0024224122638060374\n",
      "Iteration 24900: loss = 0.0024224020008487782\n",
      "Iteration 24901: loss = 0.0024223917380027074\n",
      "Iteration 24902: loss = 0.002422381475267824\n",
      "Iteration 24903: loss = 0.002422371212644127\n",
      "Iteration 24904: loss = 0.002422360950131614\n",
      "Iteration 24905: loss = 0.0024223506877302852\n",
      "Iteration 24906: loss = 0.0024223404254401382\n",
      "Iteration 24907: loss = 0.0024223301632611714\n",
      "Iteration 24908: loss = 0.0024223199011933843\n",
      "Iteration 24909: loss = 0.0024223096392367747\n",
      "Iteration 24910: loss = 0.002422299377391342\n",
      "Iteration 24911: loss = 0.002422289115657084\n",
      "Iteration 24912: loss = 0.0024222788540340006\n",
      "Iteration 24913: loss = 0.002422268592522089\n",
      "Iteration 24914: loss = 0.0024222583311213483\n",
      "Iteration 24915: loss = 0.002422248069831778\n",
      "Iteration 24916: loss = 0.0024222378086533763\n",
      "Iteration 24917: loss = 0.0024222275475861412\n",
      "Iteration 24918: loss = 0.0024222172866300716\n",
      "Iteration 24919: loss = 0.002422207025785167\n",
      "Iteration 24920: loss = 0.0024221967650514247\n",
      "Iteration 24921: loss = 0.0024221865044288435\n",
      "Iteration 24922: loss = 0.0024221762439174234\n",
      "Iteration 24923: loss = 0.0024221659835171622\n",
      "Iteration 24924: loss = 0.0024221557232280587\n",
      "Iteration 24925: loss = 0.002422145463050111\n",
      "Iteration 24926: loss = 0.002422135202983318\n",
      "Iteration 24927: loss = 0.0024221249430276784\n",
      "Iteration 24928: loss = 0.0024221146831831914\n",
      "Iteration 24929: loss = 0.002422104423449855\n",
      "Iteration 24930: loss = 0.002422094163827668\n",
      "Iteration 24931: loss = 0.002422083904316629\n",
      "Iteration 24932: loss = 0.002422073644916737\n",
      "Iteration 24933: loss = 0.0024220633856279895\n",
      "Iteration 24934: loss = 0.002422053126450387\n",
      "Iteration 24935: loss = 0.002422042867383927\n",
      "Iteration 24936: loss = 0.0024220326084286076\n",
      "Iteration 24937: loss = 0.002422022349584428\n",
      "Iteration 24938: loss = 0.002422012090851387\n",
      "Iteration 24939: loss = 0.002422001832229484\n",
      "Iteration 24940: loss = 0.002421991573718716\n",
      "Iteration 24941: loss = 0.0024219813153190826\n",
      "Iteration 24942: loss = 0.002421971057030583\n",
      "Iteration 24943: loss = 0.002421960798853214\n",
      "Iteration 24944: loss = 0.0024219505407869764\n",
      "Iteration 24945: loss = 0.0024219402828318673\n",
      "Iteration 24946: loss = 0.0024219300249878864\n",
      "Iteration 24947: loss = 0.002421919767255031\n",
      "Iteration 24948: loss = 0.0024219095096333016\n",
      "Iteration 24949: loss = 0.0024218992521226955\n",
      "Iteration 24950: loss = 0.0024218889947232114\n",
      "Iteration 24951: loss = 0.002421878737434848\n",
      "Iteration 24952: loss = 0.0024218684802576043\n",
      "Iteration 24953: loss = 0.002421858223191479\n",
      "Iteration 24954: loss = 0.00242184796623647\n",
      "Iteration 24955: loss = 0.002421837709392577\n",
      "Iteration 24956: loss = 0.002421827452659798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24957: loss = 0.0024218171960381316\n",
      "Iteration 24958: loss = 0.0024218069395275767\n",
      "Iteration 24959: loss = 0.002421796683128132\n",
      "Iteration 24960: loss = 0.0024217864268397957\n",
      "Iteration 24961: loss = 0.002421776170662567\n",
      "Iteration 24962: loss = 0.0024217659145964444\n",
      "Iteration 24963: loss = 0.0024217556586414264\n",
      "Iteration 24964: loss = 0.002421745402797511\n",
      "Iteration 24965: loss = 0.0024217351470646985\n",
      "Iteration 24966: loss = 0.0024217248914429855\n",
      "Iteration 24967: loss = 0.002421714635932373\n",
      "Iteration 24968: loss = 0.002421704380532857\n",
      "Iteration 24969: loss = 0.0024216941252444384\n",
      "Iteration 24970: loss = 0.0024216838700671143\n",
      "Iteration 24971: loss = 0.002421673615000884\n",
      "Iteration 24972: loss = 0.002421663360045747\n",
      "Iteration 24973: loss = 0.0024216531052016995\n",
      "Iteration 24974: loss = 0.002421642850468743\n",
      "Iteration 24975: loss = 0.002421632595846874\n",
      "Iteration 24976: loss = 0.0024216223413360926\n",
      "Iteration 24977: loss = 0.002421612086936397\n",
      "Iteration 24978: loss = 0.002421601832647785\n",
      "Iteration 24979: loss = 0.002421591578470256\n",
      "Iteration 24980: loss = 0.002421581324403809\n",
      "Iteration 24981: loss = 0.0024215710704484418\n",
      "Iteration 24982: loss = 0.0024215608166041538\n",
      "Iteration 24983: loss = 0.002421550562870943\n",
      "Iteration 24984: loss = 0.0024215403092488087\n",
      "Iteration 24985: loss = 0.0024215300557377485\n",
      "Iteration 24986: loss = 0.002421519802337762\n",
      "Iteration 24987: loss = 0.0024215095490488477\n",
      "Iteration 24988: loss = 0.002421499295871004\n",
      "Iteration 24989: loss = 0.00242148904280423\n",
      "Iteration 24990: loss = 0.0024214787898485237\n",
      "Iteration 24991: loss = 0.0024214685370038836\n",
      "Iteration 24992: loss = 0.0024214582842703096\n",
      "Iteration 24993: loss = 0.0024214480316477986\n",
      "Iteration 24994: loss = 0.002421437779136351\n",
      "Iteration 24995: loss = 0.0024214275267359637\n",
      "Iteration 24996: loss = 0.0024214172744466372\n",
      "Iteration 24997: loss = 0.0024214070222683686\n",
      "Iteration 24998: loss = 0.0024213967702011578\n",
      "Iteration 24999: loss = 0.0024213865182450022\n",
      "Iteration 25000: loss = 0.0024213762663999015\n",
      "Iteration 25001: loss = 0.002421366014665853\n",
      "Iteration 25002: loss = 0.002421355763042857\n",
      "Iteration 25003: loss = 0.002421345511530911\n",
      "Iteration 25004: loss = 0.002421335260130014\n",
      "Iteration 25005: loss = 0.0024213250088401645\n",
      "Iteration 25006: loss = 0.002421314757661362\n",
      "Iteration 25007: loss = 0.002421304506593604\n",
      "Iteration 25008: loss = 0.0024212942556368893\n",
      "Iteration 25009: loss = 0.0024212840047912174\n",
      "Iteration 25010: loss = 0.002421273754056586\n",
      "Iteration 25011: loss = 0.0024212635034329944\n",
      "Iteration 25012: loss = 0.0024212532529204406\n",
      "Iteration 25013: loss = 0.002421243002518923\n",
      "Iteration 25014: loss = 0.002421232752228442\n",
      "Iteration 25015: loss = 0.002421222502048994\n",
      "Iteration 25016: loss = 0.00242121225198058\n",
      "Iteration 25017: loss = 0.002421202002023196\n",
      "Iteration 25018: loss = 0.002421191752176843\n",
      "Iteration 25019: loss = 0.002421181502441519\n",
      "Iteration 25020: loss = 0.002421171252817221\n",
      "Iteration 25021: loss = 0.00242116100330395\n",
      "Iteration 25022: loss = 0.0024211507539017027\n",
      "Iteration 25023: loss = 0.0024211405046104797\n",
      "Iteration 25024: loss = 0.0024211302554302777\n",
      "Iteration 25025: loss = 0.0024211200063610967\n",
      "Iteration 25026: loss = 0.0024211097574029345\n",
      "Iteration 25027: loss = 0.0024210995085557903\n",
      "Iteration 25028: loss = 0.0024210892598196627\n",
      "Iteration 25029: loss = 0.00242107901119455\n",
      "Iteration 25030: loss = 0.0024210687626804507\n",
      "Iteration 25031: loss = 0.0024210585142773644\n",
      "Iteration 25032: loss = 0.0024210482659852888\n",
      "Iteration 25033: loss = 0.002421038017804223\n",
      "Iteration 25034: loss = 0.002421027769734166\n",
      "Iteration 25035: loss = 0.002421017521775115\n",
      "Iteration 25036: loss = 0.0024210072739270705\n",
      "Iteration 25037: loss = 0.002420997026190029\n",
      "Iteration 25038: loss = 0.0024209867785639915\n",
      "Iteration 25039: loss = 0.0024209765310489553\n",
      "Iteration 25040: loss = 0.0024209662836449193\n",
      "Iteration 25041: loss = 0.0024209560363518817\n",
      "Iteration 25042: loss = 0.002420945789169842\n",
      "Iteration 25043: loss = 0.002420935542098798\n",
      "Iteration 25044: loss = 0.0024209252951387494\n",
      "Iteration 25045: loss = 0.002420915048289694\n",
      "Iteration 25046: loss = 0.0024209048015516304\n",
      "Iteration 25047: loss = 0.0024208945549245575\n",
      "Iteration 25048: loss = 0.002420884308408474\n",
      "Iteration 25049: loss = 0.0024208740620033785\n",
      "Iteration 25050: loss = 0.0024208638157092698\n",
      "Iteration 25051: loss = 0.002420853569526146\n",
      "Iteration 25052: loss = 0.0024208433234540064\n",
      "Iteration 25053: loss = 0.002420833077492849\n",
      "Iteration 25054: loss = 0.002420822831642673\n",
      "Iteration 25055: loss = 0.0024208125859034773\n",
      "Iteration 25056: loss = 0.0024208023402752595\n",
      "Iteration 25057: loss = 0.0024207920947580193\n",
      "Iteration 25058: loss = 0.002420781849351754\n",
      "Iteration 25059: loss = 0.0024207716040564645\n",
      "Iteration 25060: loss = 0.002420761358872147\n",
      "Iteration 25061: loss = 0.0024207511137988016\n",
      "Iteration 25062: loss = 0.002420740868836426\n",
      "Iteration 25063: loss = 0.00242073062398502\n",
      "Iteration 25064: loss = 0.002420720379244582\n",
      "Iteration 25065: loss = 0.0024207101346151094\n",
      "Iteration 25066: loss = 0.002420699890096602\n",
      "Iteration 25067: loss = 0.0024206896456890584\n",
      "Iteration 25068: loss = 0.0024206794013924768\n",
      "Iteration 25069: loss = 0.002420669157206856\n",
      "Iteration 25070: loss = 0.002420658913132195\n",
      "Iteration 25071: loss = 0.002420648669168492\n",
      "Iteration 25072: loss = 0.002420638425315746\n",
      "Iteration 25073: loss = 0.002420628181573955\n",
      "Iteration 25074: loss = 0.0024206179379431185\n",
      "Iteration 25075: loss = 0.0024206076944232346\n",
      "Iteration 25076: loss = 0.002420597451014302\n",
      "Iteration 25077: loss = 0.0024205872077163195\n",
      "Iteration 25078: loss = 0.0024205769645292852\n",
      "Iteration 25079: loss = 0.002420566721453199\n",
      "Iteration 25080: loss = 0.0024205564784880586\n",
      "Iteration 25081: loss = 0.0024205462356338622\n",
      "Iteration 25082: loss = 0.00242053599289061\n",
      "Iteration 25083: loss = 0.0024205257502582993\n",
      "Iteration 25084: loss = 0.0024205155077369283\n",
      "Iteration 25085: loss = 0.002420505265326497\n",
      "Iteration 25086: loss = 0.0024204950230270043\n",
      "Iteration 25087: loss = 0.0024204847808384465\n",
      "Iteration 25088: loss = 0.002420474538760825\n",
      "Iteration 25089: loss = 0.0024204642967941372\n",
      "Iteration 25090: loss = 0.002420454054938382\n",
      "Iteration 25091: loss = 0.0024204438131935568\n",
      "Iteration 25092: loss = 0.002420433571559662\n",
      "Iteration 25093: loss = 0.002420423330036696\n",
      "Iteration 25094: loss = 0.0024204130886246564\n",
      "Iteration 25095: loss = 0.0024204028473235424\n",
      "Iteration 25096: loss = 0.0024203926061333527\n",
      "Iteration 25097: loss = 0.002420382365054086\n",
      "Iteration 25098: loss = 0.0024203721240857404\n",
      "Iteration 25099: loss = 0.0024203618832283154\n",
      "Iteration 25100: loss = 0.0024203516424818093\n",
      "Iteration 25101: loss = 0.0024203414018462207\n",
      "Iteration 25102: loss = 0.0024203311613215485\n",
      "Iteration 25103: loss = 0.0024203209209077907\n",
      "Iteration 25104: loss = 0.0024203106806049467\n",
      "Iteration 25105: loss = 0.002420300440413014\n",
      "Iteration 25106: loss = 0.0024202902003319926\n",
      "Iteration 25107: loss = 0.0024202799603618807\n",
      "Iteration 25108: loss = 0.002420269720502676\n",
      "Iteration 25109: loss = 0.0024202594807543787\n",
      "Iteration 25110: loss = 0.002420249241116986\n",
      "Iteration 25111: loss = 0.002420239001590498\n",
      "Iteration 25112: loss = 0.0024202287621749126\n",
      "Iteration 25113: loss = 0.0024202185228702276\n",
      "Iteration 25114: loss = 0.0024202082836764435\n",
      "Iteration 25115: loss = 0.0024201980445935577\n",
      "Iteration 25116: loss = 0.0024201878056215684\n",
      "Iteration 25117: loss = 0.0024201775667604753\n",
      "Iteration 25118: loss = 0.0024201673280102765\n",
      "Iteration 25119: loss = 0.0024201570893709716\n",
      "Iteration 25120: loss = 0.002420146850842558\n",
      "Iteration 25121: loss = 0.002420136612425034\n",
      "Iteration 25122: loss = 0.0024201263741183993\n",
      "Iteration 25123: loss = 0.0024201161359226533\n",
      "Iteration 25124: loss = 0.002420105897837793\n",
      "Iteration 25125: loss = 0.0024200956598638174\n",
      "Iteration 25126: loss = 0.0024200854220007254\n",
      "Iteration 25127: loss = 0.002420075184248516\n",
      "Iteration 25128: loss = 0.002420064946607187\n",
      "Iteration 25129: loss = 0.0024200547090767388\n",
      "Iteration 25130: loss = 0.0024200444716571674\n",
      "Iteration 25131: loss = 0.0024200342343484735\n",
      "Iteration 25132: loss = 0.002420023997150655\n",
      "Iteration 25133: loss = 0.0024200137600637103\n",
      "Iteration 25134: loss = 0.0024200035230876387\n",
      "Iteration 25135: loss = 0.0024199932862224386\n",
      "Iteration 25136: loss = 0.002419983049468108\n",
      "Iteration 25137: loss = 0.0024199728128246473\n",
      "Iteration 25138: loss = 0.002419962576292053\n",
      "Iteration 25139: loss = 0.0024199523398703254\n",
      "Iteration 25140: loss = 0.0024199421035594622\n",
      "Iteration 25141: loss = 0.0024199318673594618\n",
      "Iteration 25142: loss = 0.002419921631270323\n",
      "Iteration 25143: loss = 0.002419911395292046\n",
      "Iteration 25144: loss = 0.0024199011594246275\n",
      "Iteration 25145: loss = 0.002419890923668067\n",
      "Iteration 25146: loss = 0.002419880688022363\n",
      "Iteration 25147: loss = 0.0024198704524875142\n",
      "Iteration 25148: loss = 0.0024198602170635193\n",
      "Iteration 25149: loss = 0.002419849981750377\n",
      "Iteration 25150: loss = 0.0024198397465480855\n",
      "Iteration 25151: loss = 0.002419829511456644\n",
      "Iteration 25152: loss = 0.0024198192764760505\n",
      "Iteration 25153: loss = 0.0024198090416063045\n",
      "Iteration 25154: loss = 0.002419798806847404\n",
      "Iteration 25155: loss = 0.0024197885721993473\n",
      "Iteration 25156: loss = 0.0024197783376621344\n",
      "Iteration 25157: loss = 0.0024197681032357625\n",
      "Iteration 25158: loss = 0.0024197578689202307\n",
      "Iteration 25159: loss = 0.0024197476347155387\n",
      "Iteration 25160: loss = 0.0024197374006216838\n",
      "Iteration 25161: loss = 0.002419727166638665\n",
      "Iteration 25162: loss = 0.002419716932766481\n",
      "Iteration 25163: loss = 0.0024197066990051305\n",
      "Iteration 25164: loss = 0.0024196964653546127\n",
      "Iteration 25165: loss = 0.0024196862318149256\n",
      "Iteration 25166: loss = 0.002419675998386067\n",
      "Iteration 25167: loss = 0.0024196657650680372\n",
      "Iteration 25168: loss = 0.0024196555318608337\n",
      "Iteration 25169: loss = 0.0024196452987644557\n",
      "Iteration 25170: loss = 0.002419635065778902\n",
      "Iteration 25171: loss = 0.0024196248329041714\n",
      "Iteration 25172: loss = 0.002419614600140261\n",
      "Iteration 25173: loss = 0.002419604367487171\n",
      "Iteration 25174: loss = 0.0024195941349448996\n",
      "Iteration 25175: loss = 0.0024195839025134454\n",
      "Iteration 25176: loss = 0.002419573670192807\n",
      "Iteration 25177: loss = 0.0024195634379829837\n",
      "Iteration 25178: loss = 0.002419553205883973\n",
      "Iteration 25179: loss = 0.002419542973895775\n",
      "Iteration 25180: loss = 0.0024195327420183863\n",
      "Iteration 25181: loss = 0.0024195225102518073\n",
      "Iteration 25182: loss = 0.002419512278596036\n",
      "Iteration 25183: loss = 0.0024195020470510705\n",
      "Iteration 25184: loss = 0.0024194918156169106\n",
      "Iteration 25185: loss = 0.0024194815842935543\n",
      "Iteration 25186: loss = 0.0024194713530810005\n",
      "Iteration 25187: loss = 0.002419461121979248\n",
      "Iteration 25188: loss = 0.0024194508909882945\n",
      "Iteration 25189: loss = 0.0024194406601081397\n",
      "Iteration 25190: loss = 0.0024194304293387813\n",
      "Iteration 25191: loss = 0.0024194201986802192\n",
      "Iteration 25192: loss = 0.0024194099681324513\n",
      "Iteration 25193: loss = 0.0024193997376954755\n",
      "Iteration 25194: loss = 0.002419389507369291\n",
      "Iteration 25195: loss = 0.002419379277153898\n",
      "Iteration 25196: loss = 0.0024193690470492926\n",
      "Iteration 25197: loss = 0.0024193588170554757\n",
      "Iteration 25198: loss = 0.0024193485871724443\n",
      "Iteration 25199: loss = 0.002419338357400197\n",
      "Iteration 25200: loss = 0.0024193281277387346\n",
      "Iteration 25201: loss = 0.0024193178981880524\n",
      "Iteration 25202: loss = 0.0024193076687481527\n",
      "Iteration 25203: loss = 0.002419297439419031\n",
      "Iteration 25204: loss = 0.002419287210200688\n",
      "Iteration 25205: loss = 0.002419276981093121\n",
      "Iteration 25206: loss = 0.00241926675209633\n",
      "Iteration 25207: loss = 0.0024192565232103126\n",
      "Iteration 25208: loss = 0.0024192462944350674\n",
      "Iteration 25209: loss = 0.002419236065770594\n",
      "Iteration 25210: loss = 0.0024192258372168897\n",
      "Iteration 25211: loss = 0.002419215608773954\n",
      "Iteration 25212: loss = 0.002419205380441786\n",
      "Iteration 25213: loss = 0.0024191951522203833\n",
      "Iteration 25214: loss = 0.0024191849241097453\n",
      "Iteration 25215: loss = 0.0024191746961098703\n",
      "Iteration 25216: loss = 0.0024191644682207574\n",
      "Iteration 25217: loss = 0.0024191542404424045\n",
      "Iteration 25218: loss = 0.0024191440127748105\n",
      "Iteration 25219: loss = 0.0024191337852179744\n",
      "Iteration 25220: loss = 0.0024191235577718942\n",
      "Iteration 25221: loss = 0.0024191133304365692\n",
      "Iteration 25222: loss = 0.002419103103211998\n",
      "Iteration 25223: loss = 0.0024190928760981787\n",
      "Iteration 25224: loss = 0.002419082649095111\n",
      "Iteration 25225: loss = 0.002419072422202792\n",
      "Iteration 25226: loss = 0.0024190621954212218\n",
      "Iteration 25227: loss = 0.0024190519687503982\n",
      "Iteration 25228: loss = 0.0024190417421903194\n",
      "Iteration 25229: loss = 0.0024190315157409854\n",
      "Iteration 25230: loss = 0.0024190212894023943\n",
      "Iteration 25231: loss = 0.0024190110631745437\n",
      "Iteration 25232: loss = 0.0024190008370574344\n",
      "Iteration 25233: loss = 0.0024189906110510632\n",
      "Iteration 25234: loss = 0.002418980385155429\n",
      "Iteration 25235: loss = 0.0024189701593705314\n",
      "Iteration 25236: loss = 0.0024189599336963685\n",
      "Iteration 25237: loss = 0.002418949708132939\n",
      "Iteration 25238: loss = 0.0024189394826802414\n",
      "Iteration 25239: loss = 0.0024189292573382733\n",
      "Iteration 25240: loss = 0.0024189190321070356\n",
      "Iteration 25241: loss = 0.002418908806986525\n",
      "Iteration 25242: loss = 0.0024188985819767416\n",
      "Iteration 25243: loss = 0.0024188883570776826\n",
      "Iteration 25244: loss = 0.0024188781322893485\n",
      "Iteration 25245: loss = 0.002418867907611736\n",
      "Iteration 25246: loss = 0.002418857683044845\n",
      "Iteration 25247: loss = 0.002418847458588674\n",
      "Iteration 25248: loss = 0.0024188372342432204\n",
      "Iteration 25249: loss = 0.0024188270100084846\n",
      "Iteration 25250: loss = 0.0024188167858844646\n",
      "Iteration 25251: loss = 0.0024188065618711584\n",
      "Iteration 25252: loss = 0.002418796337968566\n",
      "Iteration 25253: loss = 0.0024187861141766846\n",
      "Iteration 25254: loss = 0.0024187758904955134\n",
      "Iteration 25255: loss = 0.002418765666925051\n",
      "Iteration 25256: loss = 0.002418755443465297\n",
      "Iteration 25257: loss = 0.002418745220116249\n",
      "Iteration 25258: loss = 0.0024187349968779054\n",
      "Iteration 25259: loss = 0.0024187247737502654\n",
      "Iteration 25260: loss = 0.0024187145507333276\n",
      "Iteration 25261: loss = 0.002418704327827091\n",
      "Iteration 25262: loss = 0.002418694105031553\n",
      "Iteration 25263: loss = 0.002418683882346714\n",
      "Iteration 25264: loss = 0.0024186736597725713\n",
      "Iteration 25265: loss = 0.0024186634373091244\n",
      "Iteration 25266: loss = 0.002418653214956371\n",
      "Iteration 25267: loss = 0.00241864299271431\n",
      "Iteration 25268: loss = 0.0024186327705829416\n",
      "Iteration 25269: loss = 0.002418622548562262\n",
      "Iteration 25270: loss = 0.0024186123266522717\n",
      "Iteration 25271: loss = 0.0024186021048529685\n",
      "Iteration 25272: loss = 0.002418591883164351\n",
      "Iteration 25273: loss = 0.0024185816615864184\n",
      "Iteration 25274: loss = 0.002418571440119169\n",
      "Iteration 25275: loss = 0.002418561218762601\n",
      "Iteration 25276: loss = 0.0024185509975167144\n",
      "Iteration 25277: loss = 0.0024185407763815062\n",
      "Iteration 25278: loss = 0.002418530555356976\n",
      "Iteration 25279: loss = 0.002418520334443122\n",
      "Iteration 25280: loss = 0.0024185101136399434\n",
      "Iteration 25281: loss = 0.002418499892947439\n",
      "Iteration 25282: loss = 0.0024184896723656057\n",
      "Iteration 25283: loss = 0.002418479451894445\n",
      "Iteration 25284: loss = 0.002418469231533953\n",
      "Iteration 25285: loss = 0.0024184590112841294\n",
      "Iteration 25286: loss = 0.0024184487911449726\n",
      "Iteration 25287: loss = 0.002418438571116482\n",
      "Iteration 25288: loss = 0.0024184283511986555\n",
      "Iteration 25289: loss = 0.0024184181313914917\n",
      "Iteration 25290: loss = 0.0024184079116949895\n",
      "Iteration 25291: loss = 0.002418397692109148\n",
      "Iteration 25292: loss = 0.0024183874726339642\n",
      "Iteration 25293: loss = 0.0024183772532694386\n",
      "Iteration 25294: loss = 0.00241836703401557\n",
      "Iteration 25295: loss = 0.002418356814872355\n",
      "Iteration 25296: loss = 0.002418346595839794\n",
      "Iteration 25297: loss = 0.002418336376917885\n",
      "Iteration 25298: loss = 0.0024183261581066265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25299: loss = 0.0024183159394060177\n",
      "Iteration 25300: loss = 0.002418305720816057\n",
      "Iteration 25301: loss = 0.0024182955023367425\n",
      "Iteration 25302: loss = 0.0024182852839680744\n",
      "Iteration 25303: loss = 0.0024182750657100486\n",
      "Iteration 25304: loss = 0.0024182648475626666\n",
      "Iteration 25305: loss = 0.0024182546295259256\n",
      "Iteration 25306: loss = 0.002418244411599825\n",
      "Iteration 25307: loss = 0.0024182341937843623\n",
      "Iteration 25308: loss = 0.002418223976079537\n",
      "Iteration 25309: loss = 0.0024182137584853472\n",
      "Iteration 25310: loss = 0.0024182035410017927\n",
      "Iteration 25311: loss = 0.002418193323628871\n",
      "Iteration 25312: loss = 0.002418183106366581\n",
      "Iteration 25313: loss = 0.0024181728892149214\n",
      "Iteration 25314: loss = 0.002418162672173891\n",
      "Iteration 25315: loss = 0.002418152455243488\n",
      "Iteration 25316: loss = 0.0024181422384237116\n",
      "Iteration 25317: loss = 0.002418132021714561\n",
      "Iteration 25318: loss = 0.002418121805116033\n",
      "Iteration 25319: loss = 0.002418111588628128\n",
      "Iteration 25320: loss = 0.0024181013722508436\n",
      "Iteration 25321: loss = 0.002418091155984179\n",
      "Iteration 25322: loss = 0.0024180809398281324\n",
      "Iteration 25323: loss = 0.0024180707237827035\n",
      "Iteration 25324: loss = 0.0024180605078478897\n",
      "Iteration 25325: loss = 0.00241805029202369\n",
      "Iteration 25326: loss = 0.002418040076310103\n",
      "Iteration 25327: loss = 0.002418029860707128\n",
      "Iteration 25328: loss = 0.0024180196452147627\n",
      "Iteration 25329: loss = 0.002418009429833007\n",
      "Iteration 25330: loss = 0.002417999214561858\n",
      "Iteration 25331: loss = 0.0024179889994013154\n",
      "Iteration 25332: loss = 0.0024179787843513767\n",
      "Iteration 25333: loss = 0.0024179685694120426\n",
      "Iteration 25334: loss = 0.00241795835458331\n",
      "Iteration 25335: loss = 0.002417948139865178\n",
      "Iteration 25336: loss = 0.002417937925257646\n",
      "Iteration 25337: loss = 0.0024179277107607114\n",
      "Iteration 25338: loss = 0.0024179174963743734\n",
      "Iteration 25339: loss = 0.002417907282098631\n",
      "Iteration 25340: loss = 0.0024178970679334818\n",
      "Iteration 25341: loss = 0.002417886853878926\n",
      "Iteration 25342: loss = 0.002417876639934961\n",
      "Iteration 25343: loss = 0.002417866426101586\n",
      "Iteration 25344: loss = 0.0024178562123788\n",
      "Iteration 25345: loss = 0.0024178459987666003\n",
      "Iteration 25346: loss = 0.0024178357852649873\n",
      "Iteration 25347: loss = 0.002417825571873958\n",
      "Iteration 25348: loss = 0.002417815358593512\n",
      "Iteration 25349: loss = 0.0024178051454236483\n",
      "Iteration 25350: loss = 0.0024177949323643646\n",
      "Iteration 25351: loss = 0.0024177847194156595\n",
      "Iteration 25352: loss = 0.002417774506577533\n",
      "Iteration 25353: loss = 0.0024177642938499823\n",
      "Iteration 25354: loss = 0.0024177540812330063\n",
      "Iteration 25355: loss = 0.002417743868726604\n",
      "Iteration 25356: loss = 0.0024177336563307745\n",
      "Iteration 25357: loss = 0.0024177234440455157\n",
      "Iteration 25358: loss = 0.002417713231870827\n",
      "Iteration 25359: loss = 0.0024177030198067055\n",
      "Iteration 25360: loss = 0.0024176928078531515\n",
      "Iteration 25361: loss = 0.002417682596010163\n",
      "Iteration 25362: loss = 0.0024176723842777386\n",
      "Iteration 25363: loss = 0.002417662172655877\n",
      "Iteration 25364: loss = 0.0024176519611445774\n",
      "Iteration 25365: loss = 0.002417641749743837\n",
      "Iteration 25366: loss = 0.002417631538453656\n",
      "Iteration 25367: loss = 0.002417621327274032\n",
      "Iteration 25368: loss = 0.0024176111162049646\n",
      "Iteration 25369: loss = 0.0024176009052464514\n",
      "Iteration 25370: loss = 0.0024175906943984917\n",
      "Iteration 25371: loss = 0.0024175804836610846\n",
      "Iteration 25372: loss = 0.0024175702730342276\n",
      "Iteration 25373: loss = 0.0024175600625179197\n",
      "Iteration 25374: loss = 0.0024175498521121605\n",
      "Iteration 25375: loss = 0.0024175396418169475\n",
      "Iteration 25376: loss = 0.002417529431632279\n",
      "Iteration 25377: loss = 0.0024175192215581554\n",
      "Iteration 25378: loss = 0.0024175090115945742\n",
      "Iteration 25379: loss = 0.002417498801741534\n",
      "Iteration 25380: loss = 0.002417488591999034\n",
      "Iteration 25381: loss = 0.002417478382367072\n",
      "Iteration 25382: loss = 0.002417468172845647\n",
      "Iteration 25383: loss = 0.002417457963434758\n",
      "Iteration 25384: loss = 0.002417447754134404\n",
      "Iteration 25385: loss = 0.0024174375449445827\n",
      "Iteration 25386: loss = 0.002417427335865293\n",
      "Iteration 25387: loss = 0.002417417126896533\n",
      "Iteration 25388: loss = 0.002417406918038303\n",
      "Iteration 25389: loss = 0.0024173967092906007\n",
      "Iteration 25390: loss = 0.002417386500653424\n",
      "Iteration 25391: loss = 0.002417376292126773\n",
      "Iteration 25392: loss = 0.0024173660837106454\n",
      "Iteration 25393: loss = 0.0024173558754050396\n",
      "Iteration 25394: loss = 0.0024173456672099553\n",
      "Iteration 25395: loss = 0.00241733545912539\n",
      "Iteration 25396: loss = 0.002417325251151343\n",
      "Iteration 25397: loss = 0.002417315043287813\n",
      "Iteration 25398: loss = 0.0024173048355347997\n",
      "Iteration 25399: loss = 0.002417294627892299\n",
      "Iteration 25400: loss = 0.0024172844203603117\n",
      "Iteration 25401: loss = 0.002417274212938836\n",
      "Iteration 25402: loss = 0.0024172640056278694\n",
      "Iteration 25403: loss = 0.0024172537984274126\n",
      "Iteration 25404: loss = 0.0024172435913374633\n",
      "Iteration 25405: loss = 0.002417233384358019\n",
      "Iteration 25406: loss = 0.0024172231774890803\n",
      "Iteration 25407: loss = 0.0024172129707306445\n",
      "Iteration 25408: loss = 0.002417202764082711\n",
      "Iteration 25409: loss = 0.002417192557545278\n",
      "Iteration 25410: loss = 0.002417182351118345\n",
      "Iteration 25411: loss = 0.0024171721448019083\n",
      "Iteration 25412: loss = 0.0024171619385959693\n",
      "Iteration 25413: loss = 0.0024171517325005257\n",
      "Iteration 25414: loss = 0.0024171415265155753\n",
      "Iteration 25415: loss = 0.0024171313206411177\n",
      "Iteration 25416: loss = 0.0024171211148771516\n",
      "Iteration 25417: loss = 0.0024171109092236747\n",
      "Iteration 25418: loss = 0.002417100703680687\n",
      "Iteration 25419: loss = 0.002417090498248186\n",
      "Iteration 25420: loss = 0.002417080292926171\n",
      "Iteration 25421: loss = 0.0024170700877146397\n",
      "Iteration 25422: loss = 0.002417059882613592\n",
      "Iteration 25423: loss = 0.0024170496776230264\n",
      "Iteration 25424: loss = 0.0024170394727429404\n",
      "Iteration 25425: loss = 0.002417029267973334\n",
      "Iteration 25426: loss = 0.0024170190633142053\n",
      "Iteration 25427: loss = 0.002417008858765552\n",
      "Iteration 25428: loss = 0.002416998654327375\n",
      "Iteration 25429: loss = 0.0024169884499996705\n",
      "Iteration 25430: loss = 0.0024169782457824384\n",
      "Iteration 25431: loss = 0.002416968041675678\n",
      "Iteration 25432: loss = 0.0024169578376793868\n",
      "Iteration 25433: loss = 0.0024169476337935632\n",
      "Iteration 25434: loss = 0.0024169374300182073\n",
      "Iteration 25435: loss = 0.0024169272263533164\n",
      "Iteration 25436: loss = 0.0024169170227988897\n",
      "Iteration 25437: loss = 0.0024169068193549263\n",
      "Iteration 25438: loss = 0.0024168966160214235\n",
      "Iteration 25439: loss = 0.0024168864127983814\n",
      "Iteration 25440: loss = 0.002416876209685798\n",
      "Iteration 25441: loss = 0.002416866006683672\n",
      "Iteration 25442: loss = 0.002416855803792001\n",
      "Iteration 25443: loss = 0.002416845601010786\n",
      "Iteration 25444: loss = 0.002416835398340024\n",
      "Iteration 25445: loss = 0.0024168251957797137\n",
      "Iteration 25446: loss = 0.0024168149933298543\n",
      "Iteration 25447: loss = 0.002416804790990444\n",
      "Iteration 25448: loss = 0.0024167945887614817\n",
      "Iteration 25449: loss = 0.0024167843866429663\n",
      "Iteration 25450: loss = 0.0024167741846348964\n",
      "Iteration 25451: loss = 0.0024167639827372693\n",
      "Iteration 25452: loss = 0.002416753780950085\n",
      "Iteration 25453: loss = 0.002416743579273343\n",
      "Iteration 25454: loss = 0.0024167333777070396\n",
      "Iteration 25455: loss = 0.0024167231762511746\n",
      "Iteration 25456: loss = 0.0024167129749057474\n",
      "Iteration 25457: loss = 0.002416702773670756\n",
      "Iteration 25458: loss = 0.0024166925725461986\n",
      "Iteration 25459: loss = 0.002416682371532074\n",
      "Iteration 25460: loss = 0.002416672170628382\n",
      "Iteration 25461: loss = 0.0024166619698351203\n",
      "Iteration 25462: loss = 0.0024166517691522867\n",
      "Iteration 25463: loss = 0.0024166415685798817\n",
      "Iteration 25464: loss = 0.0024166313681179027\n",
      "Iteration 25465: loss = 0.0024166211677663484\n",
      "Iteration 25466: loss = 0.002416610967525218\n",
      "Iteration 25467: loss = 0.00241660076739451\n",
      "Iteration 25468: loss = 0.0024165905673742224\n",
      "Iteration 25469: loss = 0.0024165803674643547\n",
      "Iteration 25470: loss = 0.0024165701676649052\n",
      "Iteration 25471: loss = 0.0024165599679758726\n",
      "Iteration 25472: loss = 0.0024165497683972556\n",
      "Iteration 25473: loss = 0.002416539568929052\n",
      "Iteration 25474: loss = 0.002416529369571262\n",
      "Iteration 25475: loss = 0.0024165191703238833\n",
      "Iteration 25476: loss = 0.0024165089711869147\n",
      "Iteration 25477: loss = 0.0024164987721603548\n",
      "Iteration 25478: loss = 0.002416488573244203\n",
      "Iteration 25479: loss = 0.002416478374438456\n",
      "Iteration 25480: loss = 0.0024164681757431142\n",
      "Iteration 25481: loss = 0.0024164579771581763\n",
      "Iteration 25482: loss = 0.0024164477786836392\n",
      "Iteration 25483: loss = 0.002416437580319504\n",
      "Iteration 25484: loss = 0.0024164273820657675\n",
      "Iteration 25485: loss = 0.002416417183922429\n",
      "Iteration 25486: loss = 0.002416406985889487\n",
      "Iteration 25487: loss = 0.0024163967879669403\n",
      "Iteration 25488: loss = 0.002416386590154788\n",
      "Iteration 25489: loss = 0.002416376392453027\n",
      "Iteration 25490: loss = 0.002416366194861658\n",
      "Iteration 25491: loss = 0.002416355997380679\n",
      "Iteration 25492: loss = 0.0024163458000100884\n",
      "Iteration 25493: loss = 0.0024163356027498846\n",
      "Iteration 25494: loss = 0.0024163254056000674\n",
      "Iteration 25495: loss = 0.002416315208560634\n",
      "Iteration 25496: loss = 0.002416305011631584\n",
      "Iteration 25497: loss = 0.0024162948148129153\n",
      "Iteration 25498: loss = 0.0024162846181046273\n",
      "Iteration 25499: loss = 0.0024162744215067184\n",
      "Iteration 25500: loss = 0.0024162642250191873\n",
      "Iteration 25501: loss = 0.002416254028642032\n",
      "Iteration 25502: loss = 0.0024162438323752514\n",
      "Iteration 25503: loss = 0.0024162336362188452\n",
      "Iteration 25504: loss = 0.0024162234401728113\n",
      "Iteration 25505: loss = 0.002416213244237148\n",
      "Iteration 25506: loss = 0.0024162030484118544\n",
      "Iteration 25507: loss = 0.0024161928526969293\n",
      "Iteration 25508: loss = 0.0024161826570923708\n",
      "Iteration 25509: loss = 0.0024161724615981775\n",
      "Iteration 25510: loss = 0.0024161622662143487\n",
      "Iteration 25511: loss = 0.0024161520709408833\n",
      "Iteration 25512: loss = 0.0024161418757777785\n",
      "Iteration 25513: loss = 0.0024161316807250342\n",
      "Iteration 25514: loss = 0.0024161214857826483\n",
      "Iteration 25515: loss = 0.0024161112909506207\n",
      "Iteration 25516: loss = 0.0024161010962289488\n",
      "Iteration 25517: loss = 0.0024160909016176313\n",
      "Iteration 25518: loss = 0.002416080707116668\n",
      "Iteration 25519: loss = 0.0024160705127260554\n",
      "Iteration 25520: loss = 0.0024160603184457943\n",
      "Iteration 25521: loss = 0.0024160501242758824\n",
      "Iteration 25522: loss = 0.0024160399302163185\n",
      "Iteration 25523: loss = 0.002416029736267101\n",
      "Iteration 25524: loss = 0.0024160195424282292\n",
      "Iteration 25525: loss = 0.0024160093486997013\n",
      "Iteration 25526: loss = 0.002415999155081515\n",
      "Iteration 25527: loss = 0.0024159889615736714\n",
      "Iteration 25528: loss = 0.002415978768176167\n",
      "Iteration 25529: loss = 0.0024159685748890007\n",
      "Iteration 25530: loss = 0.002415958381712172\n",
      "Iteration 25531: loss = 0.00241594818864568\n",
      "Iteration 25532: loss = 0.0024159379956895212\n",
      "Iteration 25533: loss = 0.002415927802843696\n",
      "Iteration 25534: loss = 0.0024159176101082023\n",
      "Iteration 25535: loss = 0.002415907417483039\n",
      "Iteration 25536: loss = 0.0024158972249682053\n",
      "Iteration 25537: loss = 0.0024158870325636985\n",
      "Iteration 25538: loss = 0.002415876840269519\n",
      "Iteration 25539: loss = 0.002415866648085664\n",
      "Iteration 25540: loss = 0.002415856456012133\n",
      "Iteration 25541: loss = 0.002415846264048924\n",
      "Iteration 25542: loss = 0.0024158360721960362\n",
      "Iteration 25543: loss = 0.002415825880453468\n",
      "Iteration 25544: loss = 0.0024158156888212175\n",
      "Iteration 25545: loss = 0.0024158054972992847\n",
      "Iteration 25546: loss = 0.002415795305887667\n",
      "Iteration 25547: loss = 0.0024157851145863636\n",
      "Iteration 25548: loss = 0.002415774923395373\n",
      "Iteration 25549: loss = 0.0024157647323146945\n",
      "Iteration 25550: loss = 0.0024157545413443253\n",
      "Iteration 25551: loss = 0.002415744350484266\n",
      "Iteration 25552: loss = 0.0024157341597345135\n",
      "Iteration 25553: loss = 0.0024157239690950675\n",
      "Iteration 25554: loss = 0.0024157137785659256\n",
      "Iteration 25555: loss = 0.0024157035881470874\n",
      "Iteration 25556: loss = 0.0024156933978385516\n",
      "Iteration 25557: loss = 0.0024156832076403156\n",
      "Iteration 25558: loss = 0.0024156730175523802\n",
      "Iteration 25559: loss = 0.0024156628275747425\n",
      "Iteration 25560: loss = 0.002415652637707401\n",
      "Iteration 25561: loss = 0.002415642447950355\n",
      "Iteration 25562: loss = 0.0024156322583036034\n",
      "Iteration 25563: loss = 0.0024156220687671435\n",
      "Iteration 25564: loss = 0.002415611879340976\n",
      "Iteration 25565: loss = 0.0024156016900250972\n",
      "Iteration 25566: loss = 0.0024155915008195077\n",
      "Iteration 25567: loss = 0.002415581311724206\n",
      "Iteration 25568: loss = 0.002415571122739189\n",
      "Iteration 25569: loss = 0.002415560933864457\n",
      "Iteration 25570: loss = 0.0024155507451000084\n",
      "Iteration 25571: loss = 0.0024155405564458415\n",
      "Iteration 25572: loss = 0.002415530367901955\n",
      "Iteration 25573: loss = 0.0024155201794683477\n",
      "Iteration 25574: loss = 0.002415509991145018\n",
      "Iteration 25575: loss = 0.0024154998029319646\n",
      "Iteration 25576: loss = 0.0024154896148291865\n",
      "Iteration 25577: loss = 0.002415479426836682\n",
      "Iteration 25578: loss = 0.0024154692389544504\n",
      "Iteration 25579: loss = 0.0024154590511824892\n",
      "Iteration 25580: loss = 0.0024154488635207984\n",
      "Iteration 25581: loss = 0.0024154386759693753\n",
      "Iteration 25582: loss = 0.0024154284885282195\n",
      "Iteration 25583: loss = 0.0024154183011973296\n",
      "Iteration 25584: loss = 0.0024154081139767026\n",
      "Iteration 25585: loss = 0.0024153979268663403\n",
      "Iteration 25586: loss = 0.002415387739866238\n",
      "Iteration 25587: loss = 0.0024153775529763967\n",
      "Iteration 25588: loss = 0.0024153673661968145\n",
      "Iteration 25589: loss = 0.0024153571795274896\n",
      "Iteration 25590: loss = 0.0024153469929684207\n",
      "Iteration 25591: loss = 0.0024153368065196073\n",
      "Iteration 25592: loss = 0.0024153266201810465\n",
      "Iteration 25593: loss = 0.002415316433952738\n",
      "Iteration 25594: loss = 0.0024153062478346806\n",
      "Iteration 25595: loss = 0.002415296061826873\n",
      "Iteration 25596: loss = 0.0024152858759293127\n",
      "Iteration 25597: loss = 0.0024152756901419993\n",
      "Iteration 25598: loss = 0.0024152655044649314\n",
      "Iteration 25599: loss = 0.002415255318898108\n",
      "Iteration 25600: loss = 0.002415245133441527\n",
      "Iteration 25601: loss = 0.0024152349480951866\n",
      "Iteration 25602: loss = 0.002415224762859087\n",
      "Iteration 25603: loss = 0.002415214577733226\n",
      "Iteration 25604: loss = 0.002415204392717602\n",
      "Iteration 25605: loss = 0.0024151942078122148\n",
      "Iteration 25606: loss = 0.0024151840230170613\n",
      "Iteration 25607: loss = 0.0024151738383321407\n",
      "Iteration 25608: loss = 0.002415163653757452\n",
      "Iteration 25609: loss = 0.002415153469292995\n",
      "Iteration 25610: loss = 0.0024151432849387664\n",
      "Iteration 25611: loss = 0.0024151331006947658\n",
      "Iteration 25612: loss = 0.002415122916560991\n",
      "Iteration 25613: loss = 0.002415112732537443\n",
      "Iteration 25614: loss = 0.002415102548624117\n",
      "Iteration 25615: loss = 0.0024150923648210147\n",
      "Iteration 25616: loss = 0.002415082181128133\n",
      "Iteration 25617: loss = 0.002415071997545471\n",
      "Iteration 25618: loss = 0.0024150618140730278\n",
      "Iteration 25619: loss = 0.0024150516307108014\n",
      "Iteration 25620: loss = 0.0024150414474587907\n",
      "Iteration 25621: loss = 0.0024150312643169943\n",
      "Iteration 25622: loss = 0.002415021081285411\n",
      "Iteration 25623: loss = 0.0024150108983640394\n",
      "Iteration 25624: loss = 0.0024150007155528774\n",
      "Iteration 25625: loss = 0.0024149905328519246\n",
      "Iteration 25626: loss = 0.00241498035026118\n",
      "Iteration 25627: loss = 0.002414970167780641\n",
      "Iteration 25628: loss = 0.0024149599854103075\n",
      "Iteration 25629: loss = 0.002414949803150177\n",
      "Iteration 25630: loss = 0.0024149396210002495\n",
      "Iteration 25631: loss = 0.002414929438960522\n",
      "Iteration 25632: loss = 0.002414919257030995\n",
      "Iteration 25633: loss = 0.0024149090752116653\n",
      "Iteration 25634: loss = 0.0024148988935025323\n",
      "Iteration 25635: loss = 0.0024148887119035956\n",
      "Iteration 25636: loss = 0.0024148785304148527\n",
      "Iteration 25637: loss = 0.002414868349036302\n",
      "Iteration 25638: loss = 0.0024148581677679434\n",
      "Iteration 25639: loss = 0.0024148479866097747\n",
      "Iteration 25640: loss = 0.0024148378055617948\n",
      "Iteration 25641: loss = 0.002414827624624002\n",
      "Iteration 25642: loss = 0.002414817443796395\n",
      "Iteration 25643: loss = 0.002414807263078973\n",
      "Iteration 25644: loss = 0.0024147970824717345\n",
      "Iteration 25645: loss = 0.0024147869019746776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25646: loss = 0.0024147767215878017\n",
      "Iteration 25647: loss = 0.0024147665413111045\n",
      "Iteration 25648: loss = 0.002414756361144586\n",
      "Iteration 25649: loss = 0.0024147461810882435\n",
      "Iteration 25650: loss = 0.0024147360011420762\n",
      "Iteration 25651: loss = 0.0024147258213060834\n",
      "Iteration 25652: loss = 0.0024147156415802624\n",
      "Iteration 25653: loss = 0.0024147054619646133\n",
      "Iteration 25654: loss = 0.0024146952824591335\n",
      "Iteration 25655: loss = 0.0024146851030638224\n",
      "Iteration 25656: loss = 0.0024146749237786785\n",
      "Iteration 25657: loss = 0.0024146647446037\n",
      "Iteration 25658: loss = 0.002414654565538886\n",
      "Iteration 25659: loss = 0.0024146443865842356\n",
      "Iteration 25660: loss = 0.0024146342077397465\n",
      "Iteration 25661: loss = 0.002414624029005418\n",
      "Iteration 25662: loss = 0.002414613850381248\n",
      "Iteration 25663: loss = 0.0024146036718672367\n",
      "Iteration 25664: loss = 0.002414593493463381\n",
      "Iteration 25665: loss = 0.002414583315169681\n",
      "Iteration 25666: loss = 0.0024145731369861344\n",
      "Iteration 25667: loss = 0.0024145629589127394\n",
      "Iteration 25668: loss = 0.002414552780949496\n",
      "Iteration 25669: loss = 0.002414542603096402\n",
      "Iteration 25670: loss = 0.002414532425353456\n",
      "Iteration 25671: loss = 0.0024145222477206575\n",
      "Iteration 25672: loss = 0.0024145120701980034\n",
      "Iteration 25673: loss = 0.0024145018927854947\n",
      "Iteration 25674: loss = 0.002414491715483129\n",
      "Iteration 25675: loss = 0.002414481538290904\n",
      "Iteration 25676: loss = 0.00241447136120882\n",
      "Iteration 25677: loss = 0.0024144611842368737\n",
      "Iteration 25678: loss = 0.0024144510073750656\n",
      "Iteration 25679: loss = 0.0024144408306233933\n",
      "Iteration 25680: loss = 0.002414430653981856\n",
      "Iteration 25681: loss = 0.0024144204774504525\n",
      "Iteration 25682: loss = 0.0024144103010291804\n",
      "Iteration 25683: loss = 0.002414400124718039\n",
      "Iteration 25684: loss = 0.0024143899485170274\n",
      "Iteration 25685: loss = 0.002414379772426143\n",
      "Iteration 25686: loss = 0.0024143695964453865\n",
      "Iteration 25687: loss = 0.002414359420574755\n",
      "Iteration 25688: loss = 0.002414349244814247\n",
      "Iteration 25689: loss = 0.002414339069163862\n",
      "Iteration 25690: loss = 0.002414328893623598\n",
      "Iteration 25691: loss = 0.002414318718193454\n",
      "Iteration 25692: loss = 0.002414308542873429\n",
      "Iteration 25693: loss = 0.0024142983676635208\n",
      "Iteration 25694: loss = 0.0024142881925637285\n",
      "Iteration 25695: loss = 0.002414278017574051\n",
      "Iteration 25696: loss = 0.002414267842694487\n",
      "Iteration 25697: loss = 0.002414257667925034\n",
      "Iteration 25698: loss = 0.0024142474932656918\n",
      "Iteration 25699: loss = 0.0024142373187164582\n",
      "Iteration 25700: loss = 0.0024142271442773337\n",
      "Iteration 25701: loss = 0.002414216969948315\n",
      "Iteration 25702: loss = 0.0024142067957294015\n",
      "Iteration 25703: loss = 0.002414196621620592\n",
      "Iteration 25704: loss = 0.0024141864476218845\n",
      "Iteration 25705: loss = 0.0024141762737332774\n",
      "Iteration 25706: loss = 0.002414166099954771\n",
      "Iteration 25707: loss = 0.0024141559262863626\n",
      "Iteration 25708: loss = 0.002414145752728052\n",
      "Iteration 25709: loss = 0.002414135579279836\n",
      "Iteration 25710: loss = 0.0024141254059417147\n",
      "Iteration 25711: loss = 0.002414115232713686\n",
      "Iteration 25712: loss = 0.0024141050595957495\n",
      "Iteration 25713: loss = 0.0024140948865879032\n",
      "Iteration 25714: loss = 0.002414084713690146\n",
      "Iteration 25715: loss = 0.002414074540902476\n",
      "Iteration 25716: loss = 0.002414064368224892\n",
      "Iteration 25717: loss = 0.0024140541956573936\n",
      "Iteration 25718: loss = 0.002414044023199978\n",
      "Iteration 25719: loss = 0.002414033850852645\n",
      "Iteration 25720: loss = 0.002414023678615393\n",
      "Iteration 25721: loss = 0.00241401350648822\n",
      "Iteration 25722: loss = 0.0024140033344711258\n",
      "Iteration 25723: loss = 0.0024139931625641077\n",
      "Iteration 25724: loss = 0.0024139829907671656\n",
      "Iteration 25725: loss = 0.002413972819080297\n",
      "Iteration 25726: loss = 0.0024139626475035013\n",
      "Iteration 25727: loss = 0.0024139524760367777\n",
      "Iteration 25728: loss = 0.0024139423046801232\n",
      "Iteration 25729: loss = 0.002413932133433538\n",
      "Iteration 25730: loss = 0.0024139219622970206\n",
      "Iteration 25731: loss = 0.0024139117912705677\n",
      "Iteration 25732: loss = 0.0024139016203541808\n",
      "Iteration 25733: loss = 0.002413891449547857\n",
      "Iteration 25734: loss = 0.0024138812788515947\n",
      "Iteration 25735: loss = 0.0024138711082653933\n",
      "Iteration 25736: loss = 0.0024138609377892515\n",
      "Iteration 25737: loss = 0.0024138507674231666\n",
      "Iteration 25738: loss = 0.0024138405971671395\n",
      "Iteration 25739: loss = 0.002413830427021167\n",
      "Iteration 25740: loss = 0.0024138202569852483\n",
      "Iteration 25741: loss = 0.002413810087059382\n",
      "Iteration 25742: loss = 0.0024137999172435675\n",
      "Iteration 25743: loss = 0.002413789747537803\n",
      "Iteration 25744: loss = 0.002413779577942086\n",
      "Iteration 25745: loss = 0.0024137694084564168\n",
      "Iteration 25746: loss = 0.0024137592390807935\n",
      "Iteration 25747: loss = 0.0024137490698152137\n",
      "Iteration 25748: loss = 0.002413738900659678\n",
      "Iteration 25749: loss = 0.0024137287316141837\n",
      "Iteration 25750: loss = 0.0024137185626787296\n",
      "Iteration 25751: loss = 0.0024137083938533146\n",
      "Iteration 25752: loss = 0.0024136982251379375\n",
      "Iteration 25753: loss = 0.002413688056532598\n",
      "Iteration 25754: loss = 0.002413677888037291\n",
      "Iteration 25755: loss = 0.0024136677196520194\n",
      "Iteration 25756: loss = 0.0024136575513767793\n",
      "Iteration 25757: loss = 0.002413647383211571\n",
      "Iteration 25758: loss = 0.0024136372151563916\n",
      "Iteration 25759: loss = 0.0024136270472112408\n",
      "Iteration 25760: loss = 0.0024136168793761174\n",
      "Iteration 25761: loss = 0.0024136067116510185\n",
      "Iteration 25762: loss = 0.0024135965440359444\n",
      "Iteration 25763: loss = 0.0024135863765308925\n",
      "Iteration 25764: loss = 0.0024135762091358633\n",
      "Iteration 25765: loss = 0.0024135660418508537\n",
      "Iteration 25766: loss = 0.002413555874675863\n",
      "Iteration 25767: loss = 0.00241354570761089\n",
      "Iteration 25768: loss = 0.002413535540655933\n",
      "Iteration 25769: loss = 0.0024135253738109905\n",
      "Iteration 25770: loss = 0.0024135152070760614\n",
      "Iteration 25771: loss = 0.002413505040451145\n",
      "Iteration 25772: loss = 0.002413494873936239\n",
      "Iteration 25773: loss = 0.0024134847075313426\n",
      "Iteration 25774: loss = 0.002413474541236454\n",
      "Iteration 25775: loss = 0.002413464375051572\n",
      "Iteration 25776: loss = 0.002413454208976696\n",
      "Iteration 25777: loss = 0.0024134440430118235\n",
      "Iteration 25778: loss = 0.002413433877156954\n",
      "Iteration 25779: loss = 0.002413423711412085\n",
      "Iteration 25780: loss = 0.002413413545777217\n",
      "Iteration 25781: loss = 0.002413403380252347\n",
      "Iteration 25782: loss = 0.002413393214837475\n",
      "Iteration 25783: loss = 0.002413383049532598\n",
      "Iteration 25784: loss = 0.0024133728843377163\n",
      "Iteration 25785: loss = 0.002413362719252827\n",
      "Iteration 25786: loss = 0.0024133525542779305\n",
      "Iteration 25787: loss = 0.0024133423894130246\n",
      "Iteration 25788: loss = 0.002413332224658107\n",
      "Iteration 25789: loss = 0.002413322060013178\n",
      "Iteration 25790: loss = 0.002413311895478235\n",
      "Iteration 25791: loss = 0.0024133017310532775\n",
      "Iteration 25792: loss = 0.0024132915667383036\n",
      "Iteration 25793: loss = 0.0024132814025333124\n",
      "Iteration 25794: loss = 0.0024132712384383023\n",
      "Iteration 25795: loss = 0.0024132610744532716\n",
      "Iteration 25796: loss = 0.0024132509105782196\n",
      "Iteration 25797: loss = 0.002413240746813145\n",
      "Iteration 25798: loss = 0.002413230583158046\n",
      "Iteration 25799: loss = 0.002413220419612921\n",
      "Iteration 25800: loss = 0.0024132102561777694\n",
      "Iteration 25801: loss = 0.002413200092852589\n",
      "Iteration 25802: loss = 0.002413189929637379\n",
      "Iteration 25803: loss = 0.0024131797665321386\n",
      "Iteration 25804: loss = 0.002413169603536865\n",
      "Iteration 25805: loss = 0.0024131594406515584\n",
      "Iteration 25806: loss = 0.0024131492778762167\n",
      "Iteration 25807: loss = 0.002413139115210838\n",
      "Iteration 25808: loss = 0.002413128952655422\n",
      "Iteration 25809: loss = 0.002413118790209967\n",
      "Iteration 25810: loss = 0.0024131086278744718\n",
      "Iteration 25811: loss = 0.0024130984656489337\n",
      "Iteration 25812: loss = 0.0024130883035333537\n",
      "Iteration 25813: loss = 0.002413078141527729\n",
      "Iteration 25814: loss = 0.0024130679796320583\n",
      "Iteration 25815: loss = 0.00241305781784634\n",
      "Iteration 25816: loss = 0.0024130476561705734\n",
      "Iteration 25817: loss = 0.002413037494604757\n",
      "Iteration 25818: loss = 0.0024130273331488894\n",
      "Iteration 25819: loss = 0.0024130171718029694\n",
      "Iteration 25820: loss = 0.002413007010566995\n",
      "Iteration 25821: loss = 0.002412996849440966\n",
      "Iteration 25822: loss = 0.0024129866884248807\n",
      "Iteration 25823: loss = 0.0024129765275187364\n",
      "Iteration 25824: loss = 0.0024129663667225337\n",
      "Iteration 25825: loss = 0.00241295620603627\n",
      "Iteration 25826: loss = 0.002412946045459944\n",
      "Iteration 25827: loss = 0.0024129358849935547\n",
      "Iteration 25828: loss = 0.0024129257246371006\n",
      "Iteration 25829: loss = 0.002412915564390581\n",
      "Iteration 25830: loss = 0.0024129054042539943\n",
      "Iteration 25831: loss = 0.002412895244227338\n",
      "Iteration 25832: loss = 0.0024128850843106123\n",
      "Iteration 25833: loss = 0.002412874924503815\n",
      "Iteration 25834: loss = 0.002412864764806945\n",
      "Iteration 25835: loss = 0.00241285460522\n",
      "Iteration 25836: loss = 0.0024128444457429806\n",
      "Iteration 25837: loss = 0.002412834286375884\n",
      "Iteration 25838: loss = 0.0024128241271187097\n",
      "Iteration 25839: loss = 0.0024128139679714554\n",
      "Iteration 25840: loss = 0.002412803808934121\n",
      "Iteration 25841: loss = 0.002412793650006703\n",
      "Iteration 25842: loss = 0.002412783491189202\n",
      "Iteration 25843: loss = 0.0024127733324816165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25844: loss = 0.002412763173883945\n",
      "Iteration 25845: loss = 0.0024127530153961857\n",
      "Iteration 25846: loss = 0.002412742857018337\n",
      "Iteration 25847: loss = 0.0024127326987503986\n",
      "Iteration 25848: loss = 0.002412722540592368\n",
      "Iteration 25849: loss = 0.002412712382544245\n",
      "Iteration 25850: loss = 0.002412702224606028\n",
      "Iteration 25851: loss = 0.002412692066777715\n",
      "Iteration 25852: loss = 0.002412681909059305\n",
      "Iteration 25853: loss = 0.002412671751450796\n",
      "Iteration 25854: loss = 0.0024126615939521878\n",
      "Iteration 25855: loss = 0.002412651436563479\n",
      "Iteration 25856: loss = 0.0024126412792846667\n",
      "Iteration 25857: loss = 0.002412631122115752\n",
      "Iteration 25858: loss = 0.0024126209650567314\n",
      "Iteration 25859: loss = 0.0024126108081076046\n",
      "Iteration 25860: loss = 0.0024126006512683695\n",
      "Iteration 25861: loss = 0.002412590494539026\n",
      "Iteration 25862: loss = 0.0024125803379195718\n",
      "Iteration 25863: loss = 0.0024125701814100065\n",
      "Iteration 25864: loss = 0.002412560025010327\n",
      "Iteration 25865: loss = 0.0024125498687205325\n",
      "Iteration 25866: loss = 0.0024125397125406237\n",
      "Iteration 25867: loss = 0.002412529556470597\n",
      "Iteration 25868: loss = 0.0024125194005104513\n",
      "Iteration 25869: loss = 0.0024125092446601864\n",
      "Iteration 25870: loss = 0.0024124990889198\n",
      "Iteration 25871: loss = 0.002412488933289291\n",
      "Iteration 25872: loss = 0.0024124787777686585\n",
      "Iteration 25873: loss = 0.0024124686223579\n",
      "Iteration 25874: loss = 0.002412458467057015\n",
      "Iteration 25875: loss = 0.002412448311866002\n",
      "Iteration 25876: loss = 0.00241243815678486\n",
      "Iteration 25877: loss = 0.002412428001813588\n",
      "Iteration 25878: loss = 0.0024124178469521825\n",
      "Iteration 25879: loss = 0.0024124076922006446\n",
      "Iteration 25880: loss = 0.002412397537558972\n",
      "Iteration 25881: loss = 0.002412387383027163\n",
      "Iteration 25882: loss = 0.0024123772286052166\n",
      "Iteration 25883: loss = 0.002412367074293132\n",
      "Iteration 25884: loss = 0.0024123569200909063\n",
      "Iteration 25885: loss = 0.0024123467659985403\n",
      "Iteration 25886: loss = 0.002412336612016031\n",
      "Iteration 25887: loss = 0.0024123264581433776\n",
      "Iteration 25888: loss = 0.0024123163043805783\n",
      "Iteration 25889: loss = 0.0024123061507276334\n",
      "Iteration 25890: loss = 0.0024122959971845386\n",
      "Iteration 25891: loss = 0.002412285843751296\n",
      "Iteration 25892: loss = 0.0024122756904279017\n",
      "Iteration 25893: loss = 0.0024122655372143553\n",
      "Iteration 25894: loss = 0.002412255384110655\n",
      "Iteration 25895: loss = 0.0024122452311168\n",
      "Iteration 25896: loss = 0.002412235078232789\n",
      "Iteration 25897: loss = 0.002412224925458621\n",
      "Iteration 25898: loss = 0.002412214772794293\n",
      "Iteration 25899: loss = 0.002412204620239805\n",
      "Iteration 25900: loss = 0.002412194467795156\n",
      "Iteration 25901: loss = 0.0024121843154603436\n",
      "Iteration 25902: loss = 0.0024121741632353666\n",
      "Iteration 25903: loss = 0.0024121640111202246\n",
      "Iteration 25904: loss = 0.002412153859114915\n",
      "Iteration 25905: loss = 0.0024121437072194365\n",
      "Iteration 25906: loss = 0.002412133555433789\n",
      "Iteration 25907: loss = 0.002412123403757971\n",
      "Iteration 25908: loss = 0.00241211325219198\n",
      "Iteration 25909: loss = 0.002412103100735815\n",
      "Iteration 25910: loss = 0.0024120929493894757\n",
      "Iteration 25911: loss = 0.0024120827981529594\n",
      "Iteration 25912: loss = 0.002412072647026266\n",
      "Iteration 25913: loss = 0.002412062496009393\n",
      "Iteration 25914: loss = 0.002412052345102339\n",
      "Iteration 25915: loss = 0.002412042194305104\n",
      "Iteration 25916: loss = 0.0024120320436176856\n",
      "Iteration 25917: loss = 0.0024120218930400827\n",
      "Iteration 25918: loss = 0.002412011742572294\n",
      "Iteration 25919: loss = 0.0024120015922143184\n",
      "Iteration 25920: loss = 0.0024119914419661534\n",
      "Iteration 25921: loss = 0.0024119812918277986\n",
      "Iteration 25922: loss = 0.0024119711417992535\n",
      "Iteration 25923: loss = 0.002411960991880515\n",
      "Iteration 25924: loss = 0.0024119508420715834\n",
      "Iteration 25925: loss = 0.002411940692372456\n",
      "Iteration 25926: loss = 0.002411930542783132\n",
      "Iteration 25927: loss = 0.00241192039330361\n",
      "Iteration 25928: loss = 0.002411910243933889\n",
      "Iteration 25929: loss = 0.002411900094673967\n",
      "Iteration 25930: loss = 0.0024118899455238428\n",
      "Iteration 25931: loss = 0.002411879796483516\n",
      "Iteration 25932: loss = 0.002411869647552984\n",
      "Iteration 25933: loss = 0.0024118594987322457\n",
      "Iteration 25934: loss = 0.0024118493500213008\n",
      "Iteration 25935: loss = 0.0024118392014201464\n",
      "Iteration 25936: loss = 0.0024118290529287828\n",
      "Iteration 25937: loss = 0.002411818904547207\n",
      "Iteration 25938: loss = 0.0024118087562754187\n",
      "Iteration 25939: loss = 0.0024117986081134165\n",
      "Iteration 25940: loss = 0.002411788460061198\n",
      "Iteration 25941: loss = 0.002411778312118764\n",
      "Iteration 25942: loss = 0.002411768164286111\n",
      "Iteration 25943: loss = 0.002411758016563239\n",
      "Iteration 25944: loss = 0.0024117478689501463\n",
      "Iteration 25945: loss = 0.002411737721446831\n",
      "Iteration 25946: loss = 0.002411727574053292\n",
      "Iteration 25947: loss = 0.002411717426769528\n",
      "Iteration 25948: loss = 0.0024117072795955385\n",
      "Iteration 25949: loss = 0.002411697132531321\n",
      "Iteration 25950: loss = 0.002411686985576875\n",
      "Iteration 25951: loss = 0.002411676838732198\n",
      "Iteration 25952: loss = 0.00241166669199729\n",
      "Iteration 25953: loss = 0.002411656545372149\n",
      "Iteration 25954: loss = 0.0024116463988567735\n",
      "Iteration 25955: loss = 0.002411636252451163\n",
      "Iteration 25956: loss = 0.0024116261061553146\n",
      "Iteration 25957: loss = 0.0024116159599692284\n",
      "Iteration 25958: loss = 0.0024116058138929026\n",
      "Iteration 25959: loss = 0.0024115956679263352\n",
      "Iteration 25960: loss = 0.0024115855220695264\n",
      "Iteration 25961: loss = 0.0024115753763224736\n",
      "Iteration 25962: loss = 0.002411565230685175\n",
      "Iteration 25963: loss = 0.0024115550851576306\n",
      "Iteration 25964: loss = 0.0024115449397398383\n",
      "Iteration 25965: loss = 0.0024115347944317967\n",
      "Iteration 25966: loss = 0.0024115246492335055\n",
      "Iteration 25967: loss = 0.002411514504144962\n",
      "Iteration 25968: loss = 0.0024115043591661656\n",
      "Iteration 25969: loss = 0.002411494214297114\n",
      "Iteration 25970: loss = 0.0024114840695378076\n",
      "Iteration 25971: loss = 0.0024114739248882436\n",
      "Iteration 25972: loss = 0.0024114637803484213\n",
      "Iteration 25973: loss = 0.0024114536359183384\n",
      "Iteration 25974: loss = 0.0024114434915979946\n",
      "Iteration 25975: loss = 0.0024114333473873885\n",
      "Iteration 25976: loss = 0.0024114232032865193\n",
      "Iteration 25977: loss = 0.0024114130592953835\n",
      "Iteration 25978: loss = 0.002411402915413982\n",
      "Iteration 25979: loss = 0.0024113927716423126\n",
      "Iteration 25980: loss = 0.0024113826279803735\n",
      "Iteration 25981: loss = 0.002411372484428164\n",
      "Iteration 25982: loss = 0.002411362340985683\n",
      "Iteration 25983: loss = 0.0024113521976529277\n",
      "Iteration 25984: loss = 0.0024113420544298984\n",
      "Iteration 25985: loss = 0.002411331911316593\n",
      "Iteration 25986: loss = 0.00241132176831301\n",
      "Iteration 25987: loss = 0.002411311625419149\n",
      "Iteration 25988: loss = 0.0024113014826350073\n",
      "Iteration 25989: loss = 0.0024112913399605843\n",
      "Iteration 25990: loss = 0.00241128119739588\n",
      "Iteration 25991: loss = 0.0024112710549408895\n",
      "Iteration 25992: loss = 0.0024112609125956147\n",
      "Iteration 25993: loss = 0.0024112507703600533\n",
      "Iteration 25994: loss = 0.002411240628234203\n",
      "Iteration 25995: loss = 0.002411230486218064\n",
      "Iteration 25996: loss = 0.0024112203443116343\n",
      "Iteration 25997: loss = 0.002411210202514912\n",
      "Iteration 25998: loss = 0.0024112000608278966\n",
      "Iteration 25999: loss = 0.0024111899192505865\n",
      "Iteration 26000: loss = 0.00241117977778298\n",
      "Iteration 26001: loss = 0.0024111696364250757\n",
      "Iteration 26002: loss = 0.002411159495176873\n",
      "Iteration 26003: loss = 0.0024111493540383703\n",
      "Iteration 26004: loss = 0.0024111392130095654\n",
      "Iteration 26005: loss = 0.002411129072090458\n",
      "Iteration 26006: loss = 0.0024111189312810465\n",
      "Iteration 26007: loss = 0.002411108790581329\n",
      "Iteration 26008: loss = 0.0024110986499913047\n",
      "Iteration 26009: loss = 0.002411088509510972\n",
      "Iteration 26010: loss = 0.0024110783691403304\n",
      "Iteration 26011: loss = 0.0024110682288793774\n",
      "Iteration 26012: loss = 0.002411058088728112\n",
      "Iteration 26013: loss = 0.002411047948686533\n",
      "Iteration 26014: loss = 0.002411037808754639\n",
      "Iteration 26015: loss = 0.0024110276689324286\n",
      "Iteration 26016: loss = 0.002411017529219901\n",
      "Iteration 26017: loss = 0.0024110073896170536\n",
      "Iteration 26018: loss = 0.0024109972501238862\n",
      "Iteration 26019: loss = 0.002410987110740398\n",
      "Iteration 26020: loss = 0.002410976971466586\n",
      "Iteration 26021: loss = 0.0024109668323024492\n",
      "Iteration 26022: loss = 0.002410956693247987\n",
      "Iteration 26023: loss = 0.002410946554303198\n",
      "Iteration 26024: loss = 0.0024109364154680808\n",
      "Iteration 26025: loss = 0.0024109262767426335\n",
      "Iteration 26026: loss = 0.0024109161381268545\n",
      "Iteration 26027: loss = 0.0024109059996207435\n",
      "Iteration 26028: loss = 0.002410895861224298\n",
      "Iteration 26029: loss = 0.0024108857229375186\n",
      "Iteration 26030: loss = 0.0024108755847604023\n",
      "Iteration 26031: loss = 0.0024108654466929477\n",
      "Iteration 26032: loss = 0.0024108553087351546\n",
      "Iteration 26033: loss = 0.0024108451708870202\n",
      "Iteration 26034: loss = 0.0024108350331485447\n",
      "Iteration 26035: loss = 0.0024108248955197254\n",
      "Iteration 26036: loss = 0.0024108147580005615\n",
      "Iteration 26037: loss = 0.002410804620591052\n",
      "Iteration 26038: loss = 0.002410794483291195\n",
      "Iteration 26039: loss = 0.00241078434610099\n",
      "Iteration 26040: loss = 0.0024107742090204343\n",
      "Iteration 26041: loss = 0.002410764072049528\n",
      "Iteration 26042: loss = 0.0024107539351882683\n",
      "Iteration 26043: loss = 0.0024107437984366557\n",
      "Iteration 26044: loss = 0.002410733661794687\n",
      "Iteration 26045: loss = 0.002410723525262362\n",
      "Iteration 26046: loss = 0.002410713388839679\n",
      "Iteration 26047: loss = 0.002410703252526636\n",
      "Iteration 26048: loss = 0.0024106931163232325\n",
      "Iteration 26049: loss = 0.0024106829802294674\n",
      "Iteration 26050: loss = 0.0024106728442453386\n",
      "Iteration 26051: loss = 0.002410662708370845\n",
      "Iteration 26052: loss = 0.0024106525726059857\n",
      "Iteration 26053: loss = 0.002410642436950759\n",
      "Iteration 26054: loss = 0.002410632301405163\n",
      "Iteration 26055: loss = 0.0024106221659691974\n",
      "Iteration 26056: loss = 0.002410612030642861\n",
      "Iteration 26057: loss = 0.002410601895426151\n",
      "Iteration 26058: loss = 0.002410591760319066\n",
      "Iteration 26059: loss = 0.0024105816253216067\n",
      "Iteration 26060: loss = 0.0024105714904337707\n",
      "Iteration 26061: loss = 0.0024105613556555557\n",
      "Iteration 26062: loss = 0.0024105512209869617\n",
      "Iteration 26063: loss = 0.0024105410864279867\n",
      "Iteration 26064: loss = 0.0024105309519786297\n",
      "Iteration 26065: loss = 0.002410520817638889\n",
      "Iteration 26066: loss = 0.002410510683408764\n",
      "Iteration 26067: loss = 0.002410500549288252\n",
      "Iteration 26068: loss = 0.0024104904152773524\n",
      "Iteration 26069: loss = 0.0024104802813760644\n",
      "Iteration 26070: loss = 0.0024104701475843857\n",
      "Iteration 26071: loss = 0.0024104600139023156\n",
      "Iteration 26072: loss = 0.0024104498803298523\n",
      "Iteration 26073: loss = 0.002410439746866995\n",
      "Iteration 26074: loss = 0.002410429613513742\n",
      "Iteration 26075: loss = 0.002410419480270092\n",
      "Iteration 26076: loss = 0.002410409347136044\n",
      "Iteration 26077: loss = 0.0024103992141115958\n",
      "Iteration 26078: loss = 0.002410389081196746\n",
      "Iteration 26079: loss = 0.002410378948391495\n",
      "Iteration 26080: loss = 0.00241036881569584\n",
      "Iteration 26081: loss = 0.0024103586831097797\n",
      "Iteration 26082: loss = 0.0024103485506333138\n",
      "Iteration 26083: loss = 0.002410338418266439\n",
      "Iteration 26084: loss = 0.0024103282860091562\n",
      "Iteration 26085: loss = 0.0024103181538614625\n",
      "Iteration 26086: loss = 0.002410308021823357\n",
      "Iteration 26087: loss = 0.002410297889894838\n",
      "Iteration 26088: loss = 0.002410287758075905\n",
      "Iteration 26089: loss = 0.0024102776263665564\n",
      "Iteration 26090: loss = 0.0024102674947667903\n",
      "Iteration 26091: loss = 0.002410257363276606\n",
      "Iteration 26092: loss = 0.0024102472318960014\n",
      "Iteration 26093: loss = 0.002410237100624976\n",
      "Iteration 26094: loss = 0.002410226969463528\n",
      "Iteration 26095: loss = 0.0024102168384116563\n",
      "Iteration 26096: loss = 0.002410206707469359\n",
      "Iteration 26097: loss = 0.002410196576636636\n",
      "Iteration 26098: loss = 0.002410186445913484\n",
      "Iteration 26099: loss = 0.002410176315299904\n",
      "Iteration 26100: loss = 0.0024101661847958917\n",
      "Iteration 26101: loss = 0.0024101560544014487\n",
      "Iteration 26102: loss = 0.0024101459241165726\n",
      "Iteration 26103: loss = 0.002410135793941261\n",
      "Iteration 26104: loss = 0.002410125663875514\n",
      "Iteration 26105: loss = 0.00241011553391933\n",
      "Iteration 26106: loss = 0.002410105404072707\n",
      "Iteration 26107: loss = 0.0024100952743356446\n",
      "Iteration 26108: loss = 0.0024100851447081397\n",
      "Iteration 26109: loss = 0.002410075015190193\n",
      "Iteration 26110: loss = 0.002410064885781802\n",
      "Iteration 26111: loss = 0.0024100547564829657\n",
      "Iteration 26112: loss = 0.002410044627293683\n",
      "Iteration 26113: loss = 0.0024100344982139516\n",
      "Iteration 26114: loss = 0.0024100243692437716\n",
      "Iteration 26115: loss = 0.00241001424038314\n",
      "Iteration 26116: loss = 0.002410004111632057\n",
      "Iteration 26117: loss = 0.00240999398299052\n",
      "Iteration 26118: loss = 0.0024099838544585288\n",
      "Iteration 26119: loss = 0.0024099737260360812\n",
      "Iteration 26120: loss = 0.0024099635977231763\n",
      "Iteration 26121: loss = 0.0024099534695198126\n",
      "Iteration 26122: loss = 0.002409943341425989\n",
      "Iteration 26123: loss = 0.0024099332134417035\n",
      "Iteration 26124: loss = 0.002409923085566956\n",
      "Iteration 26125: loss = 0.002409912957801743\n",
      "Iteration 26126: loss = 0.0024099028301460654\n",
      "Iteration 26127: loss = 0.0024098927025999204\n",
      "Iteration 26128: loss = 0.002409882575163308\n",
      "Iteration 26129: loss = 0.0024098724478362255\n",
      "Iteration 26130: loss = 0.002409862320618672\n",
      "Iteration 26131: loss = 0.002409852193510647\n",
      "Iteration 26132: loss = 0.0024098420665121475\n",
      "Iteration 26133: loss = 0.0024098319396231734\n",
      "Iteration 26134: loss = 0.0024098218128437232\n",
      "Iteration 26135: loss = 0.0024098116861737957\n",
      "Iteration 26136: loss = 0.002409801559613389\n",
      "Iteration 26137: loss = 0.002409791433162502\n",
      "Iteration 26138: loss = 0.002409781306821133\n",
      "Iteration 26139: loss = 0.0024097711805892816\n",
      "Iteration 26140: loss = 0.0024097610544669456\n",
      "Iteration 26141: loss = 0.002409750928454124\n",
      "Iteration 26142: loss = 0.0024097408025508154\n",
      "Iteration 26143: loss = 0.0024097306767570186\n",
      "Iteration 26144: loss = 0.0024097205510727315\n",
      "Iteration 26145: loss = 0.0024097104254979544\n",
      "Iteration 26146: loss = 0.0024097003000326843\n",
      "Iteration 26147: loss = 0.0024096901746769204\n",
      "Iteration 26148: loss = 0.002409680049430662\n",
      "Iteration 26149: loss = 0.0024096699242939063\n",
      "Iteration 26150: loss = 0.002409659799266654\n",
      "Iteration 26151: loss = 0.0024096496743489016\n",
      "Iteration 26152: loss = 0.0024096395495406494\n",
      "Iteration 26153: loss = 0.002409629424841895\n",
      "Iteration 26154: loss = 0.002409619300252638\n",
      "Iteration 26155: loss = 0.0024096091757728763\n",
      "Iteration 26156: loss = 0.0024095990514026084\n",
      "Iteration 26157: loss = 0.002409588927141833\n",
      "Iteration 26158: loss = 0.0024095788029905507\n",
      "Iteration 26159: loss = 0.0024095686789487574\n",
      "Iteration 26160: loss = 0.0024095585550164534\n",
      "Iteration 26161: loss = 0.002409548431193637\n",
      "Iteration 26162: loss = 0.0024095383074803057\n",
      "Iteration 26163: loss = 0.00240952818387646\n",
      "Iteration 26164: loss = 0.002409518060382098\n",
      "Iteration 26165: loss = 0.002409507936997218\n",
      "Iteration 26166: loss = 0.0024094978137218183\n",
      "Iteration 26167: loss = 0.002409487690555898\n",
      "Iteration 26168: loss = 0.0024094775674994564\n",
      "Iteration 26169: loss = 0.002409467444552491\n",
      "Iteration 26170: loss = 0.0024094573217150013\n",
      "Iteration 26171: loss = 0.002409447198986986\n",
      "Iteration 26172: loss = 0.0024094370763684423\n",
      "Iteration 26173: loss = 0.002409426953859372\n",
      "Iteration 26174: loss = 0.0024094168314597697\n",
      "Iteration 26175: loss = 0.002409406709169637\n",
      "Iteration 26176: loss = 0.002409396586988971\n",
      "Iteration 26177: loss = 0.0024093864649177713\n",
      "Iteration 26178: loss = 0.0024093763429560364\n",
      "Iteration 26179: loss = 0.002409366221103765\n",
      "Iteration 26180: loss = 0.0024093560993609553\n",
      "Iteration 26181: loss = 0.0024093459777276066\n",
      "Iteration 26182: loss = 0.0024093358562037167\n",
      "Iteration 26183: loss = 0.002409325734789285\n",
      "Iteration 26184: loss = 0.0024093156134843098\n",
      "Iteration 26185: loss = 0.0024093054922887896\n",
      "Iteration 26186: loss = 0.002409295371202724\n",
      "Iteration 26187: loss = 0.002409285250226111\n",
      "Iteration 26188: loss = 0.0024092751293589487\n",
      "Iteration 26189: loss = 0.0024092650086012363\n",
      "Iteration 26190: loss = 0.0024092548879529724\n",
      "Iteration 26191: loss = 0.002409244767414156\n",
      "Iteration 26192: loss = 0.0024092346469847857\n",
      "Iteration 26193: loss = 0.002409224526664859\n",
      "Iteration 26194: loss = 0.0024092144064543765\n",
      "Iteration 26195: loss = 0.002409204286353335\n",
      "Iteration 26196: loss = 0.002409194166361734\n",
      "Iteration 26197: loss = 0.002409184046479573\n",
      "Iteration 26198: loss = 0.002409173926706849\n",
      "Iteration 26199: loss = 0.0024091638070435624\n",
      "Iteration 26200: loss = 0.0024091536874897104\n",
      "Iteration 26201: loss = 0.0024091435680452917\n",
      "Iteration 26202: loss = 0.002409133448710306\n",
      "Iteration 26203: loss = 0.002409123329484751\n",
      "Iteration 26204: loss = 0.0024091132103686262\n",
      "Iteration 26205: loss = 0.0024091030913619294\n",
      "Iteration 26206: loss = 0.00240909297246466\n",
      "Iteration 26207: loss = 0.002409082853676816\n",
      "Iteration 26208: loss = 0.0024090727349983967\n",
      "Iteration 26209: loss = 0.0024090626164294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26210: loss = 0.0024090524979698256\n",
      "Iteration 26211: loss = 0.002409042379619671\n",
      "Iteration 26212: loss = 0.0024090322613789356\n",
      "Iteration 26213: loss = 0.002409022143247618\n",
      "Iteration 26214: loss = 0.002409012025225717\n",
      "Iteration 26215: loss = 0.00240900190731323\n",
      "Iteration 26216: loss = 0.0024089917895101574\n",
      "Iteration 26217: loss = 0.002408981671816497\n",
      "Iteration 26218: loss = 0.0024089715542322475\n",
      "Iteration 26219: loss = 0.002408961436757408\n",
      "Iteration 26220: loss = 0.0024089513193919767\n",
      "Iteration 26221: loss = 0.002408941202135952\n",
      "Iteration 26222: loss = 0.002408931084989333\n",
      "Iteration 26223: loss = 0.0024089209679521175\n",
      "Iteration 26224: loss = 0.0024089108510243064\n",
      "Iteration 26225: loss = 0.0024089007342058956\n",
      "Iteration 26226: loss = 0.0024088906174968856\n",
      "Iteration 26227: loss = 0.0024088805008972746\n",
      "Iteration 26228: loss = 0.002408870384407061\n",
      "Iteration 26229: loss = 0.002408860268026243\n",
      "Iteration 26230: loss = 0.002408850151754821\n",
      "Iteration 26231: loss = 0.0024088400355927915\n",
      "Iteration 26232: loss = 0.002408829919540155\n",
      "Iteration 26233: loss = 0.002408819803596908\n",
      "Iteration 26234: loss = 0.002408809687763052\n",
      "Iteration 26235: loss = 0.0024087995720385837\n",
      "Iteration 26236: loss = 0.002408789456423502\n",
      "Iteration 26237: loss = 0.002408779340917805\n",
      "Iteration 26238: loss = 0.0024087692255214937\n",
      "Iteration 26239: loss = 0.002408759110234565\n",
      "Iteration 26240: loss = 0.002408748995057016\n",
      "Iteration 26241: loss = 0.0024087388799888484\n",
      "Iteration 26242: loss = 0.0024087287650300594\n",
      "Iteration 26243: loss = 0.0024087186501806474\n",
      "Iteration 26244: loss = 0.0024087085354406123\n",
      "Iteration 26245: loss = 0.0024086984208099515\n",
      "Iteration 26246: loss = 0.0024086883062886637\n",
      "Iteration 26247: loss = 0.0024086781918767485\n",
      "Iteration 26248: loss = 0.0024086680775742037\n",
      "Iteration 26249: loss = 0.0024086579633810284\n",
      "Iteration 26250: loss = 0.002408647849297221\n",
      "Iteration 26251: loss = 0.00240863773532278\n",
      "Iteration 26252: loss = 0.0024086276214577047\n",
      "Iteration 26253: loss = 0.0024086175077019938\n",
      "Iteration 26254: loss = 0.002408607394055645\n",
      "Iteration 26255: loss = 0.0024085972805186575\n",
      "Iteration 26256: loss = 0.00240858716709103\n",
      "Iteration 26257: loss = 0.0024085770537727613\n",
      "Iteration 26258: loss = 0.0024085669405638495\n",
      "Iteration 26259: loss = 0.0024085568274642938\n",
      "Iteration 26260: loss = 0.002408546714474093\n",
      "Iteration 26261: loss = 0.002408536601593245\n",
      "Iteration 26262: loss = 0.0024085264888217494\n",
      "Iteration 26263: loss = 0.0024085163761596038\n",
      "Iteration 26264: loss = 0.0024085062636068078\n",
      "Iteration 26265: loss = 0.002408496151163359\n",
      "Iteration 26266: loss = 0.002408486038829258\n",
      "Iteration 26267: loss = 0.0024084759266045012\n",
      "Iteration 26268: loss = 0.002408465814489089\n",
      "Iteration 26269: loss = 0.0024084557024830196\n",
      "Iteration 26270: loss = 0.00240844559058629\n",
      "Iteration 26271: loss = 0.002408435478798901\n",
      "Iteration 26272: loss = 0.0024084253671208507\n",
      "Iteration 26273: loss = 0.0024084152555521377\n",
      "Iteration 26274: loss = 0.0024084051440927596\n",
      "Iteration 26275: loss = 0.0024083950327427168\n",
      "Iteration 26276: loss = 0.0024083849215020066\n",
      "Iteration 26277: loss = 0.0024083748103706283\n",
      "Iteration 26278: loss = 0.002408364699348581\n",
      "Iteration 26279: loss = 0.0024083545884358627\n",
      "Iteration 26280: loss = 0.0024083444776324715\n",
      "Iteration 26281: loss = 0.002408334366938407\n",
      "Iteration 26282: loss = 0.0024083242563536678\n",
      "Iteration 26283: loss = 0.002408314145878252\n",
      "Iteration 26284: loss = 0.002408304035512159\n",
      "Iteration 26285: loss = 0.0024082939252553868\n",
      "Iteration 26286: loss = 0.0024082838151079345\n",
      "Iteration 26287: loss = 0.0024082737050698006\n",
      "Iteration 26288: loss = 0.0024082635951409833\n",
      "Iteration 26289: loss = 0.0024082534853214817\n",
      "Iteration 26290: loss = 0.0024082433756112947\n",
      "Iteration 26291: loss = 0.002408233266010421\n",
      "Iteration 26292: loss = 0.0024082231565188583\n",
      "Iteration 26293: loss = 0.0024082130471366064\n",
      "Iteration 26294: loss = 0.002408202937863664\n",
      "Iteration 26295: loss = 0.0024081928287000283\n",
      "Iteration 26296: loss = 0.0024081827196456994\n",
      "Iteration 26297: loss = 0.002408172610700675\n",
      "Iteration 26298: loss = 0.0024081625018649547\n",
      "Iteration 26299: loss = 0.0024081523931385363\n",
      "Iteration 26300: loss = 0.0024081422845214193\n",
      "Iteration 26301: loss = 0.0024081321760136016\n",
      "Iteration 26302: loss = 0.0024081220676150823\n",
      "Iteration 26303: loss = 0.0024081119593258593\n",
      "Iteration 26304: loss = 0.002408101851145933\n",
      "Iteration 26305: loss = 0.0024080917430753\n",
      "Iteration 26306: loss = 0.0024080816351139604\n",
      "Iteration 26307: loss = 0.002408071527261912\n",
      "Iteration 26308: loss = 0.0024080614195191538\n",
      "Iteration 26309: loss = 0.002408051311885685\n",
      "Iteration 26310: loss = 0.002408041204361503\n",
      "Iteration 26311: loss = 0.0024080310969466076\n",
      "Iteration 26312: loss = 0.0024080209896409966\n",
      "Iteration 26313: loss = 0.0024080108824446698\n",
      "Iteration 26314: loss = 0.0024080007753576244\n",
      "Iteration 26315: loss = 0.0024079906683798606\n",
      "Iteration 26316: loss = 0.0024079805615113757\n",
      "Iteration 26317: loss = 0.0024079704547521692\n",
      "Iteration 26318: loss = 0.0024079603481022395\n",
      "Iteration 26319: loss = 0.0024079502415615853\n",
      "Iteration 26320: loss = 0.002407940135130205\n",
      "Iteration 26321: loss = 0.0024079300288080974\n",
      "Iteration 26322: loss = 0.002407919922595262\n",
      "Iteration 26323: loss = 0.0024079098164916956\n",
      "Iteration 26324: loss = 0.0024078997104973986\n",
      "Iteration 26325: loss = 0.002407889604612369\n",
      "Iteration 26326: loss = 0.0024078794988366048\n",
      "Iteration 26327: loss = 0.0024078693931701067\n",
      "Iteration 26328: loss = 0.002407859287612871\n",
      "Iteration 26329: loss = 0.002407849182164897\n",
      "Iteration 26330: loss = 0.0024078390768261845\n",
      "Iteration 26331: loss = 0.002407828971596731\n",
      "Iteration 26332: loss = 0.0024078188664765354\n",
      "Iteration 26333: loss = 0.0024078087614655967\n",
      "Iteration 26334: loss = 0.0024077986565639135\n",
      "Iteration 26335: loss = 0.002407788551771484\n",
      "Iteration 26336: loss = 0.002407778447088307\n",
      "Iteration 26337: loss = 0.0024077683425143817\n",
      "Iteration 26338: loss = 0.002407758238049706\n",
      "Iteration 26339: loss = 0.0024077481336942795\n",
      "Iteration 26340: loss = 0.0024077380294481\n",
      "Iteration 26341: loss = 0.0024077279253111664\n",
      "Iteration 26342: loss = 0.0024077178212834776\n",
      "Iteration 26343: loss = 0.0024077077173650318\n",
      "Iteration 26344: loss = 0.002407697613555828\n",
      "Iteration 26345: loss = 0.002407687509855865\n",
      "Iteration 26346: loss = 0.002407677406265141\n",
      "Iteration 26347: loss = 0.002407667302783655\n",
      "Iteration 26348: loss = 0.0024076571994114055\n",
      "Iteration 26349: loss = 0.002407647096148391\n",
      "Iteration 26350: loss = 0.0024076369929946113\n",
      "Iteration 26351: loss = 0.002407626889950063\n",
      "Iteration 26352: loss = 0.002407616787014747\n",
      "Iteration 26353: loss = 0.00240760668418866\n",
      "Iteration 26354: loss = 0.002407596581471802\n",
      "Iteration 26355: loss = 0.002407586478864171\n",
      "Iteration 26356: loss = 0.0024075763763657656\n",
      "Iteration 26357: loss = 0.002407566273976585\n",
      "Iteration 26358: loss = 0.0024075561716966273\n",
      "Iteration 26359: loss = 0.0024075460695258915\n",
      "Iteration 26360: loss = 0.002407535967464376\n",
      "Iteration 26361: loss = 0.00240752586551208\n",
      "Iteration 26362: loss = 0.0024075157636690024\n",
      "Iteration 26363: loss = 0.0024075056619351407\n",
      "Iteration 26364: loss = 0.0024074955603104937\n",
      "Iteration 26365: loss = 0.002407485458795061\n",
      "Iteration 26366: loss = 0.0024074753573888404\n",
      "Iteration 26367: loss = 0.002407465256091831\n",
      "Iteration 26368: loss = 0.002407455154904031\n",
      "Iteration 26369: loss = 0.00240744505382544\n",
      "Iteration 26370: loss = 0.002407434952856056\n",
      "Iteration 26371: loss = 0.002407424851995878\n",
      "Iteration 26372: loss = 0.002407414751244904\n",
      "Iteration 26373: loss = 0.002407404650603133\n",
      "Iteration 26374: loss = 0.002407394550070564\n",
      "Iteration 26375: loss = 0.0024073844496471947\n",
      "Iteration 26376: loss = 0.0024073743493330255\n",
      "Iteration 26377: loss = 0.002407364249128053\n",
      "Iteration 26378: loss = 0.002407354149032277\n",
      "Iteration 26379: loss = 0.0024073440490456964\n",
      "Iteration 26380: loss = 0.0024073339491683093\n",
      "Iteration 26381: loss = 0.002407323849400115\n",
      "Iteration 26382: loss = 0.002407313749741111\n",
      "Iteration 26383: loss = 0.0024073036501912966\n",
      "Iteration 26384: loss = 0.0024072935507506712\n",
      "Iteration 26385: loss = 0.0024072834514192324\n",
      "Iteration 26386: loss = 0.002407273352196979\n",
      "Iteration 26387: loss = 0.00240726325308391\n",
      "Iteration 26388: loss = 0.002407253154080024\n",
      "Iteration 26389: loss = 0.0024072430551853197\n",
      "Iteration 26390: loss = 0.0024072329563997958\n",
      "Iteration 26391: loss = 0.002407222857723451\n",
      "Iteration 26392: loss = 0.002407212759156283\n",
      "Iteration 26393: loss = 0.002407202660698292\n",
      "Iteration 26394: loss = 0.0024071925623494754\n",
      "Iteration 26395: loss = 0.0024071824641098325\n",
      "Iteration 26396: loss = 0.0024071723659793622\n",
      "Iteration 26397: loss = 0.0024071622679580624\n",
      "Iteration 26398: loss = 0.0024071521700459323\n",
      "Iteration 26399: loss = 0.0024071420722429704\n",
      "Iteration 26400: loss = 0.0024071319745491756\n",
      "Iteration 26401: loss = 0.002407121876964546\n",
      "Iteration 26402: loss = 0.0024071117794890805\n",
      "Iteration 26403: loss = 0.002407101682122778\n",
      "Iteration 26404: loss = 0.002407091584865637\n",
      "Iteration 26405: loss = 0.002407081487717656\n",
      "Iteration 26406: loss = 0.0024070713906788337\n",
      "Iteration 26407: loss = 0.00240706129374917\n",
      "Iteration 26408: loss = 0.002407051196928661\n",
      "Iteration 26409: loss = 0.0024070411002173075\n",
      "Iteration 26410: loss = 0.0024070310036151074\n",
      "Iteration 26411: loss = 0.0024070209071220595\n",
      "Iteration 26412: loss = 0.002407010810738162\n",
      "Iteration 26413: loss = 0.0024070007144634145\n",
      "Iteration 26414: loss = 0.002406990618297815\n",
      "Iteration 26415: loss = 0.002406980522241362\n",
      "Iteration 26416: loss = 0.0024069704262940548\n",
      "Iteration 26417: loss = 0.0024069603304558915\n",
      "Iteration 26418: loss = 0.0024069502347268705\n",
      "Iteration 26419: loss = 0.002406940139106991\n",
      "Iteration 26420: loss = 0.0024069300435962523\n",
      "Iteration 26421: loss = 0.002406919948194652\n",
      "Iteration 26422: loss = 0.002406909852902189\n",
      "Iteration 26423: loss = 0.002406899757718862\n",
      "Iteration 26424: loss = 0.0024068896626446698\n",
      "Iteration 26425: loss = 0.002406879567679611\n",
      "Iteration 26426: loss = 0.002406869472823684\n",
      "Iteration 26427: loss = 0.002406859378076888\n",
      "Iteration 26428: loss = 0.002406849283439221\n",
      "Iteration 26429: loss = 0.002406839188910682\n",
      "Iteration 26430: loss = 0.00240682909449127\n",
      "Iteration 26431: loss = 0.002406819000180983\n",
      "Iteration 26432: loss = 0.0024068089059798205\n",
      "Iteration 26433: loss = 0.0024067988118877805\n",
      "Iteration 26434: loss = 0.0024067887179048614\n",
      "Iteration 26435: loss = 0.0024067786240310625\n",
      "Iteration 26436: loss = 0.002406768530266382\n",
      "Iteration 26437: loss = 0.002406758436610819\n",
      "Iteration 26438: loss = 0.0024067483430643723\n",
      "Iteration 26439: loss = 0.00240673824962704\n",
      "Iteration 26440: loss = 0.002406728156298821\n",
      "Iteration 26441: loss = 0.0024067180630797137\n",
      "Iteration 26442: loss = 0.0024067079699697175\n",
      "Iteration 26443: loss = 0.0024066978769688293\n",
      "Iteration 26444: loss = 0.00240668778407705\n",
      "Iteration 26445: loss = 0.0024066776912943775\n",
      "Iteration 26446: loss = 0.0024066675986208095\n",
      "Iteration 26447: loss = 0.002406657506056346\n",
      "Iteration 26448: loss = 0.002406647413600985\n",
      "Iteration 26449: loss = 0.0024066373212547247\n",
      "Iteration 26450: loss = 0.002406627229017565\n",
      "Iteration 26451: loss = 0.0024066171368895034\n",
      "Iteration 26452: loss = 0.0024066070448705394\n",
      "Iteration 26453: loss = 0.0024065969529606704\n",
      "Iteration 26454: loss = 0.0024065868611598965\n",
      "Iteration 26455: loss = 0.002406576769468216\n",
      "Iteration 26456: loss = 0.002406566677885627\n",
      "Iteration 26457: loss = 0.0024065565864121287\n",
      "Iteration 26458: loss = 0.0024065464950477193\n",
      "Iteration 26459: loss = 0.002406536403792398\n",
      "Iteration 26460: loss = 0.002406526312646163\n",
      "Iteration 26461: loss = 0.0024065162216090133\n",
      "Iteration 26462: loss = 0.0024065061306809466\n",
      "Iteration 26463: loss = 0.0024064960398619637\n",
      "Iteration 26464: loss = 0.002406485949152061\n",
      "Iteration 26465: loss = 0.0024064758585512383\n",
      "Iteration 26466: loss = 0.002406465768059494\n",
      "Iteration 26467: loss = 0.0024064556776768275\n",
      "Iteration 26468: loss = 0.002406445587403236\n",
      "Iteration 26469: loss = 0.002406435497238719\n",
      "Iteration 26470: loss = 0.002406425407183275\n",
      "Iteration 26471: loss = 0.0024064153172369033\n",
      "Iteration 26472: loss = 0.0024064052273996017\n",
      "Iteration 26473: loss = 0.002406395137671369\n",
      "Iteration 26474: loss = 0.002406385048052204\n",
      "Iteration 26475: loss = 0.002406374958542106\n",
      "Iteration 26476: loss = 0.002406364869141072\n",
      "Iteration 26477: loss = 0.0024063547798491027\n",
      "Iteration 26478: loss = 0.002406344690666195\n",
      "Iteration 26479: loss = 0.002406334601592349\n",
      "Iteration 26480: loss = 0.0024063245126275624\n",
      "Iteration 26481: loss = 0.002406314423771834\n",
      "Iteration 26482: loss = 0.002406304335025163\n",
      "Iteration 26483: loss = 0.0024062942463875477\n",
      "Iteration 26484: loss = 0.0024062841578589866\n",
      "Iteration 26485: loss = 0.002406274069439479\n",
      "Iteration 26486: loss = 0.0024062639811290223\n",
      "Iteration 26487: loss = 0.0024062538929276157\n",
      "Iteration 26488: loss = 0.0024062438048352585\n",
      "Iteration 26489: loss = 0.0024062337168519496\n",
      "Iteration 26490: loss = 0.0024062236289776858\n",
      "Iteration 26491: loss = 0.002406213541212468\n",
      "Iteration 26492: loss = 0.002406203453556293\n",
      "Iteration 26493: loss = 0.0024061933660091605\n",
      "Iteration 26494: loss = 0.002406183278571069\n",
      "Iteration 26495: loss = 0.002406173191242017\n",
      "Iteration 26496: loss = 0.0024061631040220035\n",
      "Iteration 26497: loss = 0.0024061530169110276\n",
      "Iteration 26498: loss = 0.0024061429299090864\n",
      "Iteration 26499: loss = 0.0024061328430161795\n",
      "Iteration 26500: loss = 0.002406122756232305\n",
      "Iteration 26501: loss = 0.002406112669557464\n",
      "Iteration 26502: loss = 0.0024061025829916517\n",
      "Iteration 26503: loss = 0.002406092496534868\n",
      "Iteration 26504: loss = 0.002406082410187113\n",
      "Iteration 26505: loss = 0.0024060723239483837\n",
      "Iteration 26506: loss = 0.0024060622378186793\n",
      "Iteration 26507: loss = 0.0024060521517979983\n",
      "Iteration 26508: loss = 0.00240604206588634\n",
      "Iteration 26509: loss = 0.002406031980083702\n",
      "Iteration 26510: loss = 0.0024060218943900836\n",
      "Iteration 26511: loss = 0.002406011808805483\n",
      "Iteration 26512: loss = 0.0024060017233299004\n",
      "Iteration 26513: loss = 0.002405991637963333\n",
      "Iteration 26514: loss = 0.002405981552705779\n",
      "Iteration 26515: loss = 0.0024059714675572384\n",
      "Iteration 26516: loss = 0.002405961382517709\n",
      "Iteration 26517: loss = 0.0024059512975871896\n",
      "Iteration 26518: loss = 0.0024059412127656796\n",
      "Iteration 26519: loss = 0.002405931128053177\n",
      "Iteration 26520: loss = 0.0024059210434496806\n",
      "Iteration 26521: loss = 0.0024059109589551885\n",
      "Iteration 26522: loss = 0.0024059008745697\n",
      "Iteration 26523: loss = 0.002405890790293214\n",
      "Iteration 26524: loss = 0.002405880706125728\n",
      "Iteration 26525: loss = 0.0024058706220672424\n",
      "Iteration 26526: loss = 0.0024058605381177545\n",
      "Iteration 26527: loss = 0.0024058504542772636\n",
      "Iteration 26528: loss = 0.002405840370545768\n",
      "Iteration 26529: loss = 0.0024058302869232663\n",
      "Iteration 26530: loss = 0.0024058202034097573\n",
      "Iteration 26531: loss = 0.0024058101200052397\n",
      "Iteration 26532: loss = 0.002405800036709712\n",
      "Iteration 26533: loss = 0.002405789953523174\n",
      "Iteration 26534: loss = 0.0024057798704456225\n",
      "Iteration 26535: loss = 0.0024057697874770574\n",
      "Iteration 26536: loss = 0.0024057597046174767\n",
      "Iteration 26537: loss = 0.0024057496218668796\n",
      "Iteration 26538: loss = 0.002405739539225265\n",
      "Iteration 26539: loss = 0.002405729456692631\n",
      "Iteration 26540: loss = 0.0024057193742689754\n",
      "Iteration 26541: loss = 0.002405709291954299\n",
      "Iteration 26542: loss = 0.002405699209748598\n",
      "Iteration 26543: loss = 0.002405689127651874\n",
      "Iteration 26544: loss = 0.0024056790456641226\n",
      "Iteration 26545: loss = 0.0024056689637853446\n",
      "Iteration 26546: loss = 0.002405658882015538\n",
      "Iteration 26547: loss = 0.002405648800354701\n",
      "Iteration 26548: loss = 0.0024056387188028327\n",
      "Iteration 26549: loss = 0.002405628637359932\n",
      "Iteration 26550: loss = 0.0024056185560259974\n",
      "Iteration 26551: loss = 0.002405608474801027\n",
      "Iteration 26552: loss = 0.0024055983936850205\n",
      "Iteration 26553: loss = 0.0024055883126779753\n",
      "Iteration 26554: loss = 0.002405578231779891\n",
      "Iteration 26555: loss = 0.0024055681509907658\n",
      "Iteration 26556: loss = 0.002405558070310599\n",
      "Iteration 26557: loss = 0.0024055479897393886\n",
      "Iteration 26558: loss = 0.002405537909277133\n",
      "Iteration 26559: loss = 0.002405527828923832\n",
      "Iteration 26560: loss = 0.0024055177486794832\n",
      "Iteration 26561: loss = 0.0024055076685440863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26562: loss = 0.0024054975885176386\n",
      "Iteration 26563: loss = 0.00240548750860014\n",
      "Iteration 26564: loss = 0.0024054774287915882\n",
      "Iteration 26565: loss = 0.002405467349091983\n",
      "Iteration 26566: loss = 0.002405457269501322\n",
      "Iteration 26567: loss = 0.0024054471900196036\n",
      "Iteration 26568: loss = 0.002405437110646828\n",
      "Iteration 26569: loss = 0.0024054270313829925\n",
      "Iteration 26570: loss = 0.002405416952228096\n",
      "Iteration 26571: loss = 0.0024054068731821383\n",
      "Iteration 26572: loss = 0.002405396794245116\n",
      "Iteration 26573: loss = 0.0024053867154170297\n",
      "Iteration 26574: loss = 0.002405376636697877\n",
      "Iteration 26575: loss = 0.002405366558087657\n",
      "Iteration 26576: loss = 0.002405356479586368\n",
      "Iteration 26577: loss = 0.0024053464011940086\n",
      "Iteration 26578: loss = 0.002405336322910578\n",
      "Iteration 26579: loss = 0.002405326244736075\n",
      "Iteration 26580: loss = 0.002405316166670497\n",
      "Iteration 26581: loss = 0.0024053060887138443\n",
      "Iteration 26582: loss = 0.0024052960108661143\n",
      "Iteration 26583: loss = 0.002405285933127306\n",
      "Iteration 26584: loss = 0.0024052758554974187\n",
      "Iteration 26585: loss = 0.00240526577797645\n",
      "Iteration 26586: loss = 0.0024052557005643993\n",
      "Iteration 26587: loss = 0.0024052456232612654\n",
      "Iteration 26588: loss = 0.002405235546067046\n",
      "Iteration 26589: loss = 0.002405225468981741\n",
      "Iteration 26590: loss = 0.0024052153920053484\n",
      "Iteration 26591: loss = 0.0024052053151378664\n",
      "Iteration 26592: loss = 0.0024051952383792948\n",
      "Iteration 26593: loss = 0.0024051851617296316\n",
      "Iteration 26594: loss = 0.0024051750851888753\n",
      "Iteration 26595: loss = 0.0024051650087570245\n",
      "Iteration 26596: loss = 0.0024051549324340784\n",
      "Iteration 26597: loss = 0.0024051448562200355\n",
      "Iteration 26598: loss = 0.0024051347801148943\n",
      "Iteration 26599: loss = 0.0024051247041186534\n",
      "Iteration 26600: loss = 0.002405114628231312\n",
      "Iteration 26601: loss = 0.002405104552452868\n",
      "Iteration 26602: loss = 0.0024050944767833203\n",
      "Iteration 26603: loss = 0.002405084401222668\n",
      "Iteration 26604: loss = 0.0024050743257709095\n",
      "Iteration 26605: loss = 0.002405064250428042\n",
      "Iteration 26606: loss = 0.0024050541751940674\n",
      "Iteration 26607: loss = 0.0024050441000689816\n",
      "Iteration 26608: loss = 0.0024050340250527844\n",
      "Iteration 26609: loss = 0.0024050239501454745\n",
      "Iteration 26610: loss = 0.0024050138753470497\n",
      "Iteration 26611: loss = 0.0024050038006575097\n",
      "Iteration 26612: loss = 0.0024049937260768525\n",
      "Iteration 26613: loss = 0.0024049836516050775\n",
      "Iteration 26614: loss = 0.0024049735772421824\n",
      "Iteration 26615: loss = 0.0024049635029881667\n",
      "Iteration 26616: loss = 0.002404953428843028\n",
      "Iteration 26617: loss = 0.002404943354806766\n",
      "Iteration 26618: loss = 0.002404933280879379\n",
      "Iteration 26619: loss = 0.002404923207060866\n",
      "Iteration 26620: loss = 0.002404913133351225\n",
      "Iteration 26621: loss = 0.002404903059750455\n",
      "Iteration 26622: loss = 0.0024048929862585545\n",
      "Iteration 26623: loss = 0.0024048829128755222\n",
      "Iteration 26624: loss = 0.002404872839601358\n",
      "Iteration 26625: loss = 0.002404862766436058\n",
      "Iteration 26626: loss = 0.002404852693379623\n",
      "Iteration 26627: loss = 0.0024048426204320516\n",
      "Iteration 26628: loss = 0.0024048325475933404\n",
      "Iteration 26629: loss = 0.00240482247486349\n",
      "Iteration 26630: loss = 0.0024048124022424994\n",
      "Iteration 26631: loss = 0.0024048023297303656\n",
      "Iteration 26632: loss = 0.0024047922573270882\n",
      "Iteration 26633: loss = 0.0024047821850326657\n",
      "Iteration 26634: loss = 0.002404772112847097\n",
      "Iteration 26635: loss = 0.0024047620407703805\n",
      "Iteration 26636: loss = 0.0024047519688025153\n",
      "Iteration 26637: loss = 0.0024047418969434992\n",
      "Iteration 26638: loss = 0.0024047318251933314\n",
      "Iteration 26639: loss = 0.0024047217535520106\n",
      "Iteration 26640: loss = 0.0024047116820195354\n",
      "Iteration 26641: loss = 0.0024047016105959045\n",
      "Iteration 26642: loss = 0.0024046915392811163\n",
      "Iteration 26643: loss = 0.00240468146807517\n",
      "Iteration 26644: loss = 0.0024046713969780634\n",
      "Iteration 26645: loss = 0.002404661325989796\n",
      "Iteration 26646: loss = 0.0024046512551103663\n",
      "Iteration 26647: loss = 0.0024046411843397725\n",
      "Iteration 26648: loss = 0.002404631113678014\n",
      "Iteration 26649: loss = 0.0024046210431250886\n",
      "Iteration 26650: loss = 0.002404610972680996\n",
      "Iteration 26651: loss = 0.0024046009023457335\n",
      "Iteration 26652: loss = 0.0024045908321193013\n",
      "Iteration 26653: loss = 0.002404580762001697\n",
      "Iteration 26654: loss = 0.0024045706919929195\n",
      "Iteration 26655: loss = 0.0024045606220929674\n",
      "Iteration 26656: loss = 0.0024045505523018398\n",
      "Iteration 26657: loss = 0.0024045404826195348\n",
      "Iteration 26658: loss = 0.0024045304130460516\n",
      "Iteration 26659: loss = 0.0024045203435813884\n",
      "Iteration 26660: loss = 0.0024045102742255436\n",
      "Iteration 26661: loss = 0.0024045002049785168\n",
      "Iteration 26662: loss = 0.0024044901358403065\n",
      "Iteration 26663: loss = 0.0024044800668109106\n",
      "Iteration 26664: loss = 0.0024044699978903283\n",
      "Iteration 26665: loss = 0.0024044599290785574\n",
      "Iteration 26666: loss = 0.0024044498603755984\n",
      "Iteration 26667: loss = 0.0024044397917814485\n",
      "Iteration 26668: loss = 0.0024044297232961066\n",
      "Iteration 26669: loss = 0.0024044196549195714\n",
      "Iteration 26670: loss = 0.002404409586651842\n",
      "Iteration 26671: loss = 0.0024043995184929168\n",
      "Iteration 26672: loss = 0.002404389450442794\n",
      "Iteration 26673: loss = 0.0024043793825014727\n",
      "Iteration 26674: loss = 0.002404369314668952\n",
      "Iteration 26675: loss = 0.0024043592469452295\n",
      "Iteration 26676: loss = 0.002404349179330305\n",
      "Iteration 26677: loss = 0.0024043391118241763\n",
      "Iteration 26678: loss = 0.002404329044426842\n",
      "Iteration 26679: loss = 0.002404318977138302\n",
      "Iteration 26680: loss = 0.002404308909958554\n",
      "Iteration 26681: loss = 0.0024042988428875957\n",
      "Iteration 26682: loss = 0.002404288775925428\n",
      "Iteration 26683: loss = 0.0024042787090720477\n",
      "Iteration 26684: loss = 0.0024042686423274544\n",
      "Iteration 26685: loss = 0.0024042585756916465\n",
      "Iteration 26686: loss = 0.002404248509164623\n",
      "Iteration 26687: loss = 0.0024042384427463816\n",
      "Iteration 26688: loss = 0.002404228376436922\n",
      "Iteration 26689: loss = 0.002404218310236243\n",
      "Iteration 26690: loss = 0.002404208244144341\n",
      "Iteration 26691: loss = 0.002404198178161218\n",
      "Iteration 26692: loss = 0.00240418811228687\n",
      "Iteration 26693: loss = 0.002404178046521298\n",
      "Iteration 26694: loss = 0.002404167980864498\n",
      "Iteration 26695: loss = 0.0024041579153164716\n",
      "Iteration 26696: loss = 0.002404147849877215\n",
      "Iteration 26697: loss = 0.002404137784546728\n",
      "Iteration 26698: loss = 0.0024041277193250082\n",
      "Iteration 26699: loss = 0.0024041176542120564\n",
      "Iteration 26700: loss = 0.002404107589207869\n",
      "Iteration 26701: loss = 0.0024040975243124463\n",
      "Iteration 26702: loss = 0.002404087459525786\n",
      "Iteration 26703: loss = 0.0024040773948478867\n",
      "Iteration 26704: loss = 0.0024040673302787477\n",
      "Iteration 26705: loss = 0.0024040572658183676\n",
      "Iteration 26706: loss = 0.0024040472014667446\n",
      "Iteration 26707: loss = 0.002404037137223878\n",
      "Iteration 26708: loss = 0.0024040270730897656\n",
      "Iteration 26709: loss = 0.002404017009064407\n",
      "Iteration 26710: loss = 0.0024040069451477996\n",
      "Iteration 26711: loss = 0.002403996881339944\n",
      "Iteration 26712: loss = 0.002403986817640837\n",
      "Iteration 26713: loss = 0.002403976754050478\n",
      "Iteration 26714: loss = 0.0024039666905688655\n",
      "Iteration 26715: loss = 0.0024039566271959983\n",
      "Iteration 26716: loss = 0.0024039465639318757\n",
      "Iteration 26717: loss = 0.002403936500776495\n",
      "Iteration 26718: loss = 0.002403926437729857\n",
      "Iteration 26719: loss = 0.002403916374791957\n",
      "Iteration 26720: loss = 0.002403906311962797\n",
      "Iteration 26721: loss = 0.002403896249242374\n",
      "Iteration 26722: loss = 0.002403886186630687\n",
      "Iteration 26723: loss = 0.0024038761241277342\n",
      "Iteration 26724: loss = 0.0024038660617335147\n",
      "Iteration 26725: loss = 0.002403855999448028\n",
      "Iteration 26726: loss = 0.002403845937271271\n",
      "Iteration 26727: loss = 0.0024038358752032445\n",
      "Iteration 26728: loss = 0.002403825813243945\n",
      "Iteration 26729: loss = 0.002403815751393372\n",
      "Iteration 26730: loss = 0.0024038056896515244\n",
      "Iteration 26731: loss = 0.002403795628018401\n",
      "Iteration 26732: loss = 0.002403785566494\n",
      "Iteration 26733: loss = 0.0024037755050783202\n",
      "Iteration 26734: loss = 0.002403765443771361\n",
      "Iteration 26735: loss = 0.0024037553825731195\n",
      "Iteration 26736: loss = 0.0024037453214835955\n",
      "Iteration 26737: loss = 0.0024037352605027875\n",
      "Iteration 26738: loss = 0.0024037251996306946\n",
      "Iteration 26739: loss = 0.0024037151388673138\n",
      "Iteration 26740: loss = 0.002403705078212646\n",
      "Iteration 26741: loss = 0.0024036950176666884\n",
      "Iteration 26742: loss = 0.00240368495722944\n",
      "Iteration 26743: loss = 0.002403674896900899\n",
      "Iteration 26744: loss = 0.0024036648366810654\n",
      "Iteration 26745: loss = 0.0024036547765699367\n",
      "Iteration 26746: loss = 0.002403644716567512\n",
      "Iteration 26747: loss = 0.0024036346566737895\n",
      "Iteration 26748: loss = 0.0024036245968887685\n",
      "Iteration 26749: loss = 0.0024036145372124474\n",
      "Iteration 26750: loss = 0.002403604477644825\n",
      "Iteration 26751: loss = 0.0024035944181859\n",
      "Iteration 26752: loss = 0.0024035843588356702\n",
      "Iteration 26753: loss = 0.0024035742995941354\n",
      "Iteration 26754: loss = 0.002403564240461293\n",
      "Iteration 26755: loss = 0.0024035541814371435\n",
      "Iteration 26756: loss = 0.0024035441225216847\n",
      "Iteration 26757: loss = 0.002403534063714914\n",
      "Iteration 26758: loss = 0.0024035240050168318\n",
      "Iteration 26759: loss = 0.0024035139464274363\n",
      "Iteration 26760: loss = 0.002403503887946726\n",
      "Iteration 26761: loss = 0.0024034938295746993\n",
      "Iteration 26762: loss = 0.002403483771311355\n",
      "Iteration 26763: loss = 0.0024034737131566923\n",
      "Iteration 26764: loss = 0.0024034636551107094\n",
      "Iteration 26765: loss = 0.002403453597173405\n",
      "Iteration 26766: loss = 0.0024034435393447775\n",
      "Iteration 26767: loss = 0.0024034334816248255\n",
      "Iteration 26768: loss = 0.0024034234240135487\n",
      "Iteration 26769: loss = 0.0024034133665109445\n",
      "Iteration 26770: loss = 0.0024034033091170125\n",
      "Iteration 26771: loss = 0.0024033932518317514\n",
      "Iteration 26772: loss = 0.0024033831946551586\n",
      "Iteration 26773: loss = 0.002403373137587234\n",
      "Iteration 26774: loss = 0.0024033630806279765\n",
      "Iteration 26775: loss = 0.0024033530237773833\n",
      "Iteration 26776: loss = 0.0024033429670354545\n",
      "Iteration 26777: loss = 0.0024033329104021874\n",
      "Iteration 26778: loss = 0.002403322853877582\n",
      "Iteration 26779: loss = 0.0024033127974616366\n",
      "Iteration 26780: loss = 0.00240330274115435\n",
      "Iteration 26781: loss = 0.0024032926849557198\n",
      "Iteration 26782: loss = 0.002403282628865745\n",
      "Iteration 26783: loss = 0.0024032725728844254\n",
      "Iteration 26784: loss = 0.0024032625170117587\n",
      "Iteration 26785: loss = 0.0024032524612477442\n",
      "Iteration 26786: loss = 0.00240324240559238\n",
      "Iteration 26787: loss = 0.0024032323500456643\n",
      "Iteration 26788: loss = 0.0024032222946075974\n",
      "Iteration 26789: loss = 0.0024032122392781763\n",
      "Iteration 26790: loss = 0.002403202184057401\n",
      "Iteration 26791: loss = 0.002403192128945269\n",
      "Iteration 26792: loss = 0.002403182073941779\n",
      "Iteration 26793: loss = 0.002403172019046931\n",
      "Iteration 26794: loss = 0.002403161964260722\n",
      "Iteration 26795: loss = 0.002403151909583152\n",
      "Iteration 26796: loss = 0.0024031418550142184\n",
      "Iteration 26797: loss = 0.0024031318005539215\n",
      "Iteration 26798: loss = 0.0024031217462022587\n",
      "Iteration 26799: loss = 0.002403111691959229\n",
      "Iteration 26800: loss = 0.00240310163782483\n",
      "Iteration 26801: loss = 0.002403091583799063\n",
      "Iteration 26802: loss = 0.002403081529881925\n",
      "Iteration 26803: loss = 0.002403071476073414\n",
      "Iteration 26804: loss = 0.0024030614223735296\n",
      "Iteration 26805: loss = 0.00240305136878227\n",
      "Iteration 26806: loss = 0.0024030413152996348\n",
      "Iteration 26807: loss = 0.0024030312619256217\n",
      "Iteration 26808: loss = 0.0024030212086602306\n",
      "Iteration 26809: loss = 0.0024030111555034582\n",
      "Iteration 26810: loss = 0.0024030011024553043\n",
      "Iteration 26811: loss = 0.002402991049515767\n",
      "Iteration 26812: loss = 0.0024029809966848465\n",
      "Iteration 26813: loss = 0.00240297094396254\n",
      "Iteration 26814: loss = 0.0024029608913488467\n",
      "Iteration 26815: loss = 0.002402950838843765\n",
      "Iteration 26816: loss = 0.0024029407864472932\n",
      "Iteration 26817: loss = 0.002402930734159431\n",
      "Iteration 26818: loss = 0.0024029206819801766\n",
      "Iteration 26819: loss = 0.0024029106299095285\n",
      "Iteration 26820: loss = 0.0024029005779474854\n",
      "Iteration 26821: loss = 0.002402890526094046\n",
      "Iteration 26822: loss = 0.0024028804743492093\n",
      "Iteration 26823: loss = 0.0024028704227129737\n",
      "Iteration 26824: loss = 0.0024028603711853374\n",
      "Iteration 26825: loss = 0.002402850319766299\n",
      "Iteration 26826: loss = 0.0024028402684558585\n",
      "Iteration 26827: loss = 0.0024028302172540137\n",
      "Iteration 26828: loss = 0.002402820166160763\n",
      "Iteration 26829: loss = 0.002402810115176106\n",
      "Iteration 26830: loss = 0.00240280006430004\n",
      "Iteration 26831: loss = 0.0024027900135325645\n",
      "Iteration 26832: loss = 0.002402779962873678\n",
      "Iteration 26833: loss = 0.002402769912323379\n",
      "Iteration 26834: loss = 0.002402759861881667\n",
      "Iteration 26835: loss = 0.0024027498115485398\n",
      "Iteration 26836: loss = 0.002402739761323996\n",
      "Iteration 26837: loss = 0.002402729711208035\n",
      "Iteration 26838: loss = 0.0024027196612006548\n",
      "Iteration 26839: loss = 0.0024027096113018536\n",
      "Iteration 26840: loss = 0.0024026995615116322\n",
      "Iteration 26841: loss = 0.0024026895118299868\n",
      "Iteration 26842: loss = 0.0024026794622569177\n",
      "Iteration 26843: loss = 0.0024026694127924223\n",
      "Iteration 26844: loss = 0.0024026593634365\n",
      "Iteration 26845: loss = 0.0024026493141891497\n",
      "Iteration 26846: loss = 0.00240263926505037\n",
      "Iteration 26847: loss = 0.0024026292160201587\n",
      "Iteration 26848: loss = 0.0024026191670985155\n",
      "Iteration 26849: loss = 0.0024026091182854387\n",
      "Iteration 26850: loss = 0.002402599069580927\n",
      "Iteration 26851: loss = 0.0024025890209849786\n",
      "Iteration 26852: loss = 0.0024025789724975923\n",
      "Iteration 26853: loss = 0.0024025689241187675\n",
      "Iteration 26854: loss = 0.0024025588758485026\n",
      "Iteration 26855: loss = 0.002402548827686796\n",
      "Iteration 26856: loss = 0.0024025387796336455\n",
      "Iteration 26857: loss = 0.0024025287316890515\n",
      "Iteration 26858: loss = 0.0024025186838530117\n",
      "Iteration 26859: loss = 0.002402508636125525\n",
      "Iteration 26860: loss = 0.0024024985885065897\n",
      "Iteration 26861: loss = 0.002402488540996205\n",
      "Iteration 26862: loss = 0.0024024784935943685\n",
      "Iteration 26863: loss = 0.0024024684463010808\n",
      "Iteration 26864: loss = 0.002402458399116339\n",
      "Iteration 26865: loss = 0.002402448352040142\n",
      "Iteration 26866: loss = 0.0024024383050724888\n",
      "Iteration 26867: loss = 0.0024024282582133776\n",
      "Iteration 26868: loss = 0.002402418211462808\n",
      "Iteration 26869: loss = 0.0024024081648207777\n",
      "Iteration 26870: loss = 0.0024023981182872854\n",
      "Iteration 26871: loss = 0.002402388071862331\n",
      "Iteration 26872: loss = 0.0024023780255459115\n",
      "Iteration 26873: loss = 0.0024023679793380263\n",
      "Iteration 26874: loss = 0.0024023579332386746\n",
      "Iteration 26875: loss = 0.002402347887247854\n",
      "Iteration 26876: loss = 0.002402337841365564\n",
      "Iteration 26877: loss = 0.002402327795591803\n",
      "Iteration 26878: loss = 0.0024023177499265694\n",
      "Iteration 26879: loss = 0.0024023077043698626\n",
      "Iteration 26880: loss = 0.0024022976589216805\n",
      "Iteration 26881: loss = 0.002402287613582022\n",
      "Iteration 26882: loss = 0.002402277568350886\n",
      "Iteration 26883: loss = 0.00240226752322827\n",
      "Iteration 26884: loss = 0.002402257478214175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26885: loss = 0.0024022474333085975\n",
      "Iteration 26886: loss = 0.0024022373885115374\n",
      "Iteration 26887: loss = 0.002402227343822993\n",
      "Iteration 26888: loss = 0.0024022172992429628\n",
      "Iteration 26889: loss = 0.002402207254771445\n",
      "Iteration 26890: loss = 0.002402197210408439\n",
      "Iteration 26891: loss = 0.0024021871661539435\n",
      "Iteration 26892: loss = 0.0024021771220079573\n",
      "Iteration 26893: loss = 0.0024021670779704784\n",
      "Iteration 26894: loss = 0.0024021570340415055\n",
      "Iteration 26895: loss = 0.002402146990221038\n",
      "Iteration 26896: loss = 0.0024021369465090747\n",
      "Iteration 26897: loss = 0.0024021269029056123\n",
      "Iteration 26898: loss = 0.002402116859410652\n",
      "Iteration 26899: loss = 0.0024021068160241907\n",
      "Iteration 26900: loss = 0.0024020967727462275\n",
      "Iteration 26901: loss = 0.002402086729576762\n",
      "Iteration 26902: loss = 0.0024020766865157916\n",
      "Iteration 26903: loss = 0.0024020666435633155\n",
      "Iteration 26904: loss = 0.002402056600719333\n",
      "Iteration 26905: loss = 0.0024020465579838416\n",
      "Iteration 26906: loss = 0.00240203651535684\n",
      "Iteration 26907: loss = 0.002402026472838328\n",
      "Iteration 26908: loss = 0.0024020164304283035\n",
      "Iteration 26909: loss = 0.0024020063881267655\n",
      "Iteration 26910: loss = 0.002401996345933712\n",
      "Iteration 26911: loss = 0.002401986303849142\n",
      "Iteration 26912: loss = 0.002401976261873055\n",
      "Iteration 26913: loss = 0.0024019662200054485\n",
      "Iteration 26914: loss = 0.0024019561782463216\n",
      "Iteration 26915: loss = 0.002401946136595673\n",
      "Iteration 26916: loss = 0.002401936095053501\n",
      "Iteration 26917: loss = 0.002401926053619805\n",
      "Iteration 26918: loss = 0.0024019160122945834\n",
      "Iteration 26919: loss = 0.0024019059710778346\n",
      "Iteration 26920: loss = 0.0024018959299695578\n",
      "Iteration 26921: loss = 0.0024018858889697505\n",
      "Iteration 26922: loss = 0.0024018758480784125\n",
      "Iteration 26923: loss = 0.002401865807295542\n",
      "Iteration 26924: loss = 0.002401855766621138\n",
      "Iteration 26925: loss = 0.002401845726055199\n",
      "Iteration 26926: loss = 0.0024018356855977233\n",
      "Iteration 26927: loss = 0.0024018256452487097\n",
      "Iteration 26928: loss = 0.002401815605008157\n",
      "Iteration 26929: loss = 0.0024018055648760647\n",
      "Iteration 26930: loss = 0.00240179552485243\n",
      "Iteration 26931: loss = 0.0024017854849372523\n",
      "Iteration 26932: loss = 0.002401775445130531\n",
      "Iteration 26933: loss = 0.0024017654054322626\n",
      "Iteration 26934: loss = 0.002401755365842448\n",
      "Iteration 26935: loss = 0.002401745326361085\n",
      "Iteration 26936: loss = 0.0024017352869881723\n",
      "Iteration 26937: loss = 0.0024017252477237083\n",
      "Iteration 26938: loss = 0.0024017152085676918\n",
      "Iteration 26939: loss = 0.0024017051695201215\n",
      "Iteration 26940: loss = 0.0024016951305809962\n",
      "Iteration 26941: loss = 0.0024016850917503146\n",
      "Iteration 26942: loss = 0.0024016750530280757\n",
      "Iteration 26943: loss = 0.0024016650144142774\n",
      "Iteration 26944: loss = 0.0024016549759089184\n",
      "Iteration 26945: loss = 0.002401644937511998\n",
      "Iteration 26946: loss = 0.002401634899223514\n",
      "Iteration 26947: loss = 0.0024016248610434663\n",
      "Iteration 26948: loss = 0.0024016148229718524\n",
      "Iteration 26949: loss = 0.0024016047850086717\n",
      "Iteration 26950: loss = 0.0024015947471539224\n",
      "Iteration 26951: loss = 0.0024015847094076034\n",
      "Iteration 26952: loss = 0.0024015746717697133\n",
      "Iteration 26953: loss = 0.002401564634240251\n",
      "Iteration 26954: loss = 0.002401554596819215\n",
      "Iteration 26955: loss = 0.002401544559506603\n",
      "Iteration 26956: loss = 0.0024015345223024156\n",
      "Iteration 26957: loss = 0.0024015244852066504\n",
      "Iteration 26958: loss = 0.0024015144482193054\n",
      "Iteration 26959: loss = 0.0024015044113403806\n",
      "Iteration 26960: loss = 0.0024014943745698744\n",
      "Iteration 26961: loss = 0.0024014843379077844\n",
      "Iteration 26962: loss = 0.00240147430135411\n",
      "Iteration 26963: loss = 0.0024014642649088504\n",
      "Iteration 26964: loss = 0.0024014542285720033\n",
      "Iteration 26965: loss = 0.002401444192343568\n",
      "Iteration 26966: loss = 0.0024014341562235433\n",
      "Iteration 26967: loss = 0.0024014241202119264\n",
      "Iteration 26968: loss = 0.002401414084308718\n",
      "Iteration 26969: loss = 0.002401404048513916\n",
      "Iteration 26970: loss = 0.0024013940128275184\n",
      "Iteration 26971: loss = 0.0024013839772495248\n",
      "Iteration 26972: loss = 0.002401373941779933\n",
      "Iteration 26973: loss = 0.0024013639064187427\n",
      "Iteration 26974: loss = 0.002401353871165951\n",
      "Iteration 26975: loss = 0.0024013438360215586\n",
      "Iteration 26976: loss = 0.0024013338009855625\n",
      "Iteration 26977: loss = 0.0024013237660579626\n",
      "Iteration 26978: loss = 0.0024013137312387566\n",
      "Iteration 26979: loss = 0.002401303696527943\n",
      "Iteration 26980: loss = 0.002401293661925522\n",
      "Iteration 26981: loss = 0.0024012836274314906\n",
      "Iteration 26982: loss = 0.0024012735930458483\n",
      "Iteration 26983: loss = 0.0024012635587685933\n",
      "Iteration 26984: loss = 0.0024012535245997255\n",
      "Iteration 26985: loss = 0.002401243490539241\n",
      "Iteration 26986: loss = 0.0024012334565871414\n",
      "Iteration 26987: loss = 0.0024012234227434237\n",
      "Iteration 26988: loss = 0.002401213389008087\n",
      "Iteration 26989: loss = 0.0024012033553811297\n",
      "Iteration 26990: loss = 0.0024011933218625508\n",
      "Iteration 26991: loss = 0.0024011832884523487\n",
      "Iteration 26992: loss = 0.002401173255150522\n",
      "Iteration 26993: loss = 0.0024011632219570703\n",
      "Iteration 26994: loss = 0.0024011531888719906\n",
      "Iteration 26995: loss = 0.0024011431558952833\n",
      "Iteration 26996: loss = 0.002401133123026946\n",
      "Iteration 26997: loss = 0.0024011230902669777\n",
      "Iteration 26998: loss = 0.0024011130576153763\n",
      "Iteration 26999: loss = 0.002401103025072142\n",
      "Iteration 27000: loss = 0.002401092992637272\n",
      "Iteration 27001: loss = 0.0024010829603107664\n",
      "Iteration 27002: loss = 0.0024010729280926225\n",
      "Iteration 27003: loss = 0.0024010628959828394\n",
      "Iteration 27004: loss = 0.0024010528639814162\n",
      "Iteration 27005: loss = 0.0024010428320883513\n",
      "Iteration 27006: loss = 0.002401032800303643\n",
      "Iteration 27007: loss = 0.0024010227686272903\n",
      "Iteration 27008: loss = 0.0024010127370592926\n",
      "Iteration 27009: loss = 0.002401002705599647\n",
      "Iteration 27010: loss = 0.002400992674248353\n",
      "Iteration 27011: loss = 0.0024009826430054096\n",
      "Iteration 27012: loss = 0.002400972611870815\n",
      "Iteration 27013: loss = 0.002400962580844568\n",
      "Iteration 27014: loss = 0.0024009525499266676\n",
      "Iteration 27015: loss = 0.002400942519117112\n",
      "Iteration 27016: loss = 0.0024009324884159\n",
      "Iteration 27017: loss = 0.0024009224578230297\n",
      "Iteration 27018: loss = 0.0024009124273385012\n",
      "Iteration 27019: loss = 0.002400902396962312\n",
      "Iteration 27020: loss = 0.0024008923666944608\n",
      "Iteration 27021: loss = 0.002400882336534946\n",
      "Iteration 27022: loss = 0.0024008723064837677\n",
      "Iteration 27023: loss = 0.0024008622765409235\n",
      "Iteration 27024: loss = 0.002400852246706413\n",
      "Iteration 27025: loss = 0.0024008422169802326\n",
      "Iteration 27026: loss = 0.0024008321873623833\n",
      "Iteration 27027: loss = 0.0024008221578528627\n",
      "Iteration 27028: loss = 0.00240081212845167\n",
      "Iteration 27029: loss = 0.002400802099158803\n",
      "Iteration 27030: loss = 0.0024007920699742617\n",
      "Iteration 27031: loss = 0.0024007820408980434\n",
      "Iteration 27032: loss = 0.0024007720119301476\n",
      "Iteration 27033: loss = 0.002400761983070573\n",
      "Iteration 27034: loss = 0.002400751954319318\n",
      "Iteration 27035: loss = 0.0024007419256763807\n",
      "Iteration 27036: loss = 0.0024007318971417608\n",
      "Iteration 27037: loss = 0.0024007218687154565\n",
      "Iteration 27038: loss = 0.0024007118403974667\n",
      "Iteration 27039: loss = 0.002400701812187789\n",
      "Iteration 27040: loss = 0.0024006917840864233\n",
      "Iteration 27041: loss = 0.002400681756093368\n",
      "Iteration 27042: loss = 0.002400671728208622\n",
      "Iteration 27043: loss = 0.0024006617004321833\n",
      "Iteration 27044: loss = 0.002400651672764051\n",
      "Iteration 27045: loss = 0.002400641645204223\n",
      "Iteration 27046: loss = 0.0024006316177527\n",
      "Iteration 27047: loss = 0.0024006215904094777\n",
      "Iteration 27048: loss = 0.0024006115631745573\n",
      "Iteration 27049: loss = 0.002400601536047936\n",
      "Iteration 27050: loss = 0.0024005915090296137\n",
      "Iteration 27051: loss = 0.002400581482119588\n",
      "Iteration 27052: loss = 0.0024005714553178583\n",
      "Iteration 27053: loss = 0.0024005614286244223\n",
      "Iteration 27054: loss = 0.00240055140203928\n",
      "Iteration 27055: loss = 0.0024005413755624283\n",
      "Iteration 27056: loss = 0.0024005313491938674\n",
      "Iteration 27057: loss = 0.0024005213229335953\n",
      "Iteration 27058: loss = 0.0024005112967816107\n",
      "Iteration 27059: loss = 0.002400501270737913\n",
      "Iteration 27060: loss = 0.0024004912448024997\n",
      "Iteration 27061: loss = 0.0024004812189753707\n",
      "Iteration 27062: loss = 0.002400471193256523\n",
      "Iteration 27063: loss = 0.002400461167645957\n",
      "Iteration 27064: loss = 0.0024004511421436706\n",
      "Iteration 27065: loss = 0.0024004411167496626\n",
      "Iteration 27066: loss = 0.0024004310914639317\n",
      "Iteration 27067: loss = 0.0024004210662864757\n",
      "Iteration 27068: loss = 0.0024004110412172946\n",
      "Iteration 27069: loss = 0.0024004010162563864\n",
      "Iteration 27070: loss = 0.0024003909914037495\n",
      "Iteration 27071: loss = 0.002400380966659383\n",
      "Iteration 27072: loss = 0.002400370942023286\n",
      "Iteration 27073: loss = 0.002400360917495456\n",
      "Iteration 27074: loss = 0.0024003508930758926\n",
      "Iteration 27075: loss = 0.002400340868764594\n",
      "Iteration 27076: loss = 0.002400330844561559\n",
      "Iteration 27077: loss = 0.0024003208204667874\n",
      "Iteration 27078: loss = 0.0024003107964802755\n",
      "Iteration 27079: loss = 0.0024003007726020238\n",
      "Iteration 27080: loss = 0.00240029074883203\n",
      "Iteration 27081: loss = 0.002400280725170294\n",
      "Iteration 27082: loss = 0.0024002707016168127\n",
      "Iteration 27083: loss = 0.0024002606781715866\n",
      "Iteration 27084: loss = 0.0024002506548346132\n",
      "Iteration 27085: loss = 0.0024002406316058914\n",
      "Iteration 27086: loss = 0.0024002306084854198\n",
      "Iteration 27087: loss = 0.002400220585473197\n",
      "Iteration 27088: loss = 0.002400210562569222\n",
      "Iteration 27089: loss = 0.002400200539773494\n",
      "Iteration 27090: loss = 0.00240019051708601\n",
      "Iteration 27091: loss = 0.0024001804945067706\n",
      "Iteration 27092: loss = 0.002400170472035773\n",
      "Iteration 27093: loss = 0.0024001604496730164\n",
      "Iteration 27094: loss = 0.0024001504274184992\n",
      "Iteration 27095: loss = 0.002400140405272221\n",
      "Iteration 27096: loss = 0.0024001303832341795\n",
      "Iteration 27097: loss = 0.002400120361304374\n",
      "Iteration 27098: loss = 0.0024001103394828026\n",
      "Iteration 27099: loss = 0.002400100317769464\n",
      "Iteration 27100: loss = 0.002400090296164357\n",
      "Iteration 27101: loss = 0.0024000802746674808\n",
      "Iteration 27102: loss = 0.002400070253278834\n",
      "Iteration 27103: loss = 0.002400060231998414\n",
      "Iteration 27104: loss = 0.0024000502108262206\n",
      "Iteration 27105: loss = 0.0024000401897622523\n",
      "Iteration 27106: loss = 0.0024000301688065074\n",
      "Iteration 27107: loss = 0.002400020147958985\n",
      "Iteration 27108: loss = 0.0024000101272196837\n",
      "Iteration 27109: loss = 0.0024000001065886023\n",
      "Iteration 27110: loss = 0.002399990086065739\n",
      "Iteration 27111: loss = 0.002399980065651093\n",
      "Iteration 27112: loss = 0.0023999700453446625\n",
      "Iteration 27113: loss = 0.002399960025146447\n",
      "Iteration 27114: loss = 0.0023999500050564436\n",
      "Iteration 27115: loss = 0.0023999399850746527\n",
      "Iteration 27116: loss = 0.0023999299652010713\n",
      "Iteration 27117: loss = 0.0023999199454356993\n",
      "Iteration 27118: loss = 0.002399909925778536\n",
      "Iteration 27119: loss = 0.002399899906229578\n",
      "Iteration 27120: loss = 0.002399889886788825\n",
      "Iteration 27121: loss = 0.002399879867456276\n",
      "Iteration 27122: loss = 0.0023998698482319294\n",
      "Iteration 27123: loss = 0.0023998598291157837\n",
      "Iteration 27124: loss = 0.002399849810107838\n",
      "Iteration 27125: loss = 0.00239983979120809\n",
      "Iteration 27126: loss = 0.00239982977241654\n",
      "Iteration 27127: loss = 0.002399819753733185\n",
      "Iteration 27128: loss = 0.0023998097351580253\n",
      "Iteration 27129: loss = 0.002399799716691058\n",
      "Iteration 27130: loss = 0.0023997896983322825\n",
      "Iteration 27131: loss = 0.002399779680081698\n",
      "Iteration 27132: loss = 0.002399769661939301\n",
      "Iteration 27133: loss = 0.0023997596439050924\n",
      "Iteration 27134: loss = 0.0023997496259790707\n",
      "Iteration 27135: loss = 0.002399739608161234\n",
      "Iteration 27136: loss = 0.002399729590451581\n",
      "Iteration 27137: loss = 0.0023997195728501105\n",
      "Iteration 27138: loss = 0.002399709555356821\n",
      "Iteration 27139: loss = 0.0023996995379717114\n",
      "Iteration 27140: loss = 0.00239968952069478\n",
      "Iteration 27141: loss = 0.0023996795035260256\n",
      "Iteration 27142: loss = 0.002399669486465447\n",
      "Iteration 27143: loss = 0.002399659469513043\n",
      "Iteration 27144: loss = 0.0023996494526688115\n",
      "Iteration 27145: loss = 0.002399639435932753\n",
      "Iteration 27146: loss = 0.0023996294193048637\n",
      "Iteration 27147: loss = 0.002399619402785144\n",
      "Iteration 27148: loss = 0.002399609386373592\n",
      "Iteration 27149: loss = 0.0023995993700702068\n",
      "Iteration 27150: loss = 0.0023995893538749863\n",
      "Iteration 27151: loss = 0.0023995793377879298\n",
      "Iteration 27152: loss = 0.0023995693218090354\n",
      "Iteration 27153: loss = 0.0023995593059383024\n",
      "Iteration 27154: loss = 0.0023995492901757293\n",
      "Iteration 27155: loss = 0.0023995392745213145\n",
      "Iteration 27156: loss = 0.0023995292589750567\n",
      "Iteration 27157: loss = 0.002399519243536955\n",
      "Iteration 27158: loss = 0.002399509228207007\n",
      "Iteration 27159: loss = 0.0023994992129852126\n",
      "Iteration 27160: loss = 0.00239948919787157\n",
      "Iteration 27161: loss = 0.002399479182866078\n",
      "Iteration 27162: loss = 0.0023994691679687353\n",
      "Iteration 27163: loss = 0.0023994591531795403\n",
      "Iteration 27164: loss = 0.0023994491384984914\n",
      "Iteration 27165: loss = 0.002399439123925588\n",
      "Iteration 27166: loss = 0.0023994291094608282\n",
      "Iteration 27167: loss = 0.002399419095104211\n",
      "Iteration 27168: loss = 0.002399409080855735\n",
      "Iteration 27169: loss = 0.0023993990667153984\n",
      "Iteration 27170: loss = 0.0023993890526832003\n",
      "Iteration 27171: loss = 0.0023993790387591397\n",
      "Iteration 27172: loss = 0.002399369024943215\n",
      "Iteration 27173: loss = 0.0023993590112354247\n",
      "Iteration 27174: loss = 0.0023993489976357673\n",
      "Iteration 27175: loss = 0.0023993389841442427\n",
      "Iteration 27176: loss = 0.0023993289707608468\n",
      "Iteration 27177: loss = 0.0023993189574855814\n",
      "Iteration 27178: loss = 0.0023993089443184435\n",
      "Iteration 27179: loss = 0.002399298931259432\n",
      "Iteration 27180: loss = 0.0023992889183085462\n",
      "Iteration 27181: loss = 0.0023992789054657834\n",
      "Iteration 27182: loss = 0.002399268892731144\n",
      "Iteration 27183: loss = 0.002399258880104625\n",
      "Iteration 27184: loss = 0.0023992488675862265\n",
      "Iteration 27185: loss = 0.0023992388551759463\n",
      "Iteration 27186: loss = 0.0023992288428737836\n",
      "Iteration 27187: loss = 0.0023992188306797353\n",
      "Iteration 27188: loss = 0.0023992088185938033\n",
      "Iteration 27189: loss = 0.002399198806615983\n",
      "Iteration 27190: loss = 0.0023991887947462757\n",
      "Iteration 27191: loss = 0.002399178782984679\n",
      "Iteration 27192: loss = 0.002399168771331191\n",
      "Iteration 27193: loss = 0.002399158759785811\n",
      "Iteration 27194: loss = 0.0023991487483485375\n",
      "Iteration 27195: loss = 0.002399138737019369\n",
      "Iteration 27196: loss = 0.002399128725798305\n",
      "Iteration 27197: loss = 0.002399118714685343\n",
      "Iteration 27198: loss = 0.0023991087036804824\n",
      "Iteration 27199: loss = 0.0023990986927837216\n",
      "Iteration 27200: loss = 0.0023990886819950596\n",
      "Iteration 27201: loss = 0.0023990786713144947\n",
      "Iteration 27202: loss = 0.0023990686607420256\n",
      "Iteration 27203: loss = 0.002399058650277651\n",
      "Iteration 27204: loss = 0.0023990486399213693\n",
      "Iteration 27205: loss = 0.0023990386296731804\n",
      "Iteration 27206: loss = 0.002399028619533082\n",
      "Iteration 27207: loss = 0.002399018609501072\n",
      "Iteration 27208: loss = 0.0023990085995771507\n",
      "Iteration 27209: loss = 0.0023989985897613156\n",
      "Iteration 27210: loss = 0.0023989885800535654\n",
      "Iteration 27211: loss = 0.0023989785704539\n",
      "Iteration 27212: loss = 0.002398968560962317\n",
      "Iteration 27213: loss = 0.002398958551578815\n",
      "Iteration 27214: loss = 0.0023989485423033924\n",
      "Iteration 27215: loss = 0.002398938533136049\n",
      "Iteration 27216: loss = 0.002398928524076783\n",
      "Iteration 27217: loss = 0.0023989185151255922\n",
      "Iteration 27218: loss = 0.002398908506282477\n",
      "Iteration 27219: loss = 0.002398898497547435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27220: loss = 0.002398888488920464\n",
      "Iteration 27221: loss = 0.002398878480401564\n",
      "Iteration 27222: loss = 0.002398868471990734\n",
      "Iteration 27223: loss = 0.0023988584636879715\n",
      "Iteration 27224: loss = 0.0023988484554932753\n",
      "Iteration 27225: loss = 0.0023988384474066446\n",
      "Iteration 27226: loss = 0.0023988284394280777\n",
      "Iteration 27227: loss = 0.0023988184315575744\n",
      "Iteration 27228: loss = 0.002398808423795131\n",
      "Iteration 27229: loss = 0.0023987984161407483\n",
      "Iteration 27230: loss = 0.0023987884085944246\n",
      "Iteration 27231: loss = 0.0023987784011561576\n",
      "Iteration 27232: loss = 0.002398768393825947\n",
      "Iteration 27233: loss = 0.0023987583866037906\n",
      "Iteration 27234: loss = 0.0023987483794896875\n",
      "Iteration 27235: loss = 0.002398738372483637\n",
      "Iteration 27236: loss = 0.0023987283655856366\n",
      "Iteration 27237: loss = 0.002398718358795686\n",
      "Iteration 27238: loss = 0.0023987083521137833\n",
      "Iteration 27239: loss = 0.0023986983455399273\n",
      "Iteration 27240: loss = 0.002398688339074116\n",
      "Iteration 27241: loss = 0.0023986783327163493\n",
      "Iteration 27242: loss = 0.002398668326466625\n",
      "Iteration 27243: loss = 0.0023986583203249425\n",
      "Iteration 27244: loss = 0.0023986483142912998\n",
      "Iteration 27245: loss = 0.0023986383083656956\n",
      "Iteration 27246: loss = 0.0023986283025481288\n",
      "Iteration 27247: loss = 0.0023986182968385983\n",
      "Iteration 27248: loss = 0.0023986082912371017\n",
      "Iteration 27249: loss = 0.0023985982857436397\n",
      "Iteration 27250: loss = 0.002398588280358209\n",
      "Iteration 27251: loss = 0.002398578275080809\n",
      "Iteration 27252: loss = 0.0023985682699114385\n",
      "Iteration 27253: loss = 0.002398558264850096\n",
      "Iteration 27254: loss = 0.0023985482598967802\n",
      "Iteration 27255: loss = 0.0023985382550514903\n",
      "Iteration 27256: loss = 0.002398528250314224\n",
      "Iteration 27257: loss = 0.0023985182456849803\n",
      "Iteration 27258: loss = 0.0023985082411637584\n",
      "Iteration 27259: loss = 0.0023984982367505564\n",
      "Iteration 27260: loss = 0.0023984882324453727\n",
      "Iteration 27261: loss = 0.002398478228248207\n",
      "Iteration 27262: loss = 0.0023984682241590567\n",
      "Iteration 27263: loss = 0.002398458220177922\n",
      "Iteration 27264: loss = 0.0023984482163048\n",
      "Iteration 27265: loss = 0.002398438212539691\n",
      "Iteration 27266: loss = 0.0023984282088825915\n",
      "Iteration 27267: loss = 0.0023984182053335026\n",
      "Iteration 27268: loss = 0.002398408201892421\n",
      "Iteration 27269: loss = 0.002398398198559347\n",
      "Iteration 27270: loss = 0.0023983881953342774\n",
      "Iteration 27271: loss = 0.0023983781922172127\n",
      "Iteration 27272: loss = 0.0023983681892081506\n",
      "Iteration 27273: loss = 0.00239835818630709\n",
      "Iteration 27274: loss = 0.0023983481835140293\n",
      "Iteration 27275: loss = 0.0023983381808289675\n",
      "Iteration 27276: loss = 0.002398328178251903\n",
      "Iteration 27277: loss = 0.002398318175782835\n",
      "Iteration 27278: loss = 0.002398308173421762\n",
      "Iteration 27279: loss = 0.0023982981711686818\n",
      "Iteration 27280: loss = 0.0023982881690235935\n",
      "Iteration 27281: loss = 0.002398278166986497\n",
      "Iteration 27282: loss = 0.002398268165057389\n",
      "Iteration 27283: loss = 0.0023982581632362703\n",
      "Iteration 27284: loss = 0.0023982481615231373\n",
      "Iteration 27285: loss = 0.002398238159917991\n",
      "Iteration 27286: loss = 0.002398228158420828\n",
      "Iteration 27287: loss = 0.002398218157031648\n",
      "Iteration 27288: loss = 0.002398208155750449\n",
      "Iteration 27289: loss = 0.0023981981545772306\n",
      "Iteration 27290: loss = 0.0023981881535119916\n",
      "Iteration 27291: loss = 0.0023981781525547296\n",
      "Iteration 27292: loss = 0.0023981681517054438\n",
      "Iteration 27293: loss = 0.0023981581509641327\n",
      "Iteration 27294: loss = 0.0023981481503307957\n",
      "Iteration 27295: loss = 0.0023981381498054304\n",
      "Iteration 27296: loss = 0.0023981281493880365\n",
      "Iteration 27297: loss = 0.0023981181490786113\n",
      "Iteration 27298: loss = 0.002398108148877155\n",
      "Iteration 27299: loss = 0.0023980981487836655\n",
      "Iteration 27300: loss = 0.002398088148798141\n",
      "Iteration 27301: loss = 0.0023980781489205813\n",
      "Iteration 27302: loss = 0.0023980681491509843\n",
      "Iteration 27303: loss = 0.0023980581494893486\n",
      "Iteration 27304: loss = 0.0023980481499356735\n",
      "Iteration 27305: loss = 0.002398038150489957\n",
      "Iteration 27306: loss = 0.0023980281511521984\n",
      "Iteration 27307: loss = 0.002398018151922396\n",
      "Iteration 27308: loss = 0.0023980081528005488\n",
      "Iteration 27309: loss = 0.0023979981537866546\n",
      "Iteration 27310: loss = 0.002397988154880713\n",
      "Iteration 27311: loss = 0.002397978156082722\n",
      "Iteration 27312: loss = 0.002397968157392681\n",
      "Iteration 27313: loss = 0.002397958158810588\n",
      "Iteration 27314: loss = 0.0023979481603364416\n",
      "Iteration 27315: loss = 0.002397938161970242\n",
      "Iteration 27316: loss = 0.0023979281637119855\n",
      "Iteration 27317: loss = 0.002397918165561672\n",
      "Iteration 27318: loss = 0.0023979081675193003\n",
      "Iteration 27319: loss = 0.0023978981695848695\n",
      "Iteration 27320: loss = 0.0023978881717583774\n",
      "Iteration 27321: loss = 0.0023978781740398225\n",
      "Iteration 27322: loss = 0.002397868176429204\n",
      "Iteration 27323: loss = 0.002397858178926521\n",
      "Iteration 27324: loss = 0.0023978481815317716\n",
      "Iteration 27325: loss = 0.0023978381842449546\n",
      "Iteration 27326: loss = 0.002397828187066068\n",
      "Iteration 27327: loss = 0.002397818189995111\n",
      "Iteration 27328: loss = 0.002397808193032083\n",
      "Iteration 27329: loss = 0.0023977981961769814\n",
      "Iteration 27330: loss = 0.0023977881994298057\n",
      "Iteration 27331: loss = 0.0023977782027905545\n",
      "Iteration 27332: loss = 0.002397768206259226\n",
      "Iteration 27333: loss = 0.002397758209835819\n",
      "Iteration 27334: loss = 0.002397748213520333\n",
      "Iteration 27335: loss = 0.002397738217312766\n",
      "Iteration 27336: loss = 0.0023977282212131164\n",
      "Iteration 27337: loss = 0.002397718225221383\n",
      "Iteration 27338: loss = 0.0023977082293375647\n",
      "Iteration 27339: loss = 0.0023976982335616602\n",
      "Iteration 27340: loss = 0.0023976882378936686\n",
      "Iteration 27341: loss = 0.0023976782423335875\n",
      "Iteration 27342: loss = 0.0023976682468814163\n",
      "Iteration 27343: loss = 0.0023976582515371535\n",
      "Iteration 27344: loss = 0.002397648256300797\n",
      "Iteration 27345: loss = 0.002397638261172347\n",
      "Iteration 27346: loss = 0.0023976282661518017\n",
      "Iteration 27347: loss = 0.002397618271239159\n",
      "Iteration 27348: loss = 0.002397608276434418\n",
      "Iteration 27349: loss = 0.0023975982817375776\n",
      "Iteration 27350: loss = 0.002397588287148637\n",
      "Iteration 27351: loss = 0.002397578292667593\n",
      "Iteration 27352: loss = 0.0023975682982944457\n",
      "Iteration 27353: loss = 0.0023975583040291943\n",
      "Iteration 27354: loss = 0.002397548309871836\n",
      "Iteration 27355: loss = 0.0023975383158223703\n",
      "Iteration 27356: loss = 0.0023975283218807954\n",
      "Iteration 27357: loss = 0.0023975183280471103\n",
      "Iteration 27358: loss = 0.0023975083343213137\n",
      "Iteration 27359: loss = 0.002397498340703405\n",
      "Iteration 27360: loss = 0.0023974883471933814\n",
      "Iteration 27361: loss = 0.0023974783537912426\n",
      "Iteration 27362: loss = 0.0023974683604969863\n",
      "Iteration 27363: loss = 0.0023974583673106124\n",
      "Iteration 27364: loss = 0.002397448374232119\n",
      "Iteration 27365: loss = 0.0023974383812615046\n",
      "Iteration 27366: loss = 0.002397428388398768\n",
      "Iteration 27367: loss = 0.0023974183956439075\n",
      "Iteration 27368: loss = 0.002397408402996923\n",
      "Iteration 27369: loss = 0.002397398410457812\n",
      "Iteration 27370: loss = 0.0023973884180265737\n",
      "Iteration 27371: loss = 0.002397378425703206\n",
      "Iteration 27372: loss = 0.0023973684334877084\n",
      "Iteration 27373: loss = 0.00239735844138008\n",
      "Iteration 27374: loss = 0.0023973484493803178\n",
      "Iteration 27375: loss = 0.0023973384574884227\n",
      "Iteration 27376: loss = 0.0023973284657043906\n",
      "Iteration 27377: loss = 0.002397318474028223\n",
      "Iteration 27378: loss = 0.0023973084824599165\n",
      "Iteration 27379: loss = 0.002397298490999471\n",
      "Iteration 27380: loss = 0.002397288499646885\n",
      "Iteration 27381: loss = 0.002397278508402157\n",
      "Iteration 27382: loss = 0.0023972685172652846\n",
      "Iteration 27383: loss = 0.002397258526236268\n",
      "Iteration 27384: loss = 0.002397248535315105\n",
      "Iteration 27385: loss = 0.002397238544501795\n",
      "Iteration 27386: loss = 0.002397228553796336\n",
      "Iteration 27387: loss = 0.0023972185631987272\n",
      "Iteration 27388: loss = 0.002397208572708967\n",
      "Iteration 27389: loss = 0.002397198582327054\n",
      "Iteration 27390: loss = 0.0023971885920529867\n",
      "Iteration 27391: loss = 0.0023971786018867638\n",
      "Iteration 27392: loss = 0.002397168611828385\n",
      "Iteration 27393: loss = 0.0023971586218778478\n",
      "Iteration 27394: loss = 0.002397148632035151\n",
      "Iteration 27395: loss = 0.002397138642300294\n",
      "Iteration 27396: loss = 0.0023971286526732745\n",
      "Iteration 27397: loss = 0.0023971186631540917\n",
      "Iteration 27398: loss = 0.002397108673742745\n",
      "Iteration 27399: loss = 0.002397098684439231\n",
      "Iteration 27400: loss = 0.0023970886952435506\n",
      "Iteration 27401: loss = 0.0023970787061557016\n",
      "Iteration 27402: loss = 0.002397068717175682\n",
      "Iteration 27403: loss = 0.002397058728303491\n",
      "Iteration 27404: loss = 0.002397048739539128\n",
      "Iteration 27405: loss = 0.0023970387508825907\n",
      "Iteration 27406: loss = 0.002397028762333878\n",
      "Iteration 27407: loss = 0.002397018773892989\n",
      "Iteration 27408: loss = 0.0023970087855599213\n",
      "Iteration 27409: loss = 0.0023969987973346753\n",
      "Iteration 27410: loss = 0.0023969888092172475\n",
      "Iteration 27411: loss = 0.002396978821207639\n",
      "Iteration 27412: loss = 0.0023969688333058464\n",
      "Iteration 27413: loss = 0.00239695884551187\n",
      "Iteration 27414: loss = 0.0023969488578257066\n",
      "Iteration 27415: loss = 0.0023969388702473567\n",
      "Iteration 27416: loss = 0.0023969288827768182\n",
      "Iteration 27417: loss = 0.0023969188954140897\n",
      "Iteration 27418: loss = 0.0023969089081591695\n",
      "Iteration 27419: loss = 0.0023968989210120567\n",
      "Iteration 27420: loss = 0.002396888933972751\n",
      "Iteration 27421: loss = 0.0023968789470412495\n",
      "Iteration 27422: loss = 0.0023968689602175512\n",
      "Iteration 27423: loss = 0.002396858973501655\n",
      "Iteration 27424: loss = 0.00239684898689356\n",
      "Iteration 27425: loss = 0.002396839000393264\n",
      "Iteration 27426: loss = 0.0023968290140007663\n",
      "Iteration 27427: loss = 0.002396819027716066\n",
      "Iteration 27428: loss = 0.0023968090415391607\n",
      "Iteration 27429: loss = 0.00239679905547005\n",
      "Iteration 27430: loss = 0.002396789069508731\n",
      "Iteration 27431: loss = 0.002396779083655204\n",
      "Iteration 27432: loss = 0.002396769097909468\n",
      "Iteration 27433: loss = 0.00239675911227152\n",
      "Iteration 27434: loss = 0.0023967491267413593\n",
      "Iteration 27435: loss = 0.0023967391413189853\n",
      "Iteration 27436: loss = 0.002396729156004396\n",
      "Iteration 27437: loss = 0.0023967191707975903\n",
      "Iteration 27438: loss = 0.0023967091856985662\n",
      "Iteration 27439: loss = 0.002396699200707324\n",
      "Iteration 27440: loss = 0.0023966892158238605\n",
      "Iteration 27441: loss = 0.0023966792310481763\n",
      "Iteration 27442: loss = 0.0023966692463802675\n",
      "Iteration 27443: loss = 0.0023966592618201357\n",
      "Iteration 27444: loss = 0.002396649277367777\n",
      "Iteration 27445: loss = 0.0023966392930231915\n",
      "Iteration 27446: loss = 0.002396629308786378\n",
      "Iteration 27447: loss = 0.0023966193246573344\n",
      "Iteration 27448: loss = 0.00239660934063606\n",
      "Iteration 27449: loss = 0.0023965993567225527\n",
      "Iteration 27450: loss = 0.002396589372916812\n",
      "Iteration 27451: loss = 0.0023965793892188362\n",
      "Iteration 27452: loss = 0.002396569405628624\n",
      "Iteration 27453: loss = 0.002396559422146174\n",
      "Iteration 27454: loss = 0.002396549438771485\n",
      "Iteration 27455: loss = 0.002396539455504556\n",
      "Iteration 27456: loss = 0.0023965294723453847\n",
      "Iteration 27457: loss = 0.0023965194892939705\n",
      "Iteration 27458: loss = 0.0023965095063503126\n",
      "Iteration 27459: loss = 0.0023964995235144083\n",
      "Iteration 27460: loss = 0.002396489540786257\n",
      "Iteration 27461: loss = 0.0023964795581658577\n",
      "Iteration 27462: loss = 0.0023964695756532083\n",
      "Iteration 27463: loss = 0.0023964595932483078\n",
      "Iteration 27464: loss = 0.002396449610951156\n",
      "Iteration 27465: loss = 0.00239643962876175\n",
      "Iteration 27466: loss = 0.0023964296466800887\n",
      "Iteration 27467: loss = 0.002396419664706171\n",
      "Iteration 27468: loss = 0.0023964096828399963\n",
      "Iteration 27469: loss = 0.0023963997010815626\n",
      "Iteration 27470: loss = 0.0023963897194308682\n",
      "Iteration 27471: loss = 0.0023963797378879124\n",
      "Iteration 27472: loss = 0.0023963697564526936\n",
      "Iteration 27473: loss = 0.002396359775125211\n",
      "Iteration 27474: loss = 0.002396349793905462\n",
      "Iteration 27475: loss = 0.002396339812793446\n",
      "Iteration 27476: loss = 0.0023963298317891623\n",
      "Iteration 27477: loss = 0.0023963198508926086\n",
      "Iteration 27478: loss = 0.0023963098701037847\n",
      "Iteration 27479: loss = 0.002396299889422688\n",
      "Iteration 27480: loss = 0.0023962899088493185\n",
      "Iteration 27481: loss = 0.002396279928383673\n",
      "Iteration 27482: loss = 0.002396269948025752\n",
      "Iteration 27483: loss = 0.0023962599677755532\n",
      "Iteration 27484: loss = 0.0023962499876330756\n",
      "Iteration 27485: loss = 0.002396240007598318\n",
      "Iteration 27486: loss = 0.002396230027671279\n",
      "Iteration 27487: loss = 0.002396220047851956\n",
      "Iteration 27488: loss = 0.0023962100681403503\n",
      "Iteration 27489: loss = 0.002396200088536458\n",
      "Iteration 27490: loss = 0.0023961901090402793\n",
      "Iteration 27491: loss = 0.002396180129651812\n",
      "Iteration 27492: loss = 0.0023961701503710563\n",
      "Iteration 27493: loss = 0.0023961601711980085\n",
      "Iteration 27494: loss = 0.0023961501921326693\n",
      "Iteration 27495: loss = 0.0023961402131750372\n",
      "Iteration 27496: loss = 0.002396130234325109\n",
      "Iteration 27497: loss = 0.0023961202555828854\n",
      "Iteration 27498: loss = 0.0023961102769483648\n",
      "Iteration 27499: loss = 0.0023961002984215444\n",
      "Iteration 27500: loss = 0.0023960903200024248\n",
      "Iteration 27501: loss = 0.0023960803416910033\n",
      "Iteration 27502: loss = 0.002396070363487279\n",
      "Iteration 27503: loss = 0.002396060385391251\n",
      "Iteration 27504: loss = 0.0023960504074029172\n",
      "Iteration 27505: loss = 0.002396040429522277\n",
      "Iteration 27506: loss = 0.002396030451749328\n",
      "Iteration 27507: loss = 0.0023960204740840698\n",
      "Iteration 27508: loss = 0.0023960104965265014\n",
      "Iteration 27509: loss = 0.002396000519076621\n",
      "Iteration 27510: loss = 0.0023959905417344266\n",
      "Iteration 27511: loss = 0.002395980564499918\n",
      "Iteration 27512: loss = 0.002395970587373093\n",
      "Iteration 27513: loss = 0.002395960610353951\n",
      "Iteration 27514: loss = 0.0023959506334424903\n",
      "Iteration 27515: loss = 0.0023959406566387096\n",
      "Iteration 27516: loss = 0.002395930679942608\n",
      "Iteration 27517: loss = 0.002395920703354183\n",
      "Iteration 27518: loss = 0.002395910726873434\n",
      "Iteration 27519: loss = 0.0023959007505003596\n",
      "Iteration 27520: loss = 0.002395890774234959\n",
      "Iteration 27521: loss = 0.00239588079807723\n",
      "Iteration 27522: loss = 0.0023958708220271725\n",
      "Iteration 27523: loss = 0.0023958608460847835\n",
      "Iteration 27524: loss = 0.002395850870250063\n",
      "Iteration 27525: loss = 0.0023958408945230093\n",
      "Iteration 27526: loss = 0.0023958309189036206\n",
      "Iteration 27527: loss = 0.002395820943391896\n",
      "Iteration 27528: loss = 0.0023958109679878344\n",
      "Iteration 27529: loss = 0.0023958009926914343\n",
      "Iteration 27530: loss = 0.002395791017502694\n",
      "Iteration 27531: loss = 0.002395781042421613\n",
      "Iteration 27532: loss = 0.002395771067448188\n",
      "Iteration 27533: loss = 0.0023957610925824215\n",
      "Iteration 27534: loss = 0.0023957511178243083\n",
      "Iteration 27535: loss = 0.002395741143173848\n",
      "Iteration 27536: loss = 0.002395731168631041\n",
      "Iteration 27537: loss = 0.002395721194195884\n",
      "Iteration 27538: loss = 0.002395711219868377\n",
      "Iteration 27539: loss = 0.002395701245648518\n",
      "Iteration 27540: loss = 0.0023956912715363053\n",
      "Iteration 27541: loss = 0.002395681297531739\n",
      "Iteration 27542: loss = 0.0023956713236348167\n",
      "Iteration 27543: loss = 0.002395661349845537\n",
      "Iteration 27544: loss = 0.0023956513761638986\n",
      "Iteration 27545: loss = 0.0023956414025899002\n",
      "Iteration 27546: loss = 0.002395631429123541\n",
      "Iteration 27547: loss = 0.0023956214557648197\n",
      "Iteration 27548: loss = 0.002395611482513734\n",
      "Iteration 27549: loss = 0.0023956015093702836\n",
      "Iteration 27550: loss = 0.0023955915363344667\n",
      "Iteration 27551: loss = 0.002395581563406282\n",
      "Iteration 27552: loss = 0.002395571590585728\n",
      "Iteration 27553: loss = 0.002395561617872804\n",
      "Iteration 27554: loss = 0.0023955516452675075\n",
      "Iteration 27555: loss = 0.0023955416727698387\n",
      "Iteration 27556: loss = 0.002395531700379795\n",
      "Iteration 27557: loss = 0.0023955217280973756\n",
      "Iteration 27558: loss = 0.0023955117559225796\n",
      "Iteration 27559: loss = 0.002395501783855405\n",
      "Iteration 27560: loss = 0.0023954918118958505\n",
      "Iteration 27561: loss = 0.002395481840043915\n",
      "Iteration 27562: loss = 0.0023954718682995973\n",
      "Iteration 27563: loss = 0.002395461896662896\n",
      "Iteration 27564: loss = 0.0023954519251338097\n",
      "Iteration 27565: loss = 0.0023954419537123364\n",
      "Iteration 27566: loss = 0.0023954319823984763\n",
      "Iteration 27567: loss = 0.0023954220111922267\n",
      "Iteration 27568: loss = 0.002395412040093587\n",
      "Iteration 27569: loss = 0.0023954020691025553\n",
      "Iteration 27570: loss = 0.0023953920982191313\n",
      "Iteration 27571: loss = 0.002395382127443312\n",
      "Iteration 27572: loss = 0.0023953721567750983\n",
      "Iteration 27573: loss = 0.002395362186214487\n",
      "Iteration 27574: loss = 0.002395352215761477\n",
      "Iteration 27575: loss = 0.0023953422454160685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27576: loss = 0.0023953322751782582\n",
      "Iteration 27577: loss = 0.0023953223050480464\n",
      "Iteration 27578: loss = 0.00239531233502543\n",
      "Iteration 27579: loss = 0.002395302365110409\n",
      "Iteration 27580: loss = 0.0023952923953029816\n",
      "Iteration 27581: loss = 0.0023952824256031472\n",
      "Iteration 27582: loss = 0.0023952724560109043\n",
      "Iteration 27583: loss = 0.0023952624865262503\n",
      "Iteration 27584: loss = 0.0023952525171491855\n",
      "Iteration 27585: loss = 0.0023952425478797074\n",
      "Iteration 27586: loss = 0.002395232578717815\n",
      "Iteration 27587: loss = 0.002395222609663507\n",
      "Iteration 27588: loss = 0.0023952126407167824\n",
      "Iteration 27589: loss = 0.0023952026718776394\n",
      "Iteration 27590: loss = 0.0023951927031460774\n",
      "Iteration 27591: loss = 0.002395182734522094\n",
      "Iteration 27592: loss = 0.0023951727660056887\n",
      "Iteration 27593: loss = 0.00239516279759686\n",
      "Iteration 27594: loss = 0.0023951528292956064\n",
      "Iteration 27595: loss = 0.0023951428611019266\n",
      "Iteration 27596: loss = 0.0023951328930158196\n",
      "Iteration 27597: loss = 0.0023951229250372837\n",
      "Iteration 27598: loss = 0.0023951129571663176\n",
      "Iteration 27599: loss = 0.0023951029894029204\n",
      "Iteration 27600: loss = 0.0023950930217470903\n",
      "Iteration 27601: loss = 0.002395083054198826\n",
      "Iteration 27602: loss = 0.0023950730867581265\n",
      "Iteration 27603: loss = 0.00239506311942499\n",
      "Iteration 27604: loss = 0.0023950531521994157\n",
      "Iteration 27605: loss = 0.002395043185081402\n",
      "Iteration 27606: loss = 0.002395033218070947\n",
      "Iteration 27607: loss = 0.002395023251168051\n",
      "Iteration 27608: loss = 0.0023950132843727104\n",
      "Iteration 27609: loss = 0.0023950033176849263\n",
      "Iteration 27610: loss = 0.002394993351104696\n",
      "Iteration 27611: loss = 0.002394983384632017\n",
      "Iteration 27612: loss = 0.002394973418266891\n",
      "Iteration 27613: loss = 0.0023949634520093145\n",
      "Iteration 27614: loss = 0.0023949534858592864\n",
      "Iteration 27615: loss = 0.002394943519816806\n",
      "Iteration 27616: loss = 0.0023949335538818715\n",
      "Iteration 27617: loss = 0.0023949235880544816\n",
      "Iteration 27618: loss = 0.002394913622334635\n",
      "Iteration 27619: loss = 0.0023949036567223305\n",
      "Iteration 27620: loss = 0.0023948936912175666\n",
      "Iteration 27621: loss = 0.002394883725820343\n",
      "Iteration 27622: loss = 0.0023948737605306567\n",
      "Iteration 27623: loss = 0.002394863795348507\n",
      "Iteration 27624: loss = 0.0023948538302738936\n",
      "Iteration 27625: loss = 0.0023948438653068134\n",
      "Iteration 27626: loss = 0.0023948339004472665\n",
      "Iteration 27627: loss = 0.0023948239356952504\n",
      "Iteration 27628: loss = 0.002394813971050765\n",
      "Iteration 27629: loss = 0.0023948040065138085\n",
      "Iteration 27630: loss = 0.0023947940420843794\n",
      "Iteration 27631: loss = 0.002394784077762476\n",
      "Iteration 27632: loss = 0.002394774113548098\n",
      "Iteration 27633: loss = 0.002394764149441243\n",
      "Iteration 27634: loss = 0.0023947541854419103\n",
      "Iteration 27635: loss = 0.002394744221550099\n",
      "Iteration 27636: loss = 0.0023947342577658065\n",
      "Iteration 27637: loss = 0.002394724294089032\n",
      "Iteration 27638: loss = 0.0023947143305197752\n",
      "Iteration 27639: loss = 0.002394704367058034\n",
      "Iteration 27640: loss = 0.0023946944037038062\n",
      "Iteration 27641: loss = 0.0023946844404570916\n",
      "Iteration 27642: loss = 0.0023946744773178886\n",
      "Iteration 27643: loss = 0.002394664514286196\n",
      "Iteration 27644: loss = 0.002394654551362013\n",
      "Iteration 27645: loss = 0.002394644588545336\n",
      "Iteration 27646: loss = 0.002394634625836167\n",
      "Iteration 27647: loss = 0.0023946246632345013\n",
      "Iteration 27648: loss = 0.00239461470074034\n",
      "Iteration 27649: loss = 0.0023946047383536805\n",
      "Iteration 27650: loss = 0.0023945947760745227\n",
      "Iteration 27651: loss = 0.0023945848139028643\n",
      "Iteration 27652: loss = 0.002394574851838704\n",
      "Iteration 27653: loss = 0.002394564889882041\n",
      "Iteration 27654: loss = 0.0023945549280328738\n",
      "Iteration 27655: loss = 0.0023945449662912005\n",
      "Iteration 27656: loss = 0.0023945350046570206\n",
      "Iteration 27657: loss = 0.0023945250431303315\n",
      "Iteration 27658: loss = 0.0023945150817111342\n",
      "Iteration 27659: loss = 0.0023945051203994247\n",
      "Iteration 27660: loss = 0.0023944951591952034\n",
      "Iteration 27661: loss = 0.0023944851980984686\n",
      "Iteration 27662: loss = 0.002394475237109219\n",
      "Iteration 27663: loss = 0.002394465276227453\n",
      "Iteration 27664: loss = 0.002394455315453169\n",
      "Iteration 27665: loss = 0.0023944453547863668\n",
      "Iteration 27666: loss = 0.0023944353942270433\n",
      "Iteration 27667: loss = 0.0023944254337751993\n",
      "Iteration 27668: loss = 0.002394415473430832\n",
      "Iteration 27669: loss = 0.0023944055131939405\n",
      "Iteration 27670: loss = 0.002394395553064524\n",
      "Iteration 27671: loss = 0.0023943855930425796\n",
      "Iteration 27672: loss = 0.0023943756331281075\n",
      "Iteration 27673: loss = 0.0023943656733211058\n",
      "Iteration 27674: loss = 0.0023943557136215736\n",
      "Iteration 27675: loss = 0.002394345754029509\n",
      "Iteration 27676: loss = 0.002394335794544911\n",
      "Iteration 27677: loss = 0.0023943258351677783\n",
      "Iteration 27678: loss = 0.0023943158758981085\n",
      "Iteration 27679: loss = 0.002394305916735902\n",
      "Iteration 27680: loss = 0.002394295957681157\n",
      "Iteration 27681: loss = 0.002394285998733872\n",
      "Iteration 27682: loss = 0.0023942760398940445\n",
      "Iteration 27683: loss = 0.002394266081161675\n",
      "Iteration 27684: loss = 0.002394256122536761\n",
      "Iteration 27685: loss = 0.0023942461640193025\n",
      "Iteration 27686: loss = 0.002394236205609296\n",
      "Iteration 27687: loss = 0.002394226247306743\n",
      "Iteration 27688: loss = 0.00239421628911164\n",
      "Iteration 27689: loss = 0.0023942063310239854\n",
      "Iteration 27690: loss = 0.00239419637304378\n",
      "Iteration 27691: loss = 0.0023941864151710204\n",
      "Iteration 27692: loss = 0.0023941764574057062\n",
      "Iteration 27693: loss = 0.002394166499747836\n",
      "Iteration 27694: loss = 0.0023941565421974086\n",
      "Iteration 27695: loss = 0.002394146584754423\n",
      "Iteration 27696: loss = 0.0023941366274188767\n",
      "Iteration 27697: loss = 0.002394126670190769\n",
      "Iteration 27698: loss = 0.002394116713070099\n",
      "Iteration 27699: loss = 0.002394106756056865\n",
      "Iteration 27700: loss = 0.002394096799151066\n",
      "Iteration 27701: loss = 0.0023940868423527\n",
      "Iteration 27702: loss = 0.0023940768856617663\n",
      "Iteration 27703: loss = 0.0023940669290782634\n",
      "Iteration 27704: loss = 0.0023940569726021896\n",
      "Iteration 27705: loss = 0.002394047016233544\n",
      "Iteration 27706: loss = 0.0023940370599723256\n",
      "Iteration 27707: loss = 0.002394027103818532\n",
      "Iteration 27708: loss = 0.0023940171477721626\n",
      "Iteration 27709: loss = 0.0023940071918332165\n",
      "Iteration 27710: loss = 0.0023939972360016914\n",
      "Iteration 27711: loss = 0.0023939872802775865\n",
      "Iteration 27712: loss = 0.0023939773246609004\n",
      "Iteration 27713: loss = 0.0023939673691516323\n",
      "Iteration 27714: loss = 0.00239395741374978\n",
      "Iteration 27715: loss = 0.0023939474584553423\n",
      "Iteration 27716: loss = 0.002393937503268319\n",
      "Iteration 27717: loss = 0.002393927548188707\n",
      "Iteration 27718: loss = 0.002393917593216506\n",
      "Iteration 27719: loss = 0.0023939076383517146\n",
      "Iteration 27720: loss = 0.0023938976835943316\n",
      "Iteration 27721: loss = 0.0023938877289443558\n",
      "Iteration 27722: loss = 0.002393877774401785\n",
      "Iteration 27723: loss = 0.002393867819966619\n",
      "Iteration 27724: loss = 0.002393857865638855\n",
      "Iteration 27725: loss = 0.002393847911418494\n",
      "Iteration 27726: loss = 0.0023938379573055315\n",
      "Iteration 27727: loss = 0.0023938280032999694\n",
      "Iteration 27728: loss = 0.002393818049401804\n",
      "Iteration 27729: loss = 0.0023938080956110355\n",
      "Iteration 27730: loss = 0.0023937981419276623\n",
      "Iteration 27731: loss = 0.002393788188351682\n",
      "Iteration 27732: loss = 0.0023937782348830944\n",
      "Iteration 27733: loss = 0.0023937682815218975\n",
      "Iteration 27734: loss = 0.002393758328268091\n",
      "Iteration 27735: loss = 0.0023937483751216722\n",
      "Iteration 27736: loss = 0.002393738422082641\n",
      "Iteration 27737: loss = 0.002393728469150995\n",
      "Iteration 27738: loss = 0.002393718516326733\n",
      "Iteration 27739: loss = 0.0023937085636098546\n",
      "Iteration 27740: loss = 0.002393698611000358\n",
      "Iteration 27741: loss = 0.002393688658498242\n",
      "Iteration 27742: loss = 0.0023936787061035044\n",
      "Iteration 27743: loss = 0.0023936687538161456\n",
      "Iteration 27744: loss = 0.0023936588016361623\n",
      "Iteration 27745: loss = 0.0023936488495635544\n",
      "Iteration 27746: loss = 0.0023936388975983203\n",
      "Iteration 27747: loss = 0.0023936289457404587\n",
      "Iteration 27748: loss = 0.0023936189939899686\n",
      "Iteration 27749: loss = 0.002393609042346848\n",
      "Iteration 27750: loss = 0.0023935990908110953\n",
      "Iteration 27751: loss = 0.0023935891393827105\n",
      "Iteration 27752: loss = 0.0023935791880616915\n",
      "Iteration 27753: loss = 0.002393569236848037\n",
      "Iteration 27754: loss = 0.0023935592857417453\n",
      "Iteration 27755: loss = 0.0023935493347428155\n",
      "Iteration 27756: loss = 0.002393539383851247\n",
      "Iteration 27757: loss = 0.0023935294330670372\n",
      "Iteration 27758: loss = 0.0023935194823901857\n",
      "Iteration 27759: loss = 0.0023935095318206897\n",
      "Iteration 27760: loss = 0.00239349958135855\n",
      "Iteration 27761: loss = 0.0023934896310037644\n",
      "Iteration 27762: loss = 0.0023934796807563307\n",
      "Iteration 27763: loss = 0.0023934697306162486\n",
      "Iteration 27764: loss = 0.0023934597805835164\n",
      "Iteration 27765: loss = 0.0023934498306581327\n",
      "Iteration 27766: loss = 0.0023934398808400964\n",
      "Iteration 27767: loss = 0.002393429931129406\n",
      "Iteration 27768: loss = 0.002393419981526061\n",
      "Iteration 27769: loss = 0.002393410032030058\n",
      "Iteration 27770: loss = 0.0023934000826413985\n",
      "Iteration 27771: loss = 0.0023933901333600786\n",
      "Iteration 27772: loss = 0.0023933801841860983\n",
      "Iteration 27773: loss = 0.002393370235119456\n",
      "Iteration 27774: loss = 0.002393360286160151\n",
      "Iteration 27775: loss = 0.0023933503373081808\n",
      "Iteration 27776: loss = 0.0023933403885635445\n",
      "Iteration 27777: loss = 0.002393330439926241\n",
      "Iteration 27778: loss = 0.00239332049139627\n",
      "Iteration 27779: loss = 0.002393310542973628\n",
      "Iteration 27780: loss = 0.0023933005946583153\n",
      "Iteration 27781: loss = 0.00239329064645033\n",
      "Iteration 27782: loss = 0.00239328069834967\n",
      "Iteration 27783: loss = 0.0023932707503563364\n",
      "Iteration 27784: loss = 0.002393260802470325\n",
      "Iteration 27785: loss = 0.0023932508546916365\n",
      "Iteration 27786: loss = 0.0023932409070202684\n",
      "Iteration 27787: loss = 0.00239323095945622\n",
      "Iteration 27788: loss = 0.0023932210119994896\n",
      "Iteration 27789: loss = 0.0023932110646500758\n",
      "Iteration 27790: loss = 0.002393201117407978\n",
      "Iteration 27791: loss = 0.0023931911702731947\n",
      "Iteration 27792: loss = 0.0023931812232457234\n",
      "Iteration 27793: loss = 0.0023931712763255644\n",
      "Iteration 27794: loss = 0.002393161329512715\n",
      "Iteration 27795: loss = 0.0023931513828071753\n",
      "Iteration 27796: loss = 0.0023931414362089428\n",
      "Iteration 27797: loss = 0.0023931314897180167\n",
      "Iteration 27798: loss = 0.0023931215433343955\n",
      "Iteration 27799: loss = 0.002393111597058078\n",
      "Iteration 27800: loss = 0.0023931016508890626\n",
      "Iteration 27801: loss = 0.0023930917048273484\n",
      "Iteration 27802: loss = 0.0023930817588729337\n",
      "Iteration 27803: loss = 0.0023930718130258178\n",
      "Iteration 27804: loss = 0.002393061867285999\n",
      "Iteration 27805: loss = 0.0023930519216534742\n",
      "Iteration 27806: loss = 0.0023930419761282453\n",
      "Iteration 27807: loss = 0.002393032030710309\n",
      "Iteration 27808: loss = 0.002393022085399664\n",
      "Iteration 27809: loss = 0.00239301214019631\n",
      "Iteration 27810: loss = 0.0023930021951002454\n",
      "Iteration 27811: loss = 0.002392992250111468\n",
      "Iteration 27812: loss = 0.0023929823052299777\n",
      "Iteration 27813: loss = 0.0023929723604557717\n",
      "Iteration 27814: loss = 0.0023929624157888493\n",
      "Iteration 27815: loss = 0.00239295247122921\n",
      "Iteration 27816: loss = 0.002392942526776852\n",
      "Iteration 27817: loss = 0.0023929325824317733\n",
      "Iteration 27818: loss = 0.002392922638193973\n",
      "Iteration 27819: loss = 0.00239291269406345\n",
      "Iteration 27820: loss = 0.0023929027500402033\n",
      "Iteration 27821: loss = 0.0023928928061242305\n",
      "Iteration 27822: loss = 0.0023928828623155313\n",
      "Iteration 27823: loss = 0.0023928729186141035\n",
      "Iteration 27824: loss = 0.0023928629750199467\n",
      "Iteration 27825: loss = 0.002392853031533059\n",
      "Iteration 27826: loss = 0.0023928430881534394\n",
      "Iteration 27827: loss = 0.0023928331448810855\n",
      "Iteration 27828: loss = 0.0023928232017159973\n",
      "Iteration 27829: loss = 0.002392813258658174\n",
      "Iteration 27830: loss = 0.0023928033157076126\n",
      "Iteration 27831: loss = 0.002392793372864312\n",
      "Iteration 27832: loss = 0.0023927834301282723\n",
      "Iteration 27833: loss = 0.0023927734874994904\n",
      "Iteration 27834: loss = 0.002392763544977966\n",
      "Iteration 27835: loss = 0.002392753602563698\n",
      "Iteration 27836: loss = 0.0023927436602566845\n",
      "Iteration 27837: loss = 0.002392733718056924\n",
      "Iteration 27838: loss = 0.002392723775964416\n",
      "Iteration 27839: loss = 0.002392713833979158\n",
      "Iteration 27840: loss = 0.0023927038921011503\n",
      "Iteration 27841: loss = 0.00239269395033039\n",
      "Iteration 27842: loss = 0.002392684008666876\n",
      "Iteration 27843: loss = 0.002392674067110608\n",
      "Iteration 27844: loss = 0.002392664125661585\n",
      "Iteration 27845: loss = 0.0023926541843198035\n",
      "Iteration 27846: loss = 0.0023926442430852635\n",
      "Iteration 27847: loss = 0.002392634301957964\n",
      "Iteration 27848: loss = 0.0023926243609379036\n",
      "Iteration 27849: loss = 0.0023926144200250798\n",
      "Iteration 27850: loss = 0.0023926044792194926\n",
      "Iteration 27851: loss = 0.0023925945385211405\n",
      "Iteration 27852: loss = 0.0023925845979300216\n",
      "Iteration 27853: loss = 0.002392574657446135\n",
      "Iteration 27854: loss = 0.002392564717069479\n",
      "Iteration 27855: loss = 0.002392554776800053\n",
      "Iteration 27856: loss = 0.0023925448366378544\n",
      "Iteration 27857: loss = 0.0023925348965828834\n",
      "Iteration 27858: loss = 0.0023925249566351383\n",
      "Iteration 27859: loss = 0.0023925150167946165\n",
      "Iteration 27860: loss = 0.0023925050770613183\n",
      "Iteration 27861: loss = 0.002392495137435241\n",
      "Iteration 27862: loss = 0.002392485197916385\n",
      "Iteration 27863: loss = 0.002392475258504747\n",
      "Iteration 27864: loss = 0.0023924653192003266\n",
      "Iteration 27865: loss = 0.002392455380003123\n",
      "Iteration 27866: loss = 0.002392445440913134\n",
      "Iteration 27867: loss = 0.0023924355019303593\n",
      "Iteration 27868: loss = 0.002392425563054796\n",
      "Iteration 27869: loss = 0.0023924156242864443\n",
      "Iteration 27870: loss = 0.002392405685625303\n",
      "Iteration 27871: loss = 0.0023923957470713686\n",
      "Iteration 27872: loss = 0.0023923858086246424\n",
      "Iteration 27873: loss = 0.002392375870285121\n",
      "Iteration 27874: loss = 0.0023923659320528043\n",
      "Iteration 27875: loss = 0.0023923559939276915\n",
      "Iteration 27876: loss = 0.0023923460559097795\n",
      "Iteration 27877: loss = 0.002392336117999068\n",
      "Iteration 27878: loss = 0.002392326180195556\n",
      "Iteration 27879: loss = 0.002392316242499242\n",
      "Iteration 27880: loss = 0.002392306304910124\n",
      "Iteration 27881: loss = 0.002392296367428201\n",
      "Iteration 27882: loss = 0.0023922864300534724\n",
      "Iteration 27883: loss = 0.0023922764927859354\n",
      "Iteration 27884: loss = 0.0023922665556255903\n",
      "Iteration 27885: loss = 0.002392256618572435\n",
      "Iteration 27886: loss = 0.002392246681626468\n",
      "Iteration 27887: loss = 0.0023922367447876884\n",
      "Iteration 27888: loss = 0.0023922268080560946\n",
      "Iteration 27889: loss = 0.002392216871431685\n",
      "Iteration 27890: loss = 0.0023922069349144592\n",
      "Iteration 27891: loss = 0.002392196998504415\n",
      "Iteration 27892: loss = 0.0023921870622015512\n",
      "Iteration 27893: loss = 0.0023921771260058673\n",
      "Iteration 27894: loss = 0.0023921671899173615\n",
      "Iteration 27895: loss = 0.0023921572539360313\n",
      "Iteration 27896: loss = 0.0023921473180618775\n",
      "Iteration 27897: loss = 0.0023921373822948966\n",
      "Iteration 27898: loss = 0.002392127446635089\n",
      "Iteration 27899: loss = 0.002392117511082453\n",
      "Iteration 27900: loss = 0.0023921075756369865\n",
      "Iteration 27901: loss = 0.002392097640298689\n",
      "Iteration 27902: loss = 0.0023920877050675584\n",
      "Iteration 27903: loss = 0.0023920777699435944\n",
      "Iteration 27904: loss = 0.0023920678349267947\n",
      "Iteration 27905: loss = 0.002392057900017159\n",
      "Iteration 27906: loss = 0.0023920479652146847\n",
      "Iteration 27907: loss = 0.0023920380305193713\n",
      "Iteration 27908: loss = 0.002392028095931217\n",
      "Iteration 27909: loss = 0.0023920181614502218\n",
      "Iteration 27910: loss = 0.0023920082270763826\n",
      "Iteration 27911: loss = 0.0023919982928096994\n",
      "Iteration 27912: loss = 0.0023919883586501706\n",
      "Iteration 27913: loss = 0.002391978424597794\n",
      "Iteration 27914: loss = 0.0023919684906525684\n",
      "Iteration 27915: loss = 0.0023919585568144943\n",
      "Iteration 27916: loss = 0.0023919486230835675\n",
      "Iteration 27917: loss = 0.0023919386894597893\n",
      "Iteration 27918: loss = 0.0023919287559431577\n",
      "Iteration 27919: loss = 0.0023919188225336703\n",
      "Iteration 27920: loss = 0.002391908889231326\n",
      "Iteration 27921: loss = 0.002391898956036125\n",
      "Iteration 27922: loss = 0.0023918890229480644\n",
      "Iteration 27923: loss = 0.0023918790899671435\n",
      "Iteration 27924: loss = 0.002391869157093361\n",
      "Iteration 27925: loss = 0.002391859224326715\n",
      "Iteration 27926: loss = 0.0023918492916672053\n",
      "Iteration 27927: loss = 0.0023918393591148292\n",
      "Iteration 27928: loss = 0.002391829426669586\n",
      "Iteration 27929: loss = 0.002391819494331476\n",
      "Iteration 27930: loss = 0.0023918095621004946\n",
      "Iteration 27931: loss = 0.002391799629976643\n",
      "Iteration 27932: loss = 0.002391789697959919\n",
      "Iteration 27933: loss = 0.0023917797660503214\n",
      "Iteration 27934: loss = 0.002391769834247849\n",
      "Iteration 27935: loss = 0.0023917599025525\n",
      "Iteration 27936: loss = 0.0023917499709642737\n",
      "Iteration 27937: loss = 0.0023917400394831687\n",
      "Iteration 27938: loss = 0.002391730108109183\n",
      "Iteration 27939: loss = 0.002391720176842316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27940: loss = 0.002391710245682566\n",
      "Iteration 27941: loss = 0.002391700314629932\n",
      "Iteration 27942: loss = 0.0023916903836844125\n",
      "Iteration 27943: loss = 0.002391680452846006\n",
      "Iteration 27944: loss = 0.0023916705221147115\n",
      "Iteration 27945: loss = 0.0023916605914905274\n",
      "Iteration 27946: loss = 0.0023916506609734526\n",
      "Iteration 27947: loss = 0.0023916407305634856\n",
      "Iteration 27948: loss = 0.002391630800260625\n",
      "Iteration 27949: loss = 0.0023916208700648697\n",
      "Iteration 27950: loss = 0.0023916109399762186\n",
      "Iteration 27951: loss = 0.0023916010099946697\n",
      "Iteration 27952: loss = 0.0023915910801202223\n",
      "Iteration 27953: loss = 0.0023915811503528754\n",
      "Iteration 27954: loss = 0.0023915712206926264\n",
      "Iteration 27955: loss = 0.002391561291139475\n",
      "Iteration 27956: loss = 0.0023915513616934187\n",
      "Iteration 27957: loss = 0.0023915414323544583\n",
      "Iteration 27958: loss = 0.0023915315031225906\n",
      "Iteration 27959: loss = 0.0023915215739978156\n",
      "Iteration 27960: loss = 0.0023915116449801303\n",
      "Iteration 27961: loss = 0.0023915017160695347\n",
      "Iteration 27962: loss = 0.0023914917872660275\n",
      "Iteration 27963: loss = 0.002391481858569607\n",
      "Iteration 27964: loss = 0.002391471929980272\n",
      "Iteration 27965: loss = 0.0023914620014980205\n",
      "Iteration 27966: loss = 0.002391452073122852\n",
      "Iteration 27967: loss = 0.0023914421448547655\n",
      "Iteration 27968: loss = 0.002391432216693758\n",
      "Iteration 27969: loss = 0.00239142228863983\n",
      "Iteration 27970: loss = 0.0023914123606929797\n",
      "Iteration 27971: loss = 0.0023914024328532055\n",
      "Iteration 27972: loss = 0.002391392505120506\n",
      "Iteration 27973: loss = 0.0023913825774948797\n",
      "Iteration 27974: loss = 0.002391372649976326\n",
      "Iteration 27975: loss = 0.002391362722564843\n",
      "Iteration 27976: loss = 0.0023913527952604302\n",
      "Iteration 27977: loss = 0.0023913428680630852\n",
      "Iteration 27978: loss = 0.002391332940972806\n",
      "Iteration 27979: loss = 0.0023913230139895937\n",
      "Iteration 27980: loss = 0.002391313087113445\n",
      "Iteration 27981: loss = 0.00239130316034436\n",
      "Iteration 27982: loss = 0.002391293233682336\n",
      "Iteration 27983: loss = 0.002391283307127373\n",
      "Iteration 27984: loss = 0.0023912733806794677\n",
      "Iteration 27985: loss = 0.0023912634543386212\n",
      "Iteration 27986: loss = 0.0023912535281048306\n",
      "Iteration 27987: loss = 0.002391243601978095\n",
      "Iteration 27988: loss = 0.0023912336759584134\n",
      "Iteration 27989: loss = 0.002391223750045784\n",
      "Iteration 27990: loss = 0.0023912138242402057\n",
      "Iteration 27991: loss = 0.002391203898541677\n",
      "Iteration 27992: loss = 0.0023911939729501968\n",
      "Iteration 27993: loss = 0.0023911840474657633\n",
      "Iteration 27994: loss = 0.002391174122088376\n",
      "Iteration 27995: loss = 0.002391164196818033\n",
      "Iteration 27996: loss = 0.0023911542716547334\n",
      "Iteration 27997: loss = 0.0023911443465984756\n",
      "Iteration 27998: loss = 0.002391134421649258\n",
      "Iteration 27999: loss = 0.00239112449680708\n",
      "Iteration 28000: loss = 0.0023911145720719395\n",
      "Iteration 28001: loss = 0.0023911046474438358\n",
      "Iteration 28002: loss = 0.002391094722922766\n",
      "Iteration 28003: loss = 0.0023910847985087313\n",
      "Iteration 28004: loss = 0.0023910748742017296\n",
      "Iteration 28005: loss = 0.0023910649500017587\n",
      "Iteration 28006: loss = 0.002391055025908817\n",
      "Iteration 28007: loss = 0.0023910451019229046\n",
      "Iteration 28008: loss = 0.0023910351780440193\n",
      "Iteration 28009: loss = 0.0023910252542721604\n",
      "Iteration 28010: loss = 0.002391015330607325\n",
      "Iteration 28011: loss = 0.0023910054070495138\n",
      "Iteration 28012: loss = 0.002390995483598725\n",
      "Iteration 28013: loss = 0.0023909855602549557\n",
      "Iteration 28014: loss = 0.0023909756370182065\n",
      "Iteration 28015: loss = 0.0023909657138884758\n",
      "Iteration 28016: loss = 0.0023909557908657606\n",
      "Iteration 28017: loss = 0.0023909458679500614\n",
      "Iteration 28018: loss = 0.002390935945141376\n",
      "Iteration 28019: loss = 0.002390926022439704\n",
      "Iteration 28020: loss = 0.0023909160998450423\n",
      "Iteration 28021: loss = 0.002390906177357392\n",
      "Iteration 28022: loss = 0.0023908962549767498\n",
      "Iteration 28023: loss = 0.002390886332703115\n",
      "Iteration 28024: loss = 0.0023908764105364864\n",
      "Iteration 28025: loss = 0.002390866488476863\n",
      "Iteration 28026: loss = 0.0023908565665242427\n",
      "Iteration 28027: loss = 0.0023908466446786246\n",
      "Iteration 28028: loss = 0.0023908367229400073\n",
      "Iteration 28029: loss = 0.00239082680130839\n",
      "Iteration 28030: loss = 0.0023908168797837697\n",
      "Iteration 28031: loss = 0.002390806958366148\n",
      "Iteration 28032: loss = 0.0023907970370555206\n",
      "Iteration 28033: loss = 0.0023907871158518877\n",
      "Iteration 28034: loss = 0.0023907771947552486\n",
      "Iteration 28035: loss = 0.0023907672737656003\n",
      "Iteration 28036: loss = 0.0023907573528829423\n",
      "Iteration 28037: loss = 0.002390747432107273\n",
      "Iteration 28038: loss = 0.0023907375114385924\n",
      "Iteration 28039: loss = 0.0023907275908768974\n",
      "Iteration 28040: loss = 0.0023907176704221875\n",
      "Iteration 28041: loss = 0.0023907077500744615\n",
      "Iteration 28042: loss = 0.002390697829833717\n",
      "Iteration 28043: loss = 0.0023906879096999545\n",
      "Iteration 28044: loss = 0.002390677989673171\n",
      "Iteration 28045: loss = 0.002390668069753367\n",
      "Iteration 28046: loss = 0.002390658149940539\n",
      "Iteration 28047: loss = 0.0023906482302346876\n",
      "Iteration 28048: loss = 0.0023906383106358105\n",
      "Iteration 28049: loss = 0.002390628391143906\n",
      "Iteration 28050: loss = 0.002390618471758973\n",
      "Iteration 28051: loss = 0.0023906085524810113\n",
      "Iteration 28052: loss = 0.002390598633310019\n",
      "Iteration 28053: loss = 0.0023905887142459943\n",
      "Iteration 28054: loss = 0.002390578795288936\n",
      "Iteration 28055: loss = 0.002390568876438842\n",
      "Iteration 28056: loss = 0.0023905589576957128\n",
      "Iteration 28057: loss = 0.0023905490390595465\n",
      "Iteration 28058: loss = 0.00239053912053034\n",
      "Iteration 28059: loss = 0.0023905292021080948\n",
      "Iteration 28060: loss = 0.002390519283792808\n",
      "Iteration 28061: loss = 0.002390509365584478\n",
      "Iteration 28062: loss = 0.0023904994474831043\n",
      "Iteration 28063: loss = 0.0023904895294886855\n",
      "Iteration 28064: loss = 0.0023904796116012194\n",
      "Iteration 28065: loss = 0.0023904696938207054\n",
      "Iteration 28066: loss = 0.002390459776147142\n",
      "Iteration 28067: loss = 0.0023904498585805285\n",
      "Iteration 28068: loss = 0.002390439941120863\n",
      "Iteration 28069: loss = 0.002390430023768144\n",
      "Iteration 28070: loss = 0.00239042010652237\n",
      "Iteration 28071: loss = 0.0023904101893835405\n",
      "Iteration 28072: loss = 0.0023904002723516534\n",
      "Iteration 28073: loss = 0.0023903903554267085\n",
      "Iteration 28074: loss = 0.002390380438608703\n",
      "Iteration 28075: loss = 0.0023903705218976364\n",
      "Iteration 28076: loss = 0.002390360605293508\n",
      "Iteration 28077: loss = 0.0023903506887963145\n",
      "Iteration 28078: loss = 0.002390340772406057\n",
      "Iteration 28079: loss = 0.0023903308561227317\n",
      "Iteration 28080: loss = 0.0023903209399463395\n",
      "Iteration 28081: loss = 0.0023903110238768788\n",
      "Iteration 28082: loss = 0.0023903011079143464\n",
      "Iteration 28083: loss = 0.0023902911920587427\n",
      "Iteration 28084: loss = 0.002390281276310066\n",
      "Iteration 28085: loss = 0.002390271360668315\n",
      "Iteration 28086: loss = 0.002390261445133488\n",
      "Iteration 28087: loss = 0.002390251529705584\n",
      "Iteration 28088: loss = 0.0023902416143846013\n",
      "Iteration 28089: loss = 0.0023902316991705396\n",
      "Iteration 28090: loss = 0.0023902217840633968\n",
      "Iteration 28091: loss = 0.0023902118690631714\n",
      "Iteration 28092: loss = 0.0023902019541698627\n",
      "Iteration 28093: loss = 0.002390192039383469\n",
      "Iteration 28094: loss = 0.0023901821247039883\n",
      "Iteration 28095: loss = 0.0023901722101314204\n",
      "Iteration 28096: loss = 0.0023901622956657635\n",
      "Iteration 28097: loss = 0.0023901523813070168\n",
      "Iteration 28098: loss = 0.0023901424670551784\n",
      "Iteration 28099: loss = 0.0023901325529102463\n",
      "Iteration 28100: loss = 0.0023901226388722214\n",
      "Iteration 28101: loss = 0.0023901127249411\n",
      "Iteration 28102: loss = 0.002390102811116882\n",
      "Iteration 28103: loss = 0.0023900928973995656\n",
      "Iteration 28104: loss = 0.00239008298378915\n",
      "Iteration 28105: loss = 0.002390073070285633\n",
      "Iteration 28106: loss = 0.0023900631568890152\n",
      "Iteration 28107: loss = 0.002390053243599293\n",
      "Iteration 28108: loss = 0.002390043330416466\n",
      "Iteration 28109: loss = 0.002390033417340533\n",
      "Iteration 28110: loss = 0.0023900235043714923\n",
      "Iteration 28111: loss = 0.0023900135915093435\n",
      "Iteration 28112: loss = 0.002390003678754085\n",
      "Iteration 28113: loss = 0.002389993766105714\n",
      "Iteration 28114: loss = 0.002389983853564231\n",
      "Iteration 28115: loss = 0.0023899739411296335\n",
      "Iteration 28116: loss = 0.0023899640288019214\n",
      "Iteration 28117: loss = 0.0023899541165810917\n",
      "Iteration 28118: loss = 0.0023899442044671452\n",
      "Iteration 28119: loss = 0.0023899342924600785\n",
      "Iteration 28120: loss = 0.0023899243805598916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28121: loss = 0.0023899144687665823\n",
      "Iteration 28122: loss = 0.0023899045570801507\n",
      "Iteration 28123: loss = 0.002389894645500594\n",
      "Iteration 28124: loss = 0.002389884734027911\n",
      "Iteration 28125: loss = 0.0023898748226621014\n",
      "Iteration 28126: loss = 0.0023898649114031632\n",
      "Iteration 28127: loss = 0.002389855000251095\n",
      "Iteration 28128: loss = 0.0023898450892058955\n",
      "Iteration 28129: loss = 0.0023898351782675638\n",
      "Iteration 28130: loss = 0.0023898252674360984\n",
      "Iteration 28131: loss = 0.002389815356711497\n",
      "Iteration 28132: loss = 0.0023898054460937605\n",
      "Iteration 28133: loss = 0.002389795535582886\n",
      "Iteration 28134: loss = 0.002389785625178871\n",
      "Iteration 28135: loss = 0.0023897757148817174\n",
      "Iteration 28136: loss = 0.002389765804691421\n",
      "Iteration 28137: loss = 0.0023897558946079822\n",
      "Iteration 28138: loss = 0.0023897459846313985\n",
      "Iteration 28139: loss = 0.002389736074761669\n",
      "Iteration 28140: loss = 0.0023897261649987933\n",
      "Iteration 28141: loss = 0.0023897162553427687\n",
      "Iteration 28142: loss = 0.0023897063457935944\n",
      "Iteration 28143: loss = 0.0023896964363512695\n",
      "Iteration 28144: loss = 0.0023896865270157924\n",
      "Iteration 28145: loss = 0.0023896766177871616\n",
      "Iteration 28146: loss = 0.0023896667086653754\n",
      "Iteration 28147: loss = 0.0023896567996504335\n",
      "Iteration 28148: loss = 0.0023896468907423337\n",
      "Iteration 28149: loss = 0.002389636981941075\n",
      "Iteration 28150: loss = 0.002389627073246657\n",
      "Iteration 28151: loss = 0.0023896171646590764\n",
      "Iteration 28152: loss = 0.0023896072561783335\n",
      "Iteration 28153: loss = 0.0023895973478044266\n",
      "Iteration 28154: loss = 0.002389587439537354\n",
      "Iteration 28155: loss = 0.002389577531377114\n",
      "Iteration 28156: loss = 0.002389567623323707\n",
      "Iteration 28157: loss = 0.0023895577153771295\n",
      "Iteration 28158: loss = 0.0023895478075373826\n",
      "Iteration 28159: loss = 0.002389537899804463\n",
      "Iteration 28160: loss = 0.002389527992178369\n",
      "Iteration 28161: loss = 0.0023895180846591015\n",
      "Iteration 28162: loss = 0.0023895081772466577\n",
      "Iteration 28163: loss = 0.002389498269941037\n",
      "Iteration 28164: loss = 0.002389488362742237\n",
      "Iteration 28165: loss = 0.0023894784556502567\n",
      "Iteration 28166: loss = 0.002389468548665096\n",
      "Iteration 28167: loss = 0.0023894586417867523\n",
      "Iteration 28168: loss = 0.002389448735015225\n",
      "Iteration 28169: loss = 0.0023894388283505117\n",
      "Iteration 28170: loss = 0.0023894289217926126\n",
      "Iteration 28171: loss = 0.002389419015341525\n",
      "Iteration 28172: loss = 0.0023894091089972486\n",
      "Iteration 28173: loss = 0.0023893992027597816\n",
      "Iteration 28174: loss = 0.0023893892966291227\n",
      "Iteration 28175: loss = 0.0023893793906052704\n",
      "Iteration 28176: loss = 0.0023893694846882237\n",
      "Iteration 28177: loss = 0.0023893595788779813\n",
      "Iteration 28178: loss = 0.0023893496731745415\n",
      "Iteration 28179: loss = 0.002389339767577904\n",
      "Iteration 28180: loss = 0.0023893298620880667\n",
      "Iteration 28181: loss = 0.0023893199567050273\n",
      "Iteration 28182: loss = 0.002389310051428786\n",
      "Iteration 28183: loss = 0.002389300146259341\n",
      "Iteration 28184: loss = 0.002389290241196691\n",
      "Iteration 28185: loss = 0.002389280336240835\n",
      "Iteration 28186: loss = 0.002389270431391771\n",
      "Iteration 28187: loss = 0.002389260526649498\n",
      "Iteration 28188: loss = 0.0023892506220140144\n",
      "Iteration 28189: loss = 0.00238924071748532\n",
      "Iteration 28190: loss = 0.0023892308130634117\n",
      "Iteration 28191: loss = 0.0023892209087482894\n",
      "Iteration 28192: loss = 0.002389211004539952\n",
      "Iteration 28193: loss = 0.0023892011004383973\n",
      "Iteration 28194: loss = 0.002389191196443624\n",
      "Iteration 28195: loss = 0.0023891812925556323\n",
      "Iteration 28196: loss = 0.0023891713887744187\n",
      "Iteration 28197: loss = 0.002389161485099983\n",
      "Iteration 28198: loss = 0.002389151581532324\n",
      "Iteration 28199: loss = 0.0023891416780714406\n",
      "Iteration 28200: loss = 0.0023891317747173303\n",
      "Iteration 28201: loss = 0.0023891218714699935\n",
      "Iteration 28202: loss = 0.0023891119683294272\n",
      "Iteration 28203: loss = 0.0023891020652956306\n",
      "Iteration 28204: loss = 0.0023890921623686036\n",
      "Iteration 28205: loss = 0.0023890822595483427\n",
      "Iteration 28206: loss = 0.002389072356834848\n",
      "Iteration 28207: loss = 0.0023890624542281186\n",
      "Iteration 28208: loss = 0.002389052551728152\n",
      "Iteration 28209: loss = 0.0023890426493349474\n",
      "Iteration 28210: loss = 0.0023890327470485035\n",
      "Iteration 28211: loss = 0.002389022844868819\n",
      "Iteration 28212: loss = 0.002389012942795893\n",
      "Iteration 28213: loss = 0.002389003040829723\n",
      "Iteration 28214: loss = 0.0023889931389703086\n",
      "Iteration 28215: loss = 0.0023889832372176486\n",
      "Iteration 28216: loss = 0.0023889733355717405\n",
      "Iteration 28217: loss = 0.0023889634340325846\n",
      "Iteration 28218: loss = 0.0023889535326001785\n",
      "Iteration 28219: loss = 0.002388943631274522\n",
      "Iteration 28220: loss = 0.002388933730055612\n",
      "Iteration 28221: loss = 0.002388923828943449\n",
      "Iteration 28222: loss = 0.00238891392793803\n",
      "Iteration 28223: loss = 0.002388904027039355\n",
      "Iteration 28224: loss = 0.0023888941262474226\n",
      "Iteration 28225: loss = 0.0023888842255622308\n",
      "Iteration 28226: loss = 0.0023888743249837773\n",
      "Iteration 28227: loss = 0.002388864424512064\n",
      "Iteration 28228: loss = 0.0023888545241470866\n",
      "Iteration 28229: loss = 0.0023888446238888453\n",
      "Iteration 28230: loss = 0.0023888347237373377\n",
      "Iteration 28231: loss = 0.002388824823692564\n",
      "Iteration 28232: loss = 0.0023888149237545207\n",
      "Iteration 28233: loss = 0.002388805023923209\n",
      "Iteration 28234: loss = 0.0023887951241986258\n",
      "Iteration 28235: loss = 0.0023887852245807703\n",
      "Iteration 28236: loss = 0.002388775325069641\n",
      "Iteration 28237: loss = 0.0023887654256652367\n",
      "Iteration 28238: loss = 0.002388755526367557\n",
      "Iteration 28239: loss = 0.002388745627176599\n",
      "Iteration 28240: loss = 0.0023887357280923623\n",
      "Iteration 28241: loss = 0.0023887258291148458\n",
      "Iteration 28242: loss = 0.0023887159302440467\n",
      "Iteration 28243: loss = 0.0023887060314799653\n",
      "Iteration 28244: loss = 0.0023886961328226\n",
      "Iteration 28245: loss = 0.0023886862342719496\n",
      "Iteration 28246: loss = 0.002388676335828012\n",
      "Iteration 28247: loss = 0.0023886664374907864\n",
      "Iteration 28248: loss = 0.002388656539260271\n",
      "Iteration 28249: loss = 0.0023886466411364654\n",
      "Iteration 28250: loss = 0.0023886367431193677\n",
      "Iteration 28251: loss = 0.0023886268452089758\n",
      "Iteration 28252: loss = 0.00238861694740529\n",
      "Iteration 28253: loss = 0.002388607049708308\n",
      "Iteration 28254: loss = 0.0023885971521180284\n",
      "Iteration 28255: loss = 0.0023885872546344504\n",
      "Iteration 28256: loss = 0.0023885773572575727\n",
      "Iteration 28257: loss = 0.002388567459987393\n",
      "Iteration 28258: loss = 0.002388557562823911\n",
      "Iteration 28259: loss = 0.002388547665767125\n",
      "Iteration 28260: loss = 0.0023885377688170344\n",
      "Iteration 28261: loss = 0.002388527871973637\n",
      "Iteration 28262: loss = 0.0023885179752369313\n",
      "Iteration 28263: loss = 0.0023885080786069162\n",
      "Iteration 28264: loss = 0.002388498182083592\n",
      "Iteration 28265: loss = 0.0023884882856669543\n",
      "Iteration 28266: loss = 0.0023884783893570044\n",
      "Iteration 28267: loss = 0.0023884684931537396\n",
      "Iteration 28268: loss = 0.0023884585970571593\n",
      "Iteration 28269: loss = 0.002388448701067262\n",
      "Iteration 28270: loss = 0.002388438805184046\n",
      "Iteration 28271: loss = 0.00238842890940751\n",
      "Iteration 28272: loss = 0.0023884190137376535\n",
      "Iteration 28273: loss = 0.0023884091181744743\n",
      "Iteration 28274: loss = 0.0023883992227179715\n",
      "Iteration 28275: loss = 0.0023883893273681438\n",
      "Iteration 28276: loss = 0.00238837943212499\n",
      "Iteration 28277: loss = 0.002388369536988508\n",
      "Iteration 28278: loss = 0.002388359641958697\n",
      "Iteration 28279: loss = 0.0023883497470355564\n",
      "Iteration 28280: loss = 0.002388339852219084\n",
      "Iteration 28281: loss = 0.0023883299575092783\n",
      "Iteration 28282: loss = 0.0023883200629061384\n",
      "Iteration 28283: loss = 0.002388310168409663\n",
      "Iteration 28284: loss = 0.002388300274019851\n",
      "Iteration 28285: loss = 0.0023882903797367004\n",
      "Iteration 28286: loss = 0.0023882804855602105\n",
      "Iteration 28287: loss = 0.0023882705914903797\n",
      "Iteration 28288: loss = 0.002388260697527207\n",
      "Iteration 28289: loss = 0.0023882508036706914\n",
      "Iteration 28290: loss = 0.0023882409099208303\n",
      "Iteration 28291: loss = 0.0023882310162776227\n",
      "Iteration 28292: loss = 0.0023882211227410684\n",
      "Iteration 28293: loss = 0.002388211229311165\n",
      "Iteration 28294: loss = 0.0023882013359879115\n",
      "Iteration 28295: loss = 0.002388191442771307\n",
      "Iteration 28296: loss = 0.0023881815496613495\n",
      "Iteration 28297: loss = 0.0023881716566580384\n",
      "Iteration 28298: loss = 0.0023881617637613716\n",
      "Iteration 28299: loss = 0.0023881518709713482\n",
      "Iteration 28300: loss = 0.002388141978287967\n",
      "Iteration 28301: loss = 0.0023881320857112266\n",
      "Iteration 28302: loss = 0.0023881221932411253\n",
      "Iteration 28303: loss = 0.002388112300877662\n",
      "Iteration 28304: loss = 0.0023881024086208364\n",
      "Iteration 28305: loss = 0.0023880925164706454\n",
      "Iteration 28306: loss = 0.0023880826244270887\n",
      "Iteration 28307: loss = 0.002388072732490165\n",
      "Iteration 28308: loss = 0.0023880628406598727\n",
      "Iteration 28309: loss = 0.0023880529489362107\n",
      "Iteration 28310: loss = 0.0023880430573191775\n",
      "Iteration 28311: loss = 0.002388033165808772\n",
      "Iteration 28312: loss = 0.002388023274404992\n",
      "Iteration 28313: loss = 0.002388013383107838\n",
      "Iteration 28314: loss = 0.002388003491917307\n",
      "Iteration 28315: loss = 0.002387993600833399\n",
      "Iteration 28316: loss = 0.002387983709856111\n",
      "Iteration 28317: loss = 0.002387973818985443\n",
      "Iteration 28318: loss = 0.0023879639282213933\n",
      "Iteration 28319: loss = 0.002387954037563961\n",
      "Iteration 28320: loss = 0.0023879441470131438\n",
      "Iteration 28321: loss = 0.0023879342565689414\n",
      "Iteration 28322: loss = 0.0023879243662313517\n",
      "Iteration 28323: loss = 0.002387914476000374\n",
      "Iteration 28324: loss = 0.002387904585876007\n",
      "Iteration 28325: loss = 0.0023878946958582493\n",
      "Iteration 28326: loss = 0.002387884805947099\n",
      "Iteration 28327: loss = 0.0023878749161425545\n",
      "Iteration 28328: loss = 0.002387865026444616\n",
      "Iteration 28329: loss = 0.0023878551368532812\n",
      "Iteration 28330: loss = 0.002387845247368549\n",
      "Iteration 28331: loss = 0.0023878353579904173\n",
      "Iteration 28332: loss = 0.0023878254687188867\n",
      "Iteration 28333: loss = 0.0023878155795539536\n",
      "Iteration 28334: loss = 0.0023878056904956184\n",
      "Iteration 28335: loss = 0.002387795801543879\n",
      "Iteration 28336: loss = 0.0023877859126987346\n",
      "Iteration 28337: loss = 0.0023877760239601827\n",
      "Iteration 28338: loss = 0.002387766135328223\n",
      "Iteration 28339: loss = 0.002387756246802854\n",
      "Iteration 28340: loss = 0.0023877463583840746\n",
      "Iteration 28341: loss = 0.0023877364700718834\n",
      "Iteration 28342: loss = 0.002387726581866279\n",
      "Iteration 28343: loss = 0.00238771669376726\n",
      "Iteration 28344: loss = 0.0023877068057748243\n",
      "Iteration 28345: loss = 0.0023876969178889718\n",
      "Iteration 28346: loss = 0.0023876870301097006\n",
      "Iteration 28347: loss = 0.00238767714243701\n",
      "Iteration 28348: loss = 0.0023876672548708983\n",
      "Iteration 28349: loss = 0.0023876573674113635\n",
      "Iteration 28350: loss = 0.0023876474800584055\n",
      "Iteration 28351: loss = 0.0023876375928120223\n",
      "Iteration 28352: loss = 0.002387627705672212\n",
      "Iteration 28353: loss = 0.0023876178186389746\n",
      "Iteration 28354: loss = 0.002387607931712308\n",
      "Iteration 28355: loss = 0.002387598044892211\n",
      "Iteration 28356: loss = 0.0023875881581786823\n",
      "Iteration 28357: loss = 0.0023875782715717195\n",
      "Iteration 28358: loss = 0.0023875683850713234\n",
      "Iteration 28359: loss = 0.0023875584986774927\n",
      "Iteration 28360: loss = 0.002387548612390223\n",
      "Iteration 28361: loss = 0.002387538726209516\n",
      "Iteration 28362: loss = 0.0023875288401353696\n",
      "Iteration 28363: loss = 0.002387518954167782\n",
      "Iteration 28364: loss = 0.002387509068306752\n",
      "Iteration 28365: loss = 0.0023874991825522784\n",
      "Iteration 28366: loss = 0.00238748929690436\n",
      "Iteration 28367: loss = 0.002387479411362996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28368: loss = 0.002387469525928183\n",
      "Iteration 28369: loss = 0.0023874596405999227\n",
      "Iteration 28370: loss = 0.002387449755378211\n",
      "Iteration 28371: loss = 0.002387439870263049\n",
      "Iteration 28372: loss = 0.0023874299852544335\n",
      "Iteration 28373: loss = 0.0023874201003523643\n",
      "Iteration 28374: loss = 0.0023874102155568393\n",
      "Iteration 28375: loss = 0.002387400330867858\n",
      "Iteration 28376: loss = 0.0023873904462854184\n",
      "Iteration 28377: loss = 0.0023873805618095196\n",
      "Iteration 28378: loss = 0.0023873706774401597\n",
      "Iteration 28379: loss = 0.0023873607931773383\n",
      "Iteration 28380: loss = 0.002387350909021053\n",
      "Iteration 28381: loss = 0.002387341024971304\n",
      "Iteration 28382: loss = 0.0023873311410280884\n",
      "Iteration 28383: loss = 0.0023873212571914057\n",
      "Iteration 28384: loss = 0.0023873113734612546\n",
      "Iteration 28385: loss = 0.0023873014898376332\n",
      "Iteration 28386: loss = 0.0023872916063205404\n",
      "Iteration 28387: loss = 0.0023872817229099756\n",
      "Iteration 28388: loss = 0.0023872718396059367\n",
      "Iteration 28389: loss = 0.002387261956408423\n",
      "Iteration 28390: loss = 0.0023872520733174327\n",
      "Iteration 28391: loss = 0.0023872421903329646\n",
      "Iteration 28392: loss = 0.002387232307455017\n",
      "Iteration 28393: loss = 0.002387222424683589\n",
      "Iteration 28394: loss = 0.0023872125420186796\n",
      "Iteration 28395: loss = 0.0023872026594602873\n",
      "Iteration 28396: loss = 0.0023871927770084105\n",
      "Iteration 28397: loss = 0.002387182894663048\n",
      "Iteration 28398: loss = 0.002387173012424198\n",
      "Iteration 28399: loss = 0.00238716313029186\n",
      "Iteration 28400: loss = 0.0023871532482660327\n",
      "Iteration 28401: loss = 0.002387143366346714\n",
      "Iteration 28402: loss = 0.0023871334845339035\n",
      "Iteration 28403: loss = 0.0023871236028275987\n",
      "Iteration 28404: loss = 0.0023871137212278\n",
      "Iteration 28405: loss = 0.002387103839734504\n",
      "Iteration 28406: loss = 0.002387093958347711\n",
      "Iteration 28407: loss = 0.002387084077067419\n",
      "Iteration 28408: loss = 0.0023870741958936267\n",
      "Iteration 28409: loss = 0.002387064314826333\n",
      "Iteration 28410: loss = 0.002387054433865537\n",
      "Iteration 28411: loss = 0.0023870445530112363\n",
      "Iteration 28412: loss = 0.00238703467226343\n",
      "Iteration 28413: loss = 0.002387024791622118\n",
      "Iteration 28414: loss = 0.002387014911087297\n",
      "Iteration 28415: loss = 0.002387005030658967\n",
      "Iteration 28416: loss = 0.0023869951503371263\n",
      "Iteration 28417: loss = 0.002386985270121773\n",
      "Iteration 28418: loss = 0.002386975390012907\n",
      "Iteration 28419: loss = 0.002386965510010526\n",
      "Iteration 28420: loss = 0.002386955630114629\n",
      "Iteration 28421: loss = 0.002386945750325215\n",
      "Iteration 28422: loss = 0.0023869358706422824\n",
      "Iteration 28423: loss = 0.00238692599106583\n",
      "Iteration 28424: loss = 0.0023869161115958556\n",
      "Iteration 28425: loss = 0.00238690623223236\n",
      "Iteration 28426: loss = 0.002386896352975339\n",
      "Iteration 28427: loss = 0.002386886473824794\n",
      "Iteration 28428: loss = 0.0023868765947807225\n",
      "Iteration 28429: loss = 0.0023868667158431223\n",
      "Iteration 28430: loss = 0.0023868568370119933\n",
      "Iteration 28431: loss = 0.0023868469582873342\n",
      "Iteration 28432: loss = 0.0023868370796691425\n",
      "Iteration 28433: loss = 0.002386827201157419\n",
      "Iteration 28434: loss = 0.0023868173227521603\n",
      "Iteration 28435: loss = 0.0023868074444533664\n",
      "Iteration 28436: loss = 0.0023867975662610354\n",
      "Iteration 28437: loss = 0.0023867876881751653\n",
      "Iteration 28438: loss = 0.002386777810195756\n",
      "Iteration 28439: loss = 0.002386767932322806\n",
      "Iteration 28440: loss = 0.002386758054556314\n",
      "Iteration 28441: loss = 0.0023867481768962777\n",
      "Iteration 28442: loss = 0.002386738299342697\n",
      "Iteration 28443: loss = 0.00238672842189557\n",
      "Iteration 28444: loss = 0.002386718544554895\n",
      "Iteration 28445: loss = 0.0023867086673206713\n",
      "Iteration 28446: loss = 0.0023866987901928973\n",
      "Iteration 28447: loss = 0.002386688913171572\n",
      "Iteration 28448: loss = 0.0023866790362566948\n",
      "Iteration 28449: loss = 0.002386669159448262\n",
      "Iteration 28450: loss = 0.002386659282746274\n",
      "Iteration 28451: loss = 0.00238664940615073\n",
      "Iteration 28452: loss = 0.0023866395296616274\n",
      "Iteration 28453: loss = 0.0023866296532789656\n",
      "Iteration 28454: loss = 0.002386619777002743\n",
      "Iteration 28455: loss = 0.0023866099008329586\n",
      "Iteration 28456: loss = 0.0023866000247696104\n",
      "Iteration 28457: loss = 0.0023865901488126983\n",
      "Iteration 28458: loss = 0.0023865802729622194\n",
      "Iteration 28459: loss = 0.0023865703972181735\n",
      "Iteration 28460: loss = 0.002386560521580559\n",
      "Iteration 28461: loss = 0.002386550646049375\n",
      "Iteration 28462: loss = 0.002386540770624619\n",
      "Iteration 28463: loss = 0.0023865308953062913\n",
      "Iteration 28464: loss = 0.002386521020094389\n",
      "Iteration 28465: loss = 0.002386511144988912\n",
      "Iteration 28466: loss = 0.0023865012699898587\n",
      "Iteration 28467: loss = 0.002386491395097227\n",
      "Iteration 28468: loss = 0.0023864815203110167\n",
      "Iteration 28469: loss = 0.0023864716456312254\n",
      "Iteration 28470: loss = 0.0023864617710578524\n",
      "Iteration 28471: loss = 0.0023864518965908965\n",
      "Iteration 28472: loss = 0.0023864420222303568\n",
      "Iteration 28473: loss = 0.0023864321479762306\n",
      "Iteration 28474: loss = 0.0023864222738285185\n",
      "Iteration 28475: loss = 0.002386412399787217\n",
      "Iteration 28476: loss = 0.0023864025258523262\n",
      "Iteration 28477: loss = 0.0023863926520238444\n",
      "Iteration 28478: loss = 0.0023863827783017705\n",
      "Iteration 28479: loss = 0.0023863729046861024\n",
      "Iteration 28480: loss = 0.00238636303117684\n",
      "Iteration 28481: loss = 0.0023863531577739813\n",
      "Iteration 28482: loss = 0.002386343284477525\n",
      "Iteration 28483: loss = 0.0023863334112874694\n",
      "Iteration 28484: loss = 0.0023863235382038136\n",
      "Iteration 28485: loss = 0.002386313665226557\n",
      "Iteration 28486: loss = 0.0023863037923556978\n",
      "Iteration 28487: loss = 0.0023862939195912337\n",
      "Iteration 28488: loss = 0.002386284046933164\n",
      "Iteration 28489: loss = 0.0023862741743814883\n",
      "Iteration 28490: loss = 0.0023862643019362047\n",
      "Iteration 28491: loss = 0.0023862544295973113\n",
      "Iteration 28492: loss = 0.0023862445573648076\n",
      "Iteration 28493: loss = 0.0023862346852386914\n",
      "Iteration 28494: loss = 0.0023862248132189615\n",
      "Iteration 28495: loss = 0.002386214941305618\n",
      "Iteration 28496: loss = 0.002386205069498658\n",
      "Iteration 28497: loss = 0.0023861951977980807\n",
      "Iteration 28498: loss = 0.0023861853262038845\n",
      "Iteration 28499: loss = 0.0023861754547160694\n",
      "Iteration 28500: loss = 0.002386165583334632\n",
      "Iteration 28501: loss = 0.002386155712059573\n",
      "Iteration 28502: loss = 0.0023861458408908896\n",
      "Iteration 28503: loss = 0.002386135969828581\n",
      "Iteration 28504: loss = 0.0023861260988726463\n",
      "Iteration 28505: loss = 0.0023861162280230832\n",
      "Iteration 28506: loss = 0.0023861063572798913\n",
      "Iteration 28507: loss = 0.0023860964866430695\n",
      "Iteration 28508: loss = 0.0023860866161126157\n",
      "Iteration 28509: loss = 0.0023860767456885283\n",
      "Iteration 28510: loss = 0.002386066875370807\n",
      "Iteration 28511: loss = 0.00238605700515945\n",
      "Iteration 28512: loss = 0.002386047135054456\n",
      "Iteration 28513: loss = 0.002386037265055824\n",
      "Iteration 28514: loss = 0.0023860273951635514\n",
      "Iteration 28515: loss = 0.002386017525377639\n",
      "Iteration 28516: loss = 0.002386007655698084\n",
      "Iteration 28517: loss = 0.0023859977861248848\n",
      "Iteration 28518: loss = 0.0023859879166580416\n",
      "Iteration 28519: loss = 0.0023859780472975513\n",
      "Iteration 28520: loss = 0.002385968178043414\n",
      "Iteration 28521: loss = 0.002385958308895628\n",
      "Iteration 28522: loss = 0.0023859484398541923\n",
      "Iteration 28523: loss = 0.0023859385709191047\n",
      "Iteration 28524: loss = 0.0023859287020903635\n",
      "Iteration 28525: loss = 0.0023859188333679687\n",
      "Iteration 28526: loss = 0.0023859089647519194\n",
      "Iteration 28527: loss = 0.002385899096242212\n",
      "Iteration 28528: loss = 0.002385889227838848\n",
      "Iteration 28529: loss = 0.002385879359541824\n",
      "Iteration 28530: loss = 0.002385869491351139\n",
      "Iteration 28531: loss = 0.002385859623266792\n",
      "Iteration 28532: loss = 0.0023858497552887823\n",
      "Iteration 28533: loss = 0.002385839887417108\n",
      "Iteration 28534: loss = 0.0023858300196517674\n",
      "Iteration 28535: loss = 0.00238582015199276\n",
      "Iteration 28536: loss = 0.0023858102844400834\n",
      "Iteration 28537: loss = 0.0023858004169937377\n",
      "Iteration 28538: loss = 0.00238579054965372\n",
      "Iteration 28539: loss = 0.00238578068242003\n",
      "Iteration 28540: loss = 0.002385770815292667\n",
      "Iteration 28541: loss = 0.0023857609482716277\n",
      "Iteration 28542: loss = 0.002385751081356913\n",
      "Iteration 28543: loss = 0.00238574121454852\n",
      "Iteration 28544: loss = 0.0023857313478464477\n",
      "Iteration 28545: loss = 0.002385721481250696\n",
      "Iteration 28546: loss = 0.0023857116147612614\n",
      "Iteration 28547: loss = 0.0023857017483781447\n",
      "Iteration 28548: loss = 0.002385691882101343\n",
      "Iteration 28549: loss = 0.0023856820159308565\n",
      "Iteration 28550: loss = 0.002385672149866682\n",
      "Iteration 28551: loss = 0.0023856622839088203\n",
      "Iteration 28552: loss = 0.0023856524180572683\n",
      "Iteration 28553: loss = 0.0023856425523120254\n",
      "Iteration 28554: loss = 0.0023856326866730907\n",
      "Iteration 28555: loss = 0.002385622821140462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28556: loss = 0.0023856129557141394\n",
      "Iteration 28557: loss = 0.0023856030903941197\n",
      "Iteration 28558: loss = 0.0023855932251804026\n",
      "Iteration 28559: loss = 0.002385583360072987\n",
      "Iteration 28560: loss = 0.0023855734950718717\n",
      "Iteration 28561: loss = 0.002385563630177054\n",
      "Iteration 28562: loss = 0.002385553765388534\n",
      "Iteration 28563: loss = 0.0023855439007063103\n",
      "Iteration 28564: loss = 0.0023855340361303816\n",
      "Iteration 28565: loss = 0.002385524171660745\n",
      "Iteration 28566: loss = 0.0023855143072974015\n",
      "Iteration 28567: loss = 0.0023855044430403484\n",
      "Iteration 28568: loss = 0.002385494578889585\n",
      "Iteration 28569: loss = 0.0023854847148451087\n",
      "Iteration 28570: loss = 0.00238547485090692\n",
      "Iteration 28571: loss = 0.0023854649870750162\n",
      "Iteration 28572: loss = 0.0023854551233493965\n",
      "Iteration 28573: loss = 0.0023854452597300603\n",
      "Iteration 28574: loss = 0.0023854353962170045\n",
      "Iteration 28575: loss = 0.00238542553281023\n",
      "Iteration 28576: loss = 0.002385415669509734\n",
      "Iteration 28577: loss = 0.002385405806315516\n",
      "Iteration 28578: loss = 0.0023853959432275735\n",
      "Iteration 28579: loss = 0.0023853860802459065\n",
      "Iteration 28580: loss = 0.0023853762173705125\n",
      "Iteration 28581: loss = 0.002385366354601391\n",
      "Iteration 28582: loss = 0.002385356491938541\n",
      "Iteration 28583: loss = 0.0023853466293819606\n",
      "Iteration 28584: loss = 0.0023853367669316485\n",
      "Iteration 28585: loss = 0.0023853269045876033\n",
      "Iteration 28586: loss = 0.0023853170423498234\n",
      "Iteration 28587: loss = 0.002385307180218308\n",
      "Iteration 28588: loss = 0.0023852973181930563\n",
      "Iteration 28589: loss = 0.002385287456274066\n",
      "Iteration 28590: loss = 0.0023852775944613364\n",
      "Iteration 28591: loss = 0.002385267732754866\n",
      "Iteration 28592: loss = 0.002385257871154653\n",
      "Iteration 28593: loss = 0.0023852480096606967\n",
      "Iteration 28594: loss = 0.002385238148272996\n",
      "Iteration 28595: loss = 0.002385228286991549\n",
      "Iteration 28596: loss = 0.0023852184258163545\n",
      "Iteration 28597: loss = 0.0023852085647474114\n",
      "Iteration 28598: loss = 0.0023851987037847183\n",
      "Iteration 28599: loss = 0.0023851888429282735\n",
      "Iteration 28600: loss = 0.0023851789821780767\n",
      "Iteration 28601: loss = 0.0023851691215341256\n",
      "Iteration 28602: loss = 0.002385159260996419\n",
      "Iteration 28603: loss = 0.002385149400564956\n",
      "Iteration 28604: loss = 0.002385139540239735\n",
      "Iteration 28605: loss = 0.0023851296800207547\n",
      "Iteration 28606: loss = 0.002385119819908014\n",
      "Iteration 28607: loss = 0.0023851099599015114\n",
      "Iteration 28608: loss = 0.002385100100001246\n",
      "Iteration 28609: loss = 0.0023850902402072155\n",
      "Iteration 28610: loss = 0.0023850803805194197\n",
      "Iteration 28611: loss = 0.002385070520937856\n",
      "Iteration 28612: loss = 0.002385060661462525\n",
      "Iteration 28613: loss = 0.0023850508020934235\n",
      "Iteration 28614: loss = 0.0023850409428305508\n",
      "Iteration 28615: loss = 0.002385031083673906\n",
      "Iteration 28616: loss = 0.002385021224623488\n",
      "Iteration 28617: loss = 0.002385011365679295\n",
      "Iteration 28618: loss = 0.002385001506841325\n",
      "Iteration 28619: loss = 0.0023849916481095772\n",
      "Iteration 28620: loss = 0.002384981789484051\n",
      "Iteration 28621: loss = 0.002384971930964744\n",
      "Iteration 28622: loss = 0.0023849620725516565\n",
      "Iteration 28623: loss = 0.0023849522142447856\n",
      "Iteration 28624: loss = 0.0023849423560441305\n",
      "Iteration 28625: loss = 0.0023849324979496895\n",
      "Iteration 28626: loss = 0.002384922639961462\n",
      "Iteration 28627: loss = 0.0023849127820794462\n",
      "Iteration 28628: loss = 0.0023849029243036414\n",
      "Iteration 28629: loss = 0.002384893066634046\n",
      "Iteration 28630: loss = 0.002384883209070658\n",
      "Iteration 28631: loss = 0.002384873351613476\n",
      "Iteration 28632: loss = 0.002384863494262501\n",
      "Iteration 28633: loss = 0.0023848536370177286\n",
      "Iteration 28634: loss = 0.0023848437798791593\n",
      "Iteration 28635: loss = 0.002384833922846791\n",
      "Iteration 28636: loss = 0.0023848240659206234\n",
      "Iteration 28637: loss = 0.0023848142091006546\n",
      "Iteration 28638: loss = 0.0023848043523868826\n",
      "Iteration 28639: loss = 0.0023847944957793073\n",
      "Iteration 28640: loss = 0.002384784639277926\n",
      "Iteration 28641: loss = 0.0023847747828827388\n",
      "Iteration 28642: loss = 0.0023847649265937437\n",
      "Iteration 28643: loss = 0.0023847550704109394\n",
      "Iteration 28644: loss = 0.0023847452143343248\n",
      "Iteration 28645: loss = 0.002384735358363898\n",
      "Iteration 28646: loss = 0.002384725502499658\n",
      "Iteration 28647: loss = 0.0023847156467416038\n",
      "Iteration 28648: loss = 0.0023847057910897345\n",
      "Iteration 28649: loss = 0.0023846959355440477\n",
      "Iteration 28650: loss = 0.0023846860801045424\n",
      "Iteration 28651: loss = 0.0023846762247712173\n",
      "Iteration 28652: loss = 0.0023846663695440717\n",
      "Iteration 28653: loss = 0.0023846565144231037\n",
      "Iteration 28654: loss = 0.0023846466594083125\n",
      "Iteration 28655: loss = 0.002384636804499696\n",
      "Iteration 28656: loss = 0.002384626949697253\n",
      "Iteration 28657: loss = 0.0023846170950009827\n",
      "Iteration 28658: loss = 0.0023846072404108834\n",
      "Iteration 28659: loss = 0.002384597385926954\n",
      "Iteration 28660: loss = 0.002384587531549193\n",
      "Iteration 28661: loss = 0.0023845776772776\n",
      "Iteration 28662: loss = 0.0023845678231121724\n",
      "Iteration 28663: loss = 0.0023845579690529094\n",
      "Iteration 28664: loss = 0.0023845481150998097\n",
      "Iteration 28665: loss = 0.002384538261252872\n",
      "Iteration 28666: loss = 0.0023845284075120943\n",
      "Iteration 28667: loss = 0.0023845185538774764\n",
      "Iteration 28668: loss = 0.0023845087003490167\n",
      "Iteration 28669: loss = 0.0023844988469267137\n",
      "Iteration 28670: loss = 0.002384488993610566\n",
      "Iteration 28671: loss = 0.0023844791404005724\n",
      "Iteration 28672: loss = 0.0023844692872967317\n",
      "Iteration 28673: loss = 0.002384459434299042\n",
      "Iteration 28674: loss = 0.002384449581407503\n",
      "Iteration 28675: loss = 0.0023844397286221126\n",
      "Iteration 28676: loss = 0.00238442987594287\n",
      "Iteration 28677: loss = 0.0023844200233697735\n",
      "Iteration 28678: loss = 0.002384410170902822\n",
      "Iteration 28679: loss = 0.002384400318542013\n",
      "Iteration 28680: loss = 0.002384390466287347\n",
      "Iteration 28681: loss = 0.002384380614138822\n",
      "Iteration 28682: loss = 0.0023843707620964374\n",
      "Iteration 28683: loss = 0.0023843609101601897\n",
      "Iteration 28684: loss = 0.0023843510583300797\n",
      "Iteration 28685: loss = 0.0023843412066061054\n",
      "Iteration 28686: loss = 0.0023843313549882657\n",
      "Iteration 28687: loss = 0.002384321503476559\n",
      "Iteration 28688: loss = 0.002384311652070984\n",
      "Iteration 28689: loss = 0.002384301800771539\n",
      "Iteration 28690: loss = 0.002384291949578224\n",
      "Iteration 28691: loss = 0.002384282098491036\n",
      "Iteration 28692: loss = 0.0023842722475099745\n",
      "Iteration 28693: loss = 0.002384262396635039\n",
      "Iteration 28694: loss = 0.0023842525458662267\n",
      "Iteration 28695: loss = 0.002384242695203537\n",
      "Iteration 28696: loss = 0.002384232844646969\n",
      "Iteration 28697: loss = 0.002384222994196521\n",
      "Iteration 28698: loss = 0.002384213143852191\n",
      "Iteration 28699: loss = 0.002384203293613979\n",
      "Iteration 28700: loss = 0.0023841934434818825\n",
      "Iteration 28701: loss = 0.002384183593455901\n",
      "Iteration 28702: loss = 0.002384173743536033\n",
      "Iteration 28703: loss = 0.002384163893722277\n",
      "Iteration 28704: loss = 0.0023841540440146312\n",
      "Iteration 28705: loss = 0.0023841441944130954\n",
      "Iteration 28706: loss = 0.0023841343449176678\n",
      "Iteration 28707: loss = 0.0023841244955283467\n",
      "Iteration 28708: loss = 0.0023841146462451317\n",
      "Iteration 28709: loss = 0.0023841047970680203\n",
      "Iteration 28710: loss = 0.002384094947997012\n",
      "Iteration 28711: loss = 0.0023840850990321048\n",
      "Iteration 28712: loss = 0.002384075250173298\n",
      "Iteration 28713: loss = 0.0023840654014205907\n",
      "Iteration 28714: loss = 0.002384055552773981\n",
      "Iteration 28715: loss = 0.002384045704233467\n",
      "Iteration 28716: loss = 0.0023840358557990484\n",
      "Iteration 28717: loss = 0.0023840260074707237\n",
      "Iteration 28718: loss = 0.0023840161592484908\n",
      "Iteration 28719: loss = 0.0023840063111323496\n",
      "Iteration 28720: loss = 0.002383996463122298\n",
      "Iteration 28721: loss = 0.0023839866152183344\n",
      "Iteration 28722: loss = 0.0023839767674204582\n",
      "Iteration 28723: loss = 0.0023839669197286686\n",
      "Iteration 28724: loss = 0.0023839570721429625\n",
      "Iteration 28725: loss = 0.0023839472246633396\n",
      "Iteration 28726: loss = 0.002383937377289799\n",
      "Iteration 28727: loss = 0.002383927530022339\n",
      "Iteration 28728: loss = 0.0023839176828609576\n",
      "Iteration 28729: loss = 0.0023839078358056554\n",
      "Iteration 28730: loss = 0.0023838979888564284\n",
      "Iteration 28731: loss = 0.0023838881420132777\n",
      "Iteration 28732: loss = 0.002383878295276201\n",
      "Iteration 28733: loss = 0.0023838684486451964\n",
      "Iteration 28734: loss = 0.002383858602120264\n",
      "Iteration 28735: loss = 0.0023838487557014012\n",
      "Iteration 28736: loss = 0.002383838909388607\n",
      "Iteration 28737: loss = 0.0023838290631818804\n",
      "Iteration 28738: loss = 0.00238381921708122\n",
      "Iteration 28739: loss = 0.0023838093710866243\n",
      "Iteration 28740: loss = 0.002383799525198092\n",
      "Iteration 28741: loss = 0.002383789679415622\n",
      "Iteration 28742: loss = 0.0023837798337392135\n",
      "Iteration 28743: loss = 0.002383769988168864\n",
      "Iteration 28744: loss = 0.0023837601427045725\n",
      "Iteration 28745: loss = 0.0023837502973463385\n",
      "Iteration 28746: loss = 0.0023837404520941595\n",
      "Iteration 28747: loss = 0.002383730606948036\n",
      "Iteration 28748: loss = 0.0023837207619079644\n",
      "Iteration 28749: loss = 0.002383710916973945\n",
      "Iteration 28750: loss = 0.0023837010721459756\n",
      "Iteration 28751: loss = 0.0023836912274240556\n",
      "Iteration 28752: loss = 0.0023836813828081835\n",
      "Iteration 28753: loss = 0.0023836715382983572\n",
      "Iteration 28754: loss = 0.0023836616938945772\n",
      "Iteration 28755: loss = 0.00238365184959684\n",
      "Iteration 28756: loss = 0.0023836420054051456\n",
      "Iteration 28757: loss = 0.002383632161319492\n",
      "Iteration 28758: loss = 0.0023836223173398794\n",
      "Iteration 28759: loss = 0.0023836124734663046\n",
      "Iteration 28760: loss = 0.0023836026296987674\n",
      "Iteration 28761: loss = 0.0023835927860372656\n",
      "Iteration 28762: loss = 0.0023835829424817987\n",
      "Iteration 28763: loss = 0.0023835730990323656\n",
      "Iteration 28764: loss = 0.0023835632556889644\n",
      "Iteration 28765: loss = 0.0023835534124515933\n",
      "Iteration 28766: loss = 0.002383543569320252\n",
      "Iteration 28767: loss = 0.0023835337262949393\n",
      "Iteration 28768: loss = 0.002383523883375653\n",
      "Iteration 28769: loss = 0.0023835140405623913\n",
      "Iteration 28770: loss = 0.002383504197855155\n",
      "Iteration 28771: loss = 0.0023834943552539403\n",
      "Iteration 28772: loss = 0.0023834845127587486\n",
      "Iteration 28773: loss = 0.0023834746703695763\n",
      "Iteration 28774: loss = 0.0023834648280864233\n",
      "Iteration 28775: loss = 0.0023834549859092875\n",
      "Iteration 28776: loss = 0.0023834451438381676\n",
      "Iteration 28777: loss = 0.002383435301873063\n",
      "Iteration 28778: loss = 0.002383425460013972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28779: loss = 0.0023834156182608937\n",
      "Iteration 28780: loss = 0.002383405776613827\n",
      "Iteration 28781: loss = 0.0023833959350727687\n",
      "Iteration 28782: loss = 0.00238338609363772\n",
      "Iteration 28783: loss = 0.0023833762523086774\n",
      "Iteration 28784: loss = 0.002383366411085641\n",
      "Iteration 28785: loss = 0.002383356569968609\n",
      "Iteration 28786: loss = 0.0023833467289575802\n",
      "Iteration 28787: loss = 0.002383336888052553\n",
      "Iteration 28788: loss = 0.002383327047253527\n",
      "Iteration 28789: loss = 0.0023833172065605003\n",
      "Iteration 28790: loss = 0.00238330736597347\n",
      "Iteration 28791: loss = 0.0023832975254924386\n",
      "Iteration 28792: loss = 0.002383287685117401\n",
      "Iteration 28793: loss = 0.0023832778448483576\n",
      "Iteration 28794: loss = 0.0023832680046853073\n",
      "Iteration 28795: loss = 0.0023832581646282473\n",
      "Iteration 28796: loss = 0.002383248324677178\n",
      "Iteration 28797: loss = 0.002383238484832098\n",
      "Iteration 28798: loss = 0.002383228645093004\n",
      "Iteration 28799: loss = 0.0023832188054598974\n",
      "Iteration 28800: loss = 0.0023832089659327755\n",
      "Iteration 28801: loss = 0.0023831991265116366\n",
      "Iteration 28802: loss = 0.00238318928719648\n",
      "Iteration 28803: loss = 0.002383179447987305\n",
      "Iteration 28804: loss = 0.0023831696088841086\n",
      "Iteration 28805: loss = 0.002383159769886891\n",
      "Iteration 28806: loss = 0.0023831499309956497\n",
      "Iteration 28807: loss = 0.0023831400922103845\n",
      "Iteration 28808: loss = 0.0023831302535310935\n",
      "Iteration 28809: loss = 0.002383120414957776\n",
      "Iteration 28810: loss = 0.002383110576490429\n",
      "Iteration 28811: loss = 0.0023831007381290535\n",
      "Iteration 28812: loss = 0.002383090899873647\n",
      "Iteration 28813: loss = 0.0023830810617242078\n",
      "Iteration 28814: loss = 0.002383071223680735\n",
      "Iteration 28815: loss = 0.002383061385743228\n",
      "Iteration 28816: loss = 0.002383051547911684\n",
      "Iteration 28817: loss = 0.002383041710186103\n",
      "Iteration 28818: loss = 0.0023830318725664825\n",
      "Iteration 28819: loss = 0.002383022035052823\n",
      "Iteration 28820: loss = 0.0023830121976451215\n",
      "Iteration 28821: loss = 0.002383002360343377\n",
      "Iteration 28822: loss = 0.002382992523147589\n",
      "Iteration 28823: loss = 0.0023829826860577554\n",
      "Iteration 28824: loss = 0.002382972849073875\n",
      "Iteration 28825: loss = 0.002382963012195947\n",
      "Iteration 28826: loss = 0.0023829531754239693\n",
      "Iteration 28827: loss = 0.0023829433387579407\n",
      "Iteration 28828: loss = 0.002382933502197861\n",
      "Iteration 28829: loss = 0.002382923665743728\n",
      "Iteration 28830: loss = 0.0023829138293955396\n",
      "Iteration 28831: loss = 0.0023829039931532957\n",
      "Iteration 28832: loss = 0.0023828941570169953\n",
      "Iteration 28833: loss = 0.002382884320986636\n",
      "Iteration 28834: loss = 0.0023828744850622166\n",
      "Iteration 28835: loss = 0.0023828646492437366\n",
      "Iteration 28836: loss = 0.002382854813531194\n",
      "Iteration 28837: loss = 0.0023828449779245875\n",
      "Iteration 28838: loss = 0.0023828351424239167\n",
      "Iteration 28839: loss = 0.002382825307029179\n",
      "Iteration 28840: loss = 0.0023828154717403733\n",
      "Iteration 28841: loss = 0.0023828056365574994\n",
      "Iteration 28842: loss = 0.0023827958014805547\n",
      "Iteration 28843: loss = 0.0023827859665095383\n",
      "Iteration 28844: loss = 0.0023827761316444498\n",
      "Iteration 28845: loss = 0.002382766296885286\n",
      "Iteration 28846: loss = 0.0023827564622320475\n",
      "Iteration 28847: loss = 0.0023827466276847326\n",
      "Iteration 28848: loss = 0.002382736793243339\n",
      "Iteration 28849: loss = 0.0023827269589078654\n",
      "Iteration 28850: loss = 0.0023827171246783114\n",
      "Iteration 28851: loss = 0.0023827072905546758\n",
      "Iteration 28852: loss = 0.0023826974565369567\n",
      "Iteration 28853: loss = 0.002382687622625152\n",
      "Iteration 28854: loss = 0.0023826777888192622\n",
      "Iteration 28855: loss = 0.002382667955119285\n",
      "Iteration 28856: loss = 0.0023826581215252193\n",
      "Iteration 28857: loss = 0.0023826482880370636\n",
      "Iteration 28858: loss = 0.0023826384546548163\n",
      "Iteration 28859: loss = 0.002382628621378477\n",
      "Iteration 28860: loss = 0.002382618788208043\n",
      "Iteration 28861: loss = 0.0023826089551435147\n",
      "Iteration 28862: loss = 0.002382599122184889\n",
      "Iteration 28863: loss = 0.002382589289332166\n",
      "Iteration 28864: loss = 0.0023825794565853443\n",
      "Iteration 28865: loss = 0.0023825696239444215\n",
      "Iteration 28866: loss = 0.0023825597914093975\n",
      "Iteration 28867: loss = 0.00238254995898027\n",
      "Iteration 28868: loss = 0.002382540126657039\n",
      "Iteration 28869: loss = 0.0023825302944397012\n",
      "Iteration 28870: loss = 0.0023825204623282572\n",
      "Iteration 28871: loss = 0.0023825106303227046\n",
      "Iteration 28872: loss = 0.002382500798423043\n",
      "Iteration 28873: loss = 0.00238249096662927\n",
      "Iteration 28874: loss = 0.0023824811349413846\n",
      "Iteration 28875: loss = 0.0023824713033593855\n",
      "Iteration 28876: loss = 0.002382461471883272\n",
      "Iteration 28877: loss = 0.0023824516405130426\n",
      "Iteration 28878: loss = 0.0023824418092486957\n",
      "Iteration 28879: loss = 0.0023824319780902294\n",
      "Iteration 28880: loss = 0.0023824221470376433\n",
      "Iteration 28881: loss = 0.002382412316090936\n",
      "Iteration 28882: loss = 0.0023824024852501058\n",
      "Iteration 28883: loss = 0.002382392654515152\n",
      "Iteration 28884: loss = 0.002382382823886072\n",
      "Iteration 28885: loss = 0.002382372993362866\n",
      "Iteration 28886: loss = 0.0023823631629455327\n",
      "Iteration 28887: loss = 0.0023823533326340694\n",
      "Iteration 28888: loss = 0.0023823435024284754\n",
      "Iteration 28889: loss = 0.0023823336723287495\n",
      "Iteration 28890: loss = 0.002382323842334891\n",
      "Iteration 28891: loss = 0.0023823140124468976\n",
      "Iteration 28892: loss = 0.0023823041826647683\n",
      "Iteration 28893: loss = 0.0023822943529885022\n",
      "Iteration 28894: loss = 0.002382284523418097\n",
      "Iteration 28895: loss = 0.002382274693953553\n",
      "Iteration 28896: loss = 0.0023822648645948677\n",
      "Iteration 28897: loss = 0.00238225503534204\n",
      "Iteration 28898: loss = 0.0023822452061950682\n",
      "Iteration 28899: loss = 0.0023822353771539516\n",
      "Iteration 28900: loss = 0.0023822255482186896\n",
      "Iteration 28901: loss = 0.002382215719389279\n",
      "Iteration 28902: loss = 0.0023822058906657196\n",
      "Iteration 28903: loss = 0.0023821960620480104\n",
      "Iteration 28904: loss = 0.002382186233536149\n",
      "Iteration 28905: loss = 0.0023821764051301353\n",
      "Iteration 28906: loss = 0.0023821665768299677\n",
      "Iteration 28907: loss = 0.0023821567486356447\n",
      "Iteration 28908: loss = 0.0023821469205471645\n",
      "Iteration 28909: loss = 0.0023821370925645262\n",
      "Iteration 28910: loss = 0.0023821272646877283\n",
      "Iteration 28911: loss = 0.00238211743691677\n",
      "Iteration 28912: loss = 0.00238210760925165\n",
      "Iteration 28913: loss = 0.0023820977816923663\n",
      "Iteration 28914: loss = 0.002382087954238918\n",
      "Iteration 28915: loss = 0.002382078126891304\n",
      "Iteration 28916: loss = 0.002382068299649523\n",
      "Iteration 28917: loss = 0.0023820584725135724\n",
      "Iteration 28918: loss = 0.002382048645483453\n",
      "Iteration 28919: loss = 0.002382038818559162\n",
      "Iteration 28920: loss = 0.002382028991740698\n",
      "Iteration 28921: loss = 0.002382019165028061\n",
      "Iteration 28922: loss = 0.0023820093384212485\n",
      "Iteration 28923: loss = 0.0023819995119202593\n",
      "Iteration 28924: loss = 0.002381989685525093\n",
      "Iteration 28925: loss = 0.002381979859235747\n",
      "Iteration 28926: loss = 0.002381970033052222\n",
      "Iteration 28927: loss = 0.002381960206974514\n",
      "Iteration 28928: loss = 0.0023819503810026234\n",
      "Iteration 28929: loss = 0.002381940555136548\n",
      "Iteration 28930: loss = 0.002381930729376288\n",
      "Iteration 28931: loss = 0.002381920903721841\n",
      "Iteration 28932: loss = 0.0023819110781732053\n",
      "Iteration 28933: loss = 0.0023819012527303805\n",
      "Iteration 28934: loss = 0.0023818914273933643\n",
      "Iteration 28935: loss = 0.0023818816021621563\n",
      "Iteration 28936: loss = 0.0023818717770367546\n",
      "Iteration 28937: loss = 0.002381861952017159\n",
      "Iteration 28938: loss = 0.0023818521271033667\n",
      "Iteration 28939: loss = 0.002381842302295377\n",
      "Iteration 28940: loss = 0.002381832477593188\n",
      "Iteration 28941: loss = 0.0023818226529968003\n",
      "Iteration 28942: loss = 0.0023818128285062108\n",
      "Iteration 28943: loss = 0.002381803004121418\n",
      "Iteration 28944: loss = 0.0023817931798424223\n",
      "Iteration 28945: loss = 0.0023817833556692212\n",
      "Iteration 28946: loss = 0.002381773531601813\n",
      "Iteration 28947: loss = 0.002381763707640197\n",
      "Iteration 28948: loss = 0.002381753883784372\n",
      "Iteration 28949: loss = 0.0023817440600343368\n",
      "Iteration 28950: loss = 0.00238173423639009\n",
      "Iteration 28951: loss = 0.0023817244128516296\n",
      "Iteration 28952: loss = 0.0023817145894189547\n",
      "Iteration 28953: loss = 0.0023817047660920645\n",
      "Iteration 28954: loss = 0.002381694942870957\n",
      "Iteration 28955: loss = 0.0023816851197556313\n",
      "Iteration 28956: loss = 0.002381675296746086\n",
      "Iteration 28957: loss = 0.0023816654738423194\n",
      "Iteration 28958: loss = 0.0023816556510443307\n",
      "Iteration 28959: loss = 0.0023816458283521186\n",
      "Iteration 28960: loss = 0.0023816360057656817\n",
      "Iteration 28961: loss = 0.002381626183285018\n",
      "Iteration 28962: loss = 0.0023816163609101273\n",
      "Iteration 28963: loss = 0.002381606538641008\n",
      "Iteration 28964: loss = 0.0023815967164776586\n",
      "Iteration 28965: loss = 0.0023815868944200777\n",
      "Iteration 28966: loss = 0.002381577072468264\n",
      "Iteration 28967: loss = 0.002381567250622216\n",
      "Iteration 28968: loss = 0.002381557428881933\n",
      "Iteration 28969: loss = 0.002381547607247413\n",
      "Iteration 28970: loss = 0.0023815377857186555\n",
      "Iteration 28971: loss = 0.002381527964295658\n",
      "Iteration 28972: loss = 0.0023815181429784203\n",
      "Iteration 28973: loss = 0.002381508321766941\n",
      "Iteration 28974: loss = 0.002381498500661218\n",
      "Iteration 28975: loss = 0.0023814886796612506\n",
      "Iteration 28976: loss = 0.0023814788587670373\n",
      "Iteration 28977: loss = 0.0023814690379785775\n",
      "Iteration 28978: loss = 0.0023814592172958686\n",
      "Iteration 28979: loss = 0.00238144939671891\n",
      "Iteration 28980: loss = 0.0023814395762477005\n",
      "Iteration 28981: loss = 0.0023814297558822383\n",
      "Iteration 28982: loss = 0.002381419935622523\n",
      "Iteration 28983: loss = 0.0023814101154685527\n",
      "Iteration 28984: loss = 0.0023814002954203255\n",
      "Iteration 28985: loss = 0.0023813904754778404\n",
      "Iteration 28986: loss = 0.002381380655641097\n",
      "Iteration 28987: loss = 0.002381370835910094\n",
      "Iteration 28988: loss = 0.0023813610162848283\n",
      "Iteration 28989: loss = 0.0023813511967653005\n",
      "Iteration 28990: loss = 0.0023813413773515085\n",
      "Iteration 28991: loss = 0.002381331558043451\n",
      "Iteration 28992: loss = 0.0023813217388411263\n",
      "Iteration 28993: loss = 0.0023813119197445337\n",
      "Iteration 28994: loss = 0.002381302100753672\n",
      "Iteration 28995: loss = 0.002381292281868539\n",
      "Iteration 28996: loss = 0.002381282463089135\n",
      "Iteration 28997: loss = 0.002381272644415456\n",
      "Iteration 28998: loss = 0.0023812628258475035\n",
      "Iteration 28999: loss = 0.0023812530073852754\n",
      "Iteration 29000: loss = 0.0023812431890287695\n",
      "Iteration 29001: loss = 0.0023812333707779855\n",
      "Iteration 29002: loss = 0.002381223552632921\n",
      "Iteration 29003: loss = 0.0023812137345935757\n",
      "Iteration 29004: loss = 0.0023812039166599477\n",
      "Iteration 29005: loss = 0.002381194098832036\n",
      "Iteration 29006: loss = 0.0023811842811098396\n",
      "Iteration 29007: loss = 0.0023811744634933563\n",
      "Iteration 29008: loss = 0.0023811646459825854\n",
      "Iteration 29009: loss = 0.002381154828577525\n",
      "Iteration 29010: loss = 0.002381145011278175\n",
      "Iteration 29011: loss = 0.0023811351940845327\n",
      "Iteration 29012: loss = 0.0023811253769965984\n",
      "Iteration 29013: loss = 0.002381115560014369\n",
      "Iteration 29014: loss = 0.0023811057431378443\n",
      "Iteration 29015: loss = 0.0023810959263670223\n",
      "Iteration 29016: loss = 0.0023810861097019026\n",
      "Iteration 29017: loss = 0.0023810762931424836\n",
      "Iteration 29018: loss = 0.0023810664766887634\n",
      "Iteration 29019: loss = 0.002381056660340741\n",
      "Iteration 29020: loss = 0.0023810468440984157\n",
      "Iteration 29021: loss = 0.002381037027961785\n",
      "Iteration 29022: loss = 0.0023810272119308487\n",
      "Iteration 29023: loss = 0.002381017396005605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29024: loss = 0.0023810075801860525\n",
      "Iteration 29025: loss = 0.00238099776447219\n",
      "Iteration 29026: loss = 0.002380987948864016\n",
      "Iteration 29027: loss = 0.00238097813336153\n",
      "Iteration 29028: loss = 0.00238096831796473\n",
      "Iteration 29029: loss = 0.0023809585026736144\n",
      "Iteration 29030: loss = 0.0023809486874881826\n",
      "Iteration 29031: loss = 0.002380938872408433\n",
      "Iteration 29032: loss = 0.002380929057434364\n",
      "Iteration 29033: loss = 0.002380919242565975\n",
      "Iteration 29034: loss = 0.002380909427803264\n",
      "Iteration 29035: loss = 0.0023808996131462297\n",
      "Iteration 29036: loss = 0.0023808897985948705\n",
      "Iteration 29037: loss = 0.002380879984149187\n",
      "Iteration 29038: loss = 0.002380870169809176\n",
      "Iteration 29039: loss = 0.0023808603555748368\n",
      "Iteration 29040: loss = 0.0023808505414461673\n",
      "Iteration 29041: loss = 0.0023808407274231672\n",
      "Iteration 29042: loss = 0.0023808309135058352\n",
      "Iteration 29043: loss = 0.0023808210996941696\n",
      "Iteration 29044: loss = 0.002380811285988169\n",
      "Iteration 29045: loss = 0.0023808014723878325\n",
      "Iteration 29046: loss = 0.002380791658893159\n",
      "Iteration 29047: loss = 0.0023807818455041456\n",
      "Iteration 29048: loss = 0.0023807720322207926\n",
      "Iteration 29049: loss = 0.002380762219043098\n",
      "Iteration 29050: loss = 0.0023807524059710613\n",
      "Iteration 29051: loss = 0.0023807425930046804\n",
      "Iteration 29052: loss = 0.0023807327801439538\n",
      "Iteration 29053: loss = 0.0023807229673888813\n",
      "Iteration 29054: loss = 0.0023807131547394604\n",
      "Iteration 29055: loss = 0.00238070334219569\n",
      "Iteration 29056: loss = 0.0023806935297575695\n",
      "Iteration 29057: loss = 0.0023806837174250974\n",
      "Iteration 29058: loss = 0.0023806739051982716\n",
      "Iteration 29059: loss = 0.0023806640930770914\n",
      "Iteration 29060: loss = 0.0023806542810615554\n",
      "Iteration 29061: loss = 0.0023806444691516627\n",
      "Iteration 29062: loss = 0.0023806346573474113\n",
      "Iteration 29063: loss = 0.0023806248456488\n",
      "Iteration 29064: loss = 0.0023806150340558285\n",
      "Iteration 29065: loss = 0.002380605222568494\n",
      "Iteration 29066: loss = 0.002380595411186797\n",
      "Iteration 29067: loss = 0.0023805855999107332\n",
      "Iteration 29068: loss = 0.0023805757887403046\n",
      "Iteration 29069: loss = 0.0023805659776755077\n",
      "Iteration 29070: loss = 0.0023805561667163424\n",
      "Iteration 29071: loss = 0.002380546355862806\n",
      "Iteration 29072: loss = 0.0023805365451148993\n",
      "Iteration 29073: loss = 0.00238052673447262\n",
      "Iteration 29074: loss = 0.002380516923935966\n",
      "Iteration 29075: loss = 0.0023805071135049364\n",
      "Iteration 29076: loss = 0.00238049730317953\n",
      "Iteration 29077: loss = 0.002380487492959746\n",
      "Iteration 29078: loss = 0.002380477682845583\n",
      "Iteration 29079: loss = 0.0023804678728370386\n",
      "Iteration 29080: loss = 0.002380458062934113\n",
      "Iteration 29081: loss = 0.002380448253136804\n",
      "Iteration 29082: loss = 0.00238043844344511\n",
      "Iteration 29083: loss = 0.00238042863385903\n",
      "Iteration 29084: loss = 0.0023804188243785633\n",
      "Iteration 29085: loss = 0.0023804090150037083\n",
      "Iteration 29086: loss = 0.002380399205734463\n",
      "Iteration 29087: loss = 0.0023803893965708275\n",
      "Iteration 29088: loss = 0.0023803795875127986\n",
      "Iteration 29089: loss = 0.002380369778560376\n",
      "Iteration 29090: loss = 0.002380359969713559\n",
      "Iteration 29091: loss = 0.002380350160972346\n",
      "Iteration 29092: loss = 0.0023803403523367348\n",
      "Iteration 29093: loss = 0.0023803305438067244\n",
      "Iteration 29094: loss = 0.002380320735382314\n",
      "Iteration 29095: loss = 0.0023803109270635024\n",
      "Iteration 29096: loss = 0.002380301118850287\n",
      "Iteration 29097: loss = 0.0023802913107426687\n",
      "Iteration 29098: loss = 0.002380281502740644\n",
      "Iteration 29099: loss = 0.0023802716948442128\n",
      "Iteration 29100: loss = 0.0023802618870533737\n",
      "Iteration 29101: loss = 0.0023802520793681247\n",
      "Iteration 29102: loss = 0.0023802422717884653\n",
      "Iteration 29103: loss = 0.002380232464314394\n",
      "Iteration 29104: loss = 0.0023802226569459095\n",
      "Iteration 29105: loss = 0.00238021284968301\n",
      "Iteration 29106: loss = 0.0023802030425256943\n",
      "Iteration 29107: loss = 0.002380193235473962\n",
      "Iteration 29108: loss = 0.0023801834285278106\n",
      "Iteration 29109: loss = 0.0023801736216872396\n",
      "Iteration 29110: loss = 0.0023801638149522474\n",
      "Iteration 29111: loss = 0.0023801540083228335\n",
      "Iteration 29112: loss = 0.0023801442017989945\n",
      "Iteration 29113: loss = 0.002380134395380731\n",
      "Iteration 29114: loss = 0.0023801245890680413\n",
      "Iteration 29115: loss = 0.0023801147828609237\n",
      "Iteration 29116: loss = 0.0023801049767593766\n",
      "Iteration 29117: loss = 0.0023800951707634\n",
      "Iteration 29118: loss = 0.0023800853648729913\n",
      "Iteration 29119: loss = 0.0023800755590881496\n",
      "Iteration 29120: loss = 0.0023800657534088737\n",
      "Iteration 29121: loss = 0.002380055947835163\n",
      "Iteration 29122: loss = 0.0023800461423670147\n",
      "Iteration 29123: loss = 0.002380036337004428\n",
      "Iteration 29124: loss = 0.0023800265317474026\n",
      "Iteration 29125: loss = 0.0023800167265959358\n",
      "Iteration 29126: loss = 0.002380006921550027\n",
      "Iteration 29127: loss = 0.0023799971166096746\n",
      "Iteration 29128: loss = 0.002379987311774878\n",
      "Iteration 29129: loss = 0.0023799775070456348\n",
      "Iteration 29130: loss = 0.0023799677024219446\n",
      "Iteration 29131: loss = 0.0023799578979038062\n",
      "Iteration 29132: loss = 0.0023799480934912176\n",
      "Iteration 29133: loss = 0.0023799382891841773\n",
      "Iteration 29134: loss = 0.0023799284849826846\n",
      "Iteration 29135: loss = 0.0023799186808867384\n",
      "Iteration 29136: loss = 0.002379908876896337\n",
      "Iteration 29137: loss = 0.0023798990730114784\n",
      "Iteration 29138: loss = 0.002379889269232163\n",
      "Iteration 29139: loss = 0.002379879465558388\n",
      "Iteration 29140: loss = 0.0023798696619901525\n",
      "Iteration 29141: loss = 0.002379859858527456\n",
      "Iteration 29142: loss = 0.0023798500551702956\n",
      "Iteration 29143: loss = 0.002379840251918671\n",
      "Iteration 29144: loss = 0.0023798304487725815\n",
      "Iteration 29145: loss = 0.0023798206457320246\n",
      "Iteration 29146: loss = 0.0023798108427969993\n",
      "Iteration 29147: loss = 0.002379801039967505\n",
      "Iteration 29148: loss = 0.002379791237243539\n",
      "Iteration 29149: loss = 0.0023797814346251016\n",
      "Iteration 29150: loss = 0.0023797716321121904\n",
      "Iteration 29151: loss = 0.002379761829704804\n",
      "Iteration 29152: loss = 0.0023797520274029425\n",
      "Iteration 29153: loss = 0.002379742225206603\n",
      "Iteration 29154: loss = 0.002379732423115785\n",
      "Iteration 29155: loss = 0.002379722621130487\n",
      "Iteration 29156: loss = 0.002379712819250707\n",
      "Iteration 29157: loss = 0.0023797030174764456\n",
      "Iteration 29158: loss = 0.0023796932158076994\n",
      "Iteration 29159: loss = 0.0023796834142444678\n",
      "Iteration 29160: loss = 0.0023796736127867507\n",
      "Iteration 29161: loss = 0.002379663811434545\n",
      "Iteration 29162: loss = 0.0023796540101878503\n",
      "Iteration 29163: loss = 0.0023796442090466647\n",
      "Iteration 29164: loss = 0.0023796344080109877\n",
      "Iteration 29165: loss = 0.002379624607080818\n",
      "Iteration 29166: loss = 0.002379614806256153\n",
      "Iteration 29167: loss = 0.002379605005536993\n",
      "Iteration 29168: loss = 0.002379595204923336\n",
      "Iteration 29169: loss = 0.00237958540441518\n",
      "Iteration 29170: loss = 0.002379575604012525\n",
      "Iteration 29171: loss = 0.0023795658037153696\n",
      "Iteration 29172: loss = 0.0023795560035237114\n",
      "Iteration 29173: loss = 0.002379546203437549\n",
      "Iteration 29174: loss = 0.0023795364034568827\n",
      "Iteration 29175: loss = 0.0023795266035817105\n",
      "Iteration 29176: loss = 0.00237951680381203\n",
      "Iteration 29177: loss = 0.0023795070041478412\n",
      "Iteration 29178: loss = 0.0023794972045891425\n",
      "Iteration 29179: loss = 0.002379487405135932\n",
      "Iteration 29180: loss = 0.0023794776057882087\n",
      "Iteration 29181: loss = 0.0023794678065459716\n",
      "Iteration 29182: loss = 0.002379458007409219\n",
      "Iteration 29183: loss = 0.00237944820837795\n",
      "Iteration 29184: loss = 0.0023794384094521634\n",
      "Iteration 29185: loss = 0.0023794286106318574\n",
      "Iteration 29186: loss = 0.002379418811917031\n",
      "Iteration 29187: loss = 0.002379409013307682\n",
      "Iteration 29188: loss = 0.002379399214803811\n",
      "Iteration 29189: loss = 0.002379389416405415\n",
      "Iteration 29190: loss = 0.0023793796181124935\n",
      "Iteration 29191: loss = 0.002379369819925044\n",
      "Iteration 29192: loss = 0.0023793600218430674\n",
      "Iteration 29193: loss = 0.002379350223866561\n",
      "Iteration 29194: loss = 0.0023793404259955225\n",
      "Iteration 29195: loss = 0.002379330628229953\n",
      "Iteration 29196: loss = 0.0023793208305698493\n",
      "Iteration 29197: loss = 0.0023793110330152106\n",
      "Iteration 29198: loss = 0.0023793012355660358\n",
      "Iteration 29199: loss = 0.002379291438222324\n",
      "Iteration 29200: loss = 0.0023792816409840723\n",
      "Iteration 29201: loss = 0.0023792718438512815\n",
      "Iteration 29202: loss = 0.0023792620468239484\n",
      "Iteration 29203: loss = 0.002379252249902073\n",
      "Iteration 29204: loss = 0.0023792424530856542\n",
      "Iteration 29205: loss = 0.0023792326563746888\n",
      "Iteration 29206: loss = 0.0023792228597691776\n",
      "Iteration 29207: loss = 0.0023792130632691185\n",
      "Iteration 29208: loss = 0.0023792032668745098\n",
      "Iteration 29209: loss = 0.00237919347058535\n",
      "Iteration 29210: loss = 0.0023791836744016396\n",
      "Iteration 29211: loss = 0.002379173878323375\n",
      "Iteration 29212: loss = 0.002379164082350556\n",
      "Iteration 29213: loss = 0.0023791542864831814\n",
      "Iteration 29214: loss = 0.00237914449072125\n",
      "Iteration 29215: loss = 0.0023791346950647603\n",
      "Iteration 29216: loss = 0.0023791248995137103\n",
      "Iteration 29217: loss = 0.002379115104068099\n",
      "Iteration 29218: loss = 0.002379105308727926\n",
      "Iteration 29219: loss = 0.002379095513493189\n",
      "Iteration 29220: loss = 0.0023790857183638874\n",
      "Iteration 29221: loss = 0.002379075923340019\n",
      "Iteration 29222: loss = 0.002379066128421584\n",
      "Iteration 29223: loss = 0.0023790563336085796\n",
      "Iteration 29224: loss = 0.0023790465389010042\n",
      "Iteration 29225: loss = 0.0023790367442988584\n",
      "Iteration 29226: loss = 0.00237902694980214\n",
      "Iteration 29227: loss = 0.0023790171554108467\n",
      "Iteration 29228: loss = 0.0023790073611249787\n",
      "Iteration 29229: loss = 0.0023789975669445333\n",
      "Iteration 29230: loss = 0.00237898777286951\n",
      "Iteration 29231: loss = 0.002378977978899908\n",
      "Iteration 29232: loss = 0.002378968185035725\n",
      "Iteration 29233: loss = 0.00237895839127696\n",
      "Iteration 29234: loss = 0.002378948597623612\n",
      "Iteration 29235: loss = 0.0023789388040756786\n",
      "Iteration 29236: loss = 0.0023789290106331607\n",
      "Iteration 29237: loss = 0.002378919217296055\n",
      "Iteration 29238: loss = 0.002378909424064361\n",
      "Iteration 29239: loss = 0.002378899630938077\n",
      "Iteration 29240: loss = 0.0023788898379172017\n",
      "Iteration 29241: loss = 0.0023788800450017344\n",
      "Iteration 29242: loss = 0.0023788702521916733\n",
      "Iteration 29243: loss = 0.002378860459487017\n",
      "Iteration 29244: loss = 0.0023788506668877647\n",
      "Iteration 29245: loss = 0.002378840874393915\n",
      "Iteration 29246: loss = 0.0023788310820054667\n",
      "Iteration 29247: loss = 0.0023788212897224168\n",
      "Iteration 29248: loss = 0.0023788114975447664\n",
      "Iteration 29249: loss = 0.002378801705472513\n",
      "Iteration 29250: loss = 0.0023787919135056556\n",
      "Iteration 29251: loss = 0.0023787821216441924\n",
      "Iteration 29252: loss = 0.002378772329888123\n",
      "Iteration 29253: loss = 0.002378762538237445\n",
      "Iteration 29254: loss = 0.002378752746692158\n",
      "Iteration 29255: loss = 0.00237874295525226\n",
      "Iteration 29256: loss = 0.002378733163917751\n",
      "Iteration 29257: loss = 0.0023787233726886275\n",
      "Iteration 29258: loss = 0.0023787135815648897\n",
      "Iteration 29259: loss = 0.0023787037905465365\n",
      "Iteration 29260: loss = 0.002378693999633566\n",
      "Iteration 29261: loss = 0.0023786842088259767\n",
      "Iteration 29262: loss = 0.0023786744181237677\n",
      "Iteration 29263: loss = 0.0023786646275269375\n",
      "Iteration 29264: loss = 0.0023786548370354848\n",
      "Iteration 29265: loss = 0.002378645046649409\n",
      "Iteration 29266: loss = 0.0023786352563687076\n",
      "Iteration 29267: loss = 0.0023786254661933796\n",
      "Iteration 29268: loss = 0.0023786156761234248\n",
      "Iteration 29269: loss = 0.002378605886158841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29270: loss = 0.0023785960962996264\n",
      "Iteration 29271: loss = 0.00237858630654578\n",
      "Iteration 29272: loss = 0.0023785765168973013\n",
      "Iteration 29273: loss = 0.0023785667273541885\n",
      "Iteration 29274: loss = 0.00237855693791644\n",
      "Iteration 29275: loss = 0.0023785471485840547\n",
      "Iteration 29276: loss = 0.002378537359357031\n",
      "Iteration 29277: loss = 0.0023785275702353685\n",
      "Iteration 29278: loss = 0.002378517781219065\n",
      "Iteration 29279: loss = 0.00237850799230812\n",
      "Iteration 29280: loss = 0.0023784982035025314\n",
      "Iteration 29281: loss = 0.0023784884148022978\n",
      "Iteration 29282: loss = 0.0023784786262074187\n",
      "Iteration 29283: loss = 0.002378468837717892\n",
      "Iteration 29284: loss = 0.002378459049333717\n",
      "Iteration 29285: loss = 0.002378449261054892\n",
      "Iteration 29286: loss = 0.0023784394728814163\n",
      "Iteration 29287: loss = 0.002378429684813288\n",
      "Iteration 29288: loss = 0.002378419896850506\n",
      "Iteration 29289: loss = 0.0023784101089930685\n",
      "Iteration 29290: loss = 0.0023784003212409754\n",
      "Iteration 29291: loss = 0.002378390533594224\n",
      "Iteration 29292: loss = 0.0023783807460528135\n",
      "Iteration 29293: loss = 0.002378370958616743\n",
      "Iteration 29294: loss = 0.0023783611712860113\n",
      "Iteration 29295: loss = 0.002378351384060616\n",
      "Iteration 29296: loss = 0.0023783415969405564\n",
      "Iteration 29297: loss = 0.002378331809925832\n",
      "Iteration 29298: loss = 0.0023783220230164407\n",
      "Iteration 29299: loss = 0.0023783122362123813\n",
      "Iteration 29300: loss = 0.0023783024495136525\n",
      "Iteration 29301: loss = 0.0023782926629202523\n",
      "Iteration 29302: loss = 0.002378282876432181\n",
      "Iteration 29303: loss = 0.002378273090049436\n",
      "Iteration 29304: loss = 0.0023782633037720156\n",
      "Iteration 29305: loss = 0.0023782535175999203\n",
      "Iteration 29306: loss = 0.002378243731533147\n",
      "Iteration 29307: loss = 0.0023782339455716956\n",
      "Iteration 29308: loss = 0.002378224159715564\n",
      "Iteration 29309: loss = 0.0023782143739647524\n",
      "Iteration 29310: loss = 0.002378204588319257\n",
      "Iteration 29311: loss = 0.0023781948027790786\n",
      "Iteration 29312: loss = 0.0023781850173442147\n",
      "Iteration 29313: loss = 0.0023781752320146646\n",
      "Iteration 29314: loss = 0.0023781654467904265\n",
      "Iteration 29315: loss = 0.0023781556616714996\n",
      "Iteration 29316: loss = 0.0023781458766578825\n",
      "Iteration 29317: loss = 0.0023781360917495735\n",
      "Iteration 29318: loss = 0.0023781263069465723\n",
      "Iteration 29319: loss = 0.002378116522248876\n",
      "Iteration 29320: loss = 0.0023781067376564846\n",
      "Iteration 29321: loss = 0.0023780969531693964\n",
      "Iteration 29322: loss = 0.00237808716878761\n",
      "Iteration 29323: loss = 0.0023780773845111245\n",
      "Iteration 29324: loss = 0.0023780676003399373\n",
      "Iteration 29325: loss = 0.0023780578162740486\n",
      "Iteration 29326: loss = 0.002378048032313457\n",
      "Iteration 29327: loss = 0.0023780382484581606\n",
      "Iteration 29328: loss = 0.0023780284647081573\n",
      "Iteration 29329: loss = 0.002378018681063448\n",
      "Iteration 29330: loss = 0.0023780088975240295\n",
      "Iteration 29331: loss = 0.002377999114089901\n",
      "Iteration 29332: loss = 0.0023779893307610614\n",
      "Iteration 29333: loss = 0.00237797954753751\n",
      "Iteration 29334: loss = 0.002377969764419244\n",
      "Iteration 29335: loss = 0.002377959981406263\n",
      "Iteration 29336: loss = 0.0023779501984985654\n",
      "Iteration 29337: loss = 0.0023779404156961505\n",
      "Iteration 29338: loss = 0.0023779306329990164\n",
      "Iteration 29339: loss = 0.002377920850407162\n",
      "Iteration 29340: loss = 0.002377911067920586\n",
      "Iteration 29341: loss = 0.0023779012855392875\n",
      "Iteration 29342: loss = 0.0023778915032632644\n",
      "Iteration 29343: loss = 0.002377881721092516\n",
      "Iteration 29344: loss = 0.0023778719390270403\n",
      "Iteration 29345: loss = 0.002377862157066837\n",
      "Iteration 29346: loss = 0.0023778523752119034\n",
      "Iteration 29347: loss = 0.0023778425934622397\n",
      "Iteration 29348: loss = 0.0023778328118178437\n",
      "Iteration 29349: loss = 0.0023778230302787143\n",
      "Iteration 29350: loss = 0.0023778132488448504\n",
      "Iteration 29351: loss = 0.0023778034675162505\n",
      "Iteration 29352: loss = 0.0023777936862929136\n",
      "Iteration 29353: loss = 0.002377783905174838\n",
      "Iteration 29354: loss = 0.002377774124162022\n",
      "Iteration 29355: loss = 0.0023777643432544653\n",
      "Iteration 29356: loss = 0.002377754562452166\n",
      "Iteration 29357: loss = 0.0023777447817551227\n",
      "Iteration 29358: loss = 0.0023777350011633344\n",
      "Iteration 29359: loss = 0.0023777252206768\n",
      "Iteration 29360: loss = 0.0023777154402955174\n",
      "Iteration 29361: loss = 0.0023777056600194856\n",
      "Iteration 29362: loss = 0.0023776958798487043\n",
      "Iteration 29363: loss = 0.0023776860997831704\n",
      "Iteration 29364: loss = 0.0023776763198228843\n",
      "Iteration 29365: loss = 0.002377666539967844\n",
      "Iteration 29366: loss = 0.0023776567602180475\n",
      "Iteration 29367: loss = 0.0023776469805734946\n",
      "Iteration 29368: loss = 0.0023776372010341834\n",
      "Iteration 29369: loss = 0.0023776274216001127\n",
      "Iteration 29370: loss = 0.002377617642271282\n",
      "Iteration 29371: loss = 0.002377607863047688\n",
      "Iteration 29372: loss = 0.0023775980839293312\n",
      "Iteration 29373: loss = 0.00237758830491621\n",
      "Iteration 29374: loss = 0.002377578526008323\n",
      "Iteration 29375: loss = 0.002377568747205667\n",
      "Iteration 29376: loss = 0.002377558968508244\n",
      "Iteration 29377: loss = 0.002377549189916051\n",
      "Iteration 29378: loss = 0.0023775394114290867\n",
      "Iteration 29379: loss = 0.00237752963304735\n",
      "Iteration 29380: loss = 0.002377519854770839\n",
      "Iteration 29381: loss = 0.002377510076599553\n",
      "Iteration 29382: loss = 0.002377500298533491\n",
      "Iteration 29383: loss = 0.002377490520572651\n",
      "Iteration 29384: loss = 0.002377480742717032\n",
      "Iteration 29385: loss = 0.0023774709649666322\n",
      "Iteration 29386: loss = 0.0023774611873214517\n",
      "Iteration 29387: loss = 0.0023774514097814874\n",
      "Iteration 29388: loss = 0.0023774416323467393\n",
      "Iteration 29389: loss = 0.002377431855017205\n",
      "Iteration 29390: loss = 0.0023774220777928847\n",
      "Iteration 29391: loss = 0.002377412300673776\n",
      "Iteration 29392: loss = 0.002377402523659878\n",
      "Iteration 29393: loss = 0.002377392746751189\n",
      "Iteration 29394: loss = 0.002377382969947708\n",
      "Iteration 29395: loss = 0.0023773731932494337\n",
      "Iteration 29396: loss = 0.0023773634166563648\n",
      "Iteration 29397: loss = 0.0023773536401684994\n",
      "Iteration 29398: loss = 0.0023773438637858368\n",
      "Iteration 29399: loss = 0.0023773340875083765\n",
      "Iteration 29400: loss = 0.0023773243113361154\n",
      "Iteration 29401: loss = 0.002377314535269053\n",
      "Iteration 29402: loss = 0.002377304759307189\n",
      "Iteration 29403: loss = 0.00237729498345052\n",
      "Iteration 29404: loss = 0.002377285207699047\n",
      "Iteration 29405: loss = 0.0023772754320527667\n",
      "Iteration 29406: loss = 0.002377265656511679\n",
      "Iteration 29407: loss = 0.0023772558810757826\n",
      "Iteration 29408: loss = 0.0023772461057450754\n",
      "Iteration 29409: loss = 0.0023772363305195566\n",
      "Iteration 29410: loss = 0.002377226555399226\n",
      "Iteration 29411: loss = 0.00237721678038408\n",
      "Iteration 29412: loss = 0.0023772070054741187\n",
      "Iteration 29413: loss = 0.0023771972306693406\n",
      "Iteration 29414: loss = 0.0023771874559697435\n",
      "Iteration 29415: loss = 0.002377177681375328\n",
      "Iteration 29416: loss = 0.0023771679068860917\n",
      "Iteration 29417: loss = 0.0023771581325020327\n",
      "Iteration 29418: loss = 0.002377148358223151\n",
      "Iteration 29419: loss = 0.0023771385840494442\n",
      "Iteration 29420: loss = 0.0023771288099809123\n",
      "Iteration 29421: loss = 0.0023771190360175522\n",
      "Iteration 29422: loss = 0.0023771092621593637\n",
      "Iteration 29423: loss = 0.002377099488406345\n",
      "Iteration 29424: loss = 0.002377089714758495\n",
      "Iteration 29425: loss = 0.0023770799412158134\n",
      "Iteration 29426: loss = 0.0023770701677782976\n",
      "Iteration 29427: loss = 0.0023770603944459462\n",
      "Iteration 29428: loss = 0.002377050621218759\n",
      "Iteration 29429: loss = 0.002377040848096734\n",
      "Iteration 29430: loss = 0.00237703107507987\n",
      "Iteration 29431: loss = 0.0023770213021681655\n",
      "Iteration 29432: loss = 0.0023770115293616193\n",
      "Iteration 29433: loss = 0.0023770017566602302\n",
      "Iteration 29434: loss = 0.0023769919840639976\n",
      "Iteration 29435: loss = 0.0023769822115729186\n",
      "Iteration 29436: loss = 0.0023769724391869925\n",
      "Iteration 29437: loss = 0.002376962666906219\n",
      "Iteration 29438: loss = 0.0023769528947305955\n",
      "Iteration 29439: loss = 0.002376943122660122\n",
      "Iteration 29440: loss = 0.002376933350694796\n",
      "Iteration 29441: loss = 0.0023769235788346165\n",
      "Iteration 29442: loss = 0.002376913807079583\n",
      "Iteration 29443: loss = 0.002376904035429693\n",
      "Iteration 29444: loss = 0.0023768942638849456\n",
      "Iteration 29445: loss = 0.0023768844924453403\n",
      "Iteration 29446: loss = 0.002376874721110874\n",
      "Iteration 29447: loss = 0.0023768649498815474\n",
      "Iteration 29448: loss = 0.002376855178757358\n",
      "Iteration 29449: loss = 0.0023768454077383048\n",
      "Iteration 29450: loss = 0.0023768356368243865\n",
      "Iteration 29451: loss = 0.002376825866015602\n",
      "Iteration 29452: loss = 0.0023768160953119497\n",
      "Iteration 29453: loss = 0.0023768063247134284\n",
      "Iteration 29454: loss = 0.002376796554220036\n",
      "Iteration 29455: loss = 0.0023767867838317734\n",
      "Iteration 29456: loss = 0.0023767770135486375\n",
      "Iteration 29457: loss = 0.002376767243370627\n",
      "Iteration 29458: loss = 0.0023767574732977415\n",
      "Iteration 29459: loss = 0.0023767477033299787\n",
      "Iteration 29460: loss = 0.0023767379334673376\n",
      "Iteration 29461: loss = 0.002376728163709818\n",
      "Iteration 29462: loss = 0.002376718394057417\n",
      "Iteration 29463: loss = 0.0023767086245101337\n",
      "Iteration 29464: loss = 0.002376698855067967\n",
      "Iteration 29465: loss = 0.0023766890857309163\n",
      "Iteration 29466: loss = 0.00237667931649898\n",
      "Iteration 29467: loss = 0.0023766695473721555\n",
      "Iteration 29468: loss = 0.002376659778350443\n",
      "Iteration 29469: loss = 0.0023766500094338397\n",
      "Iteration 29470: loss = 0.0023766402406223464\n",
      "Iteration 29471: loss = 0.0023766304719159597\n",
      "Iteration 29472: loss = 0.0023766207033146802\n",
      "Iteration 29473: loss = 0.0023766109348185055\n",
      "Iteration 29474: loss = 0.0023766011664274334\n",
      "Iteration 29475: loss = 0.0023765913981414646\n",
      "Iteration 29476: loss = 0.0023765816299605963\n",
      "Iteration 29477: loss = 0.0023765718618848288\n",
      "Iteration 29478: loss = 0.0023765620939141586\n",
      "Iteration 29479: loss = 0.002376552326048586\n",
      "Iteration 29480: loss = 0.0023765425582881086\n",
      "Iteration 29481: loss = 0.002376532790632726\n",
      "Iteration 29482: loss = 0.0023765230230824367\n",
      "Iteration 29483: loss = 0.0023765132556372395\n",
      "Iteration 29484: loss = 0.0023765034882971326\n",
      "Iteration 29485: loss = 0.0023764937210621153\n",
      "Iteration 29486: loss = 0.002376483953932186\n",
      "Iteration 29487: loss = 0.002376474186907343\n",
      "Iteration 29488: loss = 0.002376464419987585\n",
      "Iteration 29489: loss = 0.0023764546531729117\n",
      "Iteration 29490: loss = 0.0023764448864633213\n",
      "Iteration 29491: loss = 0.0023764351198588126\n",
      "Iteration 29492: loss = 0.002376425353359383\n",
      "Iteration 29493: loss = 0.002376415586965033\n",
      "Iteration 29494: loss = 0.0023764058206757607\n",
      "Iteration 29495: loss = 0.0023763960544915647\n",
      "Iteration 29496: loss = 0.002376386288412443\n",
      "Iteration 29497: loss = 0.002376376522438396\n",
      "Iteration 29498: loss = 0.0023763667565694204\n",
      "Iteration 29499: loss = 0.0023763569908055162\n",
      "Iteration 29500: loss = 0.0023763472251466817\n",
      "Iteration 29501: loss = 0.002376337459592916\n",
      "Iteration 29502: loss = 0.002376327694144216\n",
      "Iteration 29503: loss = 0.002376317928800584\n",
      "Iteration 29504: loss = 0.002376308163562015\n",
      "Iteration 29505: loss = 0.00237629839842851\n",
      "Iteration 29506: loss = 0.0023762886334000666\n",
      "Iteration 29507: loss = 0.0023762788684766837\n",
      "Iteration 29508: loss = 0.00237626910365836\n",
      "Iteration 29509: loss = 0.0023762593389450954\n",
      "Iteration 29510: loss = 0.002376249574336886\n",
      "Iteration 29511: loss = 0.002376239809833733\n",
      "Iteration 29512: loss = 0.002376230045435634\n",
      "Iteration 29513: loss = 0.002376220281142588\n",
      "Iteration 29514: loss = 0.0023762105169545925\n",
      "Iteration 29515: loss = 0.0023762007528716485\n",
      "Iteration 29516: loss = 0.0023761909888937523\n",
      "Iteration 29517: loss = 0.0023761812250209045\n",
      "Iteration 29518: loss = 0.0023761714612531024\n",
      "Iteration 29519: loss = 0.0023761616975903456\n",
      "Iteration 29520: loss = 0.002376151934032633\n",
      "Iteration 29521: loss = 0.002376142170579962\n",
      "Iteration 29522: loss = 0.0023761324072323325\n",
      "Iteration 29523: loss = 0.0023761226439897422\n",
      "Iteration 29524: loss = 0.0023761128808521908\n",
      "Iteration 29525: loss = 0.002376103117819677\n",
      "Iteration 29526: loss = 0.0023760933548921982\n",
      "Iteration 29527: loss = 0.0023760835920697545\n",
      "Iteration 29528: loss = 0.002376073829352344\n",
      "Iteration 29529: loss = 0.0023760640667399653\n",
      "Iteration 29530: loss = 0.0023760543042326172\n",
      "Iteration 29531: loss = 0.002376044541830299\n",
      "Iteration 29532: loss = 0.0023760347795330084\n",
      "Iteration 29533: loss = 0.002376025017340745\n",
      "Iteration 29534: loss = 0.002376015255253507\n",
      "Iteration 29535: loss = 0.0023760054932712928\n",
      "Iteration 29536: loss = 0.0023759957313941016\n",
      "Iteration 29537: loss = 0.0023759859696219315\n",
      "Iteration 29538: loss = 0.002375976207954782\n",
      "Iteration 29539: loss = 0.0023759664463926518\n",
      "Iteration 29540: loss = 0.0023759566849355387\n",
      "Iteration 29541: loss = 0.0023759469235834422\n",
      "Iteration 29542: loss = 0.0023759371623363608\n",
      "Iteration 29543: loss = 0.0023759274011942925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29544: loss = 0.0023759176401572375\n",
      "Iteration 29545: loss = 0.0023759078792251935\n",
      "Iteration 29546: loss = 0.002375898118398159\n",
      "Iteration 29547: loss = 0.0023758883576761327\n",
      "Iteration 29548: loss = 0.002375878597059114\n",
      "Iteration 29549: loss = 0.0023758688365471013\n",
      "Iteration 29550: loss = 0.0023758590761400935\n",
      "Iteration 29551: loss = 0.002375849315838089\n",
      "Iteration 29552: loss = 0.0023758395556410855\n",
      "Iteration 29553: loss = 0.0023758297955490835\n",
      "Iteration 29554: loss = 0.002375820035562081\n",
      "Iteration 29555: loss = 0.002375810275680076\n",
      "Iteration 29556: loss = 0.0023758005159030685\n",
      "Iteration 29557: loss = 0.0023757907562310565\n",
      "Iteration 29558: loss = 0.0023757809966640383\n",
      "Iteration 29559: loss = 0.0023757712372020124\n",
      "Iteration 29560: loss = 0.002375761477844979\n",
      "Iteration 29561: loss = 0.0023757517185929358\n",
      "Iteration 29562: loss = 0.002375741959445882\n",
      "Iteration 29563: loss = 0.0023757322004038153\n",
      "Iteration 29564: loss = 0.0023757224414667345\n",
      "Iteration 29565: loss = 0.0023757126826346397\n",
      "Iteration 29566: loss = 0.0023757029239075277\n",
      "Iteration 29567: loss = 0.0023756931652853994\n",
      "Iteration 29568: loss = 0.0023756834067682514\n",
      "Iteration 29569: loss = 0.0023756736483560833\n",
      "Iteration 29570: loss = 0.0023756638900488936\n",
      "Iteration 29571: loss = 0.002375654131846682\n",
      "Iteration 29572: loss = 0.002375644373749446\n",
      "Iteration 29573: loss = 0.0023756346157571846\n",
      "Iteration 29574: loss = 0.0023756248578698964\n",
      "Iteration 29575: loss = 0.0023756151000875803\n",
      "Iteration 29576: loss = 0.002375605342410235\n",
      "Iteration 29577: loss = 0.002375595584837859\n",
      "Iteration 29578: loss = 0.002375585827370451\n",
      "Iteration 29579: loss = 0.002375576070008011\n",
      "Iteration 29580: loss = 0.0023755663127505355\n",
      "Iteration 29581: loss = 0.002375556555598024\n",
      "Iteration 29582: loss = 0.0023755467985504763\n",
      "Iteration 29583: loss = 0.0023755370416078904\n",
      "Iteration 29584: loss = 0.0023755272847702635\n",
      "Iteration 29585: loss = 0.002375517528037597\n",
      "Iteration 29586: loss = 0.0023755077714098876\n",
      "Iteration 29587: loss = 0.0023754980148871347\n",
      "Iteration 29588: loss = 0.002375488258469337\n",
      "Iteration 29589: loss = 0.002375478502156493\n",
      "Iteration 29590: loss = 0.002375468745948601\n",
      "Iteration 29591: loss = 0.0023754589898456616\n",
      "Iteration 29592: loss = 0.0023754492338476717\n",
      "Iteration 29593: loss = 0.0023754394779546295\n",
      "Iteration 29594: loss = 0.002375429722166536\n",
      "Iteration 29595: loss = 0.0023754199664833874\n",
      "Iteration 29596: loss = 0.002375410210905184\n",
      "Iteration 29597: loss = 0.0023754004554319237\n",
      "Iteration 29598: loss = 0.0023753907000636054\n",
      "Iteration 29599: loss = 0.0023753809448002284\n",
      "Iteration 29600: loss = 0.0023753711896417904\n",
      "Iteration 29601: loss = 0.002375361434588291\n",
      "Iteration 29602: loss = 0.002375351679639729\n",
      "Iteration 29603: loss = 0.0023753419247961017\n",
      "Iteration 29604: loss = 0.002375332170057409\n",
      "Iteration 29605: loss = 0.002375322415423649\n",
      "Iteration 29606: loss = 0.0023753126608948216\n",
      "Iteration 29607: loss = 0.002375302906470924\n",
      "Iteration 29608: loss = 0.002375293152151955\n",
      "Iteration 29609: loss = 0.002375283397937915\n",
      "Iteration 29610: loss = 0.002375273643828801\n",
      "Iteration 29611: loss = 0.002375263889824612\n",
      "Iteration 29612: loss = 0.002375254135925347\n",
      "Iteration 29613: loss = 0.0023752443821310047\n",
      "Iteration 29614: loss = 0.002375234628441583\n",
      "Iteration 29615: loss = 0.0023752248748570817\n",
      "Iteration 29616: loss = 0.0023752151213774996\n",
      "Iteration 29617: loss = 0.0023752053680028344\n",
      "Iteration 29618: loss = 0.0023751956147330853\n",
      "Iteration 29619: loss = 0.002375185861568251\n",
      "Iteration 29620: loss = 0.0023751761085083305\n",
      "Iteration 29621: loss = 0.0023751663555533213\n",
      "Iteration 29622: loss = 0.002375156602703224\n",
      "Iteration 29623: loss = 0.0023751468499580357\n",
      "Iteration 29624: loss = 0.0023751370973177557\n",
      "Iteration 29625: loss = 0.0023751273447823835\n",
      "Iteration 29626: loss = 0.0023751175923519157\n",
      "Iteration 29627: loss = 0.0023751078400263527\n",
      "Iteration 29628: loss = 0.0023750980878056932\n",
      "Iteration 29629: loss = 0.002375088335689935\n",
      "Iteration 29630: loss = 0.0023750785836790774\n",
      "Iteration 29631: loss = 0.0023750688317731193\n",
      "Iteration 29632: loss = 0.002375059079972058\n",
      "Iteration 29633: loss = 0.0023750493282758945\n",
      "Iteration 29634: loss = 0.0023750395766846256\n",
      "Iteration 29635: loss = 0.002375029825198251\n",
      "Iteration 29636: loss = 0.0023750200738167685\n",
      "Iteration 29637: loss = 0.0023750103225401776\n",
      "Iteration 29638: loss = 0.002375000571368477\n",
      "Iteration 29639: loss = 0.002374990820301665\n",
      "Iteration 29640: loss = 0.002374981069339741\n",
      "Iteration 29641: loss = 0.0023749713184827025\n",
      "Iteration 29642: loss = 0.002374961567730549\n",
      "Iteration 29643: loss = 0.002374951817083279\n",
      "Iteration 29644: loss = 0.0023749420665408914\n",
      "Iteration 29645: loss = 0.0023749323161033845\n",
      "Iteration 29646: loss = 0.0023749225657707577\n",
      "Iteration 29647: loss = 0.002374912815543009\n",
      "Iteration 29648: loss = 0.0023749030654201366\n",
      "Iteration 29649: loss = 0.0023748933154021414\n",
      "Iteration 29650: loss = 0.00237488356548902\n",
      "Iteration 29651: loss = 0.002374873815680771\n",
      "Iteration 29652: loss = 0.0023748640659773945\n",
      "Iteration 29653: loss = 0.0023748543163788884\n",
      "Iteration 29654: loss = 0.002374844566885252\n",
      "Iteration 29655: loss = 0.0023748348174964825\n",
      "Iteration 29656: loss = 0.0023748250682125807\n",
      "Iteration 29657: loss = 0.002374815319033544\n",
      "Iteration 29658: loss = 0.002374805569959371\n",
      "Iteration 29659: loss = 0.002374795820990061\n",
      "Iteration 29660: loss = 0.0023747860721256117\n",
      "Iteration 29661: loss = 0.0023747763233660235\n",
      "Iteration 29662: loss = 0.002374766574711294\n",
      "Iteration 29663: loss = 0.002374756826161422\n",
      "Iteration 29664: loss = 0.0023747470777164053\n",
      "Iteration 29665: loss = 0.0023747373293762446\n",
      "Iteration 29666: loss = 0.002374727581140937\n",
      "Iteration 29667: loss = 0.002374717833010482\n",
      "Iteration 29668: loss = 0.002374708084984878\n",
      "Iteration 29669: loss = 0.0023746983370641233\n",
      "Iteration 29670: loss = 0.0023746885892482173\n",
      "Iteration 29671: loss = 0.002374678841537159\n",
      "Iteration 29672: loss = 0.0023746690939309457\n",
      "Iteration 29673: loss = 0.002374659346429577\n",
      "Iteration 29674: loss = 0.002374649599033052\n",
      "Iteration 29675: loss = 0.0023746398517413684\n",
      "Iteration 29676: loss = 0.0023746301045545256\n",
      "Iteration 29677: loss = 0.002374620357472522\n",
      "Iteration 29678: loss = 0.0023746106104953566\n",
      "Iteration 29679: loss = 0.002374600863623028\n",
      "Iteration 29680: loss = 0.0023745911168555345\n",
      "Iteration 29681: loss = 0.0023745813701928757\n",
      "Iteration 29682: loss = 0.0023745716236350492\n",
      "Iteration 29683: loss = 0.002374561877182054\n",
      "Iteration 29684: loss = 0.00237455213083389\n",
      "Iteration 29685: loss = 0.0023745423845905537\n",
      "Iteration 29686: loss = 0.002374532638452046\n",
      "Iteration 29687: loss = 0.002374522892418364\n",
      "Iteration 29688: loss = 0.002374513146489507\n",
      "Iteration 29689: loss = 0.0023745034006654735\n",
      "Iteration 29690: loss = 0.002374493654946263\n",
      "Iteration 29691: loss = 0.0023744839093318734\n",
      "Iteration 29692: loss = 0.0023744741638223035\n",
      "Iteration 29693: loss = 0.002374464418417552\n",
      "Iteration 29694: loss = 0.002374454673117618\n",
      "Iteration 29695: loss = 0.0023744449279224995\n",
      "Iteration 29696: loss = 0.0023744351828321963\n",
      "Iteration 29697: loss = 0.002374425437846706\n",
      "Iteration 29698: loss = 0.002374415692966028\n",
      "Iteration 29699: loss = 0.00237440594819016\n",
      "Iteration 29700: loss = 0.0023743962035191017\n",
      "Iteration 29701: loss = 0.002374386458952852\n",
      "Iteration 29702: loss = 0.002374376714491408\n",
      "Iteration 29703: loss = 0.002374366970134771\n",
      "Iteration 29704: loss = 0.0023743572258829366\n",
      "Iteration 29705: loss = 0.002374347481735906\n",
      "Iteration 29706: loss = 0.002374337737693677\n",
      "Iteration 29707: loss = 0.002374327993756248\n",
      "Iteration 29708: loss = 0.002374318249923618\n",
      "Iteration 29709: loss = 0.0023743085061957857\n",
      "Iteration 29710: loss = 0.0023742987625727506\n",
      "Iteration 29711: loss = 0.00237428901905451\n",
      "Iteration 29712: loss = 0.0023742792756410625\n",
      "Iteration 29713: loss = 0.0023742695323324087\n",
      "Iteration 29714: loss = 0.002374259789128545\n",
      "Iteration 29715: loss = 0.002374250046029472\n",
      "Iteration 29716: loss = 0.002374240303035187\n",
      "Iteration 29717: loss = 0.00237423056014569\n",
      "Iteration 29718: loss = 0.002374220817360978\n",
      "Iteration 29719: loss = 0.0023742110746810515\n",
      "Iteration 29720: loss = 0.002374201332105908\n",
      "Iteration 29721: loss = 0.0023741915896355473\n",
      "Iteration 29722: loss = 0.0023741818472699664\n",
      "Iteration 29723: loss = 0.0023741721050091658\n",
      "Iteration 29724: loss = 0.002374162362853143\n",
      "Iteration 29725: loss = 0.002374152620801897\n",
      "Iteration 29726: loss = 0.002374142878855427\n",
      "Iteration 29727: loss = 0.0023741331370137303\n",
      "Iteration 29728: loss = 0.002374123395276808\n",
      "Iteration 29729: loss = 0.002374113653644656\n",
      "Iteration 29730: loss = 0.002374103912117275\n",
      "Iteration 29731: loss = 0.0023740941706946635\n",
      "Iteration 29732: loss = 0.0023740844293768195\n",
      "Iteration 29733: loss = 0.002374074688163742\n",
      "Iteration 29734: loss = 0.0023740649470554293\n",
      "Iteration 29735: loss = 0.0023740552060518808\n",
      "Iteration 29736: loss = 0.002374045465153095\n",
      "Iteration 29737: loss = 0.0023740357243590706\n",
      "Iteration 29738: loss = 0.0023740259836698054\n",
      "Iteration 29739: loss = 0.0023740162430853\n",
      "Iteration 29740: loss = 0.0023740065026055515\n",
      "Iteration 29741: loss = 0.002373996762230559\n",
      "Iteration 29742: loss = 0.0023739870219603204\n",
      "Iteration 29743: loss = 0.0023739772817948364\n",
      "Iteration 29744: loss = 0.002373967541734105\n",
      "Iteration 29745: loss = 0.0023739578017781234\n",
      "Iteration 29746: loss = 0.0023739480619268923\n",
      "Iteration 29747: loss = 0.0023739383221804086\n",
      "Iteration 29748: loss = 0.0023739285825386722\n",
      "Iteration 29749: loss = 0.0023739188430016824\n",
      "Iteration 29750: loss = 0.002373909103569436\n",
      "Iteration 29751: loss = 0.0023738993642419327\n",
      "Iteration 29752: loss = 0.002373889625019171\n",
      "Iteration 29753: loss = 0.0023738798859011504\n",
      "Iteration 29754: loss = 0.0023738701468878688\n",
      "Iteration 29755: loss = 0.0023738604079793254\n",
      "Iteration 29756: loss = 0.002373850669175518\n",
      "Iteration 29757: loss = 0.002373840930476446\n",
      "Iteration 29758: loss = 0.002373831191882108\n",
      "Iteration 29759: loss = 0.0023738214533925027\n",
      "Iteration 29760: loss = 0.002373811715007629\n",
      "Iteration 29761: loss = 0.0023738019767274854\n",
      "Iteration 29762: loss = 0.00237379223855207\n",
      "Iteration 29763: loss = 0.0023737825004813834\n",
      "Iteration 29764: loss = 0.002373772762515422\n",
      "Iteration 29765: loss = 0.0023737630246541855\n",
      "Iteration 29766: loss = 0.002373753286897673\n",
      "Iteration 29767: loss = 0.0023737435492458825\n",
      "Iteration 29768: loss = 0.0023737338116988126\n",
      "Iteration 29769: loss = 0.0023737240742564628\n",
      "Iteration 29770: loss = 0.0023737143369188312\n",
      "Iteration 29771: loss = 0.0023737045996859167\n",
      "Iteration 29772: loss = 0.002373694862557719\n",
      "Iteration 29773: loss = 0.002373685125534235\n",
      "Iteration 29774: loss = 0.002373675388615464\n",
      "Iteration 29775: loss = 0.002373665651801405\n",
      "Iteration 29776: loss = 0.002373655915092057\n",
      "Iteration 29777: loss = 0.0023736461784874176\n",
      "Iteration 29778: loss = 0.0023736364419874867\n",
      "Iteration 29779: loss = 0.0023736267055922623\n",
      "Iteration 29780: loss = 0.0023736169693017433\n",
      "Iteration 29781: loss = 0.0023736072331159283\n",
      "Iteration 29782: loss = 0.0023735974970348164\n",
      "Iteration 29783: loss = 0.002373587761058406\n",
      "Iteration 29784: loss = 0.0023735780251866955\n",
      "Iteration 29785: loss = 0.002373568289419684\n",
      "Iteration 29786: loss = 0.00237355855375737\n",
      "Iteration 29787: loss = 0.0023735488181997524\n",
      "Iteration 29788: loss = 0.00237353908274683\n",
      "Iteration 29789: loss = 0.002373529347398601\n",
      "Iteration 29790: loss = 0.0023735196121550646\n",
      "Iteration 29791: loss = 0.002373509877016219\n",
      "Iteration 29792: loss = 0.0023735001419820634\n",
      "Iteration 29793: loss = 0.0023734904070525963\n",
      "Iteration 29794: loss = 0.002373480672227817\n",
      "Iteration 29795: loss = 0.002373470937507723\n",
      "Iteration 29796: loss = 0.002373461202892313\n",
      "Iteration 29797: loss = 0.002373451468381588\n",
      "Iteration 29798: loss = 0.0023734417339755435\n",
      "Iteration 29799: loss = 0.00237343199967418\n",
      "Iteration 29800: loss = 0.002373422265477496\n",
      "Iteration 29801: loss = 0.00237341253138549\n",
      "Iteration 29802: loss = 0.002373402797398161\n",
      "Iteration 29803: loss = 0.0023733930635155076\n",
      "Iteration 29804: loss = 0.0023733833297375283\n",
      "Iteration 29805: loss = 0.002373373596064222\n",
      "Iteration 29806: loss = 0.0023733638624955874\n",
      "Iteration 29807: loss = 0.002373354129031623\n",
      "Iteration 29808: loss = 0.0023733443956723273\n",
      "Iteration 29809: loss = 0.0023733346624176996\n",
      "Iteration 29810: loss = 0.0023733249292677387\n",
      "Iteration 29811: loss = 0.0023733151962224423\n",
      "Iteration 29812: loss = 0.00237330546328181\n",
      "Iteration 29813: loss = 0.00237329573044584\n",
      "Iteration 29814: loss = 0.002373285997714531\n",
      "Iteration 29815: loss = 0.0023732762650878823\n",
      "Iteration 29816: loss = 0.002373266532565893\n",
      "Iteration 29817: loss = 0.0023732568001485596\n",
      "Iteration 29818: loss = 0.002373247067835883\n",
      "Iteration 29819: loss = 0.002373237335627861\n",
      "Iteration 29820: loss = 0.002373227603524492\n",
      "Iteration 29821: loss = 0.002373217871525776\n",
      "Iteration 29822: loss = 0.00237320813963171\n",
      "Iteration 29823: loss = 0.002373198407842294\n",
      "Iteration 29824: loss = 0.0023731886761575267\n",
      "Iteration 29825: loss = 0.0023731789445774055\n",
      "Iteration 29826: loss = 0.00237316921310193\n",
      "Iteration 29827: loss = 0.0023731594817310987\n",
      "Iteration 29828: loss = 0.002373149750464911\n",
      "Iteration 29829: loss = 0.0023731400193033646\n",
      "Iteration 29830: loss = 0.002373130288246459\n",
      "Iteration 29831: loss = 0.002373120557294192\n",
      "Iteration 29832: loss = 0.002373110826446563\n",
      "Iteration 29833: loss = 0.0023731010957035717\n",
      "Iteration 29834: loss = 0.0023730913650652145\n",
      "Iteration 29835: loss = 0.002373081634531491\n",
      "Iteration 29836: loss = 0.002373071904102401\n",
      "Iteration 29837: loss = 0.0023730621737779417\n",
      "Iteration 29838: loss = 0.002373052443558112\n",
      "Iteration 29839: loss = 0.002373042713442912\n",
      "Iteration 29840: loss = 0.002373032983432339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29841: loss = 0.0023730232535263927\n",
      "Iteration 29842: loss = 0.0023730135237250705\n",
      "Iteration 29843: loss = 0.0023730037940283724\n",
      "Iteration 29844: loss = 0.0023729940644362964\n",
      "Iteration 29845: loss = 0.002372984334948841\n",
      "Iteration 29846: loss = 0.0023729746055660057\n",
      "Iteration 29847: loss = 0.0023729648762877884\n",
      "Iteration 29848: loss = 0.002372955147114188\n",
      "Iteration 29849: loss = 0.002372945418045204\n",
      "Iteration 29850: loss = 0.0023729356890808337\n",
      "Iteration 29851: loss = 0.0023729259602210777\n",
      "Iteration 29852: loss = 0.0023729162314659324\n",
      "Iteration 29853: loss = 0.0023729065028153984\n",
      "Iteration 29854: loss = 0.002372896774269473\n",
      "Iteration 29855: loss = 0.002372887045828156\n",
      "Iteration 29856: loss = 0.0023728773174914453\n",
      "Iteration 29857: loss = 0.002372867589259341\n",
      "Iteration 29858: loss = 0.00237285786113184\n",
      "Iteration 29859: loss = 0.0023728481331089417\n",
      "Iteration 29860: loss = 0.002372838405190645\n",
      "Iteration 29861: loss = 0.002372828677376949\n",
      "Iteration 29862: loss = 0.0023728189496678505\n",
      "Iteration 29863: loss = 0.0023728092220633504\n",
      "Iteration 29864: loss = 0.002372799494563447\n",
      "Iteration 29865: loss = 0.002372789767168138\n",
      "Iteration 29866: loss = 0.002372780039877423\n",
      "Iteration 29867: loss = 0.0023727703126913\n",
      "Iteration 29868: loss = 0.0023727605856097684\n",
      "Iteration 29869: loss = 0.002372750858632827\n",
      "Iteration 29870: loss = 0.0023727411317604735\n",
      "Iteration 29871: loss = 0.0023727314049927064\n",
      "Iteration 29872: loss = 0.002372721678329526\n",
      "Iteration 29873: loss = 0.0023727119517709307\n",
      "Iteration 29874: loss = 0.0023727022253169183\n",
      "Iteration 29875: loss = 0.002372692498967488\n",
      "Iteration 29876: loss = 0.002372682772722638\n",
      "Iteration 29877: loss = 0.002372673046582368\n",
      "Iteration 29878: loss = 0.0023726633205466757\n",
      "Iteration 29879: loss = 0.00237265359461556\n",
      "Iteration 29880: loss = 0.00237264386878902\n",
      "Iteration 29881: loss = 0.002372634143067054\n",
      "Iteration 29882: loss = 0.0023726244174496615\n",
      "Iteration 29883: loss = 0.0023726146919368406\n",
      "Iteration 29884: loss = 0.0023726049665285893\n",
      "Iteration 29885: loss = 0.002372595241224907\n",
      "Iteration 29886: loss = 0.0023725855160257937\n",
      "Iteration 29887: loss = 0.0023725757909312454\n",
      "Iteration 29888: loss = 0.002372566065941263\n",
      "Iteration 29889: loss = 0.0023725563410558438\n",
      "Iteration 29890: loss = 0.0023725466162749878\n",
      "Iteration 29891: loss = 0.0023725368915986926\n",
      "Iteration 29892: loss = 0.002372527167026957\n",
      "Iteration 29893: loss = 0.0023725174425597803\n",
      "Iteration 29894: loss = 0.0023725077181971617\n",
      "Iteration 29895: loss = 0.0023724979939390984\n",
      "Iteration 29896: loss = 0.00237248826978559\n",
      "Iteration 29897: loss = 0.0023724785457366346\n",
      "Iteration 29898: loss = 0.0023724688217922323\n",
      "Iteration 29899: loss = 0.00237245909795238\n",
      "Iteration 29900: loss = 0.002372449374217078\n",
      "Iteration 29901: loss = 0.0023724396505863236\n",
      "Iteration 29902: loss = 0.002372429927060116\n",
      "Iteration 29903: loss = 0.0023724202036384543\n",
      "Iteration 29904: loss = 0.0023724104803213367\n",
      "Iteration 29905: loss = 0.0023724007571087626\n",
      "Iteration 29906: loss = 0.00237239103400073\n",
      "Iteration 29907: loss = 0.002372381310997238\n",
      "Iteration 29908: loss = 0.002372371588098285\n",
      "Iteration 29909: loss = 0.00237236186530387\n",
      "Iteration 29910: loss = 0.0023723521426139917\n",
      "Iteration 29911: loss = 0.002372342420028648\n",
      "Iteration 29912: loss = 0.0023723326975478385\n",
      "Iteration 29913: loss = 0.0023723229751715625\n",
      "Iteration 29914: loss = 0.002372313252899817\n",
      "Iteration 29915: loss = 0.0023723035307326016\n",
      "Iteration 29916: loss = 0.0023722938086699155\n",
      "Iteration 29917: loss = 0.002372284086711757\n",
      "Iteration 29918: loss = 0.0023722743648581235\n",
      "Iteration 29919: loss = 0.002372264643109016\n",
      "Iteration 29920: loss = 0.0023722549214644315\n",
      "Iteration 29921: loss = 0.0023722451999243694\n",
      "Iteration 29922: loss = 0.0023722354784888283\n",
      "Iteration 29923: loss = 0.002372225757157807\n",
      "Iteration 29924: loss = 0.0023722160359313043\n",
      "Iteration 29925: loss = 0.0023722063148093183\n",
      "Iteration 29926: loss = 0.002372196593791848\n",
      "Iteration 29927: loss = 0.002372186872878893\n",
      "Iteration 29928: loss = 0.0023721771520704505\n",
      "Iteration 29929: loss = 0.0023721674313665194\n",
      "Iteration 29930: loss = 0.0023721577107671\n",
      "Iteration 29931: loss = 0.0023721479902721896\n",
      "Iteration 29932: loss = 0.002372138269881787\n",
      "Iteration 29933: loss = 0.002372128549595891\n",
      "Iteration 29934: loss = 0.0023721188294145005\n",
      "Iteration 29935: loss = 0.0023721091093376143\n",
      "Iteration 29936: loss = 0.0023720993893652305\n",
      "Iteration 29937: loss = 0.002372089669497349\n",
      "Iteration 29938: loss = 0.0023720799497339673\n",
      "Iteration 29939: loss = 0.0023720702300750844\n",
      "Iteration 29940: loss = 0.0023720605105206987\n",
      "Iteration 29941: loss = 0.00237205079107081\n",
      "Iteration 29942: loss = 0.002372041071725416\n",
      "Iteration 29943: loss = 0.0023720313524845157\n",
      "Iteration 29944: loss = 0.0023720216333481085\n",
      "Iteration 29945: loss = 0.0023720119143161916\n",
      "Iteration 29946: loss = 0.002372002195388765\n",
      "Iteration 29947: loss = 0.0023719924765658266\n",
      "Iteration 29948: loss = 0.002371982757847376\n",
      "Iteration 29949: loss = 0.0023719730392334115\n",
      "Iteration 29950: loss = 0.0023719633207239304\n",
      "Iteration 29951: loss = 0.002371953602318934\n",
      "Iteration 29952: loss = 0.002371943884018419\n",
      "Iteration 29953: loss = 0.002371934165822385\n",
      "Iteration 29954: loss = 0.00237192444773083\n",
      "Iteration 29955: loss = 0.0023719147297437534\n",
      "Iteration 29956: loss = 0.002371905011861154\n",
      "Iteration 29957: loss = 0.00237189529408303\n",
      "Iteration 29958: loss = 0.0023718855764093797\n",
      "Iteration 29959: loss = 0.0023718758588402037\n",
      "Iteration 29960: loss = 0.002371866141375498\n",
      "Iteration 29961: loss = 0.0023718564240152633\n",
      "Iteration 29962: loss = 0.0023718467067594977\n",
      "Iteration 29963: loss = 0.0023718369896081996\n",
      "Iteration 29964: loss = 0.002371827272561368\n",
      "Iteration 29965: loss = 0.002371817555619002\n",
      "Iteration 29966: loss = 0.0023718078387810993\n",
      "Iteration 29967: loss = 0.00237179812204766\n",
      "Iteration 29968: loss = 0.002371788405418681\n",
      "Iteration 29969: loss = 0.002371778688894163\n",
      "Iteration 29970: loss = 0.002371768972474103\n",
      "Iteration 29971: loss = 0.002371759256158501\n",
      "Iteration 29972: loss = 0.0023717495399473544\n",
      "Iteration 29973: loss = 0.0023717398238406627\n",
      "Iteration 29974: loss = 0.0023717301078384255\n",
      "Iteration 29975: loss = 0.00237172039194064\n",
      "Iteration 29976: loss = 0.0023717106761473043\n",
      "Iteration 29977: loss = 0.0023717009604584194\n",
      "Iteration 29978: loss = 0.002371691244873983\n",
      "Iteration 29979: loss = 0.002371681529393993\n",
      "Iteration 29980: loss = 0.0023716718140184487\n",
      "Iteration 29981: loss = 0.0023716620987473493\n",
      "Iteration 29982: loss = 0.002371652383580693\n",
      "Iteration 29983: loss = 0.002371642668518478\n",
      "Iteration 29984: loss = 0.002371632953560704\n",
      "Iteration 29985: loss = 0.0023716232387073696\n",
      "Iteration 29986: loss = 0.002371613523958473\n",
      "Iteration 29987: loss = 0.002371603809314012\n",
      "Iteration 29988: loss = 0.0023715940947739876\n",
      "Iteration 29989: loss = 0.0023715843803383966\n",
      "Iteration 29990: loss = 0.0023715746660072385\n",
      "Iteration 29991: loss = 0.0023715649517805117\n",
      "Iteration 29992: loss = 0.0023715552376582156\n",
      "Iteration 29993: loss = 0.0023715455236403477\n",
      "Iteration 29994: loss = 0.002371535809726908\n",
      "Iteration 29995: loss = 0.0023715260959178944\n",
      "Iteration 29996: loss = 0.0023715163822133054\n",
      "Iteration 29997: loss = 0.0023715066686131403\n",
      "Iteration 29998: loss = 0.0023714969551173978\n",
      "Iteration 29999: loss = 0.0023714872417260765\n",
      "Iteration 30000: loss = 0.002371477528439175\n",
      "Iteration 30001: loss = 0.0023714678152566915\n",
      "Iteration 30002: loss = 0.0023714581021786257\n",
      "Iteration 30003: loss = 0.002371448389204975\n",
      "Iteration 30004: loss = 0.00237143867633574\n",
      "Iteration 30005: loss = 0.002371428963570917\n",
      "Iteration 30006: loss = 0.002371419250910507\n",
      "Iteration 30007: loss = 0.0023714095383545078\n",
      "Iteration 30008: loss = 0.0023713998259029174\n",
      "Iteration 30009: loss = 0.0023713901135557353\n",
      "Iteration 30010: loss = 0.0023713804013129605\n",
      "Iteration 30011: loss = 0.002371370689174591\n",
      "Iteration 30012: loss = 0.0023713609771406252\n",
      "Iteration 30013: loss = 0.002371351265211063\n",
      "Iteration 30014: loss = 0.002371341553385902\n",
      "Iteration 30015: loss = 0.002371331841665142\n",
      "Iteration 30016: loss = 0.0023713221300487803\n",
      "Iteration 30017: loss = 0.0023713124185368167\n",
      "Iteration 30018: loss = 0.0023713027071292493\n",
      "Iteration 30019: loss = 0.0023712929958260775\n",
      "Iteration 30020: loss = 0.0023712832846272984\n",
      "Iteration 30021: loss = 0.002371273573532913\n",
      "Iteration 30022: loss = 0.0023712638625429192\n",
      "Iteration 30023: loss = 0.002371254151657315\n",
      "Iteration 30024: loss = 0.002371244440876099\n",
      "Iteration 30025: loss = 0.0023712347301992707\n",
      "Iteration 30026: loss = 0.002371225019626828\n",
      "Iteration 30027: loss = 0.0023712153091587705\n",
      "Iteration 30028: loss = 0.0023712055987950963\n",
      "Iteration 30029: loss = 0.002371195888535805\n",
      "Iteration 30030: loss = 0.0023711861783808938\n",
      "Iteration 30031: loss = 0.002371176468330363\n",
      "Iteration 30032: loss = 0.0023711667583842092\n",
      "Iteration 30033: loss = 0.002371157048542433\n",
      "Iteration 30034: loss = 0.002371147338805033\n",
      "Iteration 30035: loss = 0.0023711376291720076\n",
      "Iteration 30036: loss = 0.0023711279196433543\n",
      "Iteration 30037: loss = 0.0023711182102190737\n",
      "Iteration 30038: loss = 0.002371108500899163\n",
      "Iteration 30039: loss = 0.0023710987916836217\n",
      "Iteration 30040: loss = 0.0023710890825724487\n",
      "Iteration 30041: loss = 0.002371079373565642\n",
      "Iteration 30042: loss = 0.0023710696646632007\n",
      "Iteration 30043: loss = 0.0023710599558651236\n",
      "Iteration 30044: loss = 0.0023710502471714088\n",
      "Iteration 30045: loss = 0.0023710405385820557\n",
      "Iteration 30046: loss = 0.002371030830097063\n",
      "Iteration 30047: loss = 0.0023710211217164287\n",
      "Iteration 30048: loss = 0.002371011413440152\n",
      "Iteration 30049: loss = 0.002371001705268232\n",
      "Iteration 30050: loss = 0.002370991997200667\n",
      "Iteration 30051: loss = 0.0023709822892374554\n",
      "Iteration 30052: loss = 0.002370972581378596\n",
      "Iteration 30053: loss = 0.0023709628736240875\n",
      "Iteration 30054: loss = 0.0023709531659739293\n",
      "Iteration 30055: loss = 0.0023709434584281196\n",
      "Iteration 30056: loss = 0.002370933750986657\n",
      "Iteration 30057: loss = 0.00237092404364954\n",
      "Iteration 30058: loss = 0.0023709143364167677\n",
      "Iteration 30059: loss = 0.002370904629288339\n",
      "Iteration 30060: loss = 0.0023708949222642516\n",
      "Iteration 30061: loss = 0.0023708852153445053\n",
      "Iteration 30062: loss = 0.0023708755085290988\n",
      "Iteration 30063: loss = 0.00237086580181803\n",
      "Iteration 30064: loss = 0.0023708560952112987\n",
      "Iteration 30065: loss = 0.0023708463887089012\n",
      "Iteration 30066: loss = 0.00237083668231084\n",
      "Iteration 30067: loss = 0.0023708269760171104\n",
      "Iteration 30068: loss = 0.002370817269827713\n",
      "Iteration 30069: loss = 0.002370807563742646\n",
      "Iteration 30070: loss = 0.002370797857761908\n",
      "Iteration 30071: loss = 0.0023707881518854976\n",
      "Iteration 30072: loss = 0.0023707784461134136\n",
      "Iteration 30073: loss = 0.002370768740445655\n",
      "Iteration 30074: loss = 0.0023707590348822196\n",
      "Iteration 30075: loss = 0.0023707493294231074\n",
      "Iteration 30076: loss = 0.002370739624068316\n",
      "Iteration 30077: loss = 0.002370729918817845\n",
      "Iteration 30078: loss = 0.0023707202136716925\n",
      "Iteration 30079: loss = 0.0023707105086298577\n",
      "Iteration 30080: loss = 0.0023707008036923387\n",
      "Iteration 30081: loss = 0.0023706910988591343\n",
      "Iteration 30082: loss = 0.002370681394130243\n",
      "Iteration 30083: loss = 0.002370671689505665\n",
      "Iteration 30084: loss = 0.0023706619849853975\n",
      "Iteration 30085: loss = 0.0023706522805694396\n",
      "Iteration 30086: loss = 0.00237064257625779\n",
      "Iteration 30087: loss = 0.002370632872050447\n",
      "Iteration 30088: loss = 0.0023706231679474104\n",
      "Iteration 30089: loss = 0.0023706134639486782\n",
      "Iteration 30090: loss = 0.0023706037600542485\n",
      "Iteration 30091: loss = 0.0023705940562641207\n",
      "Iteration 30092: loss = 0.0023705843525782936\n",
      "Iteration 30093: loss = 0.0023705746489967655\n",
      "Iteration 30094: loss = 0.002370564945519536\n",
      "Iteration 30095: loss = 0.002370555242146603\n",
      "Iteration 30096: loss = 0.0023705455388779648\n",
      "Iteration 30097: loss = 0.002370535835713621\n",
      "Iteration 30098: loss = 0.00237052613265357\n",
      "Iteration 30099: loss = 0.0023705164296978108\n",
      "Iteration 30100: loss = 0.002370506726846341\n",
      "Iteration 30101: loss = 0.0023704970240991607\n",
      "Iteration 30102: loss = 0.0023704873214562675\n",
      "Iteration 30103: loss = 0.002370477618917661\n",
      "Iteration 30104: loss = 0.002370467916483339\n",
      "Iteration 30105: loss = 0.002370458214153301\n",
      "Iteration 30106: loss = 0.0023704485119275458\n",
      "Iteration 30107: loss = 0.002370438809806071\n",
      "Iteration 30108: loss = 0.0023704291077888766\n",
      "Iteration 30109: loss = 0.002370419405875961\n",
      "Iteration 30110: loss = 0.0023704097040673218\n",
      "Iteration 30111: loss = 0.0023704000023629583\n",
      "Iteration 30112: loss = 0.0023703903007628703\n",
      "Iteration 30113: loss = 0.002370380599267055\n",
      "Iteration 30114: loss = 0.002370370897875512\n",
      "Iteration 30115: loss = 0.0023703611965882397\n",
      "Iteration 30116: loss = 0.002370351495405237\n",
      "Iteration 30117: loss = 0.002370341794326502\n",
      "Iteration 30118: loss = 0.002370332093352034\n",
      "Iteration 30119: loss = 0.002370322392481832\n",
      "Iteration 30120: loss = 0.002370312691715894\n",
      "Iteration 30121: loss = 0.0023703029910542186\n",
      "Iteration 30122: loss = 0.0023702932904968053\n",
      "Iteration 30123: loss = 0.0023702835900436523\n",
      "Iteration 30124: loss = 0.0023702738896947584\n",
      "Iteration 30125: loss = 0.0023702641894501227\n",
      "Iteration 30126: loss = 0.002370254489309743\n",
      "Iteration 30127: loss = 0.002370244789273618\n",
      "Iteration 30128: loss = 0.002370235089341747\n",
      "Iteration 30129: loss = 0.00237022538951413\n",
      "Iteration 30130: loss = 0.0023702156897907624\n",
      "Iteration 30131: loss = 0.002370205990171646\n",
      "Iteration 30132: loss = 0.0023701962906567784\n",
      "Iteration 30133: loss = 0.0023701865912461573\n",
      "Iteration 30134: loss = 0.0023701768919397828\n",
      "Iteration 30135: loss = 0.002370167192737654\n",
      "Iteration 30136: loss = 0.002370157493639768\n",
      "Iteration 30137: loss = 0.0023701477946461236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30138: loss = 0.002370138095756721\n",
      "Iteration 30139: loss = 0.0023701283969715576\n",
      "Iteration 30140: loss = 0.0023701186982906324\n",
      "Iteration 30141: loss = 0.002370108999713945\n",
      "Iteration 30142: loss = 0.0023700993012414928\n",
      "Iteration 30143: loss = 0.0023700896028732757\n",
      "Iteration 30144: loss = 0.002370079904609291\n",
      "Iteration 30145: loss = 0.0023700702064495386\n",
      "Iteration 30146: loss = 0.0023700605083940168\n",
      "Iteration 30147: loss = 0.002370050810442724\n",
      "Iteration 30148: loss = 0.0023700411125956596\n",
      "Iteration 30149: loss = 0.0023700314148528217\n",
      "Iteration 30150: loss = 0.002370021717214209\n",
      "Iteration 30151: loss = 0.00237001201967982\n",
      "Iteration 30152: loss = 0.002370002322249655\n",
      "Iteration 30153: loss = 0.0023699926249237105\n",
      "Iteration 30154: loss = 0.002369982927701987\n",
      "Iteration 30155: loss = 0.002369973230584482\n",
      "Iteration 30156: loss = 0.002369963533571195\n",
      "Iteration 30157: loss = 0.002369953836662124\n",
      "Iteration 30158: loss = 0.002369944139857268\n",
      "Iteration 30159: loss = 0.0023699344431566253\n",
      "Iteration 30160: loss = 0.0023699247465601963\n",
      "Iteration 30161: loss = 0.0023699150500679773\n",
      "Iteration 30162: loss = 0.002369905353679969\n",
      "Iteration 30163: loss = 0.0023698956573961687\n",
      "Iteration 30164: loss = 0.002369885961216576\n",
      "Iteration 30165: loss = 0.002369876265141189\n",
      "Iteration 30166: loss = 0.0023698665691700067\n",
      "Iteration 30167: loss = 0.002369856873303028\n",
      "Iteration 30168: loss = 0.002369847177540251\n",
      "Iteration 30169: loss = 0.0023698374818816757\n",
      "Iteration 30170: loss = 0.0023698277863272994\n",
      "Iteration 30171: loss = 0.002369818090877121\n",
      "Iteration 30172: loss = 0.0023698083955311397\n",
      "Iteration 30173: loss = 0.0023697987002893545\n",
      "Iteration 30174: loss = 0.002369789005151763\n",
      "Iteration 30175: loss = 0.002369779310118365\n",
      "Iteration 30176: loss = 0.002369769615189158\n",
      "Iteration 30177: loss = 0.0023697599203641416\n",
      "Iteration 30178: loss = 0.002369750225643315\n",
      "Iteration 30179: loss = 0.002369740531026676\n",
      "Iteration 30180: loss = 0.002369730836514224\n",
      "Iteration 30181: loss = 0.0023697211421059568\n",
      "Iteration 30182: loss = 0.0023697114478018733\n",
      "Iteration 30183: loss = 0.0023697017536019726\n",
      "Iteration 30184: loss = 0.0023696920595062533\n",
      "Iteration 30185: loss = 0.0023696823655147145\n",
      "Iteration 30186: loss = 0.0023696726716273537\n",
      "Iteration 30187: loss = 0.0023696629778441713\n",
      "Iteration 30188: loss = 0.0023696532841651643\n",
      "Iteration 30189: loss = 0.002369643590590333\n",
      "Iteration 30190: loss = 0.002369633897119675\n",
      "Iteration 30191: loss = 0.0023696242037531895\n",
      "Iteration 30192: loss = 0.0023696145104908743\n",
      "Iteration 30193: loss = 0.0023696048173327293\n",
      "Iteration 30194: loss = 0.002369595124278752\n",
      "Iteration 30195: loss = 0.002369585431328943\n",
      "Iteration 30196: loss = 0.0023695757384832998\n",
      "Iteration 30197: loss = 0.0023695660457418205\n",
      "Iteration 30198: loss = 0.0023695563531045045\n",
      "Iteration 30199: loss = 0.002369546660571351\n",
      "Iteration 30200: loss = 0.0023695369681423577\n",
      "Iteration 30201: loss = 0.002369527275817524\n",
      "Iteration 30202: loss = 0.0023695175835968486\n",
      "Iteration 30203: loss = 0.0023695078914803295\n",
      "Iteration 30204: loss = 0.002369498199467966\n",
      "Iteration 30205: loss = 0.0023694885075597567\n",
      "Iteration 30206: loss = 0.0023694788157557004\n",
      "Iteration 30207: loss = 0.0023694691240557955\n",
      "Iteration 30208: loss = 0.0023694594324600413\n",
      "Iteration 30209: loss = 0.0023694497409684364\n",
      "Iteration 30210: loss = 0.0023694400495809786\n",
      "Iteration 30211: loss = 0.002369430358297667\n",
      "Iteration 30212: loss = 0.002369420667118501\n",
      "Iteration 30213: loss = 0.002369410976043479\n",
      "Iteration 30214: loss = 0.002369401285072599\n",
      "Iteration 30215: loss = 0.002369391594205861\n",
      "Iteration 30216: loss = 0.0023693819034432627\n",
      "Iteration 30217: loss = 0.002369372212784802\n",
      "Iteration 30218: loss = 0.00236936252223048\n",
      "Iteration 30219: loss = 0.0023693528317802943\n",
      "Iteration 30220: loss = 0.0023693431414342422\n",
      "Iteration 30221: loss = 0.0023693334511923243\n",
      "Iteration 30222: loss = 0.002369323761054538\n",
      "Iteration 30223: loss = 0.0023693140710208836\n",
      "Iteration 30224: loss = 0.002369304381091358\n",
      "Iteration 30225: loss = 0.002369294691265961\n",
      "Iteration 30226: loss = 0.0023692850015446906\n",
      "Iteration 30227: loss = 0.002369275311927547\n",
      "Iteration 30228: loss = 0.0023692656224145268\n",
      "Iteration 30229: loss = 0.00236925593300563\n",
      "Iteration 30230: loss = 0.002369246243700855\n",
      "Iteration 30231: loss = 0.002369236554500201\n",
      "Iteration 30232: loss = 0.0023692268654036657\n",
      "Iteration 30233: loss = 0.0023692171764112483\n",
      "Iteration 30234: loss = 0.002369207487522948\n",
      "Iteration 30235: loss = 0.002369197798738763\n",
      "Iteration 30236: loss = 0.0023691881100586917\n",
      "Iteration 30237: loss = 0.002369178421482734\n",
      "Iteration 30238: loss = 0.0023691687330108873\n",
      "Iteration 30239: loss = 0.0023691590446431503\n",
      "Iteration 30240: loss = 0.0023691493563795228\n",
      "Iteration 30241: loss = 0.0023691396682200033\n",
      "Iteration 30242: loss = 0.0023691299801645894\n",
      "Iteration 30243: loss = 0.0023691202922132806\n",
      "Iteration 30244: loss = 0.0023691106043660756\n",
      "Iteration 30245: loss = 0.002369100916622973\n",
      "Iteration 30246: loss = 0.002369091228983972\n",
      "Iteration 30247: loss = 0.0023690815414490704\n",
      "Iteration 30248: loss = 0.0023690718540182675\n",
      "Iteration 30249: loss = 0.002369062166691562\n",
      "Iteration 30250: loss = 0.0023690524794689524\n",
      "Iteration 30251: loss = 0.0023690427923504366\n",
      "Iteration 30252: loss = 0.002369033105336015\n",
      "Iteration 30253: loss = 0.002369023418425685\n",
      "Iteration 30254: loss = 0.0023690137316194466\n",
      "Iteration 30255: loss = 0.002369004044917297\n",
      "Iteration 30256: loss = 0.0023689943583192364\n",
      "Iteration 30257: loss = 0.0023689846718252623\n",
      "Iteration 30258: loss = 0.0023689749854353737\n",
      "Iteration 30259: loss = 0.0023689652991495695\n",
      "Iteration 30260: loss = 0.002368955612967848\n",
      "Iteration 30261: loss = 0.0023689459268902086\n",
      "Iteration 30262: loss = 0.0023689362409166493\n",
      "Iteration 30263: loss = 0.00236892655504717\n",
      "Iteration 30264: loss = 0.0023689168692817677\n",
      "Iteration 30265: loss = 0.0023689071836204426\n",
      "Iteration 30266: loss = 0.002368897498063192\n",
      "Iteration 30267: loss = 0.002368887812610016\n",
      "Iteration 30268: loss = 0.002368878127260912\n",
      "Iteration 30269: loss = 0.0023688684420158797\n",
      "Iteration 30270: loss = 0.002368858756874918\n",
      "Iteration 30271: loss = 0.0023688490718380244\n",
      "Iteration 30272: loss = 0.002368839386905199\n",
      "Iteration 30273: loss = 0.002368829702076439\n",
      "Iteration 30274: loss = 0.0023688200173517444\n",
      "Iteration 30275: loss = 0.0023688103327311138\n",
      "Iteration 30276: loss = 0.002368800648214545\n",
      "Iteration 30277: loss = 0.002368790963802037\n",
      "Iteration 30278: loss = 0.002368781279493589\n",
      "Iteration 30279: loss = 0.0023687715952892\n",
      "Iteration 30280: loss = 0.002368761911188868\n",
      "Iteration 30281: loss = 0.0023687522271925907\n",
      "Iteration 30282: loss = 0.0023687425433003687\n",
      "Iteration 30283: loss = 0.0023687328595122003\n",
      "Iteration 30284: loss = 0.002368723175828084\n",
      "Iteration 30285: loss = 0.002368713492248018\n",
      "Iteration 30286: loss = 0.002368703808772002\n",
      "Iteration 30287: loss = 0.002368694125400033\n",
      "Iteration 30288: loss = 0.002368684442132112\n",
      "Iteration 30289: loss = 0.0023686747589682353\n",
      "Iteration 30290: loss = 0.0023686650759084035\n",
      "Iteration 30291: loss = 0.002368655392952615\n",
      "Iteration 30292: loss = 0.002368645710100867\n",
      "Iteration 30293: loss = 0.0023686360273531605\n",
      "Iteration 30294: loss = 0.0023686263447094923\n",
      "Iteration 30295: loss = 0.0023686166621698624\n",
      "Iteration 30296: loss = 0.002368606979734269\n",
      "Iteration 30297: loss = 0.00236859729740271\n",
      "Iteration 30298: loss = 0.0023685876151751857\n",
      "Iteration 30299: loss = 0.0023685779330516934\n",
      "Iteration 30300: loss = 0.002368568251032233\n",
      "Iteration 30301: loss = 0.002368558569116802\n",
      "Iteration 30302: loss = 0.0023685488873054\n",
      "Iteration 30303: loss = 0.002368539205598025\n",
      "Iteration 30304: loss = 0.002368529523994677\n",
      "Iteration 30305: loss = 0.0023685198424953533\n",
      "Iteration 30306: loss = 0.002368510161100053\n",
      "Iteration 30307: loss = 0.002368500479808775\n",
      "Iteration 30308: loss = 0.0023684907986215183\n",
      "Iteration 30309: loss = 0.002368481117538281\n",
      "Iteration 30310: loss = 0.0023684714365590624\n",
      "Iteration 30311: loss = 0.00236846175568386\n",
      "Iteration 30312: loss = 0.002368452074912674\n",
      "Iteration 30313: loss = 0.0023684423942455022\n",
      "Iteration 30314: loss = 0.002368432713682344\n",
      "Iteration 30315: loss = 0.0023684230332231973\n",
      "Iteration 30316: loss = 0.002368413352868062\n",
      "Iteration 30317: loss = 0.002368403672616935\n",
      "Iteration 30318: loss = 0.0023683939924698163\n",
      "Iteration 30319: loss = 0.0023683843124267046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30320: loss = 0.0023683746324875982\n",
      "Iteration 30321: loss = 0.002368364952652496\n",
      "Iteration 30322: loss = 0.0023683552729213964\n",
      "Iteration 30323: loss = 0.0023683455932942987\n",
      "Iteration 30324: loss = 0.0023683359137712007\n",
      "Iteration 30325: loss = 0.0023683262343521024\n",
      "Iteration 30326: loss = 0.0023683165550370013\n",
      "Iteration 30327: loss = 0.0023683068758258968\n",
      "Iteration 30328: loss = 0.002368297196718787\n",
      "Iteration 30329: loss = 0.0023682875177156717\n",
      "Iteration 30330: loss = 0.0023682778388165486\n",
      "Iteration 30331: loss = 0.002368268160021416\n",
      "Iteration 30332: loss = 0.0023682584813302736\n",
      "Iteration 30333: loss = 0.0023682488027431206\n",
      "Iteration 30334: loss = 0.0023682391242599546\n",
      "Iteration 30335: loss = 0.0023682294458807745\n",
      "Iteration 30336: loss = 0.002368219767605579\n",
      "Iteration 30337: loss = 0.0023682100894343674\n",
      "Iteration 30338: loss = 0.0023682004113671374\n",
      "Iteration 30339: loss = 0.002368190733403889\n",
      "Iteration 30340: loss = 0.0023681810555446193\n",
      "Iteration 30341: loss = 0.002368171377789328\n",
      "Iteration 30342: loss = 0.002368161700138014\n",
      "Iteration 30343: loss = 0.002368152022590676\n",
      "Iteration 30344: loss = 0.0023681423451473115\n",
      "Iteration 30345: loss = 0.0023681326678079216\n",
      "Iteration 30346: loss = 0.0023681229905725023\n",
      "Iteration 30347: loss = 0.0023681133134410537\n",
      "Iteration 30348: loss = 0.0023681036364135745\n",
      "Iteration 30349: loss = 0.0023680939594900633\n",
      "Iteration 30350: loss = 0.0023680842826705184\n",
      "Iteration 30351: loss = 0.0023680746059549394\n",
      "Iteration 30352: loss = 0.002368064929343324\n",
      "Iteration 30353: loss = 0.0023680552528356717\n",
      "Iteration 30354: loss = 0.0023680455764319808\n",
      "Iteration 30355: loss = 0.0023680359001322505\n",
      "Iteration 30356: loss = 0.002368026223936478\n",
      "Iteration 30357: loss = 0.002368016547844664\n",
      "Iteration 30358: loss = 0.0023680068718568053\n",
      "Iteration 30359: loss = 0.002367997195972903\n",
      "Iteration 30360: loss = 0.002367987520192954\n",
      "Iteration 30361: loss = 0.002367977844516957\n",
      "Iteration 30362: loss = 0.0023679681689449117\n",
      "Iteration 30363: loss = 0.0023679584934768157\n",
      "Iteration 30364: loss = 0.0023679488181126687\n",
      "Iteration 30365: loss = 0.002367939142852468\n",
      "Iteration 30366: loss = 0.0023679294676962146\n",
      "Iteration 30367: loss = 0.002367919792643905\n",
      "Iteration 30368: loss = 0.002367910117695539\n",
      "Iteration 30369: loss = 0.0023679004428511156\n",
      "Iteration 30370: loss = 0.0023678907681106325\n",
      "Iteration 30371: loss = 0.0023678810934740888\n",
      "Iteration 30372: loss = 0.0023678714189414836\n",
      "Iteration 30373: loss = 0.0023678617445128153\n",
      "Iteration 30374: loss = 0.0023678520701880825\n",
      "Iteration 30375: loss = 0.0023678423959672843\n",
      "Iteration 30376: loss = 0.0023678327218504187\n",
      "Iteration 30377: loss = 0.002367823047837485\n",
      "Iteration 30378: loss = 0.002367813373928482\n",
      "Iteration 30379: loss = 0.002367803700123408\n",
      "Iteration 30380: loss = 0.0023677940264222614\n",
      "Iteration 30381: loss = 0.002367784352825042\n",
      "Iteration 30382: loss = 0.002367774679331748\n",
      "Iteration 30383: loss = 0.0023677650059423776\n",
      "Iteration 30384: loss = 0.0023677553326569302\n",
      "Iteration 30385: loss = 0.002367745659475404\n",
      "Iteration 30386: loss = 0.0023677359863977982\n",
      "Iteration 30387: loss = 0.0023677263134241114\n",
      "Iteration 30388: loss = 0.0023677166405543415\n",
      "Iteration 30389: loss = 0.0023677069677884885\n",
      "Iteration 30390: loss = 0.0023676972951265497\n",
      "Iteration 30391: loss = 0.002367687622568525\n",
      "Iteration 30392: loss = 0.002367677950114413\n",
      "Iteration 30393: loss = 0.0023676682777642117\n",
      "Iteration 30394: loss = 0.00236765860551792\n",
      "Iteration 30395: loss = 0.002367648933375537\n",
      "Iteration 30396: loss = 0.0023676392613370613\n",
      "Iteration 30397: loss = 0.0023676295894024917\n",
      "Iteration 30398: loss = 0.002367619917571826\n",
      "Iteration 30399: loss = 0.002367610245845065\n",
      "Iteration 30400: loss = 0.0023676005742222046\n",
      "Iteration 30401: loss = 0.0023675909027032456\n",
      "Iteration 30402: loss = 0.0023675812312881858\n",
      "Iteration 30403: loss = 0.0023675715599770246\n",
      "Iteration 30404: loss = 0.002367561888769759\n",
      "Iteration 30405: loss = 0.0023675522176663903\n",
      "Iteration 30406: loss = 0.002367542546666916\n",
      "Iteration 30407: loss = 0.002367532875771334\n",
      "Iteration 30408: loss = 0.002367523204979644\n",
      "Iteration 30409: loss = 0.0023675135342918446\n",
      "Iteration 30410: loss = 0.0023675038637079337\n",
      "Iteration 30411: loss = 0.002367494193227911\n",
      "Iteration 30412: loss = 0.0023674845228517753\n",
      "Iteration 30413: loss = 0.0023674748525795244\n",
      "Iteration 30414: loss = 0.0023674651824111574\n",
      "Iteration 30415: loss = 0.002367455512346673\n",
      "Iteration 30416: loss = 0.0023674458423860695\n",
      "Iteration 30417: loss = 0.002367436172529347\n",
      "Iteration 30418: loss = 0.0023674265027765033\n",
      "Iteration 30419: loss = 0.0023674168331275364\n",
      "Iteration 30420: loss = 0.002367407163582446\n",
      "Iteration 30421: loss = 0.0023673974941412304\n",
      "Iteration 30422: loss = 0.0023673878248038883\n",
      "Iteration 30423: loss = 0.0023673781555704192\n",
      "Iteration 30424: loss = 0.0023673684864408207\n",
      "Iteration 30425: loss = 0.0023673588174150917\n",
      "Iteration 30426: loss = 0.002367349148493231\n",
      "Iteration 30427: loss = 0.0023673394796752388\n",
      "Iteration 30428: loss = 0.002367329810961111\n",
      "Iteration 30429: loss = 0.0023673201423508486\n",
      "Iteration 30430: loss = 0.002367310473844449\n",
      "Iteration 30431: loss = 0.002367300805441912\n",
      "Iteration 30432: loss = 0.0023672911371432352\n",
      "Iteration 30433: loss = 0.0023672814689484176\n",
      "Iteration 30434: loss = 0.0023672718008574583\n",
      "Iteration 30435: loss = 0.0023672621328703564\n",
      "Iteration 30436: loss = 0.0023672524649871094\n",
      "Iteration 30437: loss = 0.002367242797207717\n",
      "Iteration 30438: loss = 0.0023672331295321774\n",
      "Iteration 30439: loss = 0.0023672234619604893\n",
      "Iteration 30440: loss = 0.0023672137944926513\n",
      "Iteration 30441: loss = 0.002367204127128663\n",
      "Iteration 30442: loss = 0.002367194459868522\n",
      "Iteration 30443: loss = 0.0023671847927122284\n",
      "Iteration 30444: loss = 0.002367175125659779\n",
      "Iteration 30445: loss = 0.002367165458711174\n",
      "Iteration 30446: loss = 0.002367155791866411\n",
      "Iteration 30447: loss = 0.002367146125125491\n",
      "Iteration 30448: loss = 0.0023671364584884097\n",
      "Iteration 30449: loss = 0.002367126791955167\n",
      "Iteration 30450: loss = 0.0023671171255257623\n",
      "Iteration 30451: loss = 0.0023671074592001937\n",
      "Iteration 30452: loss = 0.00236709779297846\n",
      "Iteration 30453: loss = 0.0023670881268605596\n",
      "Iteration 30454: loss = 0.002367078460846491\n",
      "Iteration 30455: loss = 0.0023670687949362544\n",
      "Iteration 30456: loss = 0.0023670591291298474\n",
      "Iteration 30457: loss = 0.002367049463427268\n",
      "Iteration 30458: loss = 0.0023670397978285164\n",
      "Iteration 30459: loss = 0.0023670301323335907\n",
      "Iteration 30460: loss = 0.0023670204669424895\n",
      "Iteration 30461: loss = 0.002367010801655211\n",
      "Iteration 30462: loss = 0.002367001136471755\n",
      "Iteration 30463: loss = 0.00236699147139212\n",
      "Iteration 30464: loss = 0.0023669818064163037\n",
      "Iteration 30465: loss = 0.002366972141544305\n",
      "Iteration 30466: loss = 0.0023669624767761245\n",
      "Iteration 30467: loss = 0.0023669528121117588\n",
      "Iteration 30468: loss = 0.0023669431475512076\n",
      "Iteration 30469: loss = 0.002366933483094469\n",
      "Iteration 30470: loss = 0.002366923818741542\n",
      "Iteration 30471: loss = 0.002366914154492425\n",
      "Iteration 30472: loss = 0.002366904490347118\n",
      "Iteration 30473: loss = 0.0023668948263056177\n",
      "Iteration 30474: loss = 0.0023668851623679247\n",
      "Iteration 30475: loss = 0.0023668754985340366\n",
      "Iteration 30476: loss = 0.002366865834803952\n",
      "Iteration 30477: loss = 0.002366856171177671\n",
      "Iteration 30478: loss = 0.00236684650765519\n",
      "Iteration 30479: loss = 0.00236683684423651\n",
      "Iteration 30480: loss = 0.0023668271809216285\n",
      "Iteration 30481: loss = 0.002366817517710545\n",
      "Iteration 30482: loss = 0.0023668078546032565\n",
      "Iteration 30483: loss = 0.0023667981915997632\n",
      "Iteration 30484: loss = 0.002366788528700064\n",
      "Iteration 30485: loss = 0.0023667788659041565\n",
      "Iteration 30486: loss = 0.00236676920321204\n",
      "Iteration 30487: loss = 0.0023667595406237137\n",
      "Iteration 30488: loss = 0.0023667498781391755\n",
      "Iteration 30489: loss = 0.002366740215758424\n",
      "Iteration 30490: loss = 0.002366730553481459\n",
      "Iteration 30491: loss = 0.0023667208913082777\n",
      "Iteration 30492: loss = 0.0023667112292388806\n",
      "Iteration 30493: loss = 0.0023667015672732645\n",
      "Iteration 30494: loss = 0.0023666919054114296\n",
      "Iteration 30495: loss = 0.002366682243653374\n",
      "Iteration 30496: loss = 0.0023666725819990963\n",
      "Iteration 30497: loss = 0.0023666629204485957\n",
      "Iteration 30498: loss = 0.00236665325900187\n",
      "Iteration 30499: loss = 0.002366643597658919\n",
      "Iteration 30500: loss = 0.002366633936419741\n",
      "Iteration 30501: loss = 0.0023666242752843347\n",
      "Iteration 30502: loss = 0.002366614614252698\n",
      "Iteration 30503: loss = 0.002366604953324831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30504: loss = 0.0023665952925007315\n",
      "Iteration 30505: loss = 0.0023665856317803984\n",
      "Iteration 30506: loss = 0.0023665759711638304\n",
      "Iteration 30507: loss = 0.0023665663106510267\n",
      "Iteration 30508: loss = 0.0023665566502419854\n",
      "Iteration 30509: loss = 0.0023665469899367053\n",
      "Iteration 30510: loss = 0.002366537329735185\n",
      "Iteration 30511: loss = 0.0023665276696374243\n",
      "Iteration 30512: loss = 0.00236651800964342\n",
      "Iteration 30513: loss = 0.002366508349753172\n",
      "Iteration 30514: loss = 0.0023664986899666787\n",
      "Iteration 30515: loss = 0.0023664890302839393\n",
      "Iteration 30516: loss = 0.002366479370704952\n",
      "Iteration 30517: loss = 0.0023664697112297157\n",
      "Iteration 30518: loss = 0.0023664600518582294\n",
      "Iteration 30519: loss = 0.0023664503925904913\n",
      "Iteration 30520: loss = 0.0023664407334264997\n",
      "Iteration 30521: loss = 0.0023664310743662545\n",
      "Iteration 30522: loss = 0.002366421415409754\n",
      "Iteration 30523: loss = 0.0023664117565569964\n",
      "Iteration 30524: loss = 0.0023664020978079806\n",
      "Iteration 30525: loss = 0.002366392439162706\n",
      "Iteration 30526: loss = 0.00236638278062117\n",
      "Iteration 30527: loss = 0.0023663731221833725\n",
      "Iteration 30528: loss = 0.002366363463849312\n",
      "Iteration 30529: loss = 0.0023663538056189867\n",
      "Iteration 30530: loss = 0.0023663441474923953\n",
      "Iteration 30531: loss = 0.0023663344894695373\n",
      "Iteration 30532: loss = 0.0023663248315504105\n",
      "Iteration 30533: loss = 0.0023663151737350146\n",
      "Iteration 30534: loss = 0.0023663055160233477\n",
      "Iteration 30535: loss = 0.0023662958584154074\n",
      "Iteration 30536: loss = 0.0023662862009111944\n",
      "Iteration 30537: loss = 0.0023662765435107065\n",
      "Iteration 30538: loss = 0.0023662668862139426\n",
      "Iteration 30539: loss = 0.0023662572290209013\n",
      "Iteration 30540: loss = 0.0023662475719315812\n",
      "Iteration 30541: loss = 0.002366237914945981\n",
      "Iteration 30542: loss = 0.0023662282580640993\n",
      "Iteration 30543: loss = 0.0023662186012859353\n",
      "Iteration 30544: loss = 0.0023662089446114874\n",
      "Iteration 30545: loss = 0.0023661992880407547\n",
      "Iteration 30546: loss = 0.002366189631573735\n",
      "Iteration 30547: loss = 0.0023661799752104275\n",
      "Iteration 30548: loss = 0.0023661703189508314\n",
      "Iteration 30549: loss = 0.0023661606627949448\n",
      "Iteration 30550: loss = 0.0023661510067427664\n",
      "Iteration 30551: loss = 0.002366141350794296\n",
      "Iteration 30552: loss = 0.0023661316949495303\n",
      "Iteration 30553: loss = 0.002366122039208469\n",
      "Iteration 30554: loss = 0.0023661123835711116\n",
      "Iteration 30555: loss = 0.002366102728037456\n",
      "Iteration 30556: loss = 0.002366093072607501\n",
      "Iteration 30557: loss = 0.0023660834172812457\n",
      "Iteration 30558: loss = 0.002366073762058688\n",
      "Iteration 30559: loss = 0.0023660641069398274\n",
      "Iteration 30560: loss = 0.002366054451924662\n",
      "Iteration 30561: loss = 0.002366044797013191\n",
      "Iteration 30562: loss = 0.002366035142205412\n",
      "Iteration 30563: loss = 0.002366025487501326\n",
      "Iteration 30564: loss = 0.0023660158329009296\n",
      "Iteration 30565: loss = 0.0023660061784042227\n",
      "Iteration 30566: loss = 0.002365996524011203\n",
      "Iteration 30567: loss = 0.00236598686972187\n",
      "Iteration 30568: loss = 0.0023659772155362226\n",
      "Iteration 30569: loss = 0.002365967561454258\n",
      "Iteration 30570: loss = 0.0023659579074759773\n",
      "Iteration 30571: loss = 0.002365948253601377\n",
      "Iteration 30572: loss = 0.0023659385998304567\n",
      "Iteration 30573: loss = 0.0023659289461632157\n",
      "Iteration 30574: loss = 0.0023659192925996517\n",
      "Iteration 30575: loss = 0.002365909639139764\n",
      "Iteration 30576: loss = 0.002365899985783551\n",
      "Iteration 30577: loss = 0.0023658903325310117\n",
      "Iteration 30578: loss = 0.002365880679382144\n",
      "Iteration 30579: loss = 0.002365871026336948\n",
      "Iteration 30580: loss = 0.002365861373395422\n",
      "Iteration 30581: loss = 0.0023658517205575637\n",
      "Iteration 30582: loss = 0.0023658420678233727\n",
      "Iteration 30583: loss = 0.002365832415192848\n",
      "Iteration 30584: loss = 0.002365822762665987\n",
      "Iteration 30585: loss = 0.0023658131102427898\n",
      "Iteration 30586: loss = 0.0023658034579232545\n",
      "Iteration 30587: loss = 0.00236579380570738\n",
      "Iteration 30588: loss = 0.0023657841535951645\n",
      "Iteration 30589: loss = 0.002365774501586607\n",
      "Iteration 30590: loss = 0.0023657648496817067\n",
      "Iteration 30591: loss = 0.0023657551978804622\n",
      "Iteration 30592: loss = 0.0023657455461828713\n",
      "Iteration 30593: loss = 0.002365735894588933\n",
      "Iteration 30594: loss = 0.0023657262430986474\n",
      "Iteration 30595: loss = 0.0023657165917120113\n",
      "Iteration 30596: loss = 0.0023657069404290246\n",
      "Iteration 30597: loss = 0.0023656972892496853\n",
      "Iteration 30598: loss = 0.0023656876381739932\n",
      "Iteration 30599: loss = 0.0023656779872019457\n",
      "Iteration 30600: loss = 0.002365668336333543\n",
      "Iteration 30601: loss = 0.0023656586855687815\n",
      "Iteration 30602: loss = 0.002365649034907662\n",
      "Iteration 30603: loss = 0.0023656393843501824\n",
      "Iteration 30604: loss = 0.002365629733896342\n",
      "Iteration 30605: loss = 0.002365620083546138\n",
      "Iteration 30606: loss = 0.0023656104332995713\n",
      "Iteration 30607: loss = 0.002365600783156639\n",
      "Iteration 30608: loss = 0.0023655911331173407\n",
      "Iteration 30609: loss = 0.002365581483181674\n",
      "Iteration 30610: loss = 0.0023655718333496388\n",
      "Iteration 30611: loss = 0.0023655621836212335\n",
      "Iteration 30612: loss = 0.002365552533996456\n",
      "Iteration 30613: loss = 0.002365542884475306\n",
      "Iteration 30614: loss = 0.0023655332350577814\n",
      "Iteration 30615: loss = 0.0023655235857438822\n",
      "Iteration 30616: loss = 0.0023655139365336055\n",
      "Iteration 30617: loss = 0.002365504287426951\n",
      "Iteration 30618: loss = 0.002365494638423917\n",
      "Iteration 30619: loss = 0.002365484989524503\n",
      "Iteration 30620: loss = 0.002365475340728707\n",
      "Iteration 30621: loss = 0.0023654656920365272\n",
      "Iteration 30622: loss = 0.0023654560434479637\n",
      "Iteration 30623: loss = 0.002365446394963014\n",
      "Iteration 30624: loss = 0.0023654367465816774\n",
      "Iteration 30625: loss = 0.0023654270983039525\n",
      "Iteration 30626: loss = 0.002365417450129838\n",
      "Iteration 30627: loss = 0.002365407802059332\n",
      "Iteration 30628: loss = 0.002365398154092434\n",
      "Iteration 30629: loss = 0.0023653885062291436\n",
      "Iteration 30630: loss = 0.002365378858469457\n",
      "Iteration 30631: loss = 0.002365369210813375\n",
      "Iteration 30632: loss = 0.002365359563260896\n",
      "Iteration 30633: loss = 0.002365349915812017\n",
      "Iteration 30634: loss = 0.0023653402684667394\n",
      "Iteration 30635: loss = 0.0023653306212250607\n",
      "Iteration 30636: loss = 0.0023653209740869784\n",
      "Iteration 30637: loss = 0.002365311327052493\n",
      "Iteration 30638: loss = 0.0023653016801216025\n",
      "Iteration 30639: loss = 0.002365292033294306\n",
      "Iteration 30640: loss = 0.002365282386570601\n",
      "Iteration 30641: loss = 0.002365272739950487\n",
      "Iteration 30642: loss = 0.0023652630934339627\n",
      "Iteration 30643: loss = 0.0023652534470210273\n",
      "Iteration 30644: loss = 0.002365243800711679\n",
      "Iteration 30645: loss = 0.002365234154505917\n",
      "Iteration 30646: loss = 0.0023652245084037392\n",
      "Iteration 30647: loss = 0.002365214862405145\n",
      "Iteration 30648: loss = 0.0023652052165101323\n",
      "Iteration 30649: loss = 0.0023651955707187007\n",
      "Iteration 30650: loss = 0.002365185925030848\n",
      "Iteration 30651: loss = 0.002365176279446574\n",
      "Iteration 30652: loss = 0.002365166633965877\n",
      "Iteration 30653: loss = 0.0023651569885887553\n",
      "Iteration 30654: loss = 0.0023651473433152076\n",
      "Iteration 30655: loss = 0.0023651376981452333\n",
      "Iteration 30656: loss = 0.00236512805307883\n",
      "Iteration 30657: loss = 0.0023651184081159987\n",
      "Iteration 30658: loss = 0.002365108763256735\n",
      "Iteration 30659: loss = 0.0023650991185010404\n",
      "Iteration 30660: loss = 0.002365089473848911\n",
      "Iteration 30661: loss = 0.002365079829300348\n",
      "Iteration 30662: loss = 0.0023650701848553484\n",
      "Iteration 30663: loss = 0.0023650605405139114\n",
      "Iteration 30664: loss = 0.002365050896276036\n",
      "Iteration 30665: loss = 0.0023650412521417207\n",
      "Iteration 30666: loss = 0.002365031608110964\n",
      "Iteration 30667: loss = 0.0023650219641837647\n",
      "Iteration 30668: loss = 0.0023650123203601226\n",
      "Iteration 30669: loss = 0.0023650026766400345\n",
      "Iteration 30670: loss = 0.0023649930330235007\n",
      "Iteration 30671: loss = 0.0023649833895105186\n",
      "Iteration 30672: loss = 0.0023649737461010887\n",
      "Iteration 30673: loss = 0.002364964102795207\n",
      "Iteration 30674: loss = 0.002364954459592875\n",
      "Iteration 30675: loss = 0.00236494481649409\n",
      "Iteration 30676: loss = 0.0023649351734988506\n",
      "Iteration 30677: loss = 0.0023649255306071567\n",
      "Iteration 30678: loss = 0.002364915887819005\n",
      "Iteration 30679: loss = 0.0023649062451343962\n",
      "Iteration 30680: loss = 0.0023648966025533276\n",
      "Iteration 30681: loss = 0.0023648869600757993\n",
      "Iteration 30682: loss = 0.002364877317701808\n",
      "Iteration 30683: loss = 0.0023648676754313546\n",
      "Iteration 30684: loss = 0.0023648580332644364\n",
      "Iteration 30685: loss = 0.0023648483912010526\n",
      "Iteration 30686: loss = 0.0023648387492412022\n",
      "Iteration 30687: loss = 0.0023648291073848833\n",
      "Iteration 30688: loss = 0.002364819465632095\n",
      "Iteration 30689: loss = 0.0023648098239828354\n",
      "Iteration 30690: loss = 0.0023648001824371047\n",
      "Iteration 30691: loss = 0.0023647905409948993\n",
      "Iteration 30692: loss = 0.0023647808996562205\n",
      "Iteration 30693: loss = 0.002364771258421065\n",
      "Iteration 30694: loss = 0.002364761617289432\n",
      "Iteration 30695: loss = 0.0023647519762613205\n",
      "Iteration 30696: loss = 0.0023647423353367297\n",
      "Iteration 30697: loss = 0.0023647326945156577\n",
      "Iteration 30698: loss = 0.002364723053798103\n",
      "Iteration 30699: loss = 0.002364713413184065\n",
      "Iteration 30700: loss = 0.0023647037726735416\n",
      "Iteration 30701: loss = 0.0023646941322665324\n",
      "Iteration 30702: loss = 0.002364684491963035\n",
      "Iteration 30703: loss = 0.002364674851763049\n",
      "Iteration 30704: loss = 0.002364665211666573\n",
      "Iteration 30705: loss = 0.0023646555716736054\n",
      "Iteration 30706: loss = 0.0023646459317841454\n",
      "Iteration 30707: loss = 0.002364636291998191\n",
      "Iteration 30708: loss = 0.0023646266523157417\n",
      "Iteration 30709: loss = 0.0023646170127367954\n",
      "Iteration 30710: loss = 0.0023646073732613514\n",
      "Iteration 30711: loss = 0.0023645977338894084\n",
      "Iteration 30712: loss = 0.0023645880946209646\n",
      "Iteration 30713: loss = 0.0023645784554560197\n",
      "Iteration 30714: loss = 0.0023645688163945714\n",
      "Iteration 30715: loss = 0.0023645591774366192\n",
      "Iteration 30716: loss = 0.0023645495385821607\n",
      "Iteration 30717: loss = 0.0023645398998311954\n",
      "Iteration 30718: loss = 0.0023645302611837228\n",
      "Iteration 30719: loss = 0.0023645206226397394\n",
      "Iteration 30720: loss = 0.002364510984199246\n",
      "Iteration 30721: loss = 0.0023645013458622405\n",
      "Iteration 30722: loss = 0.0023644917076287215\n",
      "Iteration 30723: loss = 0.0023644820694986883\n",
      "Iteration 30724: loss = 0.002364472431472139\n",
      "Iteration 30725: loss = 0.002364462793549073\n",
      "Iteration 30726: loss = 0.0023644531557294878\n",
      "Iteration 30727: loss = 0.0023644435180133834\n",
      "Iteration 30728: loss = 0.002364433880400757\n",
      "Iteration 30729: loss = 0.002364424242891609\n",
      "Iteration 30730: loss = 0.002364414605485937\n",
      "Iteration 30731: loss = 0.0023644049681837407\n",
      "Iteration 30732: loss = 0.0023643953309850176\n",
      "Iteration 30733: loss = 0.002364385693889767\n",
      "Iteration 30734: loss = 0.0023643760568979877\n",
      "Iteration 30735: loss = 0.0023643664200096786\n",
      "Iteration 30736: loss = 0.002364356783224838\n",
      "Iteration 30737: loss = 0.002364347146543465\n",
      "Iteration 30738: loss = 0.0023643375099655583\n",
      "Iteration 30739: loss = 0.0023643278734911156\n",
      "Iteration 30740: loss = 0.0023643182371201367\n",
      "Iteration 30741: loss = 0.00236430860085262\n",
      "Iteration 30742: loss = 0.0023642989646885646\n",
      "Iteration 30743: loss = 0.0023642893286279685\n",
      "Iteration 30744: loss = 0.0023642796926708304\n",
      "Iteration 30745: loss = 0.00236427005681715\n",
      "Iteration 30746: loss = 0.002364260421066925\n",
      "Iteration 30747: loss = 0.0023642507854201543\n",
      "Iteration 30748: loss = 0.002364241149876837\n",
      "Iteration 30749: loss = 0.0023642315144369714\n",
      "Iteration 30750: loss = 0.002364221879100557\n",
      "Iteration 30751: loss = 0.0023642122438675916\n",
      "Iteration 30752: loss = 0.0023642026087380735\n",
      "Iteration 30753: loss = 0.0023641929737120036\n",
      "Iteration 30754: loss = 0.0023641833387893782\n",
      "Iteration 30755: loss = 0.002364173703970197\n",
      "Iteration 30756: loss = 0.002364164069254459\n",
      "Iteration 30757: loss = 0.0023641544346421632\n",
      "Iteration 30758: loss = 0.0023641448001333067\n",
      "Iteration 30759: loss = 0.0023641351657278896\n",
      "Iteration 30760: loss = 0.00236412553142591\n",
      "Iteration 30761: loss = 0.002364115897227367\n",
      "Iteration 30762: loss = 0.0023641062631322594\n",
      "Iteration 30763: loss = 0.002364096629140586\n",
      "Iteration 30764: loss = 0.0023640869952523446\n",
      "Iteration 30765: loss = 0.0023640773614675347\n",
      "Iteration 30766: loss = 0.0023640677277861543\n",
      "Iteration 30767: loss = 0.0023640580942082038\n",
      "Iteration 30768: loss = 0.002364048460733679\n",
      "Iteration 30769: loss = 0.002364038827362582\n",
      "Iteration 30770: loss = 0.0023640291940949095\n",
      "Iteration 30771: loss = 0.0023640195609306597\n",
      "Iteration 30772: loss = 0.0023640099278698333\n",
      "Iteration 30773: loss = 0.002364000294912427\n",
      "Iteration 30774: loss = 0.0023639906620584405\n",
      "Iteration 30775: loss = 0.0023639810293078736\n",
      "Iteration 30776: loss = 0.0023639713966607224\n",
      "Iteration 30777: loss = 0.0023639617641169884\n",
      "Iteration 30778: loss = 0.0023639521316766677\n",
      "Iteration 30779: loss = 0.002363942499339761\n",
      "Iteration 30780: loss = 0.0023639328671062657\n",
      "Iteration 30781: loss = 0.002363923234976182\n",
      "Iteration 30782: loss = 0.002363913602949507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30783: loss = 0.002363903971026241\n",
      "Iteration 30784: loss = 0.002363894339206381\n",
      "Iteration 30785: loss = 0.0023638847074899266\n",
      "Iteration 30786: loss = 0.002363875075876877\n",
      "Iteration 30787: loss = 0.00236386544436723\n",
      "Iteration 30788: loss = 0.0023638558129609848\n",
      "Iteration 30789: loss = 0.00236384618165814\n",
      "Iteration 30790: loss = 0.002363836550458694\n",
      "Iteration 30791: loss = 0.0023638269193626466\n",
      "Iteration 30792: loss = 0.002363817288369995\n",
      "Iteration 30793: loss = 0.002363807657480739\n",
      "Iteration 30794: loss = 0.002363798026694877\n",
      "Iteration 30795: loss = 0.0023637883960124072\n",
      "Iteration 30796: loss = 0.002363778765433329\n",
      "Iteration 30797: loss = 0.002363769134957642\n",
      "Iteration 30798: loss = 0.0023637595045853423\n",
      "Iteration 30799: loss = 0.0023637498743164305\n",
      "Iteration 30800: loss = 0.0023637402441509057\n",
      "Iteration 30801: loss = 0.0023637306140887656\n",
      "Iteration 30802: loss = 0.0023637209841300085\n",
      "Iteration 30803: loss = 0.0023637113542746345\n",
      "Iteration 30804: loss = 0.0023637017245226417\n",
      "Iteration 30805: loss = 0.0023636920948740285\n",
      "Iteration 30806: loss = 0.0023636824653287935\n",
      "Iteration 30807: loss = 0.0023636728358869363\n",
      "Iteration 30808: loss = 0.0023636632065484543\n",
      "Iteration 30809: loss = 0.002363653577313347\n",
      "Iteration 30810: loss = 0.002363643948181614\n",
      "Iteration 30811: loss = 0.0023636343191532527\n",
      "Iteration 30812: loss = 0.0023636246902282616\n",
      "Iteration 30813: loss = 0.0023636150614066405\n",
      "Iteration 30814: loss = 0.002363605432688388\n",
      "Iteration 30815: loss = 0.002363595804073502\n",
      "Iteration 30816: loss = 0.0023635861755619816\n",
      "Iteration 30817: loss = 0.002363576547153826\n",
      "Iteration 30818: loss = 0.002363566918849033\n",
      "Iteration 30819: loss = 0.0023635572906476024\n",
      "Iteration 30820: loss = 0.0023635476625495314\n",
      "Iteration 30821: loss = 0.0023635380345548205\n",
      "Iteration 30822: loss = 0.0023635284066634674\n",
      "Iteration 30823: loss = 0.002363518778875471\n",
      "Iteration 30824: loss = 0.0023635091511908296\n",
      "Iteration 30825: loss = 0.0023634995236095423\n",
      "Iteration 30826: loss = 0.002363489896131608\n",
      "Iteration 30827: loss = 0.002363480268757025\n",
      "Iteration 30828: loss = 0.0023634706414857928\n",
      "Iteration 30829: loss = 0.002363461014317909\n",
      "Iteration 30830: loss = 0.002363451387253373\n",
      "Iteration 30831: loss = 0.002363441760292183\n",
      "Iteration 30832: loss = 0.002363432133434338\n",
      "Iteration 30833: loss = 0.002363422506679838\n",
      "Iteration 30834: loss = 0.0023634128800286793\n",
      "Iteration 30835: loss = 0.002363403253480862\n",
      "Iteration 30836: loss = 0.002363393627036385\n",
      "Iteration 30837: loss = 0.0023633840006952464\n",
      "Iteration 30838: loss = 0.002363374374457445\n",
      "Iteration 30839: loss = 0.00236336474832298\n",
      "Iteration 30840: loss = 0.00236335512229185\n",
      "Iteration 30841: loss = 0.0023633454963640534\n",
      "Iteration 30842: loss = 0.0023633358705395886\n",
      "Iteration 30843: loss = 0.0023633262448184552\n",
      "Iteration 30844: loss = 0.0023633166192006507\n",
      "Iteration 30845: loss = 0.002363306993686175\n",
      "Iteration 30846: loss = 0.0023632973682750262\n",
      "Iteration 30847: loss = 0.0023632877429672038\n",
      "Iteration 30848: loss = 0.0023632781177627053\n",
      "Iteration 30849: loss = 0.0023632684926615305\n",
      "Iteration 30850: loss = 0.0023632588676636766\n",
      "Iteration 30851: loss = 0.002363249242769144\n",
      "Iteration 30852: loss = 0.002363239617977931\n",
      "Iteration 30853: loss = 0.0023632299932900363\n",
      "Iteration 30854: loss = 0.0023632203687054573\n",
      "Iteration 30855: loss = 0.002363210744224194\n",
      "Iteration 30856: loss = 0.0023632011198462454\n",
      "Iteration 30857: loss = 0.0023631914955716095\n",
      "Iteration 30858: loss = 0.0023631818714002855\n",
      "Iteration 30859: loss = 0.002363172247332272\n",
      "Iteration 30860: loss = 0.002363162623367567\n",
      "Iteration 30861: loss = 0.0023631529995061704\n",
      "Iteration 30862: loss = 0.0023631433757480796\n",
      "Iteration 30863: loss = 0.0023631337520932947\n",
      "Iteration 30864: loss = 0.0023631241285418125\n",
      "Iteration 30865: loss = 0.002363114505093634\n",
      "Iteration 30866: loss = 0.0023631048817487565\n",
      "Iteration 30867: loss = 0.002363095258507179\n",
      "Iteration 30868: loss = 0.0023630856353689003\n",
      "Iteration 30869: loss = 0.002363076012333919\n",
      "Iteration 30870: loss = 0.0023630663894022345\n",
      "Iteration 30871: loss = 0.002363056766573844\n",
      "Iteration 30872: loss = 0.0023630471438487476\n",
      "Iteration 30873: loss = 0.002363037521226944\n",
      "Iteration 30874: loss = 0.0023630278987084308\n",
      "Iteration 30875: loss = 0.0023630182762932076\n",
      "Iteration 30876: loss = 0.0023630086539812726\n",
      "Iteration 30877: loss = 0.002362999031772625\n",
      "Iteration 30878: loss = 0.0023629894096672635\n",
      "Iteration 30879: loss = 0.0023629797876651864\n",
      "Iteration 30880: loss = 0.0023629701657663926\n",
      "Iteration 30881: loss = 0.002362960543970881\n",
      "Iteration 30882: loss = 0.0023629509222786497\n",
      "Iteration 30883: loss = 0.002362941300689698\n",
      "Iteration 30884: loss = 0.0023629316792040246\n",
      "Iteration 30885: loss = 0.002362922057821629\n",
      "Iteration 30886: loss = 0.0023629124365425074\n",
      "Iteration 30887: loss = 0.002362902815366661\n",
      "Iteration 30888: loss = 0.002362893194294088\n",
      "Iteration 30889: loss = 0.0023628835733247866\n",
      "Iteration 30890: loss = 0.0023628739524587552\n",
      "Iteration 30891: loss = 0.0023628643316959933\n",
      "Iteration 30892: loss = 0.002362854711036499\n",
      "Iteration 30893: loss = 0.002362845090480272\n",
      "Iteration 30894: loss = 0.0023628354700273094\n",
      "Iteration 30895: loss = 0.0023628258496776115\n",
      "Iteration 30896: loss = 0.0023628162294311765\n",
      "Iteration 30897: loss = 0.0023628066092880023\n",
      "Iteration 30898: loss = 0.002362796989248089\n",
      "Iteration 30899: loss = 0.0023627873693114337\n",
      "Iteration 30900: loss = 0.002362777749478037\n",
      "Iteration 30901: loss = 0.002362768129747896\n",
      "Iteration 30902: loss = 0.0023627585101210105\n",
      "Iteration 30903: loss = 0.002362748890597378\n",
      "Iteration 30904: loss = 0.0023627392711769984\n",
      "Iteration 30905: loss = 0.0023627296518598704\n",
      "Iteration 30906: loss = 0.0023627200326459914\n",
      "Iteration 30907: loss = 0.002362710413535362\n",
      "Iteration 30908: loss = 0.0023627007945279796\n",
      "Iteration 30909: loss = 0.002362691175623843\n",
      "Iteration 30910: loss = 0.0023626815568229516\n",
      "Iteration 30911: loss = 0.002362671938125303\n",
      "Iteration 30912: loss = 0.002362662319530897\n",
      "Iteration 30913: loss = 0.0023626527010397317\n",
      "Iteration 30914: loss = 0.0023626430826518066\n",
      "Iteration 30915: loss = 0.0023626334643671197\n",
      "Iteration 30916: loss = 0.002362623846185669\n",
      "Iteration 30917: loss = 0.002362614228107455\n",
      "Iteration 30918: loss = 0.0023626046101324744\n",
      "Iteration 30919: loss = 0.0023625949922607284\n",
      "Iteration 30920: loss = 0.0023625853744922137\n",
      "Iteration 30921: loss = 0.002362575756826929\n",
      "Iteration 30922: loss = 0.0023625661392648743\n",
      "Iteration 30923: loss = 0.0023625565218060476\n",
      "Iteration 30924: loss = 0.0023625469044504474\n",
      "Iteration 30925: loss = 0.002362537287198073\n",
      "Iteration 30926: loss = 0.0023625276700489227\n",
      "Iteration 30927: loss = 0.002362518053002995\n",
      "Iteration 30928: loss = 0.0023625084360602896\n",
      "Iteration 30929: loss = 0.002362498819220804\n",
      "Iteration 30930: loss = 0.0023624892024845376\n",
      "Iteration 30931: loss = 0.0023624795858514886\n",
      "Iteration 30932: loss = 0.002362469969321656\n",
      "Iteration 30933: loss = 0.0023624603528950394\n",
      "Iteration 30934: loss = 0.0023624507365716365\n",
      "Iteration 30935: loss = 0.0023624411203514457\n",
      "Iteration 30936: loss = 0.0023624315042344666\n",
      "Iteration 30937: loss = 0.0023624218882206975\n",
      "Iteration 30938: loss = 0.0023624122723101366\n",
      "Iteration 30939: loss = 0.002362402656502784\n",
      "Iteration 30940: loss = 0.0023623930407986374\n",
      "Iteration 30941: loss = 0.0023623834251976956\n",
      "Iteration 30942: loss = 0.0023623738096999573\n",
      "Iteration 30943: loss = 0.0023623641943054216\n",
      "Iteration 30944: loss = 0.0023623545790140863\n",
      "Iteration 30945: loss = 0.002362344963825952\n",
      "Iteration 30946: loss = 0.0023623353487410154\n",
      "Iteration 30947: loss = 0.002362325733759276\n",
      "Iteration 30948: loss = 0.0023623161188807322\n",
      "Iteration 30949: loss = 0.002362306504105384\n",
      "Iteration 30950: loss = 0.0023622968894332287\n",
      "Iteration 30951: loss = 0.0023622872748642653\n",
      "Iteration 30952: loss = 0.0023622776603984923\n",
      "Iteration 30953: loss = 0.0023622680460359097\n",
      "Iteration 30954: loss = 0.0023622584317765146\n",
      "Iteration 30955: loss = 0.0023622488176203065\n",
      "Iteration 30956: loss = 0.0023622392035672845\n",
      "Iteration 30957: loss = 0.0023622295896174464\n",
      "Iteration 30958: loss = 0.002362219975770791\n",
      "Iteration 30959: loss = 0.0023622103620273183\n",
      "Iteration 30960: loss = 0.002362200748387026\n",
      "Iteration 30961: loss = 0.002362191134849912\n",
      "Iteration 30962: loss = 0.002362181521415977\n",
      "Iteration 30963: loss = 0.0023621719080852174\n",
      "Iteration 30964: loss = 0.002362162294857634\n",
      "Iteration 30965: loss = 0.002362152681733225\n",
      "Iteration 30966: loss = 0.002362143068711988\n",
      "Iteration 30967: loss = 0.0023621334557939228\n",
      "Iteration 30968: loss = 0.0023621238429790284\n",
      "Iteration 30969: loss = 0.0023621142302673024\n",
      "Iteration 30970: loss = 0.0023621046176587434\n",
      "Iteration 30971: loss = 0.002362095005153351\n",
      "Iteration 30972: loss = 0.0023620853927511245\n",
      "Iteration 30973: loss = 0.0023620757804520614\n",
      "Iteration 30974: loss = 0.0023620661682561606\n",
      "Iteration 30975: loss = 0.0023620565561634213\n",
      "Iteration 30976: loss = 0.0023620469441738416\n",
      "Iteration 30977: loss = 0.0023620373322874207\n",
      "Iteration 30978: loss = 0.0023620277205041573\n",
      "Iteration 30979: loss = 0.00236201810882405\n",
      "Iteration 30980: loss = 0.0023620084972470973\n",
      "Iteration 30981: loss = 0.002361998885773298\n",
      "Iteration 30982: loss = 0.0023619892744026513\n",
      "Iteration 30983: loss = 0.0023619796631351555\n",
      "Iteration 30984: loss = 0.0023619700519708093\n",
      "Iteration 30985: loss = 0.0023619604409096107\n",
      "Iteration 30986: loss = 0.00236195082995156\n",
      "Iteration 30987: loss = 0.002361941219096655\n",
      "Iteration 30988: loss = 0.0023619316083448947\n",
      "Iteration 30989: loss = 0.002361921997696277\n",
      "Iteration 30990: loss = 0.002361912387150802\n",
      "Iteration 30991: loss = 0.0023619027767084674\n",
      "Iteration 30992: loss = 0.002361893166369272\n",
      "Iteration 30993: loss = 0.002361883556133215\n",
      "Iteration 30994: loss = 0.0023618739460002946\n",
      "Iteration 30995: loss = 0.00236186433597051\n",
      "Iteration 30996: loss = 0.002361854726043859\n",
      "Iteration 30997: loss = 0.0023618451162203415\n",
      "Iteration 30998: loss = 0.0023618355064999558\n",
      "Iteration 30999: loss = 0.0023618258968826997\n",
      "Iteration 31000: loss = 0.002361816287368573\n",
      "Iteration 31001: loss = 0.0023618066779575743\n",
      "Iteration 31002: loss = 0.0023617970686497024\n",
      "Iteration 31003: loss = 0.0023617874594449553\n",
      "Iteration 31004: loss = 0.002361777850343333\n",
      "Iteration 31005: loss = 0.0023617682413448323\n",
      "Iteration 31006: loss = 0.0023617586324494533\n",
      "Iteration 31007: loss = 0.0023617490236571945\n",
      "Iteration 31008: loss = 0.002361739414968055\n",
      "Iteration 31009: loss = 0.0023617298063820322\n",
      "Iteration 31010: loss = 0.002361720197899126\n",
      "Iteration 31011: loss = 0.0023617105895193348\n",
      "Iteration 31012: loss = 0.002361700981242658\n",
      "Iteration 31013: loss = 0.0023616913730690925\n",
      "Iteration 31014: loss = 0.0023616817649986387\n",
      "Iteration 31015: loss = 0.0023616721570312946\n",
      "Iteration 31016: loss = 0.002361662549167059\n",
      "Iteration 31017: loss = 0.0023616529414059307\n",
      "Iteration 31018: loss = 0.0023616433337479083\n",
      "Iteration 31019: loss = 0.002361633726192991\n",
      "Iteration 31020: loss = 0.0023616241187411767\n",
      "Iteration 31021: loss = 0.0023616145113924646\n",
      "Iteration 31022: loss = 0.0023616049041468535\n",
      "Iteration 31023: loss = 0.0023615952970043414\n",
      "Iteration 31024: loss = 0.0023615856899649283\n",
      "Iteration 31025: loss = 0.0023615760830286115\n",
      "Iteration 31026: loss = 0.0023615664761953916\n",
      "Iteration 31027: loss = 0.0023615568694652653\n",
      "Iteration 31028: loss = 0.0023615472628382315\n",
      "Iteration 31029: loss = 0.00236153765631429\n",
      "Iteration 31030: loss = 0.0023615280498934404\n",
      "Iteration 31031: loss = 0.0023615184435756787\n",
      "Iteration 31032: loss = 0.002361508837361005\n",
      "Iteration 31033: loss = 0.0023614992312494185\n",
      "Iteration 31034: loss = 0.0023614896252409177\n",
      "Iteration 31035: loss = 0.0023614800193355002\n",
      "Iteration 31036: loss = 0.002361470413533166\n",
      "Iteration 31037: loss = 0.002361460807833913\n",
      "Iteration 31038: loss = 0.002361451202237741\n",
      "Iteration 31039: loss = 0.0023614415967446473\n",
      "Iteration 31040: loss = 0.0023614319913546323\n",
      "Iteration 31041: loss = 0.0023614223860676927\n",
      "Iteration 31042: loss = 0.0023614127808838288\n",
      "Iteration 31043: loss = 0.002361403175803039\n",
      "Iteration 31044: loss = 0.0023613935708253217\n",
      "Iteration 31045: loss = 0.0023613839659506752\n",
      "Iteration 31046: loss = 0.0023613743611790986\n",
      "Iteration 31047: loss = 0.002361364756510592\n",
      "Iteration 31048: loss = 0.0023613551519451517\n",
      "Iteration 31049: loss = 0.0023613455474827782\n",
      "Iteration 31050: loss = 0.002361335943123469\n",
      "Iteration 31051: loss = 0.0023613263388672237\n",
      "Iteration 31052: loss = 0.0023613167347140414\n",
      "Iteration 31053: loss = 0.002361307130663919\n",
      "Iteration 31054: loss = 0.002361297526716857\n",
      "Iteration 31055: loss = 0.002361287922872853\n",
      "Iteration 31056: loss = 0.0023612783191319067\n",
      "Iteration 31057: loss = 0.0023612687154940157\n",
      "Iteration 31058: loss = 0.0023612591119591803\n",
      "Iteration 31059: loss = 0.0023612495085273974\n",
      "Iteration 31060: loss = 0.002361239905198667\n",
      "Iteration 31061: loss = 0.002361230301972987\n",
      "Iteration 31062: loss = 0.0023612206988503567\n",
      "Iteration 31063: loss = 0.0023612110958307745\n",
      "Iteration 31064: loss = 0.002361201492914239\n",
      "Iteration 31065: loss = 0.00236119189010075\n",
      "Iteration 31066: loss = 0.0023611822873903046\n",
      "Iteration 31067: loss = 0.0023611726847829026\n",
      "Iteration 31068: loss = 0.002361163082278542\n",
      "Iteration 31069: loss = 0.002361153479877222\n",
      "Iteration 31070: loss = 0.002361143877578942\n",
      "Iteration 31071: loss = 0.002361134275383699\n",
      "Iteration 31072: loss = 0.0023611246732914925\n",
      "Iteration 31073: loss = 0.002361115071302322\n",
      "Iteration 31074: loss = 0.0023611054694161857\n",
      "Iteration 31075: loss = 0.002361095867633082\n",
      "Iteration 31076: loss = 0.0023610862659530093\n",
      "Iteration 31077: loss = 0.0023610766643759667\n",
      "Iteration 31078: loss = 0.002361067062901954\n",
      "Iteration 31079: loss = 0.002361057461530969\n",
      "Iteration 31080: loss = 0.00236104786026301\n",
      "Iteration 31081: loss = 0.0023610382590980757\n",
      "Iteration 31082: loss = 0.0023610286580361653\n",
      "Iteration 31083: loss = 0.002361019057077278\n",
      "Iteration 31084: loss = 0.0023610094562214116\n",
      "Iteration 31085: loss = 0.0023609998554685653\n",
      "Iteration 31086: loss = 0.002360990254818737\n",
      "Iteration 31087: loss = 0.0023609806542719274\n",
      "Iteration 31088: loss = 0.002360971053828133\n",
      "Iteration 31089: loss = 0.0023609614534873535\n",
      "Iteration 31090: loss = 0.0023609518532495878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31091: loss = 0.0023609422531148338\n",
      "Iteration 31092: loss = 0.002360932653083091\n",
      "Iteration 31093: loss = 0.0023609230531543585\n",
      "Iteration 31094: loss = 0.002360913453328634\n",
      "Iteration 31095: loss = 0.002360903853605917\n",
      "Iteration 31096: loss = 0.002360894253986205\n",
      "Iteration 31097: loss = 0.002360884654469498\n",
      "Iteration 31098: loss = 0.0023608750550557943\n",
      "Iteration 31099: loss = 0.0023608654557450925\n",
      "Iteration 31100: loss = 0.002360855856537391\n",
      "Iteration 31101: loss = 0.00236084625743269\n",
      "Iteration 31102: loss = 0.0023608366584309867\n",
      "Iteration 31103: loss = 0.00236082705953228\n",
      "Iteration 31104: loss = 0.0023608174607365694\n",
      "Iteration 31105: loss = 0.002360807862043852\n",
      "Iteration 31106: loss = 0.0023607982634541286\n",
      "Iteration 31107: loss = 0.0023607886649673966\n",
      "Iteration 31108: loss = 0.0023607790665836556\n",
      "Iteration 31109: loss = 0.002360769468302903\n",
      "Iteration 31110: loss = 0.002360759870125138\n",
      "Iteration 31111: loss = 0.002360750272050361\n",
      "Iteration 31112: loss = 0.002360740674078568\n",
      "Iteration 31113: loss = 0.00236073107620976\n",
      "Iteration 31114: loss = 0.002360721478443934\n",
      "Iteration 31115: loss = 0.0023607118807810897\n",
      "Iteration 31116: loss = 0.0023607022832212252\n",
      "Iteration 31117: loss = 0.00236069268576434\n",
      "Iteration 31118: loss = 0.0023606830884104328\n",
      "Iteration 31119: loss = 0.0023606734911595013\n",
      "Iteration 31120: loss = 0.002360663894011545\n",
      "Iteration 31121: loss = 0.002360654296966563\n",
      "Iteration 31122: loss = 0.0023606447000245534\n",
      "Iteration 31123: loss = 0.002360635103185514\n",
      "Iteration 31124: loss = 0.0023606255064494456\n",
      "Iteration 31125: loss = 0.002360615909816345\n",
      "Iteration 31126: loss = 0.002360606313286212\n",
      "Iteration 31127: loss = 0.0023605967168590456\n",
      "Iteration 31128: loss = 0.0023605871205348433\n",
      "Iteration 31129: loss = 0.002360577524313605\n",
      "Iteration 31130: loss = 0.0023605679281953284\n",
      "Iteration 31131: loss = 0.0023605583321800135\n",
      "Iteration 31132: loss = 0.0023605487362676576\n",
      "Iteration 31133: loss = 0.0023605391404582605\n",
      "Iteration 31134: loss = 0.0023605295447518197\n",
      "Iteration 31135: loss = 0.002360519949148336\n",
      "Iteration 31136: loss = 0.002360510353647806\n",
      "Iteration 31137: loss = 0.0023605007582502293\n",
      "Iteration 31138: loss = 0.0023604911629556045\n",
      "Iteration 31139: loss = 0.0023604815677639303\n",
      "Iteration 31140: loss = 0.002360471972675206\n",
      "Iteration 31141: loss = 0.0023604623776894294\n",
      "Iteration 31142: loss = 0.0023604527828065994\n",
      "Iteration 31143: loss = 0.0023604431880267156\n",
      "Iteration 31144: loss = 0.0023604335933497755\n",
      "Iteration 31145: loss = 0.002360423998775778\n",
      "Iteration 31146: loss = 0.0023604144043047233\n",
      "Iteration 31147: loss = 0.002360404809936608\n",
      "Iteration 31148: loss = 0.002360395215671433\n",
      "Iteration 31149: loss = 0.0023603856215091952\n",
      "Iteration 31150: loss = 0.002360376027449894\n",
      "Iteration 31151: loss = 0.0023603664334935276\n",
      "Iteration 31152: loss = 0.002360356839640096\n",
      "Iteration 31153: loss = 0.002360347245889597\n",
      "Iteration 31154: loss = 0.0023603376522420287\n",
      "Iteration 31155: loss = 0.0023603280586973908\n",
      "Iteration 31156: loss = 0.002360318465255682\n",
      "Iteration 31157: loss = 0.0023603088719169004\n",
      "Iteration 31158: loss = 0.002360299278681046\n",
      "Iteration 31159: loss = 0.0023602896855481163\n",
      "Iteration 31160: loss = 0.00236028009251811\n",
      "Iteration 31161: loss = 0.0023602704995910264\n",
      "Iteration 31162: loss = 0.0023602609067668633\n",
      "Iteration 31163: loss = 0.002360251314045621\n",
      "Iteration 31164: loss = 0.0023602417214272967\n",
      "Iteration 31165: loss = 0.00236023212891189\n",
      "Iteration 31166: loss = 0.0023602225364993995\n",
      "Iteration 31167: loss = 0.002360212944189824\n",
      "Iteration 31168: loss = 0.002360203351983161\n",
      "Iteration 31169: loss = 0.0023601937598794107\n",
      "Iteration 31170: loss = 0.0023601841678785713\n",
      "Iteration 31171: loss = 0.0023601745759806417\n",
      "Iteration 31172: loss = 0.0023601649841856207\n",
      "Iteration 31173: loss = 0.002360155392493507\n",
      "Iteration 31174: loss = 0.002360145800904298\n",
      "Iteration 31175: loss = 0.002360136209417994\n",
      "Iteration 31176: loss = 0.0023601266180345924\n",
      "Iteration 31177: loss = 0.0023601170267540943\n",
      "Iteration 31178: loss = 0.0023601074355764956\n",
      "Iteration 31179: loss = 0.002360097844501797\n",
      "Iteration 31180: loss = 0.0023600882535299962\n",
      "Iteration 31181: loss = 0.0023600786626610925\n",
      "Iteration 31182: loss = 0.0023600690718950843\n",
      "Iteration 31183: loss = 0.00236005948123197\n",
      "Iteration 31184: loss = 0.002360049890671749\n",
      "Iteration 31185: loss = 0.002360040300214419\n",
      "Iteration 31186: loss = 0.0023600307098599798\n",
      "Iteration 31187: loss = 0.00236002111960843\n",
      "Iteration 31188: loss = 0.0023600115294597672\n",
      "Iteration 31189: loss = 0.0023600019394139924\n",
      "Iteration 31190: loss = 0.002359992349471102\n",
      "Iteration 31191: loss = 0.0023599827596310955\n",
      "Iteration 31192: loss = 0.002359973169893972\n",
      "Iteration 31193: loss = 0.00235996358025973\n",
      "Iteration 31194: loss = 0.002359953990728368\n",
      "Iteration 31195: loss = 0.0023599444012998845\n",
      "Iteration 31196: loss = 0.002359934811974279\n",
      "Iteration 31197: loss = 0.0023599252227515495\n",
      "Iteration 31198: loss = 0.0023599156336316955\n",
      "Iteration 31199: loss = 0.002359906044614715\n",
      "Iteration 31200: loss = 0.0023598964557006064\n",
      "Iteration 31201: loss = 0.0023598868668893696\n",
      "Iteration 31202: loss = 0.0023598772781810023\n",
      "Iteration 31203: loss = 0.0023598676895755033\n",
      "Iteration 31204: loss = 0.002359858101072872\n",
      "Iteration 31205: loss = 0.0023598485126731074\n",
      "Iteration 31206: loss = 0.0023598389243762066\n",
      "Iteration 31207: loss = 0.0023598293361821697\n",
      "Iteration 31208: loss = 0.002359819748090995\n",
      "Iteration 31209: loss = 0.002359810160102681\n",
      "Iteration 31210: loss = 0.002359800572217227\n",
      "Iteration 31211: loss = 0.002359790984434631\n",
      "Iteration 31212: loss = 0.0023597813967548924\n",
      "Iteration 31213: loss = 0.002359771809178009\n",
      "Iteration 31214: loss = 0.0023597622217039808\n",
      "Iteration 31215: loss = 0.002359752634332806\n",
      "Iteration 31216: loss = 0.002359743047064482\n",
      "Iteration 31217: loss = 0.002359733459899009\n",
      "Iteration 31218: loss = 0.002359723872836386\n",
      "Iteration 31219: loss = 0.0023597142858766107\n",
      "Iteration 31220: loss = 0.002359704699019682\n",
      "Iteration 31221: loss = 0.0023596951122655994\n",
      "Iteration 31222: loss = 0.0023596855256143605\n",
      "Iteration 31223: loss = 0.002359675939065965\n",
      "Iteration 31224: loss = 0.0023596663526204106\n",
      "Iteration 31225: loss = 0.0023596567662776975\n",
      "Iteration 31226: loss = 0.0023596471800378226\n",
      "Iteration 31227: loss = 0.0023596375939007857\n",
      "Iteration 31228: loss = 0.002359628007866586\n",
      "Iteration 31229: loss = 0.002359618421935221\n",
      "Iteration 31230: loss = 0.002359608836106691\n",
      "Iteration 31231: loss = 0.002359599250380992\n",
      "Iteration 31232: loss = 0.0023595896647581253\n",
      "Iteration 31233: loss = 0.0023595800792380888\n",
      "Iteration 31234: loss = 0.002359570493820881\n",
      "Iteration 31235: loss = 0.002359560908506501\n",
      "Iteration 31236: loss = 0.002359551323294947\n",
      "Iteration 31237: loss = 0.0023595417381862176\n",
      "Iteration 31238: loss = 0.0023595321531803127\n",
      "Iteration 31239: loss = 0.00235952256827723\n",
      "Iteration 31240: loss = 0.002359512983476969\n",
      "Iteration 31241: loss = 0.0023595033987795274\n",
      "Iteration 31242: loss = 0.0023594938141849047\n",
      "Iteration 31243: loss = 0.002359484229693099\n",
      "Iteration 31244: loss = 0.0023594746453041095\n",
      "Iteration 31245: loss = 0.0023594650610179345\n",
      "Iteration 31246: loss = 0.002359455476834573\n",
      "Iteration 31247: loss = 0.0023594458927540243\n",
      "Iteration 31248: loss = 0.0023594363087762857\n",
      "Iteration 31249: loss = 0.0023594267249013572\n",
      "Iteration 31250: loss = 0.002359417141129237\n",
      "Iteration 31251: loss = 0.002359407557459924\n",
      "Iteration 31252: loss = 0.002359397973893417\n",
      "Iteration 31253: loss = 0.0023593883904297138\n",
      "Iteration 31254: loss = 0.0023593788070688146\n",
      "Iteration 31255: loss = 0.002359369223810717\n",
      "Iteration 31256: loss = 0.00235935964065542\n",
      "Iteration 31257: loss = 0.002359350057602922\n",
      "Iteration 31258: loss = 0.0023593404746532227\n",
      "Iteration 31259: loss = 0.0023593308918063204\n",
      "Iteration 31260: loss = 0.0023593213090622135\n",
      "Iteration 31261: loss = 0.002359311726420901\n",
      "Iteration 31262: loss = 0.0023593021438823806\n",
      "Iteration 31263: loss = 0.0023592925614466524\n",
      "Iteration 31264: loss = 0.002359282979113715\n",
      "Iteration 31265: loss = 0.0023592733968835666\n",
      "Iteration 31266: loss = 0.002359263814756206\n",
      "Iteration 31267: loss = 0.002359254232731632\n",
      "Iteration 31268: loss = 0.002359244650809843\n",
      "Iteration 31269: loss = 0.002359235068990838\n",
      "Iteration 31270: loss = 0.002359225487274616\n",
      "Iteration 31271: loss = 0.002359215905661176\n",
      "Iteration 31272: loss = 0.002359206324150515\n",
      "Iteration 31273: loss = 0.002359196742742634\n",
      "Iteration 31274: loss = 0.00235918716143753\n",
      "Iteration 31275: loss = 0.0023591775802352015\n",
      "Iteration 31276: loss = 0.0023591679991356498\n",
      "Iteration 31277: loss = 0.002359158418138871\n",
      "Iteration 31278: loss = 0.0023591488372448643\n",
      "Iteration 31279: loss = 0.0023591392564536293\n",
      "Iteration 31280: loss = 0.0023591296757651645\n",
      "Iteration 31281: loss = 0.0023591200951794677\n",
      "Iteration 31282: loss = 0.0023591105146965386\n",
      "Iteration 31283: loss = 0.002359100934316376\n",
      "Iteration 31284: loss = 0.0023590913540389767\n",
      "Iteration 31285: loss = 0.0023590817738643423\n",
      "Iteration 31286: loss = 0.00235907219379247\n",
      "Iteration 31287: loss = 0.0023590626138233583\n",
      "Iteration 31288: loss = 0.0023590530339570063\n",
      "Iteration 31289: loss = 0.0023590434541934133\n",
      "Iteration 31290: loss = 0.0023590338745325765\n",
      "Iteration 31291: loss = 0.0023590242949744957\n",
      "Iteration 31292: loss = 0.00235901471551917\n",
      "Iteration 31293: loss = 0.0023590051361665975\n",
      "Iteration 31294: loss = 0.002358995556916776\n",
      "Iteration 31295: loss = 0.0023589859777697064\n",
      "Iteration 31296: loss = 0.002358976398725386\n",
      "Iteration 31297: loss = 0.0023589668197838134\n",
      "Iteration 31298: loss = 0.0023589572409449876\n",
      "Iteration 31299: loss = 0.002358947662208908\n",
      "Iteration 31300: loss = 0.0023589380835755723\n",
      "Iteration 31301: loss = 0.0023589285050449795\n",
      "Iteration 31302: loss = 0.002358918926617129\n",
      "Iteration 31303: loss = 0.0023589093482920182\n",
      "Iteration 31304: loss = 0.002358899770069647\n",
      "Iteration 31305: loss = 0.0023588901919500134\n",
      "Iteration 31306: loss = 0.002358880613933117\n",
      "Iteration 31307: loss = 0.0023588710360189554\n",
      "Iteration 31308: loss = 0.002358861458207528\n",
      "Iteration 31309: loss = 0.0023588518804988335\n",
      "Iteration 31310: loss = 0.0023588423028928704\n",
      "Iteration 31311: loss = 0.0023588327253896376\n",
      "Iteration 31312: loss = 0.002358823147989134\n",
      "Iteration 31313: loss = 0.0023588135706913574\n",
      "Iteration 31314: loss = 0.0023588039934963074\n",
      "Iteration 31315: loss = 0.0023587944164039825\n",
      "Iteration 31316: loss = 0.0023587848394143814\n",
      "Iteration 31317: loss = 0.0023587752625275032\n",
      "Iteration 31318: loss = 0.002358765685743346\n",
      "Iteration 31319: loss = 0.0023587561090619084\n",
      "Iteration 31320: loss = 0.00235874653248319\n",
      "Iteration 31321: loss = 0.0023587369560071885\n",
      "Iteration 31322: loss = 0.002358727379633904\n",
      "Iteration 31323: loss = 0.002358717803363334\n",
      "Iteration 31324: loss = 0.0023587082271954774\n",
      "Iteration 31325: loss = 0.002358698651130333\n",
      "Iteration 31326: loss = 0.0023586890751678994\n",
      "Iteration 31327: loss = 0.0023586794993081758\n",
      "Iteration 31328: loss = 0.002358669923551161\n",
      "Iteration 31329: loss = 0.0023586603478968527\n",
      "Iteration 31330: loss = 0.0023586507723452515\n",
      "Iteration 31331: loss = 0.0023586411968963533\n",
      "Iteration 31332: loss = 0.0023586316215501594\n",
      "Iteration 31333: loss = 0.002358622046306667\n",
      "Iteration 31334: loss = 0.0023586124711658762\n",
      "Iteration 31335: loss = 0.0023586028961277844\n",
      "Iteration 31336: loss = 0.0023585933211923904\n",
      "Iteration 31337: loss = 0.0023585837463596937\n",
      "Iteration 31338: loss = 0.002358574171629693\n",
      "Iteration 31339: loss = 0.0023585645970023864\n",
      "Iteration 31340: loss = 0.0023585550224777723\n",
      "Iteration 31341: loss = 0.002358545448055851\n",
      "Iteration 31342: loss = 0.0023585358737366197\n",
      "Iteration 31343: loss = 0.0023585262995200773\n",
      "Iteration 31344: loss = 0.002358516725406223\n",
      "Iteration 31345: loss = 0.0023585071513950564\n",
      "Iteration 31346: loss = 0.002358497577486574\n",
      "Iteration 31347: loss = 0.0023584880036807763\n",
      "Iteration 31348: loss = 0.0023584784299776613\n",
      "Iteration 31349: loss = 0.0023584688563772277\n",
      "Iteration 31350: loss = 0.0023584592828794744\n",
      "Iteration 31351: loss = 0.0023584497094844004\n",
      "Iteration 31352: loss = 0.0023584401361920037\n",
      "Iteration 31353: loss = 0.002358430563002284\n",
      "Iteration 31354: loss = 0.0023584209899152387\n",
      "Iteration 31355: loss = 0.002358411416930868\n",
      "Iteration 31356: loss = 0.0023584018440491695\n",
      "Iteration 31357: loss = 0.002358392271270143\n",
      "Iteration 31358: loss = 0.0023583826985937853\n",
      "Iteration 31359: loss = 0.002358373126020097\n",
      "Iteration 31360: loss = 0.0023583635535490764\n",
      "Iteration 31361: loss = 0.002358353981180722\n",
      "Iteration 31362: loss = 0.002358344408915032\n",
      "Iteration 31363: loss = 0.002358334836752006\n",
      "Iteration 31364: loss = 0.0023583252646916424\n",
      "Iteration 31365: loss = 0.00235831569273394\n",
      "Iteration 31366: loss = 0.002358306120878897\n",
      "Iteration 31367: loss = 0.0023582965491265127\n",
      "Iteration 31368: loss = 0.002358286977476786\n",
      "Iteration 31369: loss = 0.002358277405929715\n",
      "Iteration 31370: loss = 0.0023582678344852984\n",
      "Iteration 31371: loss = 0.002358258263143536\n",
      "Iteration 31372: loss = 0.0023582486919044242\n",
      "Iteration 31373: loss = 0.0023582391207679645\n",
      "Iteration 31374: loss = 0.002358229549734154\n",
      "Iteration 31375: loss = 0.0023582199788029917\n",
      "Iteration 31376: loss = 0.0023582104079744766\n",
      "Iteration 31377: loss = 0.0023582008372486076\n",
      "Iteration 31378: loss = 0.002358191266625382\n",
      "Iteration 31379: loss = 0.0023581816961048005\n",
      "Iteration 31380: loss = 0.00235817212568686\n",
      "Iteration 31381: loss = 0.002358162555371561\n",
      "Iteration 31382: loss = 0.0023581529851589007\n",
      "Iteration 31383: loss = 0.002358143415048879\n",
      "Iteration 31384: loss = 0.0023581338450414935\n",
      "Iteration 31385: loss = 0.0023581242751367438\n",
      "Iteration 31386: loss = 0.002358114705334628\n",
      "Iteration 31387: loss = 0.0023581051356351456\n",
      "Iteration 31388: loss = 0.0023580955660382945\n",
      "Iteration 31389: loss = 0.0023580859965440736\n",
      "Iteration 31390: loss = 0.002358076427152483\n",
      "Iteration 31391: loss = 0.0023580668578635187\n",
      "Iteration 31392: loss = 0.0023580572886771813\n",
      "Iteration 31393: loss = 0.0023580477195934693\n",
      "Iteration 31394: loss = 0.0023580381506123813\n",
      "Iteration 31395: loss = 0.002358028581733916\n",
      "Iteration 31396: loss = 0.0023580190129580724\n",
      "Iteration 31397: loss = 0.0023580094442848484\n",
      "Iteration 31398: loss = 0.0023579998757142433\n",
      "Iteration 31399: loss = 0.002357990307246256\n",
      "Iteration 31400: loss = 0.002357980738880885\n",
      "Iteration 31401: loss = 0.0023579711706181285\n",
      "Iteration 31402: loss = 0.0023579616024579863\n",
      "Iteration 31403: loss = 0.0023579520344004564\n",
      "Iteration 31404: loss = 0.0023579424664455376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31405: loss = 0.0023579328985932286\n",
      "Iteration 31406: loss = 0.0023579233308435284\n",
      "Iteration 31407: loss = 0.002357913763196436\n",
      "Iteration 31408: loss = 0.002357904195651949\n",
      "Iteration 31409: loss = 0.002357894628210067\n",
      "Iteration 31410: loss = 0.0023578850608707877\n",
      "Iteration 31411: loss = 0.002357875493634112\n",
      "Iteration 31412: loss = 0.0023578659265000366\n",
      "Iteration 31413: loss = 0.0023578563594685606\n",
      "Iteration 31414: loss = 0.002357846792539683\n",
      "Iteration 31415: loss = 0.002357837225713403\n",
      "Iteration 31416: loss = 0.002357827658989718\n",
      "Iteration 31417: loss = 0.0023578180923686286\n",
      "Iteration 31418: loss = 0.0023578085258501315\n",
      "Iteration 31419: loss = 0.0023577989594342264\n",
      "Iteration 31420: loss = 0.002357789393120913\n",
      "Iteration 31421: loss = 0.002357779826910188\n",
      "Iteration 31422: loss = 0.0023577702608020517\n",
      "Iteration 31423: loss = 0.002357760694796502\n",
      "Iteration 31424: loss = 0.002357751128893538\n",
      "Iteration 31425: loss = 0.002357741563093158\n",
      "Iteration 31426: loss = 0.0023577319973953612\n",
      "Iteration 31427: loss = 0.002357722431800147\n",
      "Iteration 31428: loss = 0.002357712866307512\n",
      "Iteration 31429: loss = 0.0023577033009174564\n",
      "Iteration 31430: loss = 0.002357693735629979\n",
      "Iteration 31431: loss = 0.0023576841704450787\n",
      "Iteration 31432: loss = 0.0023576746053627527\n",
      "Iteration 31433: loss = 0.0023576650403830017\n",
      "Iteration 31434: loss = 0.0023576554755058233\n",
      "Iteration 31435: loss = 0.0023576459107312164\n",
      "Iteration 31436: loss = 0.0023576363460591793\n",
      "Iteration 31437: loss = 0.0023576267814897117\n",
      "Iteration 31438: loss = 0.0023576172170228114\n",
      "Iteration 31439: loss = 0.0023576076526584775\n",
      "Iteration 31440: loss = 0.002357598088396709\n",
      "Iteration 31441: loss = 0.002357588524237504\n",
      "Iteration 31442: loss = 0.002357578960180862\n",
      "Iteration 31443: loss = 0.0023575693962267804\n",
      "Iteration 31444: loss = 0.0023575598323752594\n",
      "Iteration 31445: loss = 0.0023575502686262973\n",
      "Iteration 31446: loss = 0.002357540704979893\n",
      "Iteration 31447: loss = 0.002357531141436044\n",
      "Iteration 31448: loss = 0.00235752157799475\n",
      "Iteration 31449: loss = 0.0023575120146560095\n",
      "Iteration 31450: loss = 0.0023575024514198217\n",
      "Iteration 31451: loss = 0.0023574928882861847\n",
      "Iteration 31452: loss = 0.002357483325255098\n",
      "Iteration 31453: loss = 0.0023574737623265596\n",
      "Iteration 31454: loss = 0.0023574641995005676\n",
      "Iteration 31455: loss = 0.0023574546367771225\n",
      "Iteration 31456: loss = 0.002357445074156222\n",
      "Iteration 31457: loss = 0.002357435511637864\n",
      "Iteration 31458: loss = 0.0023574259492220493\n",
      "Iteration 31459: loss = 0.0023574163869087747\n",
      "Iteration 31460: loss = 0.0023574068246980397\n",
      "Iteration 31461: loss = 0.002357397262589843\n",
      "Iteration 31462: loss = 0.002357387700584183\n",
      "Iteration 31463: loss = 0.002357378138681059\n",
      "Iteration 31464: loss = 0.0023573685768804695\n",
      "Iteration 31465: loss = 0.002357359015182413\n",
      "Iteration 31466: loss = 0.002357349453586889\n",
      "Iteration 31467: loss = 0.002357339892093895\n",
      "Iteration 31468: loss = 0.002357330330703429\n",
      "Iteration 31469: loss = 0.0023573207694154927\n",
      "Iteration 31470: loss = 0.0023573112082300833\n",
      "Iteration 31471: loss = 0.0023573016471471987\n",
      "Iteration 31472: loss = 0.002357292086166838\n",
      "Iteration 31473: loss = 0.0023572825252890007\n",
      "Iteration 31474: loss = 0.0023572729645136847\n",
      "Iteration 31475: loss = 0.0023572634038408896\n",
      "Iteration 31476: loss = 0.0023572538432706136\n",
      "Iteration 31477: loss = 0.002357244282802855\n",
      "Iteration 31478: loss = 0.0023572347224376125\n",
      "Iteration 31479: loss = 0.0023572251621748865\n",
      "Iteration 31480: loss = 0.0023572156020146735\n",
      "Iteration 31481: loss = 0.002357206041956973\n",
      "Iteration 31482: loss = 0.0023571964820017846\n",
      "Iteration 31483: loss = 0.0023571869221491065\n",
      "Iteration 31484: loss = 0.0023571773623989366\n",
      "Iteration 31485: loss = 0.0023571678027512746\n",
      "Iteration 31486: loss = 0.0023571582432061187\n",
      "Iteration 31487: loss = 0.002357148683763468\n",
      "Iteration 31488: loss = 0.00235713912442332\n",
      "Iteration 31489: loss = 0.002357129565185676\n",
      "Iteration 31490: loss = 0.002357120006050533\n",
      "Iteration 31491: loss = 0.002357110447017889\n",
      "Iteration 31492: loss = 0.002357100888087744\n",
      "Iteration 31493: loss = 0.0023570913292600966\n",
      "Iteration 31494: loss = 0.002357081770534945\n",
      "Iteration 31495: loss = 0.0023570722119122885\n",
      "Iteration 31496: loss = 0.0023570626533921247\n",
      "Iteration 31497: loss = 0.002357053094974454\n",
      "Iteration 31498: loss = 0.002357043536659274\n",
      "Iteration 31499: loss = 0.0023570339784465834\n",
      "Iteration 31500: loss = 0.0023570244203363817\n",
      "Iteration 31501: loss = 0.0023570148623286665\n",
      "Iteration 31502: loss = 0.0023570053044234375\n",
      "Iteration 31503: loss = 0.0023569957466206934\n",
      "Iteration 31504: loss = 0.0023569861889204324\n",
      "Iteration 31505: loss = 0.0023569766313226532\n",
      "Iteration 31506: loss = 0.0023569670738273546\n",
      "Iteration 31507: loss = 0.002356957516434535\n",
      "Iteration 31508: loss = 0.0023569479591441945\n",
      "Iteration 31509: loss = 0.0023569384019563305\n",
      "Iteration 31510: loss = 0.0023569288448709426\n",
      "Iteration 31511: loss = 0.002356919287888028\n",
      "Iteration 31512: loss = 0.0023569097310075873\n",
      "Iteration 31513: loss = 0.002356900174229618\n",
      "Iteration 31514: loss = 0.0023568906175541193\n",
      "Iteration 31515: loss = 0.00235688106098109\n",
      "Iteration 31516: loss = 0.0023568715045105284\n",
      "Iteration 31517: loss = 0.002356861948142434\n",
      "Iteration 31518: loss = 0.002356852391876804\n",
      "Iteration 31519: loss = 0.002356842835713639\n",
      "Iteration 31520: loss = 0.0023568332796529362\n",
      "Iteration 31521: loss = 0.0023568237236946954\n",
      "Iteration 31522: loss = 0.0023568141678389147\n",
      "Iteration 31523: loss = 0.002356804612085593\n",
      "Iteration 31524: loss = 0.002356795056434729\n",
      "Iteration 31525: loss = 0.0023567855008863213\n",
      "Iteration 31526: loss = 0.002356775945440369\n",
      "Iteration 31527: loss = 0.0023567663900968703\n",
      "Iteration 31528: loss = 0.002356756834855825\n",
      "Iteration 31529: loss = 0.00235674727971723\n",
      "Iteration 31530: loss = 0.0023567377246810853\n",
      "Iteration 31531: loss = 0.0023567281697473904\n",
      "Iteration 31532: loss = 0.0023567186149161417\n",
      "Iteration 31533: loss = 0.0023567090601873397\n",
      "Iteration 31534: loss = 0.0023566995055609827\n",
      "Iteration 31535: loss = 0.0023566899510370693\n",
      "Iteration 31536: loss = 0.0023566803966155983\n",
      "Iteration 31537: loss = 0.002356670842296568\n",
      "Iteration 31538: loss = 0.0023566612880799778\n",
      "Iteration 31539: loss = 0.002356651733965826\n",
      "Iteration 31540: loss = 0.002356642179954112\n",
      "Iteration 31541: loss = 0.0023566326260448336\n",
      "Iteration 31542: loss = 0.00235662307223799\n",
      "Iteration 31543: loss = 0.0023566135185335802\n",
      "Iteration 31544: loss = 0.002356603964931602\n",
      "Iteration 31545: loss = 0.002356594411432055\n",
      "Iteration 31546: loss = 0.0023565848580349375\n",
      "Iteration 31547: loss = 0.0023565753047402483\n",
      "Iteration 31548: loss = 0.0023565657515479863\n",
      "Iteration 31549: loss = 0.00235655619845815\n",
      "Iteration 31550: loss = 0.0023565466454707382\n",
      "Iteration 31551: loss = 0.0023565370925857495\n",
      "Iteration 31552: loss = 0.002356527539803183\n",
      "Iteration 31553: loss = 0.0023565179871230368\n",
      "Iteration 31554: loss = 0.0023565084345453105\n",
      "Iteration 31555: loss = 0.002356498882070002\n",
      "Iteration 31556: loss = 0.00235648932969711\n",
      "Iteration 31557: loss = 0.0023564797774266342\n",
      "Iteration 31558: loss = 0.002356470225258572\n",
      "Iteration 31559: loss = 0.0023564606731929227\n",
      "Iteration 31560: loss = 0.002356451121229686\n",
      "Iteration 31561: loss = 0.002356441569368859\n",
      "Iteration 31562: loss = 0.0023564320176104416\n",
      "Iteration 31563: loss = 0.002356422465954432\n",
      "Iteration 31564: loss = 0.002356412914400829\n",
      "Iteration 31565: loss = 0.0023564033629496313\n",
      "Iteration 31566: loss = 0.0023563938116008373\n",
      "Iteration 31567: loss = 0.0023563842603544463\n",
      "Iteration 31568: loss = 0.002356374709210457\n",
      "Iteration 31569: loss = 0.0023563651581688673\n",
      "Iteration 31570: loss = 0.0023563556072296776\n",
      "Iteration 31571: loss = 0.0023563460563928848\n",
      "Iteration 31572: loss = 0.002356336505658489\n",
      "Iteration 31573: loss = 0.0023563269550264877\n",
      "Iteration 31574: loss = 0.0023563174044968806\n",
      "Iteration 31575: loss = 0.002356307854069666\n",
      "Iteration 31576: loss = 0.0023562983037448424\n",
      "Iteration 31577: loss = 0.002356288753522409\n",
      "Iteration 31578: loss = 0.002356279203402364\n",
      "Iteration 31579: loss = 0.0023562696533847073\n",
      "Iteration 31580: loss = 0.0023562601034694364\n",
      "Iteration 31581: loss = 0.0023562505536565497\n",
      "Iteration 31582: loss = 0.002356241003946047\n",
      "Iteration 31583: loss = 0.002356231454337927\n",
      "Iteration 31584: loss = 0.0023562219048321874\n",
      "Iteration 31585: loss = 0.002356212355428828\n",
      "Iteration 31586: loss = 0.0023562028061278476\n",
      "Iteration 31587: loss = 0.002356193256929244\n",
      "Iteration 31588: loss = 0.0023561837078330166\n",
      "Iteration 31589: loss = 0.0023561741588391636\n",
      "Iteration 31590: loss = 0.0023561646099476835\n",
      "Iteration 31591: loss = 0.002356155061158576\n",
      "Iteration 31592: loss = 0.0023561455124718393\n",
      "Iteration 31593: loss = 0.002356135963887472\n",
      "Iteration 31594: loss = 0.0023561264154054735\n",
      "Iteration 31595: loss = 0.002356116867025842\n",
      "Iteration 31596: loss = 0.002356107318748576\n",
      "Iteration 31597: loss = 0.002356097770573674\n",
      "Iteration 31598: loss = 0.0023560882225011352\n",
      "Iteration 31599: loss = 0.002356078674530959\n",
      "Iteration 31600: loss = 0.002356069126663143\n",
      "Iteration 31601: loss = 0.0023560595788976865\n",
      "Iteration 31602: loss = 0.002356050031234588\n",
      "Iteration 31603: loss = 0.0023560404836738464\n",
      "Iteration 31604: loss = 0.0023560309362154605\n",
      "Iteration 31605: loss = 0.002356021388859428\n",
      "Iteration 31606: loss = 0.002356011841605749\n",
      "Iteration 31607: loss = 0.002356002294454422\n",
      "Iteration 31608: loss = 0.0023559927474054456\n",
      "Iteration 31609: loss = 0.002355983200458817\n",
      "Iteration 31610: loss = 0.002355973653614538\n",
      "Iteration 31611: loss = 0.0023559641068726044\n",
      "Iteration 31612: loss = 0.002355954560233016\n",
      "Iteration 31613: loss = 0.0023559450136957723\n",
      "Iteration 31614: loss = 0.002355935467260871\n",
      "Iteration 31615: loss = 0.0023559259209283115\n",
      "Iteration 31616: loss = 0.0023559163746980916\n",
      "Iteration 31617: loss = 0.002355906828570211\n",
      "Iteration 31618: loss = 0.002355897282544668\n",
      "Iteration 31619: loss = 0.0023558877366214614\n",
      "Iteration 31620: loss = 0.00235587819080059\n",
      "Iteration 31621: loss = 0.002355868645082052\n",
      "Iteration 31622: loss = 0.0023558590994658467\n",
      "Iteration 31623: loss = 0.0023558495539519734\n",
      "Iteration 31624: loss = 0.002355840008540429\n",
      "Iteration 31625: loss = 0.002355830463231214\n",
      "Iteration 31626: loss = 0.002355820918024326\n",
      "Iteration 31627: loss = 0.002355811372919764\n",
      "Iteration 31628: loss = 0.0023558018279175278\n",
      "Iteration 31629: loss = 0.0023557922830176143\n",
      "Iteration 31630: loss = 0.0023557827382200236\n",
      "Iteration 31631: loss = 0.002355773193524753\n",
      "Iteration 31632: loss = 0.002355763648931803\n",
      "Iteration 31633: loss = 0.0023557541044411717\n",
      "Iteration 31634: loss = 0.0023557445600528573\n",
      "Iteration 31635: loss = 0.002355735015766859\n",
      "Iteration 31636: loss = 0.002355725471583175\n",
      "Iteration 31637: loss = 0.0023557159275018045\n",
      "Iteration 31638: loss = 0.002355706383522746\n",
      "Iteration 31639: loss = 0.0023556968396459986\n",
      "Iteration 31640: loss = 0.0023556872958715607\n",
      "Iteration 31641: loss = 0.0023556777521994312\n",
      "Iteration 31642: loss = 0.002355668208629609\n",
      "Iteration 31643: loss = 0.0023556586651620913\n",
      "Iteration 31644: loss = 0.002355649121796879\n",
      "Iteration 31645: loss = 0.0023556395785339695\n",
      "Iteration 31646: loss = 0.0023556300353733627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31647: loss = 0.0023556204923150556\n",
      "Iteration 31648: loss = 0.0023556109493590486\n",
      "Iteration 31649: loss = 0.002355601406505339\n",
      "Iteration 31650: loss = 0.0023555918637539262\n",
      "Iteration 31651: loss = 0.002355582321104809\n",
      "Iteration 31652: loss = 0.002355572778557986\n",
      "Iteration 31653: loss = 0.002355563236113456\n",
      "Iteration 31654: loss = 0.0023555536937712182\n",
      "Iteration 31655: loss = 0.0023555441515312704\n",
      "Iteration 31656: loss = 0.0023555346093936114\n",
      "Iteration 31657: loss = 0.0023555250673582403\n",
      "Iteration 31658: loss = 0.002355515525425157\n",
      "Iteration 31659: loss = 0.002355505983594358\n",
      "Iteration 31660: loss = 0.0023554964418658424\n",
      "Iteration 31661: loss = 0.0023554869002396107\n",
      "Iteration 31662: loss = 0.0023554773587156595\n",
      "Iteration 31663: loss = 0.0023554678172939894\n",
      "Iteration 31664: loss = 0.0023554582759745973\n",
      "Iteration 31665: loss = 0.0023554487347574836\n",
      "Iteration 31666: loss = 0.0023554391936426457\n",
      "Iteration 31667: loss = 0.0023554296526300833\n",
      "Iteration 31668: loss = 0.002355420111719795\n",
      "Iteration 31669: loss = 0.0023554105709117784\n",
      "Iteration 31670: loss = 0.0023554010302060334\n",
      "Iteration 31671: loss = 0.002355391489602559\n",
      "Iteration 31672: loss = 0.0023553819491013528\n",
      "Iteration 31673: loss = 0.002355372408702414\n",
      "Iteration 31674: loss = 0.002355362868405741\n",
      "Iteration 31675: loss = 0.002355353328211334\n",
      "Iteration 31676: loss = 0.0023553437881191895\n",
      "Iteration 31677: loss = 0.0023553342481293076\n",
      "Iteration 31678: loss = 0.002355324708241687\n",
      "Iteration 31679: loss = 0.002355315168456326\n",
      "Iteration 31680: loss = 0.002355305628773224\n",
      "Iteration 31681: loss = 0.002355296089192378\n",
      "Iteration 31682: loss = 0.002355286549713789\n",
      "Iteration 31683: loss = 0.0023552770103374542\n",
      "Iteration 31684: loss = 0.002355267471063373\n",
      "Iteration 31685: loss = 0.0023552579318915443\n",
      "Iteration 31686: loss = 0.002355248392821966\n",
      "Iteration 31687: loss = 0.0023552388538546376\n",
      "Iteration 31688: loss = 0.0023552293149895565\n",
      "Iteration 31689: loss = 0.0023552197762267235\n",
      "Iteration 31690: loss = 0.0023552102375661355\n",
      "Iteration 31691: loss = 0.0023552006990077926\n",
      "Iteration 31692: loss = 0.002355191160551692\n",
      "Iteration 31693: loss = 0.0023551816221978345\n",
      "Iteration 31694: loss = 0.002355172083946217\n",
      "Iteration 31695: loss = 0.0023551625457968388\n",
      "Iteration 31696: loss = 0.0023551530077496993\n",
      "Iteration 31697: loss = 0.002355143469804796\n",
      "Iteration 31698: loss = 0.0023551339319621283\n",
      "Iteration 31699: loss = 0.0023551243942216945\n",
      "Iteration 31700: loss = 0.0023551148565834944\n",
      "Iteration 31701: loss = 0.002355105319047526\n",
      "Iteration 31702: loss = 0.0023550957816137873\n",
      "Iteration 31703: loss = 0.002355086244282278\n",
      "Iteration 31704: loss = 0.002355076707052997\n",
      "Iteration 31705: loss = 0.0023550671699259424\n",
      "Iteration 31706: loss = 0.002355057632901113\n",
      "Iteration 31707: loss = 0.0023550480959785075\n",
      "Iteration 31708: loss = 0.0023550385591581255\n",
      "Iteration 31709: loss = 0.002355029022439964\n",
      "Iteration 31710: loss = 0.0023550194858240232\n",
      "Iteration 31711: loss = 0.0023550099493103013\n",
      "Iteration 31712: loss = 0.0023550004128987975\n",
      "Iteration 31713: loss = 0.0023549908765895097\n",
      "Iteration 31714: loss = 0.0023549813403824374\n",
      "Iteration 31715: loss = 0.0023549718042775785\n",
      "Iteration 31716: loss = 0.002354962268274932\n",
      "Iteration 31717: loss = 0.0023549527323744974\n",
      "Iteration 31718: loss = 0.002354943196576272\n",
      "Iteration 31719: loss = 0.0023549336608802564\n",
      "Iteration 31720: loss = 0.0023549241252864474\n",
      "Iteration 31721: loss = 0.0023549145897948454\n",
      "Iteration 31722: loss = 0.0023549050544054476\n",
      "Iteration 31723: loss = 0.0023548955191182537\n",
      "Iteration 31724: loss = 0.002354885983933262\n",
      "Iteration 31725: loss = 0.0023548764488504717\n",
      "Iteration 31726: loss = 0.0023548669138698805\n",
      "Iteration 31727: loss = 0.0023548573789914885\n",
      "Iteration 31728: loss = 0.0023548478442152938\n",
      "Iteration 31729: loss = 0.0023548383095412947\n",
      "Iteration 31730: loss = 0.002354828774969491\n",
      "Iteration 31731: loss = 0.00235481924049988\n",
      "Iteration 31732: loss = 0.0023548097061324613\n",
      "Iteration 31733: loss = 0.0023548001718672338\n",
      "Iteration 31734: loss = 0.002354790637704196\n",
      "Iteration 31735: loss = 0.0023547811036433463\n",
      "Iteration 31736: loss = 0.002354771569684683\n",
      "Iteration 31737: loss = 0.0023547620358282065\n",
      "Iteration 31738: loss = 0.0023547525020739144\n",
      "Iteration 31739: loss = 0.002354742968421805\n",
      "Iteration 31740: loss = 0.0023547334348718776\n",
      "Iteration 31741: loss = 0.002354723901424131\n",
      "Iteration 31742: loss = 0.002354714368078564\n",
      "Iteration 31743: loss = 0.002354704834835175\n",
      "Iteration 31744: loss = 0.002354695301693963\n",
      "Iteration 31745: loss = 0.002354685768654926\n",
      "Iteration 31746: loss = 0.002354676235718064\n",
      "Iteration 31747: loss = 0.002354666702883375\n",
      "Iteration 31748: loss = 0.002354657170150858\n",
      "Iteration 31749: loss = 0.002354647637520511\n",
      "Iteration 31750: loss = 0.002354638104992333\n",
      "Iteration 31751: loss = 0.0023546285725663234\n",
      "Iteration 31752: loss = 0.00235461904024248\n",
      "Iteration 31753: loss = 0.002354609508020802\n",
      "Iteration 31754: loss = 0.0023545999759012884\n",
      "Iteration 31755: loss = 0.0023545904438839376\n",
      "Iteration 31756: loss = 0.0023545809119687486\n",
      "Iteration 31757: loss = 0.0023545713801557196\n",
      "Iteration 31758: loss = 0.0023545618484448493\n",
      "Iteration 31759: loss = 0.0023545523168361374\n",
      "Iteration 31760: loss = 0.0023545427853295816\n",
      "Iteration 31761: loss = 0.0023545332539251806\n",
      "Iteration 31762: loss = 0.002354523722622934\n",
      "Iteration 31763: loss = 0.00235451419142284\n",
      "Iteration 31764: loss = 0.002354504660324897\n",
      "Iteration 31765: loss = 0.0023544951293291046\n",
      "Iteration 31766: loss = 0.0023544855984354604\n",
      "Iteration 31767: loss = 0.0023544760676439645\n",
      "Iteration 31768: loss = 0.0023544665369546144\n",
      "Iteration 31769: loss = 0.002354457006367409\n",
      "Iteration 31770: loss = 0.002354447475882348\n",
      "Iteration 31771: loss = 0.0023544379454994288\n",
      "Iteration 31772: loss = 0.002354428415218651\n",
      "Iteration 31773: loss = 0.0023544188850400135\n",
      "Iteration 31774: loss = 0.0023544093549635135\n",
      "Iteration 31775: loss = 0.0023543998249891517\n",
      "Iteration 31776: loss = 0.0023543902951169257\n",
      "Iteration 31777: loss = 0.0023543807653468346\n",
      "Iteration 31778: loss = 0.002354371235678877\n",
      "Iteration 31779: loss = 0.0023543617061130514\n",
      "Iteration 31780: loss = 0.002354352176649357\n",
      "Iteration 31781: loss = 0.002354342647287792\n",
      "Iteration 31782: loss = 0.0023543331180283563\n",
      "Iteration 31783: loss = 0.0023543235888710467\n",
      "Iteration 31784: loss = 0.0023543140598158633\n",
      "Iteration 31785: loss = 0.0023543045308628044\n",
      "Iteration 31786: loss = 0.002354295002011869\n",
      "Iteration 31787: loss = 0.0023542854732630557\n",
      "Iteration 31788: loss = 0.002354275944616363\n",
      "Iteration 31789: loss = 0.0023542664160717897\n",
      "Iteration 31790: loss = 0.0023542568876293345\n",
      "Iteration 31791: loss = 0.002354247359288997\n",
      "Iteration 31792: loss = 0.002354237831050774\n",
      "Iteration 31793: loss = 0.002354228302914666\n",
      "Iteration 31794: loss = 0.002354218774880671\n",
      "Iteration 31795: loss = 0.0023542092469487877\n",
      "Iteration 31796: loss = 0.0023541997191190156\n",
      "Iteration 31797: loss = 0.0023541901913913523\n",
      "Iteration 31798: loss = 0.002354180663765797\n",
      "Iteration 31799: loss = 0.0023541711362423485\n",
      "Iteration 31800: loss = 0.0023541616088210053\n",
      "Iteration 31801: loss = 0.0023541520815017667\n",
      "Iteration 31802: loss = 0.0023541425542846304\n",
      "Iteration 31803: loss = 0.0023541330271695964\n",
      "Iteration 31804: loss = 0.0023541235001566627\n",
      "Iteration 31805: loss = 0.0023541139732458274\n",
      "Iteration 31806: loss = 0.0023541044464370905\n",
      "Iteration 31807: loss = 0.0023540949197304495\n",
      "Iteration 31808: loss = 0.0023540853931259043\n",
      "Iteration 31809: loss = 0.0023540758666234533\n",
      "Iteration 31810: loss = 0.002354066340223095\n",
      "Iteration 31811: loss = 0.0023540568139248275\n",
      "Iteration 31812: loss = 0.0023540472877286505\n",
      "Iteration 31813: loss = 0.0023540377616345625\n",
      "Iteration 31814: loss = 0.002354028235642562\n",
      "Iteration 31815: loss = 0.0023540187097526484\n",
      "Iteration 31816: loss = 0.002354009183964819\n",
      "Iteration 31817: loss = 0.002353999658279074\n",
      "Iteration 31818: loss = 0.0023539901326954113\n",
      "Iteration 31819: loss = 0.00235398060721383\n",
      "Iteration 31820: loss = 0.0023539710818343287\n",
      "Iteration 31821: loss = 0.0023539615565569055\n",
      "Iteration 31822: loss = 0.002353952031381561\n",
      "Iteration 31823: loss = 0.0023539425063082912\n",
      "Iteration 31824: loss = 0.002353932981337097\n",
      "Iteration 31825: loss = 0.0023539234564679762\n",
      "Iteration 31826: loss = 0.0023539139317009282\n",
      "Iteration 31827: loss = 0.0023539044070359505\n",
      "Iteration 31828: loss = 0.002353894882473043\n",
      "Iteration 31829: loss = 0.0023538853580122037\n",
      "Iteration 31830: loss = 0.002353875833653432\n",
      "Iteration 31831: loss = 0.002353866309396726\n",
      "Iteration 31832: loss = 0.002353856785242085\n",
      "Iteration 31833: loss = 0.0023538472611895067\n",
      "Iteration 31834: loss = 0.002353837737238991\n",
      "Iteration 31835: loss = 0.0023538282133905367\n",
      "Iteration 31836: loss = 0.002353818689644142\n",
      "Iteration 31837: loss = 0.0023538091659998043\n",
      "Iteration 31838: loss = 0.002353799642457525\n",
      "Iteration 31839: loss = 0.0023537901190173014\n",
      "Iteration 31840: loss = 0.002353780595679131\n",
      "Iteration 31841: loss = 0.002353771072443015\n",
      "Iteration 31842: loss = 0.00235376154930895\n",
      "Iteration 31843: loss = 0.0023537520262769365\n",
      "Iteration 31844: loss = 0.0023537425033469724\n",
      "Iteration 31845: loss = 0.002353732980519056\n",
      "Iteration 31846: loss = 0.0023537234577931864\n",
      "Iteration 31847: loss = 0.0023537139351693624\n",
      "Iteration 31848: loss = 0.002353704412647583\n",
      "Iteration 31849: loss = 0.0023536948902278467\n",
      "Iteration 31850: loss = 0.002353685367910152\n",
      "Iteration 31851: loss = 0.0023536758456944975\n",
      "Iteration 31852: loss = 0.002353666323580882\n",
      "Iteration 31853: loss = 0.0023536568015693053\n",
      "Iteration 31854: loss = 0.0023536472796597645\n",
      "Iteration 31855: loss = 0.0023536377578522597\n",
      "Iteration 31856: loss = 0.0023536282361467887\n",
      "Iteration 31857: loss = 0.0023536187145433507\n",
      "Iteration 31858: loss = 0.0023536091930419444\n",
      "Iteration 31859: loss = 0.002353599671642567\n",
      "Iteration 31860: loss = 0.00235359015034522\n",
      "Iteration 31861: loss = 0.0023535806291499007\n",
      "Iteration 31862: loss = 0.002353571108056607\n",
      "Iteration 31863: loss = 0.0023535615870653396\n",
      "Iteration 31864: loss = 0.002353552066176095\n",
      "Iteration 31865: loss = 0.0023535425453888735\n",
      "Iteration 31866: loss = 0.0023535330247036737\n",
      "Iteration 31867: loss = 0.0023535235041204938\n",
      "Iteration 31868: loss = 0.0023535139836393325\n",
      "Iteration 31869: loss = 0.002353504463260189\n",
      "Iteration 31870: loss = 0.002353494942983062\n",
      "Iteration 31871: loss = 0.0023534854228079495\n",
      "Iteration 31872: loss = 0.002353475902734851\n",
      "Iteration 31873: loss = 0.002353466382763765\n",
      "Iteration 31874: loss = 0.00235345686289469\n",
      "Iteration 31875: loss = 0.0023534473431276247\n",
      "Iteration 31876: loss = 0.0023534378234625684\n",
      "Iteration 31877: loss = 0.002353428303899519\n",
      "Iteration 31878: loss = 0.002353418784438476\n",
      "Iteration 31879: loss = 0.002353409265079438\n",
      "Iteration 31880: loss = 0.0023533997458224033\n",
      "Iteration 31881: loss = 0.002353390226667371\n",
      "Iteration 31882: loss = 0.0023533807076143395\n",
      "Iteration 31883: loss = 0.0023533711886633076\n",
      "Iteration 31884: loss = 0.0023533616698142744\n",
      "Iteration 31885: loss = 0.002353352151067239\n",
      "Iteration 31886: loss = 0.002353342632422199\n",
      "Iteration 31887: loss = 0.002353333113879153\n",
      "Iteration 31888: loss = 0.0023533235954381008\n",
      "Iteration 31889: loss = 0.0023533140770990407\n",
      "Iteration 31890: loss = 0.0023533045588619715\n",
      "Iteration 31891: loss = 0.0023532950407268915\n",
      "Iteration 31892: loss = 0.0023532855226938\n",
      "Iteration 31893: loss = 0.002353276004762695\n",
      "Iteration 31894: loss = 0.0023532664869335765\n",
      "Iteration 31895: loss = 0.0023532569692064423\n",
      "Iteration 31896: loss = 0.002353247451581291\n",
      "Iteration 31897: loss = 0.0023532379340581214\n",
      "Iteration 31898: loss = 0.0023532284166369326\n",
      "Iteration 31899: loss = 0.0023532188993177234\n",
      "Iteration 31900: loss = 0.0023532093821004916\n",
      "Iteration 31901: loss = 0.0023531998649852373\n",
      "Iteration 31902: loss = 0.0023531903479719583\n",
      "Iteration 31903: loss = 0.0023531808310606537\n",
      "Iteration 31904: loss = 0.002353171314251322\n",
      "Iteration 31905: loss = 0.0023531617975439617\n",
      "Iteration 31906: loss = 0.0023531522809385717\n",
      "Iteration 31907: loss = 0.002353142764435151\n",
      "Iteration 31908: loss = 0.002353133248033699\n",
      "Iteration 31909: loss = 0.002353123731734213\n",
      "Iteration 31910: loss = 0.002353114215536692\n",
      "Iteration 31911: loss = 0.0023531046994411356\n",
      "Iteration 31912: loss = 0.002353095183447542\n",
      "Iteration 31913: loss = 0.002353085667555909\n",
      "Iteration 31914: loss = 0.002353076151766237\n",
      "Iteration 31915: loss = 0.0023530666360785236\n",
      "Iteration 31916: loss = 0.0023530571204927685\n",
      "Iteration 31917: loss = 0.0023530476050089696\n",
      "Iteration 31918: loss = 0.002353038089627126\n",
      "Iteration 31919: loss = 0.002353028574347235\n",
      "Iteration 31920: loss = 0.002353019059169298\n",
      "Iteration 31921: loss = 0.002353009544093312\n",
      "Iteration 31922: loss = 0.0023530000291192756\n",
      "Iteration 31923: loss = 0.002352990514247189\n",
      "Iteration 31924: loss = 0.002352980999477049\n",
      "Iteration 31925: loss = 0.0023529714848088555\n",
      "Iteration 31926: loss = 0.002352961970242607\n",
      "Iteration 31927: loss = 0.0023529524557783024\n",
      "Iteration 31928: loss = 0.00235294294141594\n",
      "Iteration 31929: loss = 0.002352933427155518\n",
      "Iteration 31930: loss = 0.002352923912997037\n",
      "Iteration 31931: loss = 0.0023529143989404946\n",
      "Iteration 31932: loss = 0.002352904884985889\n",
      "Iteration 31933: loss = 0.0023528953711332195\n",
      "Iteration 31934: loss = 0.0023528858573824856\n",
      "Iteration 31935: loss = 0.002352876343733684\n",
      "Iteration 31936: loss = 0.002352866830186815\n",
      "Iteration 31937: loss = 0.002352857316741877\n",
      "Iteration 31938: loss = 0.0023528478033988686\n",
      "Iteration 31939: loss = 0.002352838290157789\n",
      "Iteration 31940: loss = 0.0023528287770186364\n",
      "Iteration 31941: loss = 0.0023528192639814096\n",
      "Iteration 31942: loss = 0.0023528097510461077\n",
      "Iteration 31943: loss = 0.002352800238212728\n",
      "Iteration 31944: loss = 0.002352790725481272\n",
      "Iteration 31945: loss = 0.0023527812128517353\n",
      "Iteration 31946: loss = 0.002352771700324119\n",
      "Iteration 31947: loss = 0.0023527621878984208\n",
      "Iteration 31948: loss = 0.0023527526755746393\n",
      "Iteration 31949: loss = 0.0023527431633527736\n",
      "Iteration 31950: loss = 0.0023527336512328225\n",
      "Iteration 31951: loss = 0.0023527241392147847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31952: loss = 0.002352714627298658\n",
      "Iteration 31953: loss = 0.0023527051154844424\n",
      "Iteration 31954: loss = 0.002352695603772136\n",
      "Iteration 31955: loss = 0.0023526860921617384\n",
      "Iteration 31956: loss = 0.002352676580653247\n",
      "Iteration 31957: loss = 0.0023526670692466606\n",
      "Iteration 31958: loss = 0.0023526575579419784\n",
      "Iteration 31959: loss = 0.0023526480467392\n",
      "Iteration 31960: loss = 0.0023526385356383234\n",
      "Iteration 31961: loss = 0.0023526290246393467\n",
      "Iteration 31962: loss = 0.002352619513742269\n",
      "Iteration 31963: loss = 0.0023526100029470894\n",
      "Iteration 31964: loss = 0.002352600492253806\n",
      "Iteration 31965: loss = 0.0023525909816624188\n",
      "Iteration 31966: loss = 0.0023525814711729248\n",
      "Iteration 31967: loss = 0.002352571960785324\n",
      "Iteration 31968: loss = 0.0023525624504996145\n",
      "Iteration 31969: loss = 0.0023525529403157952\n",
      "Iteration 31970: loss = 0.0023525434302338654\n",
      "Iteration 31971: loss = 0.002352533920253823\n",
      "Iteration 31972: loss = 0.002352524410375667\n",
      "Iteration 31973: loss = 0.002352514900599396\n",
      "Iteration 31974: loss = 0.002352505390925009\n",
      "Iteration 31975: loss = 0.0023524958813525045\n",
      "Iteration 31976: loss = 0.0023524863718818816\n",
      "Iteration 31977: loss = 0.002352476862513138\n",
      "Iteration 31978: loss = 0.002352467353246274\n",
      "Iteration 31979: loss = 0.002352457844081287\n",
      "Iteration 31980: loss = 0.0023524483350181773\n",
      "Iteration 31981: loss = 0.0023524388260569413\n",
      "Iteration 31982: loss = 0.0023524293171975795\n",
      "Iteration 31983: loss = 0.00235241980844009\n",
      "Iteration 31984: loss = 0.002352410299784472\n",
      "Iteration 31985: loss = 0.0023524007912307235\n",
      "Iteration 31986: loss = 0.002352391282778844\n",
      "Iteration 31987: loss = 0.002352381774428831\n",
      "Iteration 31988: loss = 0.0023523722661806853\n",
      "Iteration 31989: loss = 0.0023523627580344037\n",
      "Iteration 31990: loss = 0.0023523532499899855\n",
      "Iteration 31991: loss = 0.00235234374204743\n",
      "Iteration 31992: loss = 0.002352334234206735\n",
      "Iteration 31993: loss = 0.0023523247264678995\n",
      "Iteration 31994: loss = 0.002352315218830923\n",
      "Iteration 31995: loss = 0.002352305711295804\n",
      "Iteration 31996: loss = 0.0023522962038625395\n",
      "Iteration 31997: loss = 0.0023522866965311304\n",
      "Iteration 31998: loss = 0.0023522771893015747\n",
      "Iteration 31999: loss = 0.002352267682173871\n",
      "Iteration 32000: loss = 0.002352258175148019\n",
      "Iteration 32001: loss = 0.0023522486682240155\n",
      "Iteration 32002: loss = 0.00235223916140186\n",
      "Iteration 32003: loss = 0.0023522296546815524\n",
      "Iteration 32004: loss = 0.00235222014806309\n",
      "Iteration 32005: loss = 0.0023522106415464717\n",
      "Iteration 32006: loss = 0.002352201135131697\n",
      "Iteration 32007: loss = 0.002352191628818764\n",
      "Iteration 32008: loss = 0.002352182122607672\n",
      "Iteration 32009: loss = 0.002352172616498419\n",
      "Iteration 32010: loss = 0.0023521631104910044\n",
      "Iteration 32011: loss = 0.0023521536045854256\n",
      "Iteration 32012: loss = 0.002352144098781683\n",
      "Iteration 32013: loss = 0.002352134593079775\n",
      "Iteration 32014: loss = 0.0023521250874797\n",
      "Iteration 32015: loss = 0.0023521155819814566\n",
      "Iteration 32016: loss = 0.002352106076585043\n",
      "Iteration 32017: loss = 0.0023520965712904595\n",
      "Iteration 32018: loss = 0.0023520870660977034\n",
      "Iteration 32019: loss = 0.002352077561006774\n",
      "Iteration 32020: loss = 0.00235206805601767\n",
      "Iteration 32021: loss = 0.0023520585511303898\n",
      "Iteration 32022: loss = 0.002352049046344933\n",
      "Iteration 32023: loss = 0.002352039541661297\n",
      "Iteration 32024: loss = 0.0023520300370794825\n",
      "Iteration 32025: loss = 0.002352020532599486\n",
      "Iteration 32026: loss = 0.002352011028221307\n",
      "Iteration 32027: loss = 0.002352001523944945\n",
      "Iteration 32028: loss = 0.0023519920197703982\n",
      "Iteration 32029: loss = 0.002351982515697665\n",
      "Iteration 32030: loss = 0.0023519730117267444\n",
      "Iteration 32031: loss = 0.0023519635078576356\n",
      "Iteration 32032: loss = 0.0023519540040903364\n",
      "Iteration 32033: loss = 0.0023519445004248464\n",
      "Iteration 32034: loss = 0.0023519349968611643\n",
      "Iteration 32035: loss = 0.0023519254933992875\n",
      "Iteration 32036: loss = 0.0023519159900392164\n",
      "Iteration 32037: loss = 0.002351906486780949\n",
      "Iteration 32038: loss = 0.002351896983624484\n",
      "Iteration 32039: loss = 0.0023518874805698196\n",
      "Iteration 32040: loss = 0.002351877977616956\n",
      "Iteration 32041: loss = 0.0023518684747658907\n",
      "Iteration 32042: loss = 0.002351858972016623\n",
      "Iteration 32043: loss = 0.0023518494693691512\n",
      "Iteration 32044: loss = 0.002351839966823474\n",
      "Iteration 32045: loss = 0.002351830464379591\n",
      "Iteration 32046: loss = 0.0023518209620374997\n",
      "Iteration 32047: loss = 0.0023518114597971995\n",
      "Iteration 32048: loss = 0.002351801957658689\n",
      "Iteration 32049: loss = 0.0023517924556219674\n",
      "Iteration 32050: loss = 0.002351782953687033\n",
      "Iteration 32051: loss = 0.002351773451853884\n",
      "Iteration 32052: loss = 0.0023517639501225204\n",
      "Iteration 32053: loss = 0.0023517544484929396\n",
      "Iteration 32054: loss = 0.002351744946965141\n",
      "Iteration 32055: loss = 0.002351735445539123\n",
      "Iteration 32056: loss = 0.002351725944214885\n",
      "Iteration 32057: loss = 0.0023517164429924255\n",
      "Iteration 32058: loss = 0.0023517069418717428\n",
      "Iteration 32059: loss = 0.002351697440852836\n",
      "Iteration 32060: loss = 0.002351687939935704\n",
      "Iteration 32061: loss = 0.0023516784391203445\n",
      "Iteration 32062: loss = 0.0023516689384067567\n",
      "Iteration 32063: loss = 0.00235165943779494\n",
      "Iteration 32064: loss = 0.0023516499372848924\n",
      "Iteration 32065: loss = 0.0023516404368766138\n",
      "Iteration 32066: loss = 0.002351630936570102\n",
      "Iteration 32067: loss = 0.002351621436365355\n",
      "Iteration 32068: loss = 0.0023516119362623727\n",
      "Iteration 32069: loss = 0.002351602436261153\n",
      "Iteration 32070: loss = 0.002351592936361696\n",
      "Iteration 32071: loss = 0.002351583436563999\n",
      "Iteration 32072: loss = 0.002351573936868061\n",
      "Iteration 32073: loss = 0.0023515644372738816\n",
      "Iteration 32074: loss = 0.002351554937781458\n",
      "Iteration 32075: loss = 0.002351545438390791\n",
      "Iteration 32076: loss = 0.0023515359391018775\n",
      "Iteration 32077: loss = 0.0023515264399147167\n",
      "Iteration 32078: loss = 0.0023515169408293074\n",
      "Iteration 32079: loss = 0.0023515074418456496\n",
      "Iteration 32080: loss = 0.0023514979429637398\n",
      "Iteration 32081: loss = 0.002351488444183578\n",
      "Iteration 32082: loss = 0.0023514789455051635\n",
      "Iteration 32083: loss = 0.002351469446928493\n",
      "Iteration 32084: loss = 0.0023514599484535667\n",
      "Iteration 32085: loss = 0.002351450450080384\n",
      "Iteration 32086: loss = 0.0023514409518089418\n",
      "Iteration 32087: loss = 0.00235143145363924\n",
      "Iteration 32088: loss = 0.0023514219555712777\n",
      "Iteration 32089: loss = 0.002351412457605052\n",
      "Iteration 32090: loss = 0.0023514029597405633\n",
      "Iteration 32091: loss = 0.00235139346197781\n",
      "Iteration 32092: loss = 0.0023513839643167903\n",
      "Iteration 32093: loss = 0.0023513744667575027\n",
      "Iteration 32094: loss = 0.0023513649692999466\n",
      "Iteration 32095: loss = 0.0023513554719441212\n",
      "Iteration 32096: loss = 0.0023513459746900235\n",
      "Iteration 32097: loss = 0.002351336477537654\n",
      "Iteration 32098: loss = 0.00235132698048701\n",
      "Iteration 32099: loss = 0.002351317483538091\n",
      "Iteration 32100: loss = 0.002351307986690896\n",
      "Iteration 32101: loss = 0.0023512984899454236\n",
      "Iteration 32102: loss = 0.0023512889933016722\n",
      "Iteration 32103: loss = 0.0023512794967596408\n",
      "Iteration 32104: loss = 0.0023512700003193274\n",
      "Iteration 32105: loss = 0.0023512605039807317\n",
      "Iteration 32106: loss = 0.002351251007743852\n",
      "Iteration 32107: loss = 0.002351241511608687\n",
      "Iteration 32108: loss = 0.0023512320155752355\n",
      "Iteration 32109: loss = 0.0023512225196434962\n",
      "Iteration 32110: loss = 0.0023512130238134673\n",
      "Iteration 32111: loss = 0.0023512035280851486\n",
      "Iteration 32112: loss = 0.0023511940324585385\n",
      "Iteration 32113: loss = 0.0023511845369336352\n",
      "Iteration 32114: loss = 0.002351175041510438\n",
      "Iteration 32115: loss = 0.002351165546188945\n",
      "Iteration 32116: loss = 0.002351156050969155\n",
      "Iteration 32117: loss = 0.0023511465558510684\n",
      "Iteration 32118: loss = 0.002351137060834681\n",
      "Iteration 32119: loss = 0.0023511275659199946\n",
      "Iteration 32120: loss = 0.002351118071107006\n",
      "Iteration 32121: loss = 0.002351108576395714\n",
      "Iteration 32122: loss = 0.0023510990817861175\n",
      "Iteration 32123: loss = 0.002351089587278216\n",
      "Iteration 32124: loss = 0.0023510800928720076\n",
      "Iteration 32125: loss = 0.00235107059856749\n",
      "Iteration 32126: loss = 0.0023510611043646647\n",
      "Iteration 32127: loss = 0.0023510516102635275\n",
      "Iteration 32128: loss = 0.002351042116264079\n",
      "Iteration 32129: loss = 0.0023510326223663168\n",
      "Iteration 32130: loss = 0.0023510231285702407\n",
      "Iteration 32131: loss = 0.0023510136348758484\n",
      "Iteration 32132: loss = 0.0023510041412831396\n",
      "Iteration 32133: loss = 0.002350994647792112\n",
      "Iteration 32134: loss = 0.002350985154402765\n",
      "Iteration 32135: loss = 0.002350975661115097\n",
      "Iteration 32136: loss = 0.0023509661679291076\n",
      "Iteration 32137: loss = 0.002350956674844794\n",
      "Iteration 32138: loss = 0.0023509471818621563\n",
      "Iteration 32139: loss = 0.002350937688981193\n",
      "Iteration 32140: loss = 0.0023509281962019015\n",
      "Iteration 32141: loss = 0.0023509187035242827\n",
      "Iteration 32142: loss = 0.0023509092109483334\n",
      "Iteration 32143: loss = 0.0023508997184740533\n",
      "Iteration 32144: loss = 0.002350890226101441\n",
      "Iteration 32145: loss = 0.002350880733830495\n",
      "Iteration 32146: loss = 0.002350871241661215\n",
      "Iteration 32147: loss = 0.002350861749593598\n",
      "Iteration 32148: loss = 0.002350852257627644\n",
      "Iteration 32149: loss = 0.0023508427657633513\n",
      "Iteration 32150: loss = 0.002350833274000719\n",
      "Iteration 32151: loss = 0.0023508237823397452\n",
      "Iteration 32152: loss = 0.0023508142907804294\n",
      "Iteration 32153: loss = 0.0023508047993227696\n",
      "Iteration 32154: loss = 0.0023507953079667646\n",
      "Iteration 32155: loss = 0.002350785816712414\n",
      "Iteration 32156: loss = 0.002350776325559716\n",
      "Iteration 32157: loss = 0.0023507668345086686\n",
      "Iteration 32158: loss = 0.002350757343559271\n",
      "Iteration 32159: loss = 0.002350747852711523\n",
      "Iteration 32160: loss = 0.0023507383619654214\n",
      "Iteration 32161: loss = 0.002350728871320967\n",
      "Iteration 32162: loss = 0.0023507193807781566\n",
      "Iteration 32163: loss = 0.00235070989033699\n",
      "Iteration 32164: loss = 0.0023507003999974664\n",
      "Iteration 32165: loss = 0.002350690909759583\n",
      "Iteration 32166: loss = 0.00235068141962334\n",
      "Iteration 32167: loss = 0.0023506719295887353\n",
      "Iteration 32168: loss = 0.002350662439655768\n",
      "Iteration 32169: loss = 0.002350652949824436\n",
      "Iteration 32170: loss = 0.00235064346009474\n",
      "Iteration 32171: loss = 0.0023506339704666766\n",
      "Iteration 32172: loss = 0.0023506244809402455\n",
      "Iteration 32173: loss = 0.0023506149915154453\n",
      "Iteration 32174: loss = 0.0023506055021922747\n",
      "Iteration 32175: loss = 0.002350596012970733\n",
      "Iteration 32176: loss = 0.002350586523850818\n",
      "Iteration 32177: loss = 0.0023505770348325287\n",
      "Iteration 32178: loss = 0.0023505675459158638\n",
      "Iteration 32179: loss = 0.0023505580571008223\n",
      "Iteration 32180: loss = 0.0023505485683874036\n",
      "Iteration 32181: loss = 0.002350539079775605\n",
      "Iteration 32182: loss = 0.0023505295912654256\n",
      "Iteration 32183: loss = 0.002350520102856865\n",
      "Iteration 32184: loss = 0.0023505106145499215\n",
      "Iteration 32185: loss = 0.002350501126344593\n",
      "Iteration 32186: loss = 0.0023504916382408787\n",
      "Iteration 32187: loss = 0.0023504821502387785\n",
      "Iteration 32188: loss = 0.00235047266233829\n",
      "Iteration 32189: loss = 0.0023504631745394113\n",
      "Iteration 32190: loss = 0.0023504536868421424\n",
      "Iteration 32191: loss = 0.0023504441992464814\n",
      "Iteration 32192: loss = 0.002350434711752428\n",
      "Iteration 32193: loss = 0.0023504252243599796\n",
      "Iteration 32194: loss = 0.002350415737069135\n",
      "Iteration 32195: loss = 0.002350406249879894\n",
      "Iteration 32196: loss = 0.002350396762792254\n",
      "Iteration 32197: loss = 0.002350387275806215\n",
      "Iteration 32198: loss = 0.002350377788921775\n",
      "Iteration 32199: loss = 0.002350368302138933\n",
      "Iteration 32200: loss = 0.002350358815457688\n",
      "Iteration 32201: loss = 0.0023503493288780375\n",
      "Iteration 32202: loss = 0.0023503398423999813\n",
      "Iteration 32203: loss = 0.0023503303560235183\n",
      "Iteration 32204: loss = 0.0023503208697486467\n",
      "Iteration 32205: loss = 0.0023503113835753654\n",
      "Iteration 32206: loss = 0.0023503018975036733\n",
      "Iteration 32207: loss = 0.0023502924115335684\n",
      "Iteration 32208: loss = 0.0023502829256650507\n",
      "Iteration 32209: loss = 0.0023502734398981174\n",
      "Iteration 32210: loss = 0.0023502639542327688\n",
      "Iteration 32211: loss = 0.002350254468669003\n",
      "Iteration 32212: loss = 0.0023502449832068176\n",
      "Iteration 32213: loss = 0.002350235497846213\n",
      "Iteration 32214: loss = 0.002350226012587187\n",
      "Iteration 32215: loss = 0.002350216527429739\n",
      "Iteration 32216: loss = 0.002350207042373867\n",
      "Iteration 32217: loss = 0.0023501975574195698\n",
      "Iteration 32218: loss = 0.0023501880725668466\n",
      "Iteration 32219: loss = 0.002350178587815696\n",
      "Iteration 32220: loss = 0.0023501691031661163\n",
      "Iteration 32221: loss = 0.0023501596186181074\n",
      "Iteration 32222: loss = 0.0023501501341716665\n",
      "Iteration 32223: loss = 0.002350140649826793\n",
      "Iteration 32224: loss = 0.002350131165583486\n",
      "Iteration 32225: loss = 0.002350121681441744\n",
      "Iteration 32226: loss = 0.0023501121974015653\n",
      "Iteration 32227: loss = 0.0023501027134629486\n",
      "Iteration 32228: loss = 0.002350093229625894\n",
      "Iteration 32229: loss = 0.0023500837458903987\n",
      "Iteration 32230: loss = 0.0023500742622564615\n",
      "Iteration 32231: loss = 0.0023500647787240824\n",
      "Iteration 32232: loss = 0.002350055295293259\n",
      "Iteration 32233: loss = 0.0023500458119639902\n",
      "Iteration 32234: loss = 0.0023500363287362754\n",
      "Iteration 32235: loss = 0.0023500268456101117\n",
      "Iteration 32236: loss = 0.0023500173625854996\n",
      "Iteration 32237: loss = 0.002350007879662437\n",
      "Iteration 32238: loss = 0.0023499983968409232\n",
      "Iteration 32239: loss = 0.0023499889141209564\n",
      "Iteration 32240: loss = 0.0023499794315025354\n",
      "Iteration 32241: loss = 0.0023499699489856586\n",
      "Iteration 32242: loss = 0.002349960466570326\n",
      "Iteration 32243: loss = 0.0023499509842565343\n",
      "Iteration 32244: loss = 0.002349941502044284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32245: loss = 0.0023499320199335733\n",
      "Iteration 32246: loss = 0.002349922537924401\n",
      "Iteration 32247: loss = 0.0023499130560167654\n",
      "Iteration 32248: loss = 0.0023499035742106655\n",
      "Iteration 32249: loss = 0.0023498940925061\n",
      "Iteration 32250: loss = 0.0023498846109030677\n",
      "Iteration 32251: loss = 0.002349875129401567\n",
      "Iteration 32252: loss = 0.0023498656480015977\n",
      "Iteration 32253: loss = 0.0023498561667031573\n",
      "Iteration 32254: loss = 0.002349846685506245\n",
      "Iteration 32255: loss = 0.0023498372044108594\n",
      "Iteration 32256: loss = 0.0023498277234169996\n",
      "Iteration 32257: loss = 0.002349818242524664\n",
      "Iteration 32258: loss = 0.002349808761733851\n",
      "Iteration 32259: loss = 0.00234979928104456\n",
      "Iteration 32260: loss = 0.00234978980045679\n",
      "Iteration 32261: loss = 0.0023497803199705384\n",
      "Iteration 32262: loss = 0.0023497708395858055\n",
      "Iteration 32263: loss = 0.0023497613593025887\n",
      "Iteration 32264: loss = 0.0023497518791208874\n",
      "Iteration 32265: loss = 0.002349742399040701\n",
      "Iteration 32266: loss = 0.002349732919062026\n",
      "Iteration 32267: loss = 0.002349723439184864\n",
      "Iteration 32268: loss = 0.0023497139594092116\n",
      "Iteration 32269: loss = 0.0023497044797350687\n",
      "Iteration 32270: loss = 0.002349695000162433\n",
      "Iteration 32271: loss = 0.0023496855206913045\n",
      "Iteration 32272: loss = 0.0023496760413216806\n",
      "Iteration 32273: loss = 0.0023496665620535606\n",
      "Iteration 32274: loss = 0.0023496570828869445\n",
      "Iteration 32275: loss = 0.002349647603821829\n",
      "Iteration 32276: loss = 0.002349638124858214\n",
      "Iteration 32277: loss = 0.002349628645996097\n",
      "Iteration 32278: loss = 0.0023496191672354788\n",
      "Iteration 32279: loss = 0.0023496096885763564\n",
      "Iteration 32280: loss = 0.002349600210018729\n",
      "Iteration 32281: loss = 0.0023495907315625956\n",
      "Iteration 32282: loss = 0.002349581253207955\n",
      "Iteration 32283: loss = 0.002349571774954805\n",
      "Iteration 32284: loss = 0.0023495622968031458\n",
      "Iteration 32285: loss = 0.0023495528187529747\n",
      "Iteration 32286: loss = 0.0023495433408042914\n",
      "Iteration 32287: loss = 0.002349533862957095\n",
      "Iteration 32288: loss = 0.0023495243852113827\n",
      "Iteration 32289: loss = 0.0023495149075671547\n",
      "Iteration 32290: loss = 0.0023495054300244084\n",
      "Iteration 32291: loss = 0.0023494959525831435\n",
      "Iteration 32292: loss = 0.002349486475243359\n",
      "Iteration 32293: loss = 0.0023494769980050524\n",
      "Iteration 32294: loss = 0.0023494675208682236\n",
      "Iteration 32295: loss = 0.0023494580438328705\n",
      "Iteration 32296: loss = 0.0023494485668989927\n",
      "Iteration 32297: loss = 0.0023494390900665883\n",
      "Iteration 32298: loss = 0.002349429613335656\n",
      "Iteration 32299: loss = 0.002349420136706195\n",
      "Iteration 32300: loss = 0.002349410660178203\n",
      "Iteration 32301: loss = 0.00234940118375168\n",
      "Iteration 32302: loss = 0.002349391707426624\n",
      "Iteration 32303: loss = 0.0023493822312030343\n",
      "Iteration 32304: loss = 0.002349372755080909\n",
      "Iteration 32305: loss = 0.0023493632790602477\n",
      "Iteration 32306: loss = 0.0023493538031410474\n",
      "Iteration 32307: loss = 0.002349344327323309\n",
      "Iteration 32308: loss = 0.00234933485160703\n",
      "Iteration 32309: loss = 0.002349325375992209\n",
      "Iteration 32310: loss = 0.0023493159004788445\n",
      "Iteration 32311: loss = 0.0023493064250669367\n",
      "Iteration 32312: loss = 0.002349296949756483\n",
      "Iteration 32313: loss = 0.0023492874745474827\n",
      "Iteration 32314: loss = 0.0023492779994399343\n",
      "Iteration 32315: loss = 0.0023492685244338365\n",
      "Iteration 32316: loss = 0.0023492590495291883\n",
      "Iteration 32317: loss = 0.0023492495747259885\n",
      "Iteration 32318: loss = 0.0023492401000242357\n",
      "Iteration 32319: loss = 0.0023492306254239283\n",
      "Iteration 32320: loss = 0.0023492211509250653\n",
      "Iteration 32321: loss = 0.002349211676527645\n",
      "Iteration 32322: loss = 0.002349202202231666\n",
      "Iteration 32323: loss = 0.0023491927280371292\n",
      "Iteration 32324: loss = 0.0023491832539440306\n",
      "Iteration 32325: loss = 0.0023491737799523705\n",
      "Iteration 32326: loss = 0.0023491643060621474\n",
      "Iteration 32327: loss = 0.002349154832273359\n",
      "Iteration 32328: loss = 0.0023491453585860055\n",
      "Iteration 32329: loss = 0.0023491358850000845\n",
      "Iteration 32330: loss = 0.002349126411515595\n",
      "Iteration 32331: loss = 0.0023491169381325365\n",
      "Iteration 32332: loss = 0.0023491074648509074\n",
      "Iteration 32333: loss = 0.0023490979916707057\n",
      "Iteration 32334: loss = 0.0023490885185919303\n",
      "Iteration 32335: loss = 0.0023490790456145807\n",
      "Iteration 32336: loss = 0.0023490695727386548\n",
      "Iteration 32337: loss = 0.0023490600999641524\n",
      "Iteration 32338: loss = 0.0023490506272910716\n",
      "Iteration 32339: loss = 0.0023490411547194105\n",
      "Iteration 32340: loss = 0.0023490316822491687\n",
      "Iteration 32341: loss = 0.002349022209880344\n",
      "Iteration 32342: loss = 0.002349012737612937\n",
      "Iteration 32343: loss = 0.002349003265446944\n",
      "Iteration 32344: loss = 0.002348993793382365\n",
      "Iteration 32345: loss = 0.0023489843214191996\n",
      "Iteration 32346: loss = 0.0023489748495574454\n",
      "Iteration 32347: loss = 0.002348965377797101\n",
      "Iteration 32348: loss = 0.0023489559061381656\n",
      "Iteration 32349: loss = 0.0023489464345806373\n",
      "Iteration 32350: loss = 0.002348936963124516\n",
      "Iteration 32351: loss = 0.0023489274917697996\n",
      "Iteration 32352: loss = 0.002348918020516487\n",
      "Iteration 32353: loss = 0.0023489085493645766\n",
      "Iteration 32354: loss = 0.002348899078314068\n",
      "Iteration 32355: loss = 0.002348889607364959\n",
      "Iteration 32356: loss = 0.002348880136517249\n",
      "Iteration 32357: loss = 0.002348870665770936\n",
      "Iteration 32358: loss = 0.0023488611951260198\n",
      "Iteration 32359: loss = 0.0023488517245824985\n",
      "Iteration 32360: loss = 0.002348842254140371\n",
      "Iteration 32361: loss = 0.0023488327837996354\n",
      "Iteration 32362: loss = 0.002348823313560291\n",
      "Iteration 32363: loss = 0.0023488138434223363\n",
      "Iteration 32364: loss = 0.00234880437338577\n",
      "Iteration 32365: loss = 0.002348794903450592\n",
      "Iteration 32366: loss = 0.0023487854336167995\n",
      "Iteration 32367: loss = 0.0023487759638843916\n",
      "Iteration 32368: loss = 0.0023487664942533674\n",
      "Iteration 32369: loss = 0.0023487570247237256\n",
      "Iteration 32370: loss = 0.0023487475552954645\n",
      "Iteration 32371: loss = 0.0023487380859685833\n",
      "Iteration 32372: loss = 0.0023487286167430802\n",
      "Iteration 32373: loss = 0.002348719147618955\n",
      "Iteration 32374: loss = 0.002348709678596205\n",
      "Iteration 32375: loss = 0.0023487002096748305\n",
      "Iteration 32376: loss = 0.002348690740854828\n",
      "Iteration 32377: loss = 0.0023486812721361986\n",
      "Iteration 32378: loss = 0.00234867180351894\n",
      "Iteration 32379: loss = 0.0023486623350030505\n",
      "Iteration 32380: loss = 0.00234865286658853\n",
      "Iteration 32381: loss = 0.002348643398275376\n",
      "Iteration 32382: loss = 0.002348633930063588\n",
      "Iteration 32383: loss = 0.0023486244619531646\n",
      "Iteration 32384: loss = 0.002348614993944104\n",
      "Iteration 32385: loss = 0.002348605526036406\n",
      "Iteration 32386: loss = 0.0023485960582300684\n",
      "Iteration 32387: loss = 0.00234858659052509\n",
      "Iteration 32388: loss = 0.0023485771229214704\n",
      "Iteration 32389: loss = 0.0023485676554192074\n",
      "Iteration 32390: loss = 0.0023485581880183\n",
      "Iteration 32391: loss = 0.0023485487207187466\n",
      "Iteration 32392: loss = 0.0023485392535205468\n",
      "Iteration 32393: loss = 0.002348529786423698\n",
      "Iteration 32394: loss = 0.002348520319428201\n",
      "Iteration 32395: loss = 0.0023485108525340527\n",
      "Iteration 32396: loss = 0.0023485013857412525\n",
      "Iteration 32397: loss = 0.002348491919049799\n",
      "Iteration 32398: loss = 0.002348482452459692\n",
      "Iteration 32399: loss = 0.0023484729859709276\n",
      "Iteration 32400: loss = 0.0023484635195835067\n",
      "Iteration 32401: loss = 0.0023484540532974275\n",
      "Iteration 32402: loss = 0.0023484445871126895\n",
      "Iteration 32403: loss = 0.0023484351210292893\n",
      "Iteration 32404: loss = 0.002348425655047228\n",
      "Iteration 32405: loss = 0.0023484161891665035\n",
      "Iteration 32406: loss = 0.0023484067233871136\n",
      "Iteration 32407: loss = 0.002348397257709058\n",
      "Iteration 32408: loss = 0.002348387792132336\n",
      "Iteration 32409: loss = 0.002348378326656945\n",
      "Iteration 32410: loss = 0.002348368861282884\n",
      "Iteration 32411: loss = 0.002348359396010152\n",
      "Iteration 32412: loss = 0.002348349930838748\n",
      "Iteration 32413: loss = 0.0023483404657686706\n",
      "Iteration 32414: loss = 0.0023483310007999187\n",
      "Iteration 32415: loss = 0.0023483215359324902\n",
      "Iteration 32416: loss = 0.002348312071166385\n",
      "Iteration 32417: loss = 0.0023483026065016003\n",
      "Iteration 32418: loss = 0.0023482931419381357\n",
      "Iteration 32419: loss = 0.002348283677475991\n",
      "Iteration 32420: loss = 0.0023482742131151636\n",
      "Iteration 32421: loss = 0.0023482647488556525\n",
      "Iteration 32422: loss = 0.0023482552846974566\n",
      "Iteration 32423: loss = 0.0023482458206405743\n",
      "Iteration 32424: loss = 0.002348236356685005\n",
      "Iteration 32425: loss = 0.0023482268928307465\n",
      "Iteration 32426: loss = 0.002348217429077798\n",
      "Iteration 32427: loss = 0.0023482079654261584\n",
      "Iteration 32428: loss = 0.002348198501875826\n",
      "Iteration 32429: loss = 0.0023481890384268013\n",
      "Iteration 32430: loss = 0.00234817957507908\n",
      "Iteration 32431: loss = 0.0023481701118326626\n",
      "Iteration 32432: loss = 0.002348160648687548\n",
      "Iteration 32433: loss = 0.0023481511856437345\n",
      "Iteration 32434: loss = 0.0023481417227012207\n",
      "Iteration 32435: loss = 0.0023481322598600056\n",
      "Iteration 32436: loss = 0.002348122797120088\n",
      "Iteration 32437: loss = 0.0023481133344814666\n",
      "Iteration 32438: loss = 0.0023481038719441395\n",
      "Iteration 32439: loss = 0.0023480944095081065\n",
      "Iteration 32440: loss = 0.0023480849471733656\n",
      "Iteration 32441: loss = 0.0023480754849399153\n",
      "Iteration 32442: loss = 0.002348066022807755\n",
      "Iteration 32443: loss = 0.002348056560776884\n",
      "Iteration 32444: loss = 0.0023480470988472995\n",
      "Iteration 32445: loss = 0.0023480376370190012\n",
      "Iteration 32446: loss = 0.0023480281752919874\n",
      "Iteration 32447: loss = 0.0023480187136662566\n",
      "Iteration 32448: loss = 0.0023480092521418086\n",
      "Iteration 32449: loss = 0.0023479997907186415\n",
      "Iteration 32450: loss = 0.0023479903293967537\n",
      "Iteration 32451: loss = 0.002347980868176144\n",
      "Iteration 32452: loss = 0.002347971407056812\n",
      "Iteration 32453: loss = 0.0023479619460387556\n",
      "Iteration 32454: loss = 0.002347952485121974\n",
      "Iteration 32455: loss = 0.0023479430243064656\n",
      "Iteration 32456: loss = 0.002347933563592229\n",
      "Iteration 32457: loss = 0.002347924102979264\n",
      "Iteration 32458: loss = 0.0023479146424675674\n",
      "Iteration 32459: loss = 0.0023479051820571393\n",
      "Iteration 32460: loss = 0.0023478957217479782\n",
      "Iteration 32461: loss = 0.002347886261540083\n",
      "Iteration 32462: loss = 0.002347876801433452\n",
      "Iteration 32463: loss = 0.002347867341428084\n",
      "Iteration 32464: loss = 0.002347857881523979\n",
      "Iteration 32465: loss = 0.0023478484217211337\n",
      "Iteration 32466: loss = 0.0023478389620195475\n",
      "Iteration 32467: loss = 0.0023478295024192204\n",
      "Iteration 32468: loss = 0.0023478200429201492\n",
      "Iteration 32469: loss = 0.002347810583522334\n",
      "Iteration 32470: loss = 0.002347801124225773\n",
      "Iteration 32471: loss = 0.002347791665030465\n",
      "Iteration 32472: loss = 0.002347782205936408\n",
      "Iteration 32473: loss = 0.002347772746943603\n",
      "Iteration 32474: loss = 0.0023477632880520463\n",
      "Iteration 32475: loss = 0.002347753829261738\n",
      "Iteration 32476: loss = 0.002347744370572676\n",
      "Iteration 32477: loss = 0.00234773491198486\n",
      "Iteration 32478: loss = 0.002347725453498288\n",
      "Iteration 32479: loss = 0.0023477159951129584\n",
      "Iteration 32480: loss = 0.002347706536828871\n",
      "Iteration 32481: loss = 0.0023476970786460234\n",
      "Iteration 32482: loss = 0.0023476876205644156\n",
      "Iteration 32483: loss = 0.0023476781625840454\n",
      "Iteration 32484: loss = 0.0023476687047049114\n",
      "Iteration 32485: loss = 0.002347659246927013\n",
      "Iteration 32486: loss = 0.002347649789250349\n",
      "Iteration 32487: loss = 0.0023476403316749174\n",
      "Iteration 32488: loss = 0.0023476308742007173\n",
      "Iteration 32489: loss = 0.0023476214168277475\n",
      "Iteration 32490: loss = 0.0023476119595560066\n",
      "Iteration 32491: loss = 0.0023476025023854937\n",
      "Iteration 32492: loss = 0.0023475930453162063\n",
      "Iteration 32493: loss = 0.0023475835883481453\n",
      "Iteration 32494: loss = 0.002347574131481308\n",
      "Iteration 32495: loss = 0.0023475646747156925\n",
      "Iteration 32496: loss = 0.0023475552180512995\n",
      "Iteration 32497: loss = 0.002347545761488126\n",
      "Iteration 32498: loss = 0.002347536305026171\n",
      "Iteration 32499: loss = 0.0023475268486654348\n",
      "Iteration 32500: loss = 0.0023475173924059137\n",
      "Iteration 32501: loss = 0.002347507936247608\n",
      "Iteration 32502: loss = 0.0023474984801905166\n",
      "Iteration 32503: loss = 0.0023474890242346368\n",
      "Iteration 32504: loss = 0.002347479568379969\n",
      "Iteration 32505: loss = 0.002347470112626511\n",
      "Iteration 32506: loss = 0.002347460656974262\n",
      "Iteration 32507: loss = 0.00234745120142322\n",
      "Iteration 32508: loss = 0.002347441745973384\n",
      "Iteration 32509: loss = 0.002347432290624754\n",
      "Iteration 32510: loss = 0.002347422835377327\n",
      "Iteration 32511: loss = 0.0023474133802311023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32512: loss = 0.002347403925186079\n",
      "Iteration 32513: loss = 0.002347394470242255\n",
      "Iteration 32514: loss = 0.0023473850153996295\n",
      "Iteration 32515: loss = 0.002347375560658202\n",
      "Iteration 32516: loss = 0.002347366106017971\n",
      "Iteration 32517: loss = 0.0023473566514789334\n",
      "Iteration 32518: loss = 0.00234734719704109\n",
      "Iteration 32519: loss = 0.0023473377427044397\n",
      "Iteration 32520: loss = 0.0023473282884689797\n",
      "Iteration 32521: loss = 0.0023473188343347095\n",
      "Iteration 32522: loss = 0.002347309380301627\n",
      "Iteration 32523: loss = 0.0023472999263697333\n",
      "Iteration 32524: loss = 0.0023472904725390245\n",
      "Iteration 32525: loss = 0.0023472810188095005\n",
      "Iteration 32526: loss = 0.00234727156518116\n",
      "Iteration 32527: loss = 0.0023472621116540017\n",
      "Iteration 32528: loss = 0.002347252658228024\n",
      "Iteration 32529: loss = 0.002347243204903226\n",
      "Iteration 32530: loss = 0.002347233751679607\n",
      "Iteration 32531: loss = 0.0023472242985571644\n",
      "Iteration 32532: loss = 0.0023472148455358976\n",
      "Iteration 32533: loss = 0.0023472053926158055\n",
      "Iteration 32534: loss = 0.002347195939796887\n",
      "Iteration 32535: loss = 0.00234718648707914\n",
      "Iteration 32536: loss = 0.002347177034462564\n",
      "Iteration 32537: loss = 0.002347167581947158\n",
      "Iteration 32538: loss = 0.0023471581295329193\n",
      "Iteration 32539: loss = 0.0023471486772198486\n",
      "Iteration 32540: loss = 0.0023471392250079427\n",
      "Iteration 32541: loss = 0.0023471297728972014\n",
      "Iteration 32542: loss = 0.002347120320887624\n",
      "Iteration 32543: loss = 0.0023471108689792075\n",
      "Iteration 32544: loss = 0.002347101417171952\n",
      "Iteration 32545: loss = 0.0023470919654658565\n",
      "Iteration 32546: loss = 0.002347082513860918\n",
      "Iteration 32547: loss = 0.002347073062357137\n",
      "Iteration 32548: loss = 0.0023470636109545117\n",
      "Iteration 32549: loss = 0.0023470541596530407\n",
      "Iteration 32550: loss = 0.0023470447084527222\n",
      "Iteration 32551: loss = 0.0023470352573535555\n",
      "Iteration 32552: loss = 0.00234702580635554\n",
      "Iteration 32553: loss = 0.0023470163554586733\n",
      "Iteration 32554: loss = 0.002347006904662955\n",
      "Iteration 32555: loss = 0.002346997453968383\n",
      "Iteration 32556: loss = 0.002346988003374956\n",
      "Iteration 32557: loss = 0.002346978552882674\n",
      "Iteration 32558: loss = 0.0023469691024915348\n",
      "Iteration 32559: loss = 0.002346959652201537\n",
      "Iteration 32560: loss = 0.00234695020201268\n",
      "Iteration 32561: loss = 0.0023469407519249616\n",
      "Iteration 32562: loss = 0.002346931301938381\n",
      "Iteration 32563: loss = 0.002346921852052938\n",
      "Iteration 32564: loss = 0.0023469124022686295\n",
      "Iteration 32565: loss = 0.002346902952585455\n",
      "Iteration 32566: loss = 0.0023468935030034133\n",
      "Iteration 32567: loss = 0.0023468840535225037\n",
      "Iteration 32568: loss = 0.0023468746041427242\n",
      "Iteration 32569: loss = 0.0023468651548640735\n",
      "Iteration 32570: loss = 0.0023468557056865514\n",
      "Iteration 32571: loss = 0.0023468462566101542\n",
      "Iteration 32572: loss = 0.002346836807634883\n",
      "Iteration 32573: loss = 0.002346827358760735\n",
      "Iteration 32574: loss = 0.0023468179099877112\n",
      "Iteration 32575: loss = 0.002346808461315808\n",
      "Iteration 32576: loss = 0.002346799012745025\n",
      "Iteration 32577: loss = 0.002346789564275361\n",
      "Iteration 32578: loss = 0.002346780115906815\n",
      "Iteration 32579: loss = 0.0023467706676393845\n",
      "Iteration 32580: loss = 0.0023467612194730697\n",
      "Iteration 32581: loss = 0.002346751771407868\n",
      "Iteration 32582: loss = 0.0023467423234437798\n",
      "Iteration 32583: loss = 0.0023467328755808026\n",
      "Iteration 32584: loss = 0.0023467234278189354\n",
      "Iteration 32585: loss = 0.002346713980158177\n",
      "Iteration 32586: loss = 0.002346704532598526\n",
      "Iteration 32587: loss = 0.002346695085139981\n",
      "Iteration 32588: loss = 0.0023466856377825415\n",
      "Iteration 32589: loss = 0.0023466761905262053\n",
      "Iteration 32590: loss = 0.0023466667433709714\n",
      "Iteration 32591: loss = 0.00234665729631684\n",
      "Iteration 32592: loss = 0.002346647849363807\n",
      "Iteration 32593: loss = 0.0023466384025118735\n",
      "Iteration 32594: loss = 0.002346628955761037\n",
      "Iteration 32595: loss = 0.002346619509111297\n",
      "Iteration 32596: loss = 0.002346610062562651\n",
      "Iteration 32597: loss = 0.0023466006161150994\n",
      "Iteration 32598: loss = 0.0023465911697686396\n",
      "Iteration 32599: loss = 0.002346581723523271\n",
      "Iteration 32600: loss = 0.002346572277378993\n",
      "Iteration 32601: loss = 0.0023465628313358022\n",
      "Iteration 32602: loss = 0.0023465533853936995\n",
      "Iteration 32603: loss = 0.002346543939552683\n",
      "Iteration 32604: loss = 0.002346534493812751\n",
      "Iteration 32605: loss = 0.0023465250481739023\n",
      "Iteration 32606: loss = 0.002346515602636136\n",
      "Iteration 32607: loss = 0.0023465061571994504\n",
      "Iteration 32608: loss = 0.0023464967118638448\n",
      "Iteration 32609: loss = 0.0023464872666293175\n",
      "Iteration 32610: loss = 0.0023464778214958674\n",
      "Iteration 32611: loss = 0.0023464683764634927\n",
      "Iteration 32612: loss = 0.0023464589315321934\n",
      "Iteration 32613: loss = 0.0023464494867019674\n",
      "Iteration 32614: loss = 0.0023464400419728132\n",
      "Iteration 32615: loss = 0.0023464305973447297\n",
      "Iteration 32616: loss = 0.002346421152817716\n",
      "Iteration 32617: loss = 0.0023464117083917707\n",
      "Iteration 32618: loss = 0.0023464022640668917\n",
      "Iteration 32619: loss = 0.002346392819843079\n",
      "Iteration 32620: loss = 0.002346383375720331\n",
      "Iteration 32621: loss = 0.0023463739316986467\n",
      "Iteration 32622: loss = 0.002346364487778024\n",
      "Iteration 32623: loss = 0.0023463550439584615\n",
      "Iteration 32624: loss = 0.0023463456002399585\n",
      "Iteration 32625: loss = 0.0023463361566225145\n",
      "Iteration 32626: loss = 0.0023463267131061272\n",
      "Iteration 32627: loss = 0.0023463172696907954\n",
      "Iteration 32628: loss = 0.0023463078263765177\n",
      "Iteration 32629: loss = 0.002346298383163293\n",
      "Iteration 32630: loss = 0.00234628894005112\n",
      "Iteration 32631: loss = 0.0023462794970399987\n",
      "Iteration 32632: loss = 0.002346270054129926\n",
      "Iteration 32633: loss = 0.0023462606113209016\n",
      "Iteration 32634: loss = 0.002346251168612924\n",
      "Iteration 32635: loss = 0.002346241726005992\n",
      "Iteration 32636: loss = 0.0023462322835001038\n",
      "Iteration 32637: loss = 0.0023462228410952592\n",
      "Iteration 32638: loss = 0.0023462133987914567\n",
      "Iteration 32639: loss = 0.002346203956588694\n",
      "Iteration 32640: loss = 0.0023461945144869706\n",
      "Iteration 32641: loss = 0.0023461850724862854\n",
      "Iteration 32642: loss = 0.0023461756305866365\n",
      "Iteration 32643: loss = 0.0023461661887880236\n",
      "Iteration 32644: loss = 0.0023461567470904445\n",
      "Iteration 32645: loss = 0.0023461473054938987\n",
      "Iteration 32646: loss = 0.0023461378639983836\n",
      "Iteration 32647: loss = 0.0023461284226038996\n",
      "Iteration 32648: loss = 0.002346118981310445\n",
      "Iteration 32649: loss = 0.002346109540118018\n",
      "Iteration 32650: loss = 0.0023461000990266175\n",
      "Iteration 32651: loss = 0.002346090658036242\n",
      "Iteration 32652: loss = 0.002346081217146891\n",
      "Iteration 32653: loss = 0.0023460717763585624\n",
      "Iteration 32654: loss = 0.002346062335671256\n",
      "Iteration 32655: loss = 0.0023460528950849696\n",
      "Iteration 32656: loss = 0.002346043454599702\n",
      "Iteration 32657: loss = 0.0023460340142154525\n",
      "Iteration 32658: loss = 0.0023460245739322185\n",
      "Iteration 32659: loss = 0.002346015133750001\n",
      "Iteration 32660: loss = 0.0023460056936687964\n",
      "Iteration 32661: loss = 0.002345996253688605\n",
      "Iteration 32662: loss = 0.0023459868138094252\n",
      "Iteration 32663: loss = 0.0023459773740312556\n",
      "Iteration 32664: loss = 0.0023459679343540945\n",
      "Iteration 32665: loss = 0.0023459584947779413\n",
      "Iteration 32666: loss = 0.002345949055302794\n",
      "Iteration 32667: loss = 0.0023459396159286523\n",
      "Iteration 32668: loss = 0.0023459301766555144\n",
      "Iteration 32669: loss = 0.002345920737483379\n",
      "Iteration 32670: loss = 0.0023459112984122454\n",
      "Iteration 32671: loss = 0.002345901859442111\n",
      "Iteration 32672: loss = 0.0023458924205729758\n",
      "Iteration 32673: loss = 0.002345882981804838\n",
      "Iteration 32674: loss = 0.0023458735431376968\n",
      "Iteration 32675: loss = 0.0023458641045715502\n",
      "Iteration 32676: loss = 0.0023458546661063976\n",
      "Iteration 32677: loss = 0.0023458452277422374\n",
      "Iteration 32678: loss = 0.002345835789479068\n",
      "Iteration 32679: loss = 0.002345826351316889\n",
      "Iteration 32680: loss = 0.0023458169132556986\n",
      "Iteration 32681: loss = 0.0023458074752954958\n",
      "Iteration 32682: loss = 0.002345798037436279\n",
      "Iteration 32683: loss = 0.0023457885996780465\n",
      "Iteration 32684: loss = 0.0023457791620207983\n",
      "Iteration 32685: loss = 0.0023457697244645324\n",
      "Iteration 32686: loss = 0.0023457602870092472\n",
      "Iteration 32687: loss = 0.0023457508496549425\n",
      "Iteration 32688: loss = 0.002345741412401616\n",
      "Iteration 32689: loss = 0.0023457319752492665\n",
      "Iteration 32690: loss = 0.002345722538197894\n",
      "Iteration 32691: loss = 0.002345713101247495\n",
      "Iteration 32692: loss = 0.00234570366439807\n",
      "Iteration 32693: loss = 0.0023456942276496176\n",
      "Iteration 32694: loss = 0.0023456847910021364\n",
      "Iteration 32695: loss = 0.0023456753544556243\n",
      "Iteration 32696: loss = 0.002345665918010081\n",
      "Iteration 32697: loss = 0.002345656481665505\n",
      "Iteration 32698: loss = 0.0023456470454218947\n",
      "Iteration 32699: loss = 0.002345637609279249\n",
      "Iteration 32700: loss = 0.002345628173237567\n",
      "Iteration 32701: loss = 0.0023456187372968467\n",
      "Iteration 32702: loss = 0.0023456093014570874\n",
      "Iteration 32703: loss = 0.0023455998657182877\n",
      "Iteration 32704: loss = 0.0023455904300804463\n",
      "Iteration 32705: loss = 0.002345580994543563\n",
      "Iteration 32706: loss = 0.002345571559107634\n",
      "Iteration 32707: loss = 0.0023455621237726603\n",
      "Iteration 32708: loss = 0.00234555268853864\n",
      "Iteration 32709: loss = 0.002345543253405572\n",
      "Iteration 32710: loss = 0.0023455338183734544\n",
      "Iteration 32711: loss = 0.0023455243834422865\n",
      "Iteration 32712: loss = 0.002345514948612067\n",
      "Iteration 32713: loss = 0.0023455055138827934\n",
      "Iteration 32714: loss = 0.0023454960792544666\n",
      "Iteration 32715: loss = 0.0023454866447270846\n",
      "Iteration 32716: loss = 0.002345477210300645\n",
      "Iteration 32717: loss = 0.002345467775975147\n",
      "Iteration 32718: loss = 0.00234545834175059\n",
      "Iteration 32719: loss = 0.002345448907626973\n",
      "Iteration 32720: loss = 0.0023454394736042937\n",
      "Iteration 32721: loss = 0.0023454300396825514\n",
      "Iteration 32722: loss = 0.002345420605861744\n",
      "Iteration 32723: loss = 0.002345411172141872\n",
      "Iteration 32724: loss = 0.002345401738522933\n",
      "Iteration 32725: loss = 0.0023453923050049254\n",
      "Iteration 32726: loss = 0.002345382871587848\n",
      "Iteration 32727: loss = 0.002345373438271701\n",
      "Iteration 32728: loss = 0.0023453640050564817\n",
      "Iteration 32729: loss = 0.0023453545719421885\n",
      "Iteration 32730: loss = 0.0023453451389288216\n",
      "Iteration 32731: loss = 0.002345335706016378\n",
      "Iteration 32732: loss = 0.0023453262732048585\n",
      "Iteration 32733: loss = 0.0023453168404942598\n",
      "Iteration 32734: loss = 0.0023453074078845825\n",
      "Iteration 32735: loss = 0.0023452979753758236\n",
      "Iteration 32736: loss = 0.0023452885429679827\n",
      "Iteration 32737: loss = 0.002345279110661059\n",
      "Iteration 32738: loss = 0.00234526967845505\n",
      "Iteration 32739: loss = 0.002345260246349956\n",
      "Iteration 32740: loss = 0.002345250814345774\n",
      "Iteration 32741: loss = 0.0023452413824425044\n",
      "Iteration 32742: loss = 0.002345231950640145\n",
      "Iteration 32743: loss = 0.002345222518938694\n",
      "Iteration 32744: loss = 0.002345213087338152\n",
      "Iteration 32745: loss = 0.0023452036558385156\n",
      "Iteration 32746: loss = 0.002345194224439785\n",
      "Iteration 32747: loss = 0.002345184793141958\n",
      "Iteration 32748: loss = 0.0023451753619450344\n",
      "Iteration 32749: loss = 0.0023451659308490118\n",
      "Iteration 32750: loss = 0.0023451564998538898\n",
      "Iteration 32751: loss = 0.0023451470689596667\n",
      "Iteration 32752: loss = 0.002345137638166341\n",
      "Iteration 32753: loss = 0.0023451282074739123\n",
      "Iteration 32754: loss = 0.0023451187768823785\n",
      "Iteration 32755: loss = 0.0023451093463917392\n",
      "Iteration 32756: loss = 0.0023450999160019924\n",
      "Iteration 32757: loss = 0.002345090485713136\n",
      "Iteration 32758: loss = 0.0023450810555251706\n",
      "Iteration 32759: loss = 0.0023450716254380943\n",
      "Iteration 32760: loss = 0.0023450621954519053\n",
      "Iteration 32761: loss = 0.0023450527655666025\n",
      "Iteration 32762: loss = 0.0023450433357821853\n",
      "Iteration 32763: loss = 0.0023450339060986512\n",
      "Iteration 32764: loss = 0.002345024476516\n",
      "Iteration 32765: loss = 0.002345015047034231\n",
      "Iteration 32766: loss = 0.0023450056176533406\n",
      "Iteration 32767: loss = 0.00234499618837333\n",
      "Iteration 32768: loss = 0.0023449867591941965\n",
      "Iteration 32769: loss = 0.0023449773301159396\n",
      "Iteration 32770: loss = 0.0023449679011385573\n",
      "Iteration 32771: loss = 0.002344958472262049\n",
      "Iteration 32772: loss = 0.0023449490434864135\n",
      "Iteration 32773: loss = 0.002344939614811649\n",
      "Iteration 32774: loss = 0.002344930186237754\n",
      "Iteration 32775: loss = 0.0023449207577647285\n",
      "Iteration 32776: loss = 0.0023449113293925695\n",
      "Iteration 32777: loss = 0.0023449019011212774\n",
      "Iteration 32778: loss = 0.0023448924729508503\n",
      "Iteration 32779: loss = 0.002344883044881286\n",
      "Iteration 32780: loss = 0.002344873616912585\n",
      "Iteration 32781: loss = 0.002344864189044744\n",
      "Iteration 32782: loss = 0.002344854761277764\n",
      "Iteration 32783: loss = 0.002344845333611642\n",
      "Iteration 32784: loss = 0.0023448359060463777\n",
      "Iteration 32785: loss = 0.002344826478581969\n",
      "Iteration 32786: loss = 0.002344817051218416\n",
      "Iteration 32787: loss = 0.0023448076239557156\n",
      "Iteration 32788: loss = 0.002344798196793868\n",
      "Iteration 32789: loss = 0.002344788769732871\n",
      "Iteration 32790: loss = 0.0023447793427727242\n",
      "Iteration 32791: loss = 0.0023447699159134263\n",
      "Iteration 32792: loss = 0.002344760489154975\n",
      "Iteration 32793: loss = 0.002344751062497369\n",
      "Iteration 32794: loss = 0.002344741635940609\n",
      "Iteration 32795: loss = 0.0023447322094846917\n",
      "Iteration 32796: loss = 0.002344722783129617\n",
      "Iteration 32797: loss = 0.002344713356875383\n",
      "Iteration 32798: loss = 0.0023447039307219883\n",
      "Iteration 32799: loss = 0.0023446945046694326\n",
      "Iteration 32800: loss = 0.002344685078717714\n",
      "Iteration 32801: loss = 0.0023446756528668314\n",
      "Iteration 32802: loss = 0.002344666227116783\n",
      "Iteration 32803: loss = 0.0023446568014675677\n",
      "Iteration 32804: loss = 0.0023446473759191856\n",
      "Iteration 32805: loss = 0.002344637950471634\n",
      "Iteration 32806: loss = 0.002344628525124911\n",
      "Iteration 32807: loss = 0.0023446190998790166\n",
      "Iteration 32808: loss = 0.00234460967473395\n",
      "Iteration 32809: loss = 0.0023446002496897086\n",
      "Iteration 32810: loss = 0.002344590824746292\n",
      "Iteration 32811: loss = 0.0023445813999036984\n",
      "Iteration 32812: loss = 0.002344571975161927\n",
      "Iteration 32813: loss = 0.0023445625505209757\n",
      "Iteration 32814: loss = 0.002344553125980845\n",
      "Iteration 32815: loss = 0.0023445437015415316\n",
      "Iteration 32816: loss = 0.002344534277203036\n",
      "Iteration 32817: loss = 0.0023445248529653556\n",
      "Iteration 32818: loss = 0.0023445154288284894\n",
      "Iteration 32819: loss = 0.0023445060047924365\n",
      "Iteration 32820: loss = 0.002344496580857196\n",
      "Iteration 32821: loss = 0.002344487157022765\n",
      "Iteration 32822: loss = 0.002344477733289145\n",
      "Iteration 32823: loss = 0.0023444683096563316\n",
      "Iteration 32824: loss = 0.002344458886124326\n",
      "Iteration 32825: loss = 0.0023444494626931258\n",
      "Iteration 32826: loss = 0.00234444003936273\n",
      "Iteration 32827: loss = 0.0023444306161331365\n",
      "Iteration 32828: loss = 0.0023444211930043458\n",
      "Iteration 32829: loss = 0.002344411769976355\n",
      "Iteration 32830: loss = 0.0023444023470491635\n",
      "Iteration 32831: loss = 0.0023443929242227706\n",
      "Iteration 32832: loss = 0.002344383501497174\n",
      "Iteration 32833: loss = 0.0023443740788723732\n",
      "Iteration 32834: loss = 0.0023443646563483665\n",
      "Iteration 32835: loss = 0.002344355233925152\n",
      "Iteration 32836: loss = 0.00234434581160273\n",
      "Iteration 32837: loss = 0.0023443363893810992\n",
      "Iteration 32838: loss = 0.002344326967260256\n",
      "Iteration 32839: loss = 0.0023443175452402022\n",
      "Iteration 32840: loss = 0.0023443081233209344\n",
      "Iteration 32841: loss = 0.0023442987015024515\n",
      "Iteration 32842: loss = 0.0023442892797847542\n",
      "Iteration 32843: loss = 0.0023442798581678383\n",
      "Iteration 32844: loss = 0.0023442704366517042\n",
      "Iteration 32845: loss = 0.0023442610152363515\n",
      "Iteration 32846: loss = 0.0023442515939217775\n",
      "Iteration 32847: loss = 0.002344242172707981\n",
      "Iteration 32848: loss = 0.0023442327515949607\n",
      "Iteration 32849: loss = 0.002344223330582716\n",
      "Iteration 32850: loss = 0.002344213909671246\n",
      "Iteration 32851: loss = 0.0023442044888605483\n",
      "Iteration 32852: loss = 0.0023441950681506217\n",
      "Iteration 32853: loss = 0.002344185647541466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32854: loss = 0.0023441762270330793\n",
      "Iteration 32855: loss = 0.00234416680662546\n",
      "Iteration 32856: loss = 0.0023441573863186072\n",
      "Iteration 32857: loss = 0.00234414796611252\n",
      "Iteration 32858: loss = 0.0023441385460071964\n",
      "Iteration 32859: loss = 0.0023441291260026358\n",
      "Iteration 32860: loss = 0.0023441197060988365\n",
      "Iteration 32861: loss = 0.0023441102862957978\n",
      "Iteration 32862: loss = 0.0023441008665935174\n",
      "Iteration 32863: loss = 0.002344091446991995\n",
      "Iteration 32864: loss = 0.002344082027491229\n",
      "Iteration 32865: loss = 0.0023440726080912176\n",
      "Iteration 32866: loss = 0.0023440631887919608\n",
      "Iteration 32867: loss = 0.0023440537695934563\n",
      "Iteration 32868: loss = 0.0023440443504957028\n",
      "Iteration 32869: loss = 0.0023440349314987\n",
      "Iteration 32870: loss = 0.0023440255126024453\n",
      "Iteration 32871: loss = 0.0023440160938069388\n",
      "Iteration 32872: loss = 0.0023440066751121784\n",
      "Iteration 32873: loss = 0.0023439972565181635\n",
      "Iteration 32874: loss = 0.0023439878380248913\n",
      "Iteration 32875: loss = 0.0023439784196323627\n",
      "Iteration 32876: loss = 0.0023439690013405752\n",
      "Iteration 32877: loss = 0.0023439595831495274\n",
      "Iteration 32878: loss = 0.002343950165059218\n",
      "Iteration 32879: loss = 0.0023439407470696467\n",
      "Iteration 32880: loss = 0.002343931329180811\n",
      "Iteration 32881: loss = 0.002343921911392711\n",
      "Iteration 32882: loss = 0.0023439124937053445\n",
      "Iteration 32883: loss = 0.00234390307611871\n",
      "Iteration 32884: loss = 0.002343893658632807\n",
      "Iteration 32885: loss = 0.002343884241247634\n",
      "Iteration 32886: loss = 0.00234387482396319\n",
      "Iteration 32887: loss = 0.0023438654067794726\n",
      "Iteration 32888: loss = 0.002343855989696481\n",
      "Iteration 32889: loss = 0.0023438465727142156\n",
      "Iteration 32890: loss = 0.002343837155832673\n",
      "Iteration 32891: loss = 0.002343827739051853\n",
      "Iteration 32892: loss = 0.002343818322371754\n",
      "Iteration 32893: loss = 0.0023438089057923744\n",
      "Iteration 32894: loss = 0.002343799489313714\n",
      "Iteration 32895: loss = 0.002343790072935771\n",
      "Iteration 32896: loss = 0.002343780656658543\n",
      "Iteration 32897: loss = 0.002343771240482031\n",
      "Iteration 32898: loss = 0.0023437618244062314\n",
      "Iteration 32899: loss = 0.002343752408431145\n",
      "Iteration 32900: loss = 0.0023437429925567688\n",
      "Iteration 32901: loss = 0.0023437335767831032\n",
      "Iteration 32902: loss = 0.002343724161110146\n",
      "Iteration 32903: loss = 0.0023437147455378955\n",
      "Iteration 32904: loss = 0.002343705330066351\n",
      "Iteration 32905: loss = 0.0023436959146955114\n",
      "Iteration 32906: loss = 0.002343686499425375\n",
      "Iteration 32907: loss = 0.002343677084255941\n",
      "Iteration 32908: loss = 0.002343667669187208\n",
      "Iteration 32909: loss = 0.002343658254219175\n",
      "Iteration 32910: loss = 0.0023436488393518396\n",
      "Iteration 32911: loss = 0.002343639424585202\n",
      "Iteration 32912: loss = 0.0023436300099192596\n",
      "Iteration 32913: loss = 0.0023436205953540124\n",
      "Iteration 32914: loss = 0.0023436111808894586\n",
      "Iteration 32915: loss = 0.0023436017665255963\n",
      "Iteration 32916: loss = 0.002343592352262425\n",
      "Iteration 32917: loss = 0.002343582938099943\n",
      "Iteration 32918: loss = 0.00234357352403815\n",
      "Iteration 32919: loss = 0.0023435641100770436\n",
      "Iteration 32920: loss = 0.0023435546962166233\n",
      "Iteration 32921: loss = 0.0023435452824568876\n",
      "Iteration 32922: loss = 0.0023435358687978348\n",
      "Iteration 32923: loss = 0.002343526455239464\n",
      "Iteration 32924: loss = 0.002343517041781774\n",
      "Iteration 32925: loss = 0.0023435076284247635\n",
      "Iteration 32926: loss = 0.0023434982151684315\n",
      "Iteration 32927: loss = 0.0023434888020127764\n",
      "Iteration 32928: loss = 0.002343479388957797\n",
      "Iteration 32929: loss = 0.0023434699760034915\n",
      "Iteration 32930: loss = 0.00234346056314986\n",
      "Iteration 32931: loss = 0.0023434511503968992\n",
      "Iteration 32932: loss = 0.00234344173774461\n",
      "Iteration 32933: loss = 0.00234343232519299\n",
      "Iteration 32934: loss = 0.002343422912742038\n",
      "Iteration 32935: loss = 0.002343413500391753\n",
      "Iteration 32936: loss = 0.002343404088142134\n",
      "Iteration 32937: loss = 0.0023433946759931793\n",
      "Iteration 32938: loss = 0.0023433852639448876\n",
      "Iteration 32939: loss = 0.002343375851997258\n",
      "Iteration 32940: loss = 0.0023433664401502877\n",
      "Iteration 32941: loss = 0.002343357028403978\n",
      "Iteration 32942: loss = 0.0023433476167583254\n",
      "Iteration 32943: loss = 0.00234333820521333\n",
      "Iteration 32944: loss = 0.0023433287937689906\n",
      "Iteration 32945: loss = 0.0023433193824253054\n",
      "Iteration 32946: loss = 0.002343309971182273\n",
      "Iteration 32947: loss = 0.002343300560039892\n",
      "Iteration 32948: loss = 0.002343291148998162\n",
      "Iteration 32949: loss = 0.002343281738057081\n",
      "Iteration 32950: loss = 0.0023432723272166483\n",
      "Iteration 32951: loss = 0.002343262916476862\n",
      "Iteration 32952: loss = 0.002343253505837721\n",
      "Iteration 32953: loss = 0.0023432440952992246\n",
      "Iteration 32954: loss = 0.002343234684861371\n",
      "Iteration 32955: loss = 0.0023432252745241586\n",
      "Iteration 32956: loss = 0.0023432158642875873\n",
      "Iteration 32957: loss = 0.0023432064541516547\n",
      "Iteration 32958: loss = 0.0023431970441163603\n",
      "Iteration 32959: loss = 0.0023431876341817024\n",
      "Iteration 32960: loss = 0.00234317822434768\n",
      "Iteration 32961: loss = 0.0023431688146142915\n",
      "Iteration 32962: loss = 0.002343159404981536\n",
      "Iteration 32963: loss = 0.0023431499954494122\n",
      "Iteration 32964: loss = 0.002343140586017918\n",
      "Iteration 32965: loss = 0.002343131176687054\n",
      "Iteration 32966: loss = 0.0023431217674568174\n",
      "Iteration 32967: loss = 0.002343112358327207\n",
      "Iteration 32968: loss = 0.002343102949298222\n",
      "Iteration 32969: loss = 0.002343093540369862\n",
      "Iteration 32970: loss = 0.002343084131542123\n",
      "Iteration 32971: loss = 0.0023430747228150067\n",
      "Iteration 32972: loss = 0.002343065314188511\n",
      "Iteration 32973: loss = 0.002343055905662633\n",
      "Iteration 32974: loss = 0.0023430464972373733\n",
      "Iteration 32975: loss = 0.0023430370889127305\n",
      "Iteration 32976: loss = 0.002343027680688703\n",
      "Iteration 32977: loss = 0.0023430182725652886\n",
      "Iteration 32978: loss = 0.0023430088645424877\n",
      "Iteration 32979: loss = 0.002342999456620298\n",
      "Iteration 32980: loss = 0.002342990048798718\n",
      "Iteration 32981: loss = 0.0023429806410777475\n",
      "Iteration 32982: loss = 0.002342971233457385\n",
      "Iteration 32983: loss = 0.002342961825937628\n",
      "Iteration 32984: loss = 0.002342952418518477\n",
      "Iteration 32985: loss = 0.002342943011199929\n",
      "Iteration 32986: loss = 0.0023429336039819835\n",
      "Iteration 32987: loss = 0.0023429241968646406\n",
      "Iteration 32988: loss = 0.002342914789847897\n",
      "Iteration 32989: loss = 0.0023429053829317524\n",
      "Iteration 32990: loss = 0.0023428959761162054\n",
      "Iteration 32991: loss = 0.0023428865694012538\n",
      "Iteration 32992: loss = 0.0023428771627868987\n",
      "Iteration 32993: loss = 0.0023428677562731368\n",
      "Iteration 32994: loss = 0.002342858349859967\n",
      "Iteration 32995: loss = 0.002342848943547389\n",
      "Iteration 32996: loss = 0.0023428395373354008\n",
      "Iteration 32997: loss = 0.002342830131224002\n",
      "Iteration 32998: loss = 0.0023428207252131896\n",
      "Iteration 32999: loss = 0.0023428113193029645\n",
      "Iteration 33000: loss = 0.0023428019134933238\n",
      "Iteration 33001: loss = 0.0023427925077842667\n",
      "Iteration 33002: loss = 0.0023427831021757923\n",
      "Iteration 33003: loss = 0.0023427736966678994\n",
      "Iteration 33004: loss = 0.0023427642912605857\n",
      "Iteration 33005: loss = 0.002342754885953852\n",
      "Iteration 33006: loss = 0.002342745480747694\n",
      "Iteration 33007: loss = 0.0023427360756421134\n",
      "Iteration 33008: loss = 0.0023427266706371073\n",
      "Iteration 33009: loss = 0.0023427172657326753\n",
      "Iteration 33010: loss = 0.0023427078609288146\n",
      "Iteration 33011: loss = 0.0023426984562255264\n",
      "Iteration 33012: loss = 0.002342689051622807\n",
      "Iteration 33013: loss = 0.0023426796471206564\n",
      "Iteration 33014: loss = 0.002342670242719074\n",
      "Iteration 33015: loss = 0.0023426608384180567\n",
      "Iteration 33016: loss = 0.0023426514342176044\n",
      "Iteration 33017: loss = 0.0023426420301177163\n",
      "Iteration 33018: loss = 0.0023426326261183896\n",
      "Iteration 33019: loss = 0.0023426232222196245\n",
      "Iteration 33020: loss = 0.002342613818421419\n",
      "Iteration 33021: loss = 0.002342604414723773\n",
      "Iteration 33022: loss = 0.002342595011126683\n",
      "Iteration 33023: loss = 0.002342585607630149\n",
      "Iteration 33024: loss = 0.0023425762042341707\n",
      "Iteration 33025: loss = 0.0023425668009387445\n",
      "Iteration 33026: loss = 0.002342557397743872\n",
      "Iteration 33027: loss = 0.0023425479946495506\n",
      "Iteration 33028: loss = 0.0023425385916557777\n",
      "Iteration 33029: loss = 0.002342529188762554\n",
      "Iteration 33030: loss = 0.0023425197859698777\n",
      "Iteration 33031: loss = 0.0023425103832777463\n",
      "Iteration 33032: loss = 0.0023425009806861603\n",
      "Iteration 33033: loss = 0.002342491578195118\n",
      "Iteration 33034: loss = 0.0023424821758046173\n",
      "Iteration 33035: loss = 0.0023424727735146577\n",
      "Iteration 33036: loss = 0.0023424633713252374\n",
      "Iteration 33037: loss = 0.0023424539692363565\n",
      "Iteration 33038: loss = 0.0023424445672480115\n",
      "Iteration 33039: loss = 0.0023424351653602033\n",
      "Iteration 33040: loss = 0.0023424257635729293\n",
      "Iteration 33041: loss = 0.0023424163618861885\n",
      "Iteration 33042: loss = 0.0023424069602999806\n",
      "Iteration 33043: loss = 0.002342397558814303\n",
      "Iteration 33044: loss = 0.0023423881574291547\n",
      "Iteration 33045: loss = 0.002342378756144535\n",
      "Iteration 33046: loss = 0.0023423693549604425\n",
      "Iteration 33047: loss = 0.0023423599538768755\n",
      "Iteration 33048: loss = 0.002342350552893833\n",
      "Iteration 33049: loss = 0.002342341152011314\n",
      "Iteration 33050: loss = 0.002342331751229317\n",
      "Iteration 33051: loss = 0.002342322350547841\n",
      "Iteration 33052: loss = 0.0023423129499668843\n",
      "Iteration 33053: loss = 0.0023423035494864455\n",
      "Iteration 33054: loss = 0.002342294149106524\n",
      "Iteration 33055: loss = 0.0023422847488271184\n",
      "Iteration 33056: loss = 0.002342275348648227\n",
      "Iteration 33057: loss = 0.0023422659485698495\n",
      "Iteration 33058: loss = 0.002342256548591983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33059: loss = 0.002342247148714627\n",
      "Iteration 33060: loss = 0.002342237748937781\n",
      "Iteration 33061: loss = 0.0023422283492614436\n",
      "Iteration 33062: loss = 0.0023422189496856123\n",
      "Iteration 33063: loss = 0.0023422095502102875\n",
      "Iteration 33064: loss = 0.002342200150835466\n",
      "Iteration 33065: loss = 0.002342190751561149\n",
      "Iteration 33066: loss = 0.002342181352387333\n",
      "Iteration 33067: loss = 0.0023421719533140172\n",
      "Iteration 33068: loss = 0.0023421625543412015\n",
      "Iteration 33069: loss = 0.002342153155468884\n",
      "Iteration 33070: loss = 0.0023421437566970627\n",
      "Iteration 33071: loss = 0.0023421343580257376\n",
      "Iteration 33072: loss = 0.0023421249594549067\n",
      "Iteration 33073: loss = 0.0023421155609845688\n",
      "Iteration 33074: loss = 0.002342106162614723\n",
      "Iteration 33075: loss = 0.002342096764345367\n",
      "Iteration 33076: loss = 0.0023420873661765007\n",
      "Iteration 33077: loss = 0.002342077968108123\n",
      "Iteration 33078: loss = 0.002342068570140231\n",
      "Iteration 33079: loss = 0.0023420591722728245\n",
      "Iteration 33080: loss = 0.002342049774505903\n",
      "Iteration 33081: loss = 0.0023420403768394647\n",
      "Iteration 33082: loss = 0.0023420309792735075\n",
      "Iteration 33083: loss = 0.0023420215818080316\n",
      "Iteration 33084: loss = 0.0023420121844430347\n",
      "Iteration 33085: loss = 0.0023420027871785152\n",
      "Iteration 33086: loss = 0.0023419933900144722\n",
      "Iteration 33087: loss = 0.0023419839929509057\n",
      "Iteration 33088: loss = 0.002341974595987812\n",
      "Iteration 33089: loss = 0.0023419651991251926\n",
      "Iteration 33090: loss = 0.002341955802363044\n",
      "Iteration 33091: loss = 0.0023419464057013667\n",
      "Iteration 33092: loss = 0.002341937009140158\n",
      "Iteration 33093: loss = 0.0023419276126794173\n",
      "Iteration 33094: loss = 0.0023419182163191433\n",
      "Iteration 33095: loss = 0.002341908820059334\n",
      "Iteration 33096: loss = 0.00234189942389999\n",
      "Iteration 33097: loss = 0.0023418900278411076\n",
      "Iteration 33098: loss = 0.0023418806318826874\n",
      "Iteration 33099: loss = 0.0023418712360247277\n",
      "Iteration 33100: loss = 0.0023418618402672275\n",
      "Iteration 33101: loss = 0.0023418524446101843\n",
      "Iteration 33102: loss = 0.002341843049053598\n",
      "Iteration 33103: loss = 0.002341833653597467\n",
      "Iteration 33104: loss = 0.00234182425824179\n",
      "Iteration 33105: loss = 0.0023418148629865657\n",
      "Iteration 33106: loss = 0.0023418054678317935\n",
      "Iteration 33107: loss = 0.0023417960727774713\n",
      "Iteration 33108: loss = 0.0023417866778235982\n",
      "Iteration 33109: loss = 0.0023417772829701726\n",
      "Iteration 33110: loss = 0.0023417678882171936\n",
      "Iteration 33111: loss = 0.00234175849356466\n",
      "Iteration 33112: loss = 0.0023417490990125696\n",
      "Iteration 33113: loss = 0.0023417397045609233\n",
      "Iteration 33114: loss = 0.002341730310209717\n",
      "Iteration 33115: loss = 0.0023417209159589517\n",
      "Iteration 33116: loss = 0.002341711521808626\n",
      "Iteration 33117: loss = 0.0023417021277587374\n",
      "Iteration 33118: loss = 0.0023416927338092853\n",
      "Iteration 33119: loss = 0.002341683339960268\n",
      "Iteration 33120: loss = 0.002341673946211685\n",
      "Iteration 33121: loss = 0.0023416645525635347\n",
      "Iteration 33122: loss = 0.0023416551590158155\n",
      "Iteration 33123: loss = 0.002341645765568527\n",
      "Iteration 33124: loss = 0.002341636372221667\n",
      "Iteration 33125: loss = 0.0023416269789752345\n",
      "Iteration 33126: loss = 0.0023416175858292287\n",
      "Iteration 33127: loss = 0.002341608192783648\n",
      "Iteration 33128: loss = 0.0023415987998384913\n",
      "Iteration 33129: loss = 0.002341589406993757\n",
      "Iteration 33130: loss = 0.002341580014249444\n",
      "Iteration 33131: loss = 0.0023415706216055516\n",
      "Iteration 33132: loss = 0.002341561229062077\n",
      "Iteration 33133: loss = 0.002341551836619021\n",
      "Iteration 33134: loss = 0.0023415424442763807\n",
      "Iteration 33135: loss = 0.0023415330520341554\n",
      "Iteration 33136: loss = 0.002341523659892344\n",
      "Iteration 33137: loss = 0.0023415142678509457\n",
      "Iteration 33138: loss = 0.0023415048759099577\n",
      "Iteration 33139: loss = 0.0023414954840693804\n",
      "Iteration 33140: loss = 0.002341486092329212\n",
      "Iteration 33141: loss = 0.002341476700689451\n",
      "Iteration 33142: loss = 0.0023414673091500964\n",
      "Iteration 33143: loss = 0.0023414579177111466\n",
      "Iteration 33144: loss = 0.0023414485263726\n",
      "Iteration 33145: loss = 0.0023414391351344564\n",
      "Iteration 33146: loss = 0.002341429743996714\n",
      "Iteration 33147: loss = 0.002341420352959372\n",
      "Iteration 33148: loss = 0.0023414109620224282\n",
      "Iteration 33149: loss = 0.002341401571185882\n",
      "Iteration 33150: loss = 0.002341392180449732\n",
      "Iteration 33151: loss = 0.0023413827898139767\n",
      "Iteration 33152: loss = 0.0023413733992786153\n",
      "Iteration 33153: loss = 0.002341364008843646\n",
      "Iteration 33154: loss = 0.002341354618509068\n",
      "Iteration 33155: loss = 0.00234134522827488\n",
      "Iteration 33156: loss = 0.002341335838141081\n",
      "Iteration 33157: loss = 0.002341326448107669\n",
      "Iteration 33158: loss = 0.002341317058174644\n",
      "Iteration 33159: loss = 0.0023413076683420023\n",
      "Iteration 33160: loss = 0.0023412982786097454\n",
      "Iteration 33161: loss = 0.0023412888889778704\n",
      "Iteration 33162: loss = 0.0023412794994463765\n",
      "Iteration 33163: loss = 0.002341270110015263\n",
      "Iteration 33164: loss = 0.002341260720684527\n",
      "Iteration 33165: loss = 0.002341251331454169\n",
      "Iteration 33166: loss = 0.0023412419423241865\n",
      "Iteration 33167: loss = 0.0023412325532945802\n",
      "Iteration 33168: loss = 0.0023412231643653463\n",
      "Iteration 33169: loss = 0.002341213775536485\n",
      "Iteration 33170: loss = 0.0023412043868079953\n",
      "Iteration 33171: loss = 0.0023411949981798747\n",
      "Iteration 33172: loss = 0.0023411856096521226\n",
      "Iteration 33173: loss = 0.0023411762212247377\n",
      "Iteration 33174: loss = 0.002341166832897719\n",
      "Iteration 33175: loss = 0.0023411574446710656\n",
      "Iteration 33176: loss = 0.0023411480565447753\n",
      "Iteration 33177: loss = 0.002341138668518847\n",
      "Iteration 33178: loss = 0.00234112928059328\n",
      "Iteration 33179: loss = 0.002341119892768072\n",
      "Iteration 33180: loss = 0.0023411105050432234\n",
      "Iteration 33181: loss = 0.0023411011174187314\n",
      "Iteration 33182: loss = 0.0023410917298945957\n",
      "Iteration 33183: loss = 0.0023410823424708147\n",
      "Iteration 33184: loss = 0.002341072955147387\n",
      "Iteration 33185: loss = 0.0023410635679243117\n",
      "Iteration 33186: loss = 0.0023410541808015876\n",
      "Iteration 33187: loss = 0.0023410447937792125\n",
      "Iteration 33188: loss = 0.002341035406857186\n",
      "Iteration 33189: loss = 0.0023410260200355066\n",
      "Iteration 33190: loss = 0.002341016633314173\n",
      "Iteration 33191: loss = 0.0023410072466931845\n",
      "Iteration 33192: loss = 0.002340997860172539\n",
      "Iteration 33193: loss = 0.002340988473752236\n",
      "Iteration 33194: loss = 0.0023409790874322736\n",
      "Iteration 33195: loss = 0.0023409697012126513\n",
      "Iteration 33196: loss = 0.0023409603150933668\n",
      "Iteration 33197: loss = 0.0023409509290744195\n",
      "Iteration 33198: loss = 0.0023409415431558077\n",
      "Iteration 33199: loss = 0.0023409321573375315\n",
      "Iteration 33200: loss = 0.0023409227716195873\n",
      "Iteration 33201: loss = 0.002340913386001976\n",
      "Iteration 33202: loss = 0.002340904000484695\n",
      "Iteration 33203: loss = 0.0023408946150677446\n",
      "Iteration 33204: loss = 0.0023408852297511214\n",
      "Iteration 33205: loss = 0.002340875844534826\n",
      "Iteration 33206: loss = 0.0023408664594188563\n",
      "Iteration 33207: loss = 0.00234085707440321\n",
      "Iteration 33208: loss = 0.0023408476894878883\n",
      "Iteration 33209: loss = 0.002340838304672888\n",
      "Iteration 33210: loss = 0.002340828919958209\n",
      "Iteration 33211: loss = 0.002340819535343849\n",
      "Iteration 33212: loss = 0.0023408101508298074\n",
      "Iteration 33213: loss = 0.0023408007664160826\n",
      "Iteration 33214: loss = 0.0023407913821026738\n",
      "Iteration 33215: loss = 0.0023407819978895792\n",
      "Iteration 33216: loss = 0.002340772613776798\n",
      "Iteration 33217: loss = 0.002340763229764329\n",
      "Iteration 33218: loss = 0.0023407538458521705\n",
      "Iteration 33219: loss = 0.002340744462040322\n",
      "Iteration 33220: loss = 0.0023407350783287806\n",
      "Iteration 33221: loss = 0.0023407256947175466\n",
      "Iteration 33222: loss = 0.0023407163112066187\n",
      "Iteration 33223: loss = 0.0023407069277959942\n",
      "Iteration 33224: loss = 0.002340697544485674\n",
      "Iteration 33225: loss = 0.002340688161275655\n",
      "Iteration 33226: loss = 0.0023406787781659368\n",
      "Iteration 33227: loss = 0.002340669395156518\n",
      "Iteration 33228: loss = 0.0023406600122473976\n",
      "Iteration 33229: loss = 0.002340650629438574\n",
      "Iteration 33230: loss = 0.0023406412467300458\n",
      "Iteration 33231: loss = 0.0023406318641218124\n",
      "Iteration 33232: loss = 0.0023406224816138713\n",
      "Iteration 33233: loss = 0.0023406130992062227\n",
      "Iteration 33234: loss = 0.0023406037168988646\n",
      "Iteration 33235: loss = 0.002340594334691796\n",
      "Iteration 33236: loss = 0.002340584952585015\n",
      "Iteration 33237: loss = 0.002340575570578521\n",
      "Iteration 33238: loss = 0.0023405661886723127\n",
      "Iteration 33239: loss = 0.0023405568068663885\n",
      "Iteration 33240: loss = 0.002340547425160748\n",
      "Iteration 33241: loss = 0.002340538043555389\n",
      "Iteration 33242: loss = 0.0023405286620503097\n",
      "Iteration 33243: loss = 0.0023405192806455104\n",
      "Iteration 33244: loss = 0.0023405098993409894\n",
      "Iteration 33245: loss = 0.0023405005181367446\n",
      "Iteration 33246: loss = 0.002340491137032776\n",
      "Iteration 33247: loss = 0.0023404817560290806\n",
      "Iteration 33248: loss = 0.0023404723751256594\n",
      "Iteration 33249: loss = 0.002340462994322509\n",
      "Iteration 33250: loss = 0.00234045361361963\n",
      "Iteration 33251: loss = 0.0023404442330170194\n",
      "Iteration 33252: loss = 0.0023404348525146775\n",
      "Iteration 33253: loss = 0.0023404254721126018\n",
      "Iteration 33254: loss = 0.0023404160918107917\n",
      "Iteration 33255: loss = 0.002340406711609246\n",
      "Iteration 33256: loss = 0.002340397331507963\n",
      "Iteration 33257: loss = 0.002340387951506942\n",
      "Iteration 33258: loss = 0.002340378571606181\n",
      "Iteration 33259: loss = 0.0023403691918056795\n",
      "Iteration 33260: loss = 0.002340359812105436\n",
      "Iteration 33261: loss = 0.002340350432505449\n",
      "Iteration 33262: loss = 0.0023403410530057176\n",
      "Iteration 33263: loss = 0.00234033167360624\n",
      "Iteration 33264: loss = 0.0023403222943070155\n",
      "Iteration 33265: loss = 0.0023403129151080425\n",
      "Iteration 33266: loss = 0.00234030353600932\n",
      "Iteration 33267: loss = 0.002340294157010847\n",
      "Iteration 33268: loss = 0.0023402847781126214\n",
      "Iteration 33269: loss = 0.002340275399314643\n",
      "Iteration 33270: loss = 0.0023402660206169094\n",
      "Iteration 33271: loss = 0.00234025664201942\n",
      "Iteration 33272: loss = 0.0023402472635221734\n",
      "Iteration 33273: loss = 0.0023402378851251683\n",
      "Iteration 33274: loss = 0.0023402285068284045\n",
      "Iteration 33275: loss = 0.0023402191286318787\n",
      "Iteration 33276: loss = 0.0023402097505355914\n",
      "Iteration 33277: loss = 0.00234020037253954\n",
      "Iteration 33278: loss = 0.0023401909946437244\n",
      "Iteration 33279: loss = 0.0023401816168481426\n",
      "Iteration 33280: loss = 0.0023401722391527937\n",
      "Iteration 33281: loss = 0.0023401628615576766\n",
      "Iteration 33282: loss = 0.0023401534840627896\n",
      "Iteration 33283: loss = 0.002340144106668132\n",
      "Iteration 33284: loss = 0.002340134729373702\n",
      "Iteration 33285: loss = 0.002340125352179498\n",
      "Iteration 33286: loss = 0.0023401159750855195\n",
      "Iteration 33287: loss = 0.002340106598091765\n",
      "Iteration 33288: loss = 0.002340097221198234\n",
      "Iteration 33289: loss = 0.002340087844404924\n",
      "Iteration 33290: loss = 0.002340078467711834\n",
      "Iteration 33291: loss = 0.002340069091118963\n",
      "Iteration 33292: loss = 0.00234005971462631\n",
      "Iteration 33293: loss = 0.0023400503382338734\n",
      "Iteration 33294: loss = 0.002340040961941652\n",
      "Iteration 33295: loss = 0.0023400315857496448\n",
      "Iteration 33296: loss = 0.00234002220965785\n",
      "Iteration 33297: loss = 0.0023400128336662665\n",
      "Iteration 33298: loss = 0.0023400034577748937\n",
      "Iteration 33299: loss = 0.0023399940819837295\n",
      "Iteration 33300: loss = 0.002339984706292773\n",
      "Iteration 33301: loss = 0.002339975330702023\n",
      "Iteration 33302: loss = 0.0023399659552114783\n",
      "Iteration 33303: loss = 0.002339956579821138\n",
      "Iteration 33304: loss = 0.0023399472045309995\n",
      "Iteration 33305: loss = 0.0023399378293410627\n",
      "Iteration 33306: loss = 0.0023399284542513256\n",
      "Iteration 33307: loss = 0.0023399190792617883\n",
      "Iteration 33308: loss = 0.002339909704372448\n",
      "Iteration 33309: loss = 0.0023399003295833045\n",
      "Iteration 33310: loss = 0.002339890954894356\n",
      "Iteration 33311: loss = 0.002339881580305601\n",
      "Iteration 33312: loss = 0.0023398722058170394\n",
      "Iteration 33313: loss = 0.0023398628314286684\n",
      "Iteration 33314: loss = 0.002339853457140488\n",
      "Iteration 33315: loss = 0.002339844082952496\n",
      "Iteration 33316: loss = 0.002339834708864692\n",
      "Iteration 33317: loss = 0.0023398253348770742\n",
      "Iteration 33318: loss = 0.0023398159609896415\n",
      "Iteration 33319: loss = 0.0023398065872023924\n",
      "Iteration 33320: loss = 0.002339797213515326\n",
      "Iteration 33321: loss = 0.0023397878399284418\n",
      "Iteration 33322: loss = 0.0023397784664417366\n",
      "Iteration 33323: loss = 0.00233976909305521\n",
      "Iteration 33324: loss = 0.002339759719768862\n",
      "Iteration 33325: loss = 0.0023397503465826894\n",
      "Iteration 33326: loss = 0.0023397409734966924\n",
      "Iteration 33327: loss = 0.002339731600510869\n",
      "Iteration 33328: loss = 0.0023397222276252184\n",
      "Iteration 33329: loss = 0.002339712854839739\n",
      "Iteration 33330: loss = 0.002339703482154429\n",
      "Iteration 33331: loss = 0.0023396941095692883\n",
      "Iteration 33332: loss = 0.0023396847370843153\n",
      "Iteration 33333: loss = 0.002339675364699508\n",
      "Iteration 33334: loss = 0.0023396659924148663\n",
      "Iteration 33335: loss = 0.0023396566202303877\n",
      "Iteration 33336: loss = 0.0023396472481460725\n",
      "Iteration 33337: loss = 0.002339637876161918\n",
      "Iteration 33338: loss = 0.0023396285042779235\n",
      "Iteration 33339: loss = 0.002339619132494087\n",
      "Iteration 33340: loss = 0.002339609760810409\n",
      "Iteration 33341: loss = 0.0023396003892268867\n",
      "Iteration 33342: loss = 0.0023395910177435196\n",
      "Iteration 33343: loss = 0.002339581646360306\n",
      "Iteration 33344: loss = 0.002339572275077245\n",
      "Iteration 33345: loss = 0.0023395629038943356\n",
      "Iteration 33346: loss = 0.002339553532811575\n",
      "Iteration 33347: loss = 0.002339544161828964\n",
      "Iteration 33348: loss = 0.0023395347909465005\n",
      "Iteration 33349: loss = 0.002339525420164183\n",
      "Iteration 33350: loss = 0.00233951604948201\n",
      "Iteration 33351: loss = 0.002339506678899981\n",
      "Iteration 33352: loss = 0.0023394973084180947\n",
      "Iteration 33353: loss = 0.0023394879380363495\n",
      "Iteration 33354: loss = 0.0023394785677547433\n",
      "Iteration 33355: loss = 0.002339469197573277\n",
      "Iteration 33356: loss = 0.002339459827491947\n",
      "Iteration 33357: loss = 0.0023394504575107535\n",
      "Iteration 33358: loss = 0.0023394410876296954\n",
      "Iteration 33359: loss = 0.0023394317178487703\n",
      "Iteration 33360: loss = 0.002339422348167978\n",
      "Iteration 33361: loss = 0.002339412978587316\n",
      "Iteration 33362: loss = 0.002339403609106785\n",
      "Iteration 33363: loss = 0.002339394239726382\n",
      "Iteration 33364: loss = 0.002339384870446106\n",
      "Iteration 33365: loss = 0.0023393755012659566\n",
      "Iteration 33366: loss = 0.002339366132185932\n",
      "Iteration 33367: loss = 0.002339356763206031\n",
      "Iteration 33368: loss = 0.002339347394326252\n",
      "Iteration 33369: loss = 0.0023393380255465946\n",
      "Iteration 33370: loss = 0.002339328656867057\n",
      "Iteration 33371: loss = 0.0023393192882876375\n",
      "Iteration 33372: loss = 0.0023393099198083358\n",
      "Iteration 33373: loss = 0.0023393005514291496\n",
      "Iteration 33374: loss = 0.002339291183150078\n",
      "Iteration 33375: loss = 0.002339281814971121\n",
      "Iteration 33376: loss = 0.0023392724468922757\n",
      "Iteration 33377: loss = 0.002339263078913541\n",
      "Iteration 33378: loss = 0.0023392537110349166\n",
      "Iteration 33379: loss = 0.002339244343256401\n",
      "Iteration 33380: loss = 0.0023392349755779916\n",
      "Iteration 33381: loss = 0.00233922560799969\n",
      "Iteration 33382: loss = 0.0023392162405214915\n",
      "Iteration 33383: loss = 0.002339206873143397\n",
      "Iteration 33384: loss = 0.002339197505865405\n",
      "Iteration 33385: loss = 0.002339188138687514\n",
      "Iteration 33386: loss = 0.002339178771609722\n",
      "Iteration 33387: loss = 0.00233916940463203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33388: loss = 0.002339160037754434\n",
      "Iteration 33389: loss = 0.002339150670976934\n",
      "Iteration 33390: loss = 0.0023391413042995286\n",
      "Iteration 33391: loss = 0.0023391319377222175\n",
      "Iteration 33392: loss = 0.0023391225712449983\n",
      "Iteration 33393: loss = 0.00233911320486787\n",
      "Iteration 33394: loss = 0.0023391038385908313\n",
      "Iteration 33395: loss = 0.002339094472413881\n",
      "Iteration 33396: loss = 0.0023390851063370185\n",
      "Iteration 33397: loss = 0.002339075740360241\n",
      "Iteration 33398: loss = 0.0023390663744835487\n",
      "Iteration 33399: loss = 0.00233905700870694\n",
      "Iteration 33400: loss = 0.002339047643030413\n",
      "Iteration 33401: loss = 0.0023390382774539672\n",
      "Iteration 33402: loss = 0.0023390289119776006\n",
      "Iteration 33403: loss = 0.002339019546601313\n",
      "Iteration 33404: loss = 0.002339010181325103\n",
      "Iteration 33405: loss = 0.002339000816148968\n",
      "Iteration 33406: loss = 0.002338991451072908\n",
      "Iteration 33407: loss = 0.002338982086096921\n",
      "Iteration 33408: loss = 0.0023389727212210065\n",
      "Iteration 33409: loss = 0.0023389633564451628\n",
      "Iteration 33410: loss = 0.0023389539917693897\n",
      "Iteration 33411: loss = 0.0023389446271936836\n",
      "Iteration 33412: loss = 0.002338935262718045\n",
      "Iteration 33413: loss = 0.0023389258983424724\n",
      "Iteration 33414: loss = 0.0023389165340669646\n",
      "Iteration 33415: loss = 0.0023389071698915196\n",
      "Iteration 33416: loss = 0.0023388978058161373\n",
      "Iteration 33417: loss = 0.002338888441840816\n",
      "Iteration 33418: loss = 0.002338879077965554\n",
      "Iteration 33419: loss = 0.00233886971419035\n",
      "Iteration 33420: loss = 0.0023388603505152036\n",
      "Iteration 33421: loss = 0.002338850986940113\n",
      "Iteration 33422: loss = 0.002338841623465077\n",
      "Iteration 33423: loss = 0.0023388322600900934\n",
      "Iteration 33424: loss = 0.002338822896815163\n",
      "Iteration 33425: loss = 0.002338813533640283\n",
      "Iteration 33426: loss = 0.0023388041705654527\n",
      "Iteration 33427: loss = 0.0023387948075906705\n",
      "Iteration 33428: loss = 0.002338785444715935\n",
      "Iteration 33429: loss = 0.002338776081941246\n",
      "Iteration 33430: loss = 0.0023387667192666015\n",
      "Iteration 33431: loss = 0.002338757356692\n",
      "Iteration 33432: loss = 0.0023387479942174405\n",
      "Iteration 33433: loss = 0.0023387386318429225\n",
      "Iteration 33434: loss = 0.002338729269568443\n",
      "Iteration 33435: loss = 0.0023387199073940023\n",
      "Iteration 33436: loss = 0.002338710545319599\n",
      "Iteration 33437: loss = 0.002338701183345231\n",
      "Iteration 33438: loss = 0.0023386918214708973\n",
      "Iteration 33439: loss = 0.0023386824596965975\n",
      "Iteration 33440: loss = 0.0023386730980223293\n",
      "Iteration 33441: loss = 0.0023386637364480917\n",
      "Iteration 33442: loss = 0.0023386543749738834\n",
      "Iteration 33443: loss = 0.002338645013599704\n",
      "Iteration 33444: loss = 0.0023386356523255516\n",
      "Iteration 33445: loss = 0.002338626291151425\n",
      "Iteration 33446: loss = 0.0023386169300773224\n",
      "Iteration 33447: loss = 0.0023386075691032428\n",
      "Iteration 33448: loss = 0.002338598208229186\n",
      "Iteration 33449: loss = 0.0023385888474551494\n",
      "Iteration 33450: loss = 0.002338579486781132\n",
      "Iteration 33451: loss = 0.002338570126207133\n",
      "Iteration 33452: loss = 0.0023385607657331513\n",
      "Iteration 33453: loss = 0.0023385514053591854\n",
      "Iteration 33454: loss = 0.0023385420450852336\n",
      "Iteration 33455: loss = 0.0023385326849112947\n",
      "Iteration 33456: loss = 0.0023385233248373683\n",
      "Iteration 33457: loss = 0.0023385139648634526\n",
      "Iteration 33458: loss = 0.0023385046049895462\n",
      "Iteration 33459: loss = 0.002338495245215648\n",
      "Iteration 33460: loss = 0.0023384858855417566\n",
      "Iteration 33461: loss = 0.0023384765259678707\n",
      "Iteration 33462: loss = 0.00233846716649399\n",
      "Iteration 33463: loss = 0.002338457807120112\n",
      "Iteration 33464: loss = 0.0023384484478462357\n",
      "Iteration 33465: loss = 0.00233843908867236\n",
      "Iteration 33466: loss = 0.0023384297295984844\n",
      "Iteration 33467: loss = 0.0023384203706246064\n",
      "Iteration 33468: loss = 0.0023384110117507253\n",
      "Iteration 33469: loss = 0.00233840165297684\n",
      "Iteration 33470: loss = 0.002338392294302949\n",
      "Iteration 33471: loss = 0.0023383829357290515\n",
      "Iteration 33472: loss = 0.002338373577255145\n",
      "Iteration 33473: loss = 0.0023383642188812303\n",
      "Iteration 33474: loss = 0.002338354860607304\n",
      "Iteration 33475: loss = 0.002338345502433366\n",
      "Iteration 33476: loss = 0.0023383361443594154\n",
      "Iteration 33477: loss = 0.00233832678638545\n",
      "Iteration 33478: loss = 0.0023383174285114693\n",
      "Iteration 33479: loss = 0.0023383080707374713\n",
      "Iteration 33480: loss = 0.0023382987130634554\n",
      "Iteration 33481: loss = 0.0023382893554894204\n",
      "Iteration 33482: loss = 0.0023382799980153643\n",
      "Iteration 33483: loss = 0.002338270640641287\n",
      "Iteration 33484: loss = 0.0023382612833671855\n",
      "Iteration 33485: loss = 0.00233825192619306\n",
      "Iteration 33486: loss = 0.002338242569118909\n",
      "Iteration 33487: loss = 0.0023382332121447305\n",
      "Iteration 33488: loss = 0.0023382238552705245\n",
      "Iteration 33489: loss = 0.0023382144984962884\n",
      "Iteration 33490: loss = 0.0023382051418220228\n",
      "Iteration 33491: loss = 0.002338195785247725\n",
      "Iteration 33492: loss = 0.0023381864287733934\n",
      "Iteration 33493: loss = 0.0023381770723990276\n",
      "Iteration 33494: loss = 0.002338167716124626\n",
      "Iteration 33495: loss = 0.0023381583599501876\n",
      "Iteration 33496: loss = 0.002338149003875711\n",
      "Iteration 33497: loss = 0.002338139647901195\n",
      "Iteration 33498: loss = 0.0023381302920266376\n",
      "Iteration 33499: loss = 0.002338120936252039\n",
      "Iteration 33500: loss = 0.0023381115805773973\n",
      "Iteration 33501: loss = 0.0023381022250027104\n",
      "Iteration 33502: loss = 0.002338092869527979\n",
      "Iteration 33503: loss = 0.0023380835141532\n",
      "Iteration 33504: loss = 0.002338074158878372\n",
      "Iteration 33505: loss = 0.0023380648037034956\n",
      "Iteration 33506: loss = 0.002338055448628568\n",
      "Iteration 33507: loss = 0.0023380460936535887\n",
      "Iteration 33508: loss = 0.002338036738778555\n",
      "Iteration 33509: loss = 0.002338027384003468\n",
      "Iteration 33510: loss = 0.0023380180293283257\n",
      "Iteration 33511: loss = 0.0023380086747531257\n",
      "Iteration 33512: loss = 0.002337999320277867\n",
      "Iteration 33513: loss = 0.0023379899659025497\n",
      "Iteration 33514: loss = 0.002337980611627171\n",
      "Iteration 33515: loss = 0.0023379712574517304\n",
      "Iteration 33516: loss = 0.002337961903376227\n",
      "Iteration 33517: loss = 0.002337952549400659\n",
      "Iteration 33518: loss = 0.002337943195525025\n",
      "Iteration 33519: loss = 0.0023379338417493234\n",
      "Iteration 33520: loss = 0.0023379244880735546\n",
      "Iteration 33521: loss = 0.002337915134497716\n",
      "Iteration 33522: loss = 0.002337905781021806\n",
      "Iteration 33523: loss = 0.0023378964276458245\n",
      "Iteration 33524: loss = 0.0023378870743697693\n",
      "Iteration 33525: loss = 0.0023378777211936403\n",
      "Iteration 33526: loss = 0.0023378683681174353\n",
      "Iteration 33527: loss = 0.002337859015141153\n",
      "Iteration 33528: loss = 0.0023378496622647924\n",
      "Iteration 33529: loss = 0.002337840309488353\n",
      "Iteration 33530: loss = 0.0023378309568118315\n",
      "Iteration 33531: loss = 0.0023378216042352286\n",
      "Iteration 33532: loss = 0.0023378122517585423\n",
      "Iteration 33533: loss = 0.0023378028993817713\n",
      "Iteration 33534: loss = 0.0023377935471049147\n",
      "Iteration 33535: loss = 0.002337784194927971\n",
      "Iteration 33536: loss = 0.0023377748428509396\n",
      "Iteration 33537: loss = 0.0023377654908738176\n",
      "Iteration 33538: loss = 0.0023377561389966053\n",
      "Iteration 33539: loss = 0.002337746787219301\n",
      "Iteration 33540: loss = 0.002337737435541903\n",
      "Iteration 33541: loss = 0.0023377280839644112\n",
      "Iteration 33542: loss = 0.002337718732486823\n",
      "Iteration 33543: loss = 0.0023377093811091376\n",
      "Iteration 33544: loss = 0.0023377000298313534\n",
      "Iteration 33545: loss = 0.0023376906786534706\n",
      "Iteration 33546: loss = 0.002337681327575486\n",
      "Iteration 33547: loss = 0.0023376719765974005\n",
      "Iteration 33548: loss = 0.002337662625719211\n",
      "Iteration 33549: loss = 0.0023376532749409163\n",
      "Iteration 33550: loss = 0.0023376439242625167\n",
      "Iteration 33551: loss = 0.00233763457368401\n",
      "Iteration 33552: loss = 0.0023376252232053946\n",
      "Iteration 33553: loss = 0.0023376158728266694\n",
      "Iteration 33554: loss = 0.0023376065225478334\n",
      "Iteration 33555: loss = 0.0023375971723688855\n",
      "Iteration 33556: loss = 0.0023375878222898243\n",
      "Iteration 33557: loss = 0.002337578472310648\n",
      "Iteration 33558: loss = 0.002337569122431356\n",
      "Iteration 33559: loss = 0.0023375597726519475\n",
      "Iteration 33560: loss = 0.00233755042297242\n",
      "Iteration 33561: loss = 0.002337541073392773\n",
      "Iteration 33562: loss = 0.0023375317239130054\n",
      "Iteration 33563: loss = 0.002337522374533115\n",
      "Iteration 33564: loss = 0.0023375130252531015\n",
      "Iteration 33565: loss = 0.002337503676072964\n",
      "Iteration 33566: loss = 0.0023374943269926997\n",
      "Iteration 33567: loss = 0.0023374849780123086\n",
      "Iteration 33568: loss = 0.002337475629131789\n",
      "Iteration 33569: loss = 0.00233746628035114\n",
      "Iteration 33570: loss = 0.00233745693167036\n",
      "Iteration 33571: loss = 0.0023374475830894477\n",
      "Iteration 33572: loss = 0.0023374382346084024\n",
      "Iteration 33573: loss = 0.002337428886227222\n",
      "Iteration 33574: loss = 0.0023374195379459056\n",
      "Iteration 33575: loss = 0.0023374101897644523\n",
      "Iteration 33576: loss = 0.002337400841682861\n",
      "Iteration 33577: loss = 0.0023373914937011298\n",
      "Iteration 33578: loss = 0.002337382145819257\n",
      "Iteration 33579: loss = 0.0023373727980372425\n",
      "Iteration 33580: loss = 0.0023373634503550842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33581: loss = 0.002337354102772782\n",
      "Iteration 33582: loss = 0.0023373447552903336\n",
      "Iteration 33583: loss = 0.002337335407907738\n",
      "Iteration 33584: loss = 0.002337326060624994\n",
      "Iteration 33585: loss = 0.0023373167134421\n",
      "Iteration 33586: loss = 0.002337307366359055\n",
      "Iteration 33587: loss = 0.0023372980193758584\n",
      "Iteration 33588: loss = 0.0023372886724925073\n",
      "Iteration 33589: loss = 0.0023372793257090026\n",
      "Iteration 33590: loss = 0.0023372699790253416\n",
      "Iteration 33591: loss = 0.0023372606324415235\n",
      "Iteration 33592: loss = 0.002337251285957547\n",
      "Iteration 33593: loss = 0.0023372419395734107\n",
      "Iteration 33594: loss = 0.0023372325932891126\n",
      "Iteration 33595: loss = 0.002337223247104653\n",
      "Iteration 33596: loss = 0.00233721390102003\n",
      "Iteration 33597: loss = 0.0023372045550352424\n",
      "Iteration 33598: loss = 0.002337195209150289\n",
      "Iteration 33599: loss = 0.002337185863365168\n",
      "Iteration 33600: loss = 0.002337176517679879\n",
      "Iteration 33601: loss = 0.0023371671720944192\n",
      "Iteration 33602: loss = 0.002337157826608789\n",
      "Iteration 33603: loss = 0.002337148481222987\n",
      "Iteration 33604: loss = 0.002337139135937011\n",
      "Iteration 33605: loss = 0.0023371297907508613\n",
      "Iteration 33606: loss = 0.0023371204456645346\n",
      "Iteration 33607: loss = 0.0023371111006780304\n",
      "Iteration 33608: loss = 0.0023371017557913486\n",
      "Iteration 33609: loss = 0.0023370924110044868\n",
      "Iteration 33610: loss = 0.0023370830663174435\n",
      "Iteration 33611: loss = 0.002337073721730218\n",
      "Iteration 33612: loss = 0.0023370643772428097\n",
      "Iteration 33613: loss = 0.002337055032855216\n",
      "Iteration 33614: loss = 0.0023370456885674373\n",
      "Iteration 33615: loss = 0.00233703634437947\n",
      "Iteration 33616: loss = 0.002337027000291315\n",
      "Iteration 33617: loss = 0.00233701765630297\n",
      "Iteration 33618: loss = 0.0023370083124144344\n",
      "Iteration 33619: loss = 0.0023369989686257064\n",
      "Iteration 33620: loss = 0.002336989624936785\n",
      "Iteration 33621: loss = 0.0023369802813476685\n",
      "Iteration 33622: loss = 0.002336970937858356\n",
      "Iteration 33623: loss = 0.0023369615944688464\n",
      "Iteration 33624: loss = 0.0023369522511791385\n",
      "Iteration 33625: loss = 0.0023369429079892305\n",
      "Iteration 33626: loss = 0.002336933564899121\n",
      "Iteration 33627: loss = 0.00233692422190881\n",
      "Iteration 33628: loss = 0.002336914879018296\n",
      "Iteration 33629: loss = 0.002336905536227576\n",
      "Iteration 33630: loss = 0.0023368961935366504\n",
      "Iteration 33631: loss = 0.0023368868509455174\n",
      "Iteration 33632: loss = 0.0023368775084541766\n",
      "Iteration 33633: loss = 0.002336868166062625\n",
      "Iteration 33634: loss = 0.0023368588237708634\n",
      "Iteration 33635: loss = 0.002336849481578889\n",
      "Iteration 33636: loss = 0.002336840139486701\n",
      "Iteration 33637: loss = 0.0023368307974942987\n",
      "Iteration 33638: loss = 0.0023368214556016794\n",
      "Iteration 33639: loss = 0.0023368121138088436\n",
      "Iteration 33640: loss = 0.0023368027721157895\n",
      "Iteration 33641: loss = 0.002336793430522515\n",
      "Iteration 33642: loss = 0.002336784089029019\n",
      "Iteration 33643: loss = 0.002336774747635301\n",
      "Iteration 33644: loss = 0.0023367654063413605\n",
      "Iteration 33645: loss = 0.0023367560651471942\n",
      "Iteration 33646: loss = 0.0023367467240528023\n",
      "Iteration 33647: loss = 0.002336737383058183\n",
      "Iteration 33648: loss = 0.002336728042163335\n",
      "Iteration 33649: loss = 0.0023367187013682575\n",
      "Iteration 33650: loss = 0.0023367093606729487\n",
      "Iteration 33651: loss = 0.0023367000200774078\n",
      "Iteration 33652: loss = 0.002336690679581633\n",
      "Iteration 33653: loss = 0.0023366813391856238\n",
      "Iteration 33654: loss = 0.002336671998889378\n",
      "Iteration 33655: loss = 0.002336662658692895\n",
      "Iteration 33656: loss = 0.0023366533185961743\n",
      "Iteration 33657: loss = 0.0023366439785992127\n",
      "Iteration 33658: loss = 0.0023366346387020107\n",
      "Iteration 33659: loss = 0.002336625298904566\n",
      "Iteration 33660: loss = 0.0023366159592068786\n",
      "Iteration 33661: loss = 0.0023366066196089455\n",
      "Iteration 33662: loss = 0.0023365972801107668\n",
      "Iteration 33663: loss = 0.00233658794071234\n",
      "Iteration 33664: loss = 0.0023365786014136656\n",
      "Iteration 33665: loss = 0.002336569262214741\n",
      "Iteration 33666: loss = 0.002336559923115565\n",
      "Iteration 33667: loss = 0.0023365505841161367\n",
      "Iteration 33668: loss = 0.002336541245216456\n",
      "Iteration 33669: loss = 0.0023365319064165193\n",
      "Iteration 33670: loss = 0.002336522567716327\n",
      "Iteration 33671: loss = 0.002336513229115877\n",
      "Iteration 33672: loss = 0.0023365038906151686\n",
      "Iteration 33673: loss = 0.0023364945522142003\n",
      "Iteration 33674: loss = 0.0023364852139129704\n",
      "Iteration 33675: loss = 0.0023364758757114796\n",
      "Iteration 33676: loss = 0.002336466537609724\n",
      "Iteration 33677: loss = 0.002336457199607704\n",
      "Iteration 33678: loss = 0.002336447861705418\n",
      "Iteration 33679: loss = 0.0023364385239028644\n",
      "Iteration 33680: loss = 0.0023364291862000425\n",
      "Iteration 33681: loss = 0.0023364198485969507\n",
      "Iteration 33682: loss = 0.0023364105110935873\n",
      "Iteration 33683: loss = 0.0023364011736899527\n",
      "Iteration 33684: loss = 0.0023363918363860435\n",
      "Iteration 33685: loss = 0.00233638249918186\n",
      "Iteration 33686: loss = 0.0023363731620774\n",
      "Iteration 33687: loss = 0.0023363638250726627\n",
      "Iteration 33688: loss = 0.002336354488167647\n",
      "Iteration 33689: loss = 0.0023363451513623516\n",
      "Iteration 33690: loss = 0.002336335814656775\n",
      "Iteration 33691: loss = 0.0023363264780509154\n",
      "Iteration 33692: loss = 0.002336317141544773\n",
      "Iteration 33693: loss = 0.0023363078051383455\n",
      "Iteration 33694: loss = 0.002336298468831632\n",
      "Iteration 33695: loss = 0.002336289132624631\n",
      "Iteration 33696: loss = 0.002336279796517342\n",
      "Iteration 33697: loss = 0.0023362704605097624\n",
      "Iteration 33698: loss = 0.002336261124601892\n",
      "Iteration 33699: loss = 0.0023362517887937292\n",
      "Iteration 33700: loss = 0.0023362424530852725\n",
      "Iteration 33701: loss = 0.0023362331174765216\n",
      "Iteration 33702: loss = 0.0023362237819674744\n",
      "Iteration 33703: loss = 0.00233621444655813\n",
      "Iteration 33704: loss = 0.002336205111248486\n",
      "Iteration 33705: loss = 0.002336195776038543\n",
      "Iteration 33706: loss = 0.0023361864409282987\n",
      "Iteration 33707: loss = 0.0023361771059177525\n",
      "Iteration 33708: loss = 0.002336167771006902\n",
      "Iteration 33709: loss = 0.0023361584361957467\n",
      "Iteration 33710: loss = 0.002336149101484286\n",
      "Iteration 33711: loss = 0.0023361397668725174\n",
      "Iteration 33712: loss = 0.0023361304323604396\n",
      "Iteration 33713: loss = 0.0023361210979480533\n",
      "Iteration 33714: loss = 0.0023361117636353547\n",
      "Iteration 33715: loss = 0.002336102429422344\n",
      "Iteration 33716: loss = 0.0023360930953090202\n",
      "Iteration 33717: loss = 0.0023360837612953814\n",
      "Iteration 33718: loss = 0.0023360744273814263\n",
      "Iteration 33719: loss = 0.0023360650935671536\n",
      "Iteration 33720: loss = 0.002336055759852563\n",
      "Iteration 33721: loss = 0.002336046426237652\n",
      "Iteration 33722: loss = 0.0023360370927224196\n",
      "Iteration 33723: loss = 0.0023360277593068653\n",
      "Iteration 33724: loss = 0.0023360184259909877\n",
      "Iteration 33725: loss = 0.002336009092774785\n",
      "Iteration 33726: loss = 0.0023359997596582556\n",
      "Iteration 33727: loss = 0.0023359904266413993\n",
      "Iteration 33728: loss = 0.002335981093724214\n",
      "Iteration 33729: loss = 0.0023359717609066997\n",
      "Iteration 33730: loss = 0.0023359624281888538\n",
      "Iteration 33731: loss = 0.0023359530955706755\n",
      "Iteration 33732: loss = 0.0023359437630521636\n",
      "Iteration 33733: loss = 0.0023359344306333168\n",
      "Iteration 33734: loss = 0.0023359250983141337\n",
      "Iteration 33735: loss = 0.0023359157660946136\n",
      "Iteration 33736: loss = 0.0023359064339747546\n",
      "Iteration 33737: loss = 0.0023358971019545563\n",
      "Iteration 33738: loss = 0.0023358877700340158\n",
      "Iteration 33739: loss = 0.002335878438213134\n",
      "Iteration 33740: loss = 0.002335869106491908\n",
      "Iteration 33741: loss = 0.0023358597748703368\n",
      "Iteration 33742: loss = 0.0023358504433484205\n",
      "Iteration 33743: loss = 0.0023358411119261567\n",
      "Iteration 33744: loss = 0.002335831780603543\n",
      "Iteration 33745: loss = 0.002335822449380581\n",
      "Iteration 33746: loss = 0.002335813118257267\n",
      "Iteration 33747: loss = 0.002335803787233601\n",
      "Iteration 33748: loss = 0.002335794456309581\n",
      "Iteration 33749: loss = 0.0023357851254852064\n",
      "Iteration 33750: loss = 0.0023357757947604753\n",
      "Iteration 33751: loss = 0.0023357664641353868\n",
      "Iteration 33752: loss = 0.0023357571336099408\n",
      "Iteration 33753: loss = 0.0023357478031841338\n",
      "Iteration 33754: loss = 0.0023357384728579663\n",
      "Iteration 33755: loss = 0.0023357291426314357\n",
      "Iteration 33756: loss = 0.0023357198125045424\n",
      "Iteration 33757: loss = 0.0023357104824772833\n",
      "Iteration 33758: loss = 0.0023357011525496586\n",
      "Iteration 33759: loss = 0.002335691822721667\n",
      "Iteration 33760: loss = 0.002335682492993306\n",
      "Iteration 33761: loss = 0.0023356731633645757\n",
      "Iteration 33762: loss = 0.0023356638338354737\n",
      "Iteration 33763: loss = 0.002335654504405999\n",
      "Iteration 33764: loss = 0.002335645175076152\n",
      "Iteration 33765: loss = 0.002335635845845929\n",
      "Iteration 33766: loss = 0.0023356265167153306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33767: loss = 0.002335617187684354\n",
      "Iteration 33768: loss = 0.0023356078587529997\n",
      "Iteration 33769: loss = 0.0023355985299212648\n",
      "Iteration 33770: loss = 0.0023355892011891494\n",
      "Iteration 33771: loss = 0.0023355798725566515\n",
      "Iteration 33772: loss = 0.0023355705440237695\n",
      "Iteration 33773: loss = 0.0023355612155905032\n",
      "Iteration 33774: loss = 0.0023355518872568504\n",
      "Iteration 33775: loss = 0.0023355425590228106\n",
      "Iteration 33776: loss = 0.0023355332308883816\n",
      "Iteration 33777: loss = 0.0023355239028535636\n",
      "Iteration 33778: loss = 0.0023355145749183533\n",
      "Iteration 33779: loss = 0.0023355052470827517\n",
      "Iteration 33780: loss = 0.002335495919346756\n",
      "Iteration 33781: loss = 0.0023354865917103655\n",
      "Iteration 33782: loss = 0.0023354772641735787\n",
      "Iteration 33783: loss = 0.0023354679367363954\n",
      "Iteration 33784: loss = 0.002335458609398812\n",
      "Iteration 33785: loss = 0.00233544928216083\n",
      "Iteration 33786: loss = 0.0023354399550224465\n",
      "Iteration 33787: loss = 0.0023354306279836606\n",
      "Iteration 33788: loss = 0.0023354213010444713\n",
      "Iteration 33789: loss = 0.0023354119742048764\n",
      "Iteration 33790: loss = 0.002335402647464876\n",
      "Iteration 33791: loss = 0.0023353933208244684\n",
      "Iteration 33792: loss = 0.002335383994283652\n",
      "Iteration 33793: loss = 0.0023353746678424253\n",
      "Iteration 33794: loss = 0.0023353653415007884\n",
      "Iteration 33795: loss = 0.002335356015258738\n",
      "Iteration 33796: loss = 0.0023353466891162748\n",
      "Iteration 33797: loss = 0.0023353373630733963\n",
      "Iteration 33798: loss = 0.002335328037130102\n",
      "Iteration 33799: loss = 0.0023353187112863908\n",
      "Iteration 33800: loss = 0.00233530938554226\n",
      "Iteration 33801: loss = 0.0023353000598977103\n",
      "Iteration 33802: loss = 0.0023352907343527386\n",
      "Iteration 33803: loss = 0.0023352814089073453\n",
      "Iteration 33804: loss = 0.0023352720835615277\n",
      "Iteration 33805: loss = 0.002335262758315286\n",
      "Iteration 33806: loss = 0.0023352534331686175\n",
      "Iteration 33807: loss = 0.0023352441081215215\n",
      "Iteration 33808: loss = 0.0023352347831739977\n",
      "Iteration 33809: loss = 0.0023352254583260436\n",
      "Iteration 33810: loss = 0.0023352161335776583\n",
      "Iteration 33811: loss = 0.002335206808928841\n",
      "Iteration 33812: loss = 0.0023351974843795896\n",
      "Iteration 33813: loss = 0.002335188159929904\n",
      "Iteration 33814: loss = 0.002335178835579782\n",
      "Iteration 33815: loss = 0.0023351695113292222\n",
      "Iteration 33816: loss = 0.002335160187178225\n",
      "Iteration 33817: loss = 0.0023351508631267864\n",
      "Iteration 33818: loss = 0.0023351415391749076\n",
      "Iteration 33819: loss = 0.0023351322153225868\n",
      "Iteration 33820: loss = 0.0023351228915698217\n",
      "Iteration 33821: loss = 0.002335113567916612\n",
      "Iteration 33822: loss = 0.0023351042443629563\n",
      "Iteration 33823: loss = 0.002335094920908853\n",
      "Iteration 33824: loss = 0.0023350855975543014\n",
      "Iteration 33825: loss = 0.0023350762742992996\n",
      "Iteration 33826: loss = 0.002335066951143847\n",
      "Iteration 33827: loss = 0.0023350576280879417\n",
      "Iteration 33828: loss = 0.0023350483051315834\n",
      "Iteration 33829: loss = 0.00233503898227477\n",
      "Iteration 33830: loss = 0.0023350296595175007\n",
      "Iteration 33831: loss = 0.0023350203368597737\n",
      "Iteration 33832: loss = 0.0023350110143015882\n",
      "Iteration 33833: loss = 0.002335001691842943\n",
      "Iteration 33834: loss = 0.0023349923694838365\n",
      "Iteration 33835: loss = 0.002334983047224268\n",
      "Iteration 33836: loss = 0.002334973725064236\n",
      "Iteration 33837: loss = 0.002334964403003739\n",
      "Iteration 33838: loss = 0.0023349550810427755\n",
      "Iteration 33839: loss = 0.002334945759181345\n",
      "Iteration 33840: loss = 0.0023349364374194463\n",
      "Iteration 33841: loss = 0.0023349271157570776\n",
      "Iteration 33842: loss = 0.0023349177941942374\n",
      "Iteration 33843: loss = 0.0023349084727309253\n",
      "Iteration 33844: loss = 0.0023348991513671394\n",
      "Iteration 33845: loss = 0.0023348898301028785\n",
      "Iteration 33846: loss = 0.002334880508938142\n",
      "Iteration 33847: loss = 0.002334871187872928\n",
      "Iteration 33848: loss = 0.002334861866907236\n",
      "Iteration 33849: loss = 0.0023348525460410627\n",
      "Iteration 33850: loss = 0.0023348432252744094\n",
      "Iteration 33851: loss = 0.002334833904607274\n",
      "Iteration 33852: loss = 0.002334824584039654\n",
      "Iteration 33853: loss = 0.0023348152635715496\n",
      "Iteration 33854: loss = 0.0023348059432029603\n",
      "Iteration 33855: loss = 0.0023347966229338825\n",
      "Iteration 33856: loss = 0.0023347873027643163\n",
      "Iteration 33857: loss = 0.0023347779826942607\n",
      "Iteration 33858: loss = 0.0023347686627237132\n",
      "Iteration 33859: loss = 0.0023347593428526743\n",
      "Iteration 33860: loss = 0.0023347500230811416\n",
      "Iteration 33861: loss = 0.002334740703409114\n",
      "Iteration 33862: loss = 0.00233473138383659\n",
      "Iteration 33863: loss = 0.0023347220643635696\n",
      "Iteration 33864: loss = 0.0023347127449900498\n",
      "Iteration 33865: loss = 0.0023347034257160306\n",
      "Iteration 33866: loss = 0.0023346941065415104\n",
      "Iteration 33867: loss = 0.002334684787466488\n",
      "Iteration 33868: loss = 0.0023346754684909618\n",
      "Iteration 33869: loss = 0.0023346661496149307\n",
      "Iteration 33870: loss = 0.0023346568308383942\n",
      "Iteration 33871: loss = 0.0023346475121613494\n",
      "Iteration 33872: loss = 0.0023346381935837966\n",
      "Iteration 33873: loss = 0.0023346288751057336\n",
      "Iteration 33874: loss = 0.0023346195567271605\n",
      "Iteration 33875: loss = 0.0023346102384480746\n",
      "Iteration 33876: loss = 0.002334600920268475\n",
      "Iteration 33877: loss = 0.0023345916021883613\n",
      "Iteration 33878: loss = 0.0023345822842077302\n",
      "Iteration 33879: loss = 0.0023345729663265835\n",
      "Iteration 33880: loss = 0.002334563648544917\n",
      "Iteration 33881: loss = 0.0023345543308627313\n",
      "Iteration 33882: loss = 0.0023345450132800246\n",
      "Iteration 33883: loss = 0.002334535695796796\n",
      "Iteration 33884: loss = 0.002334526378413043\n",
      "Iteration 33885: loss = 0.0023345170611287656\n",
      "Iteration 33886: loss = 0.0023345077439439624\n",
      "Iteration 33887: loss = 0.002334498426858632\n",
      "Iteration 33888: loss = 0.0023344891098727727\n",
      "Iteration 33889: loss = 0.002334479792986384\n",
      "Iteration 33890: loss = 0.002334470476199464\n",
      "Iteration 33891: loss = 0.0023344611595120116\n",
      "Iteration 33892: loss = 0.002334451842924026\n",
      "Iteration 33893: loss = 0.0023344425264355054\n",
      "Iteration 33894: loss = 0.0023344332100464493\n",
      "Iteration 33895: loss = 0.002334423893756856\n",
      "Iteration 33896: loss = 0.002334414577566724\n",
      "Iteration 33897: loss = 0.0023344052614760516\n",
      "Iteration 33898: loss = 0.0023343959454848393\n",
      "Iteration 33899: loss = 0.0023343866295930838\n",
      "Iteration 33900: loss = 0.002334377313800785\n",
      "Iteration 33901: loss = 0.0023343679981079417\n",
      "Iteration 33902: loss = 0.0023343586825145526\n",
      "Iteration 33903: loss = 0.002334349367020616\n",
      "Iteration 33904: loss = 0.002334340051626131\n",
      "Iteration 33905: loss = 0.0023343307363310967\n",
      "Iteration 33906: loss = 0.002334321421135511\n",
      "Iteration 33907: loss = 0.002334312106039373\n",
      "Iteration 33908: loss = 0.002334302791042681\n",
      "Iteration 33909: loss = 0.0023342934761454355\n",
      "Iteration 33910: loss = 0.0023342841613476334\n",
      "Iteration 33911: loss = 0.002334274846649273\n",
      "Iteration 33912: loss = 0.002334265532050356\n",
      "Iteration 33913: loss = 0.002334256217550878\n",
      "Iteration 33914: loss = 0.00233424690315084\n",
      "Iteration 33915: loss = 0.0023342375888502393\n",
      "Iteration 33916: loss = 0.0023342282746490754\n",
      "Iteration 33917: loss = 0.0023342189605473464\n",
      "Iteration 33918: loss = 0.0023342096465450517\n",
      "Iteration 33919: loss = 0.0023342003326421892\n",
      "Iteration 33920: loss = 0.002334191018838759\n",
      "Iteration 33921: loss = 0.0023341817051347586\n",
      "Iteration 33922: loss = 0.002334172391530188\n",
      "Iteration 33923: loss = 0.0023341630780250445\n",
      "Iteration 33924: loss = 0.0023341537646193278\n",
      "Iteration 33925: loss = 0.0023341444513130362\n",
      "Iteration 33926: loss = 0.002334135138106169\n",
      "Iteration 33927: loss = 0.002334125824998725\n",
      "Iteration 33928: loss = 0.002334116511990702\n",
      "Iteration 33929: loss = 0.0023341071990820987\n",
      "Iteration 33930: loss = 0.0023340978862729156\n",
      "Iteration 33931: loss = 0.00233408857356315\n",
      "Iteration 33932: loss = 0.0023340792609528004\n",
      "Iteration 33933: loss = 0.002334069948441867\n",
      "Iteration 33934: loss = 0.002334060636030347\n",
      "Iteration 33935: loss = 0.00233405132371824\n",
      "Iteration 33936: loss = 0.002334042011505545\n",
      "Iteration 33937: loss = 0.00233403269939226\n",
      "Iteration 33938: loss = 0.002334023387378384\n",
      "Iteration 33939: loss = 0.002334014075463916\n",
      "Iteration 33940: loss = 0.0023340047636488544\n",
      "Iteration 33941: loss = 0.002333995451933198\n",
      "Iteration 33942: loss = 0.0023339861403169466\n",
      "Iteration 33943: loss = 0.0023339768288000973\n",
      "Iteration 33944: loss = 0.0023339675173826495\n",
      "Iteration 33945: loss = 0.0023339582060646026\n",
      "Iteration 33946: loss = 0.0023339488948459545\n",
      "Iteration 33947: loss = 0.002333939583726704\n",
      "Iteration 33948: loss = 0.0023339302727068507\n",
      "Iteration 33949: loss = 0.002333920961786392\n",
      "Iteration 33950: loss = 0.0023339116509653287\n",
      "Iteration 33951: loss = 0.0023339023402436576\n",
      "Iteration 33952: loss = 0.002333893029621377\n",
      "Iteration 33953: loss = 0.0023338837190984884\n",
      "Iteration 33954: loss = 0.002333874408674988\n",
      "Iteration 33955: loss = 0.002333865098350876\n",
      "Iteration 33956: loss = 0.0023338557881261504\n",
      "Iteration 33957: loss = 0.00233384647800081\n",
      "Iteration 33958: loss = 0.002333837167974854\n",
      "Iteration 33959: loss = 0.002333827858048281\n",
      "Iteration 33960: loss = 0.0023338185482210893\n",
      "Iteration 33961: loss = 0.0023338092384932786\n",
      "Iteration 33962: loss = 0.0023337999288648463\n",
      "Iteration 33963: loss = 0.002333790619335792\n",
      "Iteration 33964: loss = 0.0023337813099061147\n",
      "Iteration 33965: loss = 0.002333772000575813\n",
      "Iteration 33966: loss = 0.0023337626913448846\n",
      "Iteration 33967: loss = 0.00233375338221333\n",
      "Iteration 33968: loss = 0.002333744073181147\n",
      "Iteration 33969: loss = 0.002333734764248334\n",
      "Iteration 33970: loss = 0.0023337254554148903\n",
      "Iteration 33971: loss = 0.002333716146680814\n",
      "Iteration 33972: loss = 0.0023337068380461057\n",
      "Iteration 33973: loss = 0.0023336975295107617\n",
      "Iteration 33974: loss = 0.002333688221074782\n",
      "Iteration 33975: loss = 0.002333678912738166\n",
      "Iteration 33976: loss = 0.002333669604500911\n",
      "Iteration 33977: loss = 0.002333660296363016\n",
      "Iteration 33978: loss = 0.0023336509883244818\n",
      "Iteration 33979: loss = 0.002333641680385304\n",
      "Iteration 33980: loss = 0.002333632372545484\n",
      "Iteration 33981: loss = 0.002333623064805018\n",
      "Iteration 33982: loss = 0.0023336137571639075\n",
      "Iteration 33983: loss = 0.0023336044496221494\n",
      "Iteration 33984: loss = 0.002333595142179743\n",
      "Iteration 33985: loss = 0.0023335858348366876\n",
      "Iteration 33986: loss = 0.002333576527592981\n",
      "Iteration 33987: loss = 0.002333567220448623\n",
      "Iteration 33988: loss = 0.0023335579134036106\n",
      "Iteration 33989: loss = 0.0023335486064579437\n",
      "Iteration 33990: loss = 0.002333539299611622\n",
      "Iteration 33991: loss = 0.002333529992864643\n",
      "Iteration 33992: loss = 0.002333520686217005\n",
      "Iteration 33993: loss = 0.0023335113796687087\n",
      "Iteration 33994: loss = 0.0023335020732197505\n",
      "Iteration 33995: loss = 0.002333492766870131\n",
      "Iteration 33996: loss = 0.002333483460619848\n",
      "Iteration 33997: loss = 0.0023334741544689007\n",
      "Iteration 33998: loss = 0.002333464848417287\n",
      "Iteration 33999: loss = 0.0023334555424650073\n",
      "Iteration 34000: loss = 0.002333446236612059\n",
      "Iteration 34001: loss = 0.0023334369308584406\n",
      "Iteration 34002: loss = 0.0023334276252041522\n",
      "Iteration 34003: loss = 0.002333418319649191\n",
      "Iteration 34004: loss = 0.0023334090141935576\n",
      "Iteration 34005: loss = 0.0023333997088372488\n",
      "Iteration 34006: loss = 0.0023333904035802654\n",
      "Iteration 34007: loss = 0.002333381098422604\n",
      "Iteration 34008: loss = 0.0023333717933642644\n",
      "Iteration 34009: loss = 0.002333362488405246\n",
      "Iteration 34010: loss = 0.002333353183545546\n",
      "Iteration 34011: loss = 0.002333343878785165\n",
      "Iteration 34012: loss = 0.0023333345741241\n",
      "Iteration 34013: loss = 0.002333325269562352\n",
      "Iteration 34014: loss = 0.0023333159650999165\n",
      "Iteration 34015: loss = 0.0023333066607367945\n",
      "Iteration 34016: loss = 0.0023332973564729855\n",
      "Iteration 34017: loss = 0.002333288052308486\n",
      "Iteration 34018: loss = 0.002333278748243296\n",
      "Iteration 34019: loss = 0.0023332694442774137\n",
      "Iteration 34020: loss = 0.0023332601404108385\n",
      "Iteration 34021: loss = 0.002333250836643569\n",
      "Iteration 34022: loss = 0.002333241532975604\n",
      "Iteration 34023: loss = 0.0023332322294069415\n",
      "Iteration 34024: loss = 0.0023332229259375816\n",
      "Iteration 34025: loss = 0.002333213622567522\n",
      "Iteration 34026: loss = 0.0023332043192967615\n",
      "Iteration 34027: loss = 0.0023331950161252996\n",
      "Iteration 34028: loss = 0.002333185713053134\n",
      "Iteration 34029: loss = 0.002333176410080264\n",
      "Iteration 34030: loss = 0.002333167107206689\n",
      "Iteration 34031: loss = 0.0023331578044324063\n",
      "Iteration 34032: loss = 0.0023331485017574157\n",
      "Iteration 34033: loss = 0.0023331391991817164\n",
      "Iteration 34034: loss = 0.0023331298967053053\n",
      "Iteration 34035: loss = 0.002333120594328183\n",
      "Iteration 34036: loss = 0.0023331112920503476\n",
      "Iteration 34037: loss = 0.002333101989871798\n",
      "Iteration 34038: loss = 0.0023330926877925323\n",
      "Iteration 34039: loss = 0.0023330833858125504\n",
      "Iteration 34040: loss = 0.00233307408393185\n",
      "Iteration 34041: loss = 0.0023330647821504296\n",
      "Iteration 34042: loss = 0.0023330554804682894\n",
      "Iteration 34043: loss = 0.0023330461788854274\n",
      "Iteration 34044: loss = 0.0023330368774018423\n",
      "Iteration 34045: loss = 0.0023330275760175324\n",
      "Iteration 34046: loss = 0.0023330182747324972\n",
      "Iteration 34047: loss = 0.0023330089735467347\n",
      "Iteration 34048: loss = 0.0023329996724602447\n",
      "Iteration 34049: loss = 0.0023329903714730256\n",
      "Iteration 34050: loss = 0.0023329810705850756\n",
      "Iteration 34051: loss = 0.002332971769796394\n",
      "Iteration 34052: loss = 0.0023329624691069786\n",
      "Iteration 34053: loss = 0.0023329531685168294\n",
      "Iteration 34054: loss = 0.0023329438680259446\n",
      "Iteration 34055: loss = 0.0023329345676343233\n",
      "Iteration 34056: loss = 0.0023329252673419633\n",
      "Iteration 34057: loss = 0.002332915967148865\n",
      "Iteration 34058: loss = 0.0023329066670550254\n",
      "Iteration 34059: loss = 0.0023328973670604437\n",
      "Iteration 34060: loss = 0.00233288806716512\n",
      "Iteration 34061: loss = 0.002332878767369051\n",
      "Iteration 34062: loss = 0.002332869467672237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34063: loss = 0.002332860168074676\n",
      "Iteration 34064: loss = 0.0023328508685763675\n",
      "Iteration 34065: loss = 0.002332841569177309\n",
      "Iteration 34066: loss = 0.0023328322698775\n",
      "Iteration 34067: loss = 0.00233282297067694\n",
      "Iteration 34068: loss = 0.0023328136715756263\n",
      "Iteration 34069: loss = 0.0023328043725735588\n",
      "Iteration 34070: loss = 0.0023327950736707357\n",
      "Iteration 34071: loss = 0.002332785774867156\n",
      "Iteration 34072: loss = 0.002332776476162818\n",
      "Iteration 34073: loss = 0.0023327671775577206\n",
      "Iteration 34074: loss = 0.0023327578790518634\n",
      "Iteration 34075: loss = 0.002332748580645244\n",
      "Iteration 34076: loss = 0.0023327392823378618\n",
      "Iteration 34077: loss = 0.002332729984129715\n",
      "Iteration 34078: loss = 0.002332720686020803\n",
      "Iteration 34079: loss = 0.002332711388011125\n",
      "Iteration 34080: loss = 0.0023327020901006777\n",
      "Iteration 34081: loss = 0.0023326927922894627\n",
      "Iteration 34082: loss = 0.002332683494577476\n",
      "Iteration 34083: loss = 0.002332674196964718\n",
      "Iteration 34084: loss = 0.002332664899451187\n",
      "Iteration 34085: loss = 0.002332655602036882\n",
      "Iteration 34086: loss = 0.002332646304721802\n",
      "Iteration 34087: loss = 0.0023326370075059445\n",
      "Iteration 34088: loss = 0.0023326277103893093\n",
      "Iteration 34089: loss = 0.002332618413371895\n",
      "Iteration 34090: loss = 0.0023326091164537007\n",
      "Iteration 34091: loss = 0.002332599819634724\n",
      "Iteration 34092: loss = 0.0023325905229149646\n",
      "Iteration 34093: loss = 0.0023325812262944222\n",
      "Iteration 34094: loss = 0.002332571929773093\n",
      "Iteration 34095: loss = 0.0023325626333509774\n",
      "Iteration 34096: loss = 0.0023325533370280745\n",
      "Iteration 34097: loss = 0.0023325440408043818\n",
      "Iteration 34098: loss = 0.0023325347446798987\n",
      "Iteration 34099: loss = 0.0023325254486546245\n",
      "Iteration 34100: loss = 0.0023325161527285574\n",
      "Iteration 34101: loss = 0.002332506856901696\n",
      "Iteration 34102: loss = 0.0023324975611740393\n",
      "Iteration 34103: loss = 0.0023324882655455857\n",
      "Iteration 34104: loss = 0.0023324789700163344\n",
      "Iteration 34105: loss = 0.0023324696745862846\n",
      "Iteration 34106: loss = 0.002332460379255434\n",
      "Iteration 34107: loss = 0.002332451084023781\n",
      "Iteration 34108: loss = 0.002332441788891326\n",
      "Iteration 34109: loss = 0.002332432493858067\n",
      "Iteration 34110: loss = 0.002332423198924003\n",
      "Iteration 34111: loss = 0.0023324139040891314\n",
      "Iteration 34112: loss = 0.002332404609353453\n",
      "Iteration 34113: loss = 0.0023323953147169653\n",
      "Iteration 34114: loss = 0.002332386020179667\n",
      "Iteration 34115: loss = 0.002332376725741557\n",
      "Iteration 34116: loss = 0.0023323674314026342\n",
      "Iteration 34117: loss = 0.002332358137162898\n",
      "Iteration 34118: loss = 0.002332348843022346\n",
      "Iteration 34119: loss = 0.002332339548980978\n",
      "Iteration 34120: loss = 0.0023323302550387914\n",
      "Iteration 34121: loss = 0.002332320961195786\n",
      "Iteration 34122: loss = 0.002332311667451961\n",
      "Iteration 34123: loss = 0.0023323023738073133\n",
      "Iteration 34124: loss = 0.0023322930802618437\n",
      "Iteration 34125: loss = 0.00233228378681555\n",
      "Iteration 34126: loss = 0.002332274493468431\n",
      "Iteration 34127: loss = 0.0023322652002204856\n",
      "Iteration 34128: loss = 0.002332255907071712\n",
      "Iteration 34129: loss = 0.0023322466140221097\n",
      "Iteration 34130: loss = 0.0023322373210716775\n",
      "Iteration 34131: loss = 0.0023322280282204134\n",
      "Iteration 34132: loss = 0.002332218735468317\n",
      "Iteration 34133: loss = 0.002332209442815386\n",
      "Iteration 34134: loss = 0.0023322001502616204\n",
      "Iteration 34135: loss = 0.002332190857807018\n",
      "Iteration 34136: loss = 0.0023321815654515784\n",
      "Iteration 34137: loss = 0.0023321722731952993\n",
      "Iteration 34138: loss = 0.00233216298103818\n",
      "Iteration 34139: loss = 0.0023321536889802192\n",
      "Iteration 34140: loss = 0.002332144397021416\n",
      "Iteration 34141: loss = 0.002332135105161769\n",
      "Iteration 34142: loss = 0.0023321258134012765\n",
      "Iteration 34143: loss = 0.0023321165217399377\n",
      "Iteration 34144: loss = 0.0023321072301777514\n",
      "Iteration 34145: loss = 0.002332097938714716\n",
      "Iteration 34146: loss = 0.0023320886473508304\n",
      "Iteration 34147: loss = 0.0023320793560860933\n",
      "Iteration 34148: loss = 0.0023320700649205038\n",
      "Iteration 34149: loss = 0.00233206077385406\n",
      "Iteration 34150: loss = 0.002332051482886762\n",
      "Iteration 34151: loss = 0.002332042192018607\n",
      "Iteration 34152: loss = 0.0023320329012495944\n",
      "Iteration 34153: loss = 0.0023320236105797225\n",
      "Iteration 34154: loss = 0.0023320143200089914\n",
      "Iteration 34155: loss = 0.0023320050295373984\n",
      "Iteration 34156: loss = 0.0023319957391649427\n",
      "Iteration 34157: loss = 0.0023319864488916234\n",
      "Iteration 34158: loss = 0.0023319771587174388\n",
      "Iteration 34159: loss = 0.002331967868642388\n",
      "Iteration 34160: loss = 0.0023319585786664696\n",
      "Iteration 34161: loss = 0.0023319492887896825\n",
      "Iteration 34162: loss = 0.0023319399990120253\n",
      "Iteration 34163: loss = 0.0023319307093334963\n",
      "Iteration 34164: loss = 0.002331921419754096\n",
      "Iteration 34165: loss = 0.0023319121302738206\n",
      "Iteration 34166: loss = 0.0023319028408926704\n",
      "Iteration 34167: loss = 0.002331893551610644\n",
      "Iteration 34168: loss = 0.00233188426242774\n",
      "Iteration 34169: loss = 0.0023318749733439574\n",
      "Iteration 34170: loss = 0.0023318656843592947\n",
      "Iteration 34171: loss = 0.002331856395473751\n",
      "Iteration 34172: loss = 0.0023318471066873245\n",
      "Iteration 34173: loss = 0.002331837818000014\n",
      "Iteration 34174: loss = 0.002331828529411819\n",
      "Iteration 34175: loss = 0.0023318192409227372\n",
      "Iteration 34176: loss = 0.0023318099525327677\n",
      "Iteration 34177: loss = 0.0023318006642419104\n",
      "Iteration 34178: loss = 0.0023317913760501625\n",
      "Iteration 34179: loss = 0.0023317820879575238\n",
      "Iteration 34180: loss = 0.002331772799963992\n",
      "Iteration 34181: loss = 0.002331763512069567\n",
      "Iteration 34182: loss = 0.002331754224274247\n",
      "Iteration 34183: loss = 0.0023317449365780306\n",
      "Iteration 34184: loss = 0.002331735648980917\n",
      "Iteration 34185: loss = 0.002331726361482904\n",
      "Iteration 34186: loss = 0.002331717074083991\n",
      "Iteration 34187: loss = 0.0023317077867841783\n",
      "Iteration 34188: loss = 0.0023316984995834624\n",
      "Iteration 34189: loss = 0.002331689212481842\n",
      "Iteration 34190: loss = 0.002331679925479318\n",
      "Iteration 34191: loss = 0.0023316706385758863\n",
      "Iteration 34192: loss = 0.0023316613517715486\n",
      "Iteration 34193: loss = 0.002331652065066301\n",
      "Iteration 34194: loss = 0.0023316427784601445\n",
      "Iteration 34195: loss = 0.002331633491953077\n",
      "Iteration 34196: loss = 0.002331624205545096\n",
      "Iteration 34197: loss = 0.002331614919236202\n",
      "Iteration 34198: loss = 0.002331605633026393\n",
      "Iteration 34199: loss = 0.002331596346915668\n",
      "Iteration 34200: loss = 0.0023315870609040257\n",
      "Iteration 34201: loss = 0.0023315777749914647\n",
      "Iteration 34202: loss = 0.002331568489177984\n",
      "Iteration 34203: loss = 0.002331559203463582\n",
      "Iteration 34204: loss = 0.002331549917848258\n",
      "Iteration 34205: loss = 0.0023315406323320097\n",
      "Iteration 34206: loss = 0.0023315313469148367\n",
      "Iteration 34207: loss = 0.0023315220615967385\n",
      "Iteration 34208: loss = 0.0023315127763777125\n",
      "Iteration 34209: loss = 0.0023315034912577575\n",
      "Iteration 34210: loss = 0.0023314942062368734\n",
      "Iteration 34211: loss = 0.0023314849213150572\n",
      "Iteration 34212: loss = 0.0023314756364923093\n",
      "Iteration 34213: loss = 0.0023314663517686285\n",
      "Iteration 34214: loss = 0.002331457067144012\n",
      "Iteration 34215: loss = 0.00233144778261846\n",
      "Iteration 34216: loss = 0.0023314384981919702\n",
      "Iteration 34217: loss = 0.0023314292138645423\n",
      "Iteration 34218: loss = 0.002331419929636175\n",
      "Iteration 34219: loss = 0.0023314106455068653\n",
      "Iteration 34220: loss = 0.002331401361476614\n",
      "Iteration 34221: loss = 0.0023313920775454196\n",
      "Iteration 34222: loss = 0.0023313827937132808\n",
      "Iteration 34223: loss = 0.002331373509980195\n",
      "Iteration 34224: loss = 0.002331364226346163\n",
      "Iteration 34225: loss = 0.0023313549428111823\n",
      "Iteration 34226: loss = 0.0023313456593752513\n",
      "Iteration 34227: loss = 0.00233133637603837\n",
      "Iteration 34228: loss = 0.0023313270928005357\n",
      "Iteration 34229: loss = 0.002331317809661749\n",
      "Iteration 34230: loss = 0.0023313085266220067\n",
      "Iteration 34231: loss = 0.0023312992436813093\n",
      "Iteration 34232: loss = 0.0023312899608396533\n",
      "Iteration 34233: loss = 0.00233128067809704\n",
      "Iteration 34234: loss = 0.0023312713954534666\n",
      "Iteration 34235: loss = 0.002331262112908933\n",
      "Iteration 34236: loss = 0.002331252830463436\n",
      "Iteration 34237: loss = 0.0023312435481169765\n",
      "Iteration 34238: loss = 0.0023312342658695522\n",
      "Iteration 34239: loss = 0.0023312249837211625\n",
      "Iteration 34240: loss = 0.0023312157016718046\n",
      "Iteration 34241: loss = 0.0023312064197214795\n",
      "Iteration 34242: loss = 0.0023311971378701837\n",
      "Iteration 34243: loss = 0.002331187856117917\n",
      "Iteration 34244: loss = 0.0023311785744646795\n",
      "Iteration 34245: loss = 0.0023311692929104673\n",
      "Iteration 34246: loss = 0.0023311600114552813\n",
      "Iteration 34247: loss = 0.0023311507300991193\n",
      "Iteration 34248: loss = 0.00233114144884198\n",
      "Iteration 34249: loss = 0.0023311321676838625\n",
      "Iteration 34250: loss = 0.002331122886624765\n",
      "Iteration 34251: loss = 0.0023311136056646873\n",
      "Iteration 34252: loss = 0.0023311043248036276\n",
      "Iteration 34253: loss = 0.0023310950440415842\n",
      "Iteration 34254: loss = 0.0023310857633785566\n",
      "Iteration 34255: loss = 0.002331076482814543\n",
      "Iteration 34256: loss = 0.002331067202349542\n",
      "Iteration 34257: loss = 0.0023310579219835534\n",
      "Iteration 34258: loss = 0.002331048641716575\n",
      "Iteration 34259: loss = 0.0023310393615486058\n",
      "Iteration 34260: loss = 0.002331030081479644\n",
      "Iteration 34261: loss = 0.0023310208015096903\n",
      "Iteration 34262: loss = 0.0023310115216387418\n",
      "Iteration 34263: loss = 0.0023310022418667965\n",
      "Iteration 34264: loss = 0.0023309929621938546\n",
      "Iteration 34265: loss = 0.002330983682619915\n",
      "Iteration 34266: loss = 0.0023309744031449757\n",
      "Iteration 34267: loss = 0.0023309651237690358\n",
      "Iteration 34268: loss = 0.002330955844492094\n",
      "Iteration 34269: loss = 0.0023309465653141483\n",
      "Iteration 34270: loss = 0.002330937286235199\n",
      "Iteration 34271: loss = 0.002330928007255244\n",
      "Iteration 34272: loss = 0.0023309187283742814\n",
      "Iteration 34273: loss = 0.002330909449592311\n",
      "Iteration 34274: loss = 0.002330900170909331\n",
      "Iteration 34275: loss = 0.0023308908923253403\n",
      "Iteration 34276: loss = 0.002330881613840338\n",
      "Iteration 34277: loss = 0.0023308723354543226\n",
      "Iteration 34278: loss = 0.002330863057167293\n",
      "Iteration 34279: loss = 0.002330853778979247\n",
      "Iteration 34280: loss = 0.0023308445008901842\n",
      "Iteration 34281: loss = 0.0023308352229001036\n",
      "Iteration 34282: loss = 0.0023308259450090037\n",
      "Iteration 34283: loss = 0.0023308166672168836\n",
      "Iteration 34284: loss = 0.0023308073895237412\n",
      "Iteration 34285: loss = 0.0023307981119295752\n",
      "Iteration 34286: loss = 0.0023307888344343856\n",
      "Iteration 34287: loss = 0.00233077955703817\n",
      "Iteration 34288: loss = 0.002330770279740928\n",
      "Iteration 34289: loss = 0.0023307610025426577\n",
      "Iteration 34290: loss = 0.0023307517254433576\n",
      "Iteration 34291: loss = 0.0023307424484430277\n",
      "Iteration 34292: loss = 0.002330733171541666\n",
      "Iteration 34293: loss = 0.0023307238947392703\n",
      "Iteration 34294: loss = 0.0023307146180358415\n",
      "Iteration 34295: loss = 0.002330705341431377\n",
      "Iteration 34296: loss = 0.002330696064925875\n",
      "Iteration 34297: loss = 0.0023306867885193355\n",
      "Iteration 34298: loss = 0.0023306775122117567\n",
      "Iteration 34299: loss = 0.0023306682360031373\n",
      "Iteration 34300: loss = 0.002330658959893476\n",
      "Iteration 34301: loss = 0.002330649683882772\n",
      "Iteration 34302: loss = 0.0023306404079710243\n",
      "Iteration 34303: loss = 0.00233063113215823\n",
      "Iteration 34304: loss = 0.0023306218564443895\n",
      "Iteration 34305: loss = 0.0023306125808295014\n",
      "Iteration 34306: loss = 0.002330603305313564\n",
      "Iteration 34307: loss = 0.0023305940298965757\n",
      "Iteration 34308: loss = 0.0023305847545785356\n",
      "Iteration 34309: loss = 0.002330575479359443\n",
      "Iteration 34310: loss = 0.002330566204239296\n",
      "Iteration 34311: loss = 0.0023305569292180937\n",
      "Iteration 34312: loss = 0.002330547654295835\n",
      "Iteration 34313: loss = 0.0023305383794725186\n",
      "Iteration 34314: loss = 0.002330529104748142\n",
      "Iteration 34315: loss = 0.002330519830122706\n",
      "Iteration 34316: loss = 0.002330510555596208\n",
      "Iteration 34317: loss = 0.0023305012811686465\n",
      "Iteration 34318: loss = 0.002330492006840022\n",
      "Iteration 34319: loss = 0.002330482732610332\n",
      "Iteration 34320: loss = 0.0023304734584795746\n",
      "Iteration 34321: loss = 0.00233046418444775\n",
      "Iteration 34322: loss = 0.002330454910514856\n",
      "Iteration 34323: loss = 0.002330445636680892\n",
      "Iteration 34324: loss = 0.002330436362945856\n",
      "Iteration 34325: loss = 0.002330427089309747\n",
      "Iteration 34326: loss = 0.0023304178157725645\n",
      "Iteration 34327: loss = 0.0023304085423343068\n",
      "Iteration 34328: loss = 0.002330399268994972\n",
      "Iteration 34329: loss = 0.0023303899957545604\n",
      "Iteration 34330: loss = 0.002330380722613069\n",
      "Iteration 34331: loss = 0.002330371449570497\n",
      "Iteration 34332: loss = 0.0023303621766268437\n",
      "Iteration 34333: loss = 0.002330352903782108\n",
      "Iteration 34334: loss = 0.0023303436310362882\n",
      "Iteration 34335: loss = 0.002330334358389383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34336: loss = 0.002330325085841392\n",
      "Iteration 34337: loss = 0.002330315813392312\n",
      "Iteration 34338: loss = 0.0023303065410421437\n",
      "Iteration 34339: loss = 0.0023302972687908857\n",
      "Iteration 34340: loss = 0.0023302879966385355\n",
      "Iteration 34341: loss = 0.002330278724585093\n",
      "Iteration 34342: loss = 0.0023302694526305566\n",
      "Iteration 34343: loss = 0.0023302601807749248\n",
      "Iteration 34344: loss = 0.0023302509090181965\n",
      "Iteration 34345: loss = 0.0023302416373603705\n",
      "Iteration 34346: loss = 0.002330232365801446\n",
      "Iteration 34347: loss = 0.002330223094341421\n",
      "Iteration 34348: loss = 0.002330213822980295\n",
      "Iteration 34349: loss = 0.002330204551718066\n",
      "Iteration 34350: loss = 0.002330195280554733\n",
      "Iteration 34351: loss = 0.002330186009490295\n",
      "Iteration 34352: loss = 0.002330176738524751\n",
      "Iteration 34353: loss = 0.0023301674676580986\n",
      "Iteration 34354: loss = 0.002330158196890338\n",
      "Iteration 34355: loss = 0.002330148926221467\n",
      "Iteration 34356: loss = 0.002330139655651485\n",
      "Iteration 34357: loss = 0.00233013038518039\n",
      "Iteration 34358: loss = 0.0023301211148081817\n",
      "Iteration 34359: loss = 0.0023301118445348584\n",
      "Iteration 34360: loss = 0.0023301025743604183\n",
      "Iteration 34361: loss = 0.0023300933042848604\n",
      "Iteration 34362: loss = 0.0023300840343081844\n",
      "Iteration 34363: loss = 0.002330074764430388\n",
      "Iteration 34364: loss = 0.0023300654946514705\n",
      "Iteration 34365: loss = 0.0023300562249714296\n",
      "Iteration 34366: loss = 0.002330046955390266\n",
      "Iteration 34367: loss = 0.002330037685907977\n",
      "Iteration 34368: loss = 0.0023300284165245623\n",
      "Iteration 34369: loss = 0.00233001914724002\n",
      "Iteration 34370: loss = 0.002330009878054348\n",
      "Iteration 34371: loss = 0.0023300006089675474\n",
      "Iteration 34372: loss = 0.0023299913399796148\n",
      "Iteration 34373: loss = 0.0023299820710905497\n",
      "Iteration 34374: loss = 0.002329972802300351\n",
      "Iteration 34375: loss = 0.0023299635336090176\n",
      "Iteration 34376: loss = 0.0023299542650165477\n",
      "Iteration 34377: loss = 0.0023299449965229406\n",
      "Iteration 34378: loss = 0.0023299357281281944\n",
      "Iteration 34379: loss = 0.0023299264598323085\n",
      "Iteration 34380: loss = 0.0023299171916352822\n",
      "Iteration 34381: loss = 0.002329907923537113\n",
      "Iteration 34382: loss = 0.0023298986555377993\n",
      "Iteration 34383: loss = 0.002329889387637342\n",
      "Iteration 34384: loss = 0.002329880119835738\n",
      "Iteration 34385: loss = 0.002329870852132987\n",
      "Iteration 34386: loss = 0.002329861584529087\n",
      "Iteration 34387: loss = 0.002329852317024037\n",
      "Iteration 34388: loss = 0.002329843049617836\n",
      "Iteration 34389: loss = 0.002329833782310483\n",
      "Iteration 34390: loss = 0.0023298245151019766\n",
      "Iteration 34391: loss = 0.0023298152479923145\n",
      "Iteration 34392: loss = 0.0023298059809814973\n",
      "Iteration 34393: loss = 0.0023297967140695226\n",
      "Iteration 34394: loss = 0.002329787447256389\n",
      "Iteration 34395: loss = 0.002329778180542096\n",
      "Iteration 34396: loss = 0.0023297689139266417\n",
      "Iteration 34397: loss = 0.0023297596474100256\n",
      "Iteration 34398: loss = 0.0023297503809922454\n",
      "Iteration 34399: loss = 0.0023297411146733006\n",
      "Iteration 34400: loss = 0.00232973184845319\n",
      "Iteration 34401: loss = 0.0023297225823319116\n",
      "Iteration 34402: loss = 0.0023297133163094658\n",
      "Iteration 34403: loss = 0.0023297040503858497\n",
      "Iteration 34404: loss = 0.0023296947845610626\n",
      "Iteration 34405: loss = 0.002329685518835103\n",
      "Iteration 34406: loss = 0.0023296762532079704\n",
      "Iteration 34407: loss = 0.002329666987679663\n",
      "Iteration 34408: loss = 0.0023296577222501804\n",
      "Iteration 34409: loss = 0.0023296484569195193\n",
      "Iteration 34410: loss = 0.00232963919168768\n",
      "Iteration 34411: loss = 0.0023296299265546617\n",
      "Iteration 34412: loss = 0.0023296206615204622\n",
      "Iteration 34413: loss = 0.002329611396585081\n",
      "Iteration 34414: loss = 0.002329602131748516\n",
      "Iteration 34415: loss = 0.0023295928670107667\n",
      "Iteration 34416: loss = 0.002329583602371831\n",
      "Iteration 34417: loss = 0.002329574337831709\n",
      "Iteration 34418: loss = 0.002329565073390398\n",
      "Iteration 34419: loss = 0.0023295558090478973\n",
      "Iteration 34420: loss = 0.0023295465448042066\n",
      "Iteration 34421: loss = 0.002329537280659323\n",
      "Iteration 34422: loss = 0.0023295280166132463\n",
      "Iteration 34423: loss = 0.0023295187526659756\n",
      "Iteration 34424: loss = 0.002329509488817508\n",
      "Iteration 34425: loss = 0.0023295002250678445\n",
      "Iteration 34426: loss = 0.002329490961416982\n",
      "Iteration 34427: loss = 0.0023294816978649204\n",
      "Iteration 34428: loss = 0.0023294724344116577\n",
      "Iteration 34429: loss = 0.002329463171057194\n",
      "Iteration 34430: loss = 0.0023294539078015262\n",
      "Iteration 34431: loss = 0.002329444644644654\n",
      "Iteration 34432: loss = 0.002329435381586576\n",
      "Iteration 34433: loss = 0.0023294261186272915\n",
      "Iteration 34434: loss = 0.002329416855766798\n",
      "Iteration 34435: loss = 0.0023294075930050955\n",
      "Iteration 34436: loss = 0.0023293983303421823\n",
      "Iteration 34437: loss = 0.0023293890677780573\n",
      "Iteration 34438: loss = 0.0023293798053127188\n",
      "Iteration 34439: loss = 0.002329370542946166\n",
      "Iteration 34440: loss = 0.0023293612806783983\n",
      "Iteration 34441: loss = 0.0023293520185094126\n",
      "Iteration 34442: loss = 0.002329342756439209\n",
      "Iteration 34443: loss = 0.002329333494467786\n",
      "Iteration 34444: loss = 0.002329324232595143\n",
      "Iteration 34445: loss = 0.002329314970821278\n",
      "Iteration 34446: loss = 0.0023293057091461895\n",
      "Iteration 34447: loss = 0.002329296447569877\n",
      "Iteration 34448: loss = 0.0023292871860923384\n",
      "Iteration 34449: loss = 0.0023292779247135738\n",
      "Iteration 34450: loss = 0.0023292686634335804\n",
      "Iteration 34451: loss = 0.002329259402252358\n",
      "Iteration 34452: loss = 0.002329250141169905\n",
      "Iteration 34453: loss = 0.00232924088018622\n",
      "Iteration 34454: loss = 0.002329231619301303\n",
      "Iteration 34455: loss = 0.002329222358515151\n",
      "Iteration 34456: loss = 0.002329213097827763\n",
      "Iteration 34457: loss = 0.0023292038372391384\n",
      "Iteration 34458: loss = 0.0023291945767492765\n",
      "Iteration 34459: loss = 0.002329185316358175\n",
      "Iteration 34460: loss = 0.0023291760560658326\n",
      "Iteration 34461: loss = 0.002329166795872249\n",
      "Iteration 34462: loss = 0.0023291575357774217\n",
      "Iteration 34463: loss = 0.0023291482757813514\n",
      "Iteration 34464: loss = 0.0023291390158840354\n",
      "Iteration 34465: loss = 0.0023291297560854725\n",
      "Iteration 34466: loss = 0.0023291204963856614\n",
      "Iteration 34467: loss = 0.0023291112367846015\n",
      "Iteration 34468: loss = 0.0023291019772822908\n",
      "Iteration 34469: loss = 0.0023290927178787283\n",
      "Iteration 34470: loss = 0.0023290834585739137\n",
      "Iteration 34471: loss = 0.0023290741993678443\n",
      "Iteration 34472: loss = 0.00232906494026052\n",
      "Iteration 34473: loss = 0.002329055681251939\n",
      "Iteration 34474: loss = 0.0023290464223421\n",
      "Iteration 34475: loss = 0.0023290371635310016\n",
      "Iteration 34476: loss = 0.002329027904818643\n",
      "Iteration 34477: loss = 0.002329018646205023\n",
      "Iteration 34478: loss = 0.0023290093876901408\n",
      "Iteration 34479: loss = 0.0023290001292739938\n",
      "Iteration 34480: loss = 0.002328990870956582\n",
      "Iteration 34481: loss = 0.0023289816127379033\n",
      "Iteration 34482: loss = 0.002328972354617957\n",
      "Iteration 34483: loss = 0.0023289630965967413\n",
      "Iteration 34484: loss = 0.0023289538386742554\n",
      "Iteration 34485: loss = 0.002328944580850498\n",
      "Iteration 34486: loss = 0.0023289353231254685\n",
      "Iteration 34487: loss = 0.002328926065499165\n",
      "Iteration 34488: loss = 0.002328916807971586\n",
      "Iteration 34489: loss = 0.00232890755054273\n",
      "Iteration 34490: loss = 0.0023288982932125964\n",
      "Iteration 34491: loss = 0.002328889035981185\n",
      "Iteration 34492: loss = 0.002328879778848492\n",
      "Iteration 34493: loss = 0.002328870521814518\n",
      "Iteration 34494: loss = 0.0023288612648792615\n",
      "Iteration 34495: loss = 0.002328852008042721\n",
      "Iteration 34496: loss = 0.002328842751304896\n",
      "Iteration 34497: loss = 0.002328833494665784\n",
      "Iteration 34498: loss = 0.0023288242381253846\n",
      "Iteration 34499: loss = 0.0023288149816836964\n",
      "Iteration 34500: loss = 0.0023288057253407174\n",
      "Iteration 34501: loss = 0.002328796469096447\n",
      "Iteration 34502: loss = 0.002328787212950885\n",
      "Iteration 34503: loss = 0.002328777956904029\n",
      "Iteration 34504: loss = 0.0023287687009558773\n",
      "Iteration 34505: loss = 0.00232875944510643\n",
      "Iteration 34506: loss = 0.0023287501893556844\n",
      "Iteration 34507: loss = 0.0023287409337036406\n",
      "Iteration 34508: loss = 0.0023287316781502965\n",
      "Iteration 34509: loss = 0.0023287224226956516\n",
      "Iteration 34510: loss = 0.0023287131673397038\n",
      "Iteration 34511: loss = 0.002328703912082452\n",
      "Iteration 34512: loss = 0.0023286946569238956\n",
      "Iteration 34513: loss = 0.002328685401864033\n",
      "Iteration 34514: loss = 0.0023286761469028623\n",
      "Iteration 34515: loss = 0.002328666892040383\n",
      "Iteration 34516: loss = 0.0023286576372765944\n",
      "Iteration 34517: loss = 0.002328648382611494\n",
      "Iteration 34518: loss = 0.0023286391280450815\n",
      "Iteration 34519: loss = 0.0023286298735773554\n",
      "Iteration 34520: loss = 0.0023286206192083144\n",
      "Iteration 34521: loss = 0.002328611364937957\n",
      "Iteration 34522: loss = 0.0023286021107662824\n",
      "Iteration 34523: loss = 0.002328592856693289\n",
      "Iteration 34524: loss = 0.002328583602718976\n",
      "Iteration 34525: loss = 0.0023285743488433417\n",
      "Iteration 34526: loss = 0.0023285650950663848\n",
      "Iteration 34527: loss = 0.002328555841388105\n",
      "Iteration 34528: loss = 0.0023285465878084997\n",
      "Iteration 34529: loss = 0.0023285373343275685\n",
      "Iteration 34530: loss = 0.0023285280809453098\n",
      "Iteration 34531: loss = 0.002328518827661723\n",
      "Iteration 34532: loss = 0.002328509574476806\n",
      "Iteration 34533: loss = 0.002328500321390558\n",
      "Iteration 34534: loss = 0.002328491068402978\n",
      "Iteration 34535: loss = 0.002328481815514064\n",
      "Iteration 34536: loss = 0.0023284725627238157\n",
      "Iteration 34537: loss = 0.002328463310032231\n",
      "Iteration 34538: loss = 0.0023284540574393096\n",
      "Iteration 34539: loss = 0.0023284448049450492\n",
      "Iteration 34540: loss = 0.0023284355525494496\n",
      "Iteration 34541: loss = 0.002328426300252509\n",
      "Iteration 34542: loss = 0.0023284170480542257\n",
      "Iteration 34543: loss = 0.002328407795954599\n",
      "Iteration 34544: loss = 0.002328398543953628\n",
      "Iteration 34545: loss = 0.002328389292051311\n",
      "Iteration 34546: loss = 0.0023283800402476465\n",
      "Iteration 34547: loss = 0.002328370788542634\n",
      "Iteration 34548: loss = 0.002328361536936271\n",
      "Iteration 34549: loss = 0.002328352285428559\n",
      "Iteration 34550: loss = 0.002328343034019493\n",
      "Iteration 34551: loss = 0.0023283337827090743\n",
      "Iteration 34552: loss = 0.002328324531497301\n",
      "Iteration 34553: loss = 0.0023283152803841725\n",
      "Iteration 34554: loss = 0.002328306029369686\n",
      "Iteration 34555: loss = 0.002328296778453842\n",
      "Iteration 34556: loss = 0.002328287527636637\n",
      "Iteration 34557: loss = 0.0023282782769180727\n",
      "Iteration 34558: loss = 0.002328269026298146\n",
      "Iteration 34559: loss = 0.002328259775776856\n",
      "Iteration 34560: loss = 0.002328250525354201\n",
      "Iteration 34561: loss = 0.002328241275030181\n",
      "Iteration 34562: loss = 0.002328232024804794\n",
      "Iteration 34563: loss = 0.002328222774678038\n",
      "Iteration 34564: loss = 0.002328213524649913\n",
      "Iteration 34565: loss = 0.0023282042747204174\n",
      "Iteration 34566: loss = 0.0023281950248895487\n",
      "Iteration 34567: loss = 0.0023281857751573085\n",
      "Iteration 34568: loss = 0.002328176525523692\n",
      "Iteration 34569: loss = 0.002328167275988702\n",
      "Iteration 34570: loss = 0.002328158026552334\n",
      "Iteration 34571: loss = 0.0023281487772145873\n",
      "Iteration 34572: loss = 0.002328139527975462\n",
      "Iteration 34573: loss = 0.0023281302788349554\n",
      "Iteration 34574: loss = 0.0023281210297930676\n",
      "Iteration 34575: loss = 0.002328111780849796\n",
      "Iteration 34576: loss = 0.002328102532005141\n",
      "Iteration 34577: loss = 0.0023280932832590996\n",
      "Iteration 34578: loss = 0.002328084034611672\n",
      "Iteration 34579: loss = 0.0023280747860628557\n",
      "Iteration 34580: loss = 0.0023280655376126504\n",
      "Iteration 34581: loss = 0.0023280562892610546\n",
      "Iteration 34582: loss = 0.002328047041008067\n",
      "Iteration 34583: loss = 0.002328037792853686\n",
      "Iteration 34584: loss = 0.0023280285447979114\n",
      "Iteration 34585: loss = 0.002328019296840741\n",
      "Iteration 34586: loss = 0.0023280100489821735\n",
      "Iteration 34587: loss = 0.002328000801222208\n",
      "Iteration 34588: loss = 0.0023279915535608436\n",
      "Iteration 34589: loss = 0.002327982305998079\n",
      "Iteration 34590: loss = 0.0023279730585339126\n",
      "Iteration 34591: loss = 0.0023279638111683427\n",
      "Iteration 34592: loss = 0.002327954563901369\n",
      "Iteration 34593: loss = 0.0023279453167329898\n",
      "Iteration 34594: loss = 0.002327936069663204\n",
      "Iteration 34595: loss = 0.002327926822692011\n",
      "Iteration 34596: loss = 0.0023279175758194074\n",
      "Iteration 34597: loss = 0.0023279083290453943\n",
      "Iteration 34598: loss = 0.00232789908236997\n",
      "Iteration 34599: loss = 0.0023278898357931323\n",
      "Iteration 34600: loss = 0.00232788058931488\n",
      "Iteration 34601: loss = 0.002327871342935213\n",
      "Iteration 34602: loss = 0.0023278620966541287\n",
      "Iteration 34603: loss = 0.0023278528504716276\n",
      "Iteration 34604: loss = 0.0023278436043877074\n",
      "Iteration 34605: loss = 0.002327834358402366\n",
      "Iteration 34606: loss = 0.0023278251125156036\n",
      "Iteration 34607: loss = 0.002327815866727418\n",
      "Iteration 34608: loss = 0.0023278066210378093\n",
      "Iteration 34609: loss = 0.0023277973754467747\n",
      "Iteration 34610: loss = 0.0023277881299543137\n",
      "Iteration 34611: loss = 0.0023277788845604244\n",
      "Iteration 34612: loss = 0.002327769639265107\n",
      "Iteration 34613: loss = 0.00232776039406836\n",
      "Iteration 34614: loss = 0.0023277511489701802\n",
      "Iteration 34615: loss = 0.0023277419039705676\n",
      "Iteration 34616: loss = 0.002327732659069522\n",
      "Iteration 34617: loss = 0.002327723414267041\n",
      "Iteration 34618: loss = 0.0023277141695631233\n",
      "Iteration 34619: loss = 0.0023277049249577687\n",
      "Iteration 34620: loss = 0.0023276956804509745\n",
      "Iteration 34621: loss = 0.0023276864360427404\n",
      "Iteration 34622: loss = 0.0023276771917330645\n",
      "Iteration 34623: loss = 0.0023276679475219466\n",
      "Iteration 34624: loss = 0.0023276587034093844\n",
      "Iteration 34625: loss = 0.0023276494593953774\n",
      "Iteration 34626: loss = 0.002327640215479924\n",
      "Iteration 34627: loss = 0.0023276309716630233\n",
      "Iteration 34628: loss = 0.0023276217279446735\n",
      "Iteration 34629: loss = 0.002327612484324874\n",
      "Iteration 34630: loss = 0.002327603240803623\n",
      "Iteration 34631: loss = 0.0023275939973809194\n",
      "Iteration 34632: loss = 0.002327584754056762\n",
      "Iteration 34633: loss = 0.00232757551083115\n",
      "Iteration 34634: loss = 0.0023275662677040812\n",
      "Iteration 34635: loss = 0.0023275570246755555\n",
      "Iteration 34636: loss = 0.0023275477817455707\n",
      "Iteration 34637: loss = 0.002327538538914126\n",
      "Iteration 34638: loss = 0.0023275292961812207\n",
      "Iteration 34639: loss = 0.0023275200535468523\n",
      "Iteration 34640: loss = 0.0023275108110110206\n",
      "Iteration 34641: loss = 0.0023275015685737237\n",
      "Iteration 34642: loss = 0.002327492326234961\n",
      "Iteration 34643: loss = 0.002327483083994731\n",
      "Iteration 34644: loss = 0.0023274738418530318\n",
      "Iteration 34645: loss = 0.0023274645998098632\n",
      "Iteration 34646: loss = 0.002327455357865223\n",
      "Iteration 34647: loss = 0.002327446116019111\n",
      "Iteration 34648: loss = 0.0023274368742715254\n",
      "Iteration 34649: loss = 0.0023274276326224644\n",
      "Iteration 34650: loss = 0.0023274183910719282\n",
      "Iteration 34651: loss = 0.002327409149619914\n",
      "Iteration 34652: loss = 0.002327399908266422\n",
      "Iteration 34653: loss = 0.0023273906670114495\n",
      "Iteration 34654: loss = 0.0023273814258549966\n",
      "Iteration 34655: loss = 0.0023273721847970616\n",
      "Iteration 34656: loss = 0.002327362943837643\n",
      "Iteration 34657: loss = 0.0023273537029767388\n",
      "Iteration 34658: loss = 0.002327344462214349\n",
      "Iteration 34659: loss = 0.0023273352215504727\n",
      "Iteration 34660: loss = 0.0023273259809851077\n",
      "Iteration 34661: loss = 0.002327316740518253\n",
      "Iteration 34662: loss = 0.002327307500149907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34663: loss = 0.002327298259880069\n",
      "Iteration 34664: loss = 0.002327289019708738\n",
      "Iteration 34665: loss = 0.002327279779635912\n",
      "Iteration 34666: loss = 0.0023272705396615904\n",
      "Iteration 34667: loss = 0.002327261299785771\n",
      "Iteration 34668: loss = 0.0023272520600084536\n",
      "Iteration 34669: loss = 0.0023272428203296373\n",
      "Iteration 34670: loss = 0.0023272335807493195\n",
      "Iteration 34671: loss = 0.0023272243412675\n",
      "Iteration 34672: loss = 0.002327215101884177\n",
      "Iteration 34673: loss = 0.0023272058625993497\n",
      "Iteration 34674: loss = 0.002327196623413016\n",
      "Iteration 34675: loss = 0.002327187384325176\n",
      "Iteration 34676: loss = 0.0023271781453358276\n",
      "Iteration 34677: loss = 0.0023271689064449695\n",
      "Iteration 34678: loss = 0.0023271596676526004\n",
      "Iteration 34679: loss = 0.00232715042895872\n",
      "Iteration 34680: loss = 0.0023271411903633262\n",
      "Iteration 34681: loss = 0.002327131951866418\n",
      "Iteration 34682: loss = 0.002327122713467994\n",
      "Iteration 34683: loss = 0.002327113475168053\n",
      "Iteration 34684: loss = 0.0023271042369665943\n",
      "Iteration 34685: loss = 0.0023270949988636156\n",
      "Iteration 34686: loss = 0.0023270857608591166\n",
      "Iteration 34687: loss = 0.0023270765229530953\n",
      "Iteration 34688: loss = 0.0023270672851455508\n",
      "Iteration 34689: loss = 0.002327058047436483\n",
      "Iteration 34690: loss = 0.0023270488098258886\n",
      "Iteration 34691: loss = 0.002327039572313768\n",
      "Iteration 34692: loss = 0.0023270303349001193\n",
      "Iteration 34693: loss = 0.0023270210975849413\n",
      "Iteration 34694: loss = 0.0023270118603682323\n",
      "Iteration 34695: loss = 0.002327002623249991\n",
      "Iteration 34696: loss = 0.0023269933862302182\n",
      "Iteration 34697: loss = 0.0023269841493089105\n",
      "Iteration 34698: loss = 0.0023269749124860675\n",
      "Iteration 34699: loss = 0.002326965675761687\n",
      "Iteration 34700: loss = 0.002326956439135769\n",
      "Iteration 34701: loss = 0.0023269472026083127\n",
      "Iteration 34702: loss = 0.0023269379661793148\n",
      "Iteration 34703: loss = 0.0023269287298487754\n",
      "Iteration 34704: loss = 0.002326919493616693\n",
      "Iteration 34705: loss = 0.0023269102574830667\n",
      "Iteration 34706: loss = 0.0023269010214478953\n",
      "Iteration 34707: loss = 0.002326891785511177\n",
      "Iteration 34708: loss = 0.0023268825496729107\n",
      "Iteration 34709: loss = 0.0023268733139330954\n",
      "Iteration 34710: loss = 0.00232686407829173\n",
      "Iteration 34711: loss = 0.0023268548427488123\n",
      "Iteration 34712: loss = 0.0023268456073043424\n",
      "Iteration 34713: loss = 0.0023268363719583184\n",
      "Iteration 34714: loss = 0.0023268271367107387\n",
      "Iteration 34715: loss = 0.002326817901561603\n",
      "Iteration 34716: loss = 0.0023268086665109095\n",
      "Iteration 34717: loss = 0.0023267994315586565\n",
      "Iteration 34718: loss = 0.0023267901967048434\n",
      "Iteration 34719: loss = 0.0023267809619494694\n",
      "Iteration 34720: loss = 0.0023267717272925322\n",
      "Iteration 34721: loss = 0.0023267624927340307\n",
      "Iteration 34722: loss = 0.0023267532582739647\n",
      "Iteration 34723: loss = 0.002326744023912332\n",
      "Iteration 34724: loss = 0.0023267347896491317\n",
      "Iteration 34725: loss = 0.002326725555484362\n",
      "Iteration 34726: loss = 0.0023267163214180223\n",
      "Iteration 34727: loss = 0.0023267070874501117\n",
      "Iteration 34728: loss = 0.0023266978535806285\n",
      "Iteration 34729: loss = 0.002326688619809571\n",
      "Iteration 34730: loss = 0.0023266793861369383\n",
      "Iteration 34731: loss = 0.0023266701525627297\n",
      "Iteration 34732: loss = 0.0023266609190869436\n",
      "Iteration 34733: loss = 0.0023266516857095784\n",
      "Iteration 34734: loss = 0.0023266424524306327\n",
      "Iteration 34735: loss = 0.0023266332192501066\n",
      "Iteration 34736: loss = 0.0023266239861679974\n",
      "Iteration 34737: loss = 0.0023266147531843047\n",
      "Iteration 34738: loss = 0.002326605520299027\n",
      "Iteration 34739: loss = 0.002326596287512163\n",
      "Iteration 34740: loss = 0.0023265870548237113\n",
      "Iteration 34741: loss = 0.0023265778222336716\n",
      "Iteration 34742: loss = 0.0023265685897420414\n",
      "Iteration 34743: loss = 0.0023265593573488196\n",
      "Iteration 34744: loss = 0.002326550125054006\n",
      "Iteration 34745: loss = 0.002326540892857599\n",
      "Iteration 34746: loss = 0.002326531660759596\n",
      "Iteration 34747: loss = 0.0023265224287599977\n",
      "Iteration 34748: loss = 0.002326513196858802\n",
      "Iteration 34749: loss = 0.002326503965056008\n",
      "Iteration 34750: loss = 0.0023264947333516135\n",
      "Iteration 34751: loss = 0.002326485501745618\n",
      "Iteration 34752: loss = 0.00232647627023802\n",
      "Iteration 34753: loss = 0.002326467038828819\n",
      "Iteration 34754: loss = 0.002326457807518013\n",
      "Iteration 34755: loss = 0.002326448576305601\n",
      "Iteration 34756: loss = 0.0023264393451915815\n",
      "Iteration 34757: loss = 0.0023264301141759534\n",
      "Iteration 34758: loss = 0.002326420883258716\n",
      "Iteration 34759: loss = 0.0023264116524398673\n",
      "Iteration 34760: loss = 0.0023264024217194063\n",
      "Iteration 34761: loss = 0.0023263931910973324\n",
      "Iteration 34762: loss = 0.002326383960573643\n",
      "Iteration 34763: loss = 0.002326374730148338\n",
      "Iteration 34764: loss = 0.002326365499821416\n",
      "Iteration 34765: loss = 0.0023263562695928757\n",
      "Iteration 34766: loss = 0.0023263470394627155\n",
      "Iteration 34767: loss = 0.0023263378094309345\n",
      "Iteration 34768: loss = 0.0023263285794975315\n",
      "Iteration 34769: loss = 0.0023263193496625045\n",
      "Iteration 34770: loss = 0.0023263101199258534\n",
      "Iteration 34771: loss = 0.0023263008902875766\n",
      "Iteration 34772: loss = 0.002326291660747673\n",
      "Iteration 34773: loss = 0.002326282431306141\n",
      "Iteration 34774: loss = 0.002326273201962979\n",
      "Iteration 34775: loss = 0.0023262639727181862\n",
      "Iteration 34776: loss = 0.0023262547435717617\n",
      "Iteration 34777: loss = 0.002326245514523704\n",
      "Iteration 34778: loss = 0.0023262362855740112\n",
      "Iteration 34779: loss = 0.0023262270567226836\n",
      "Iteration 34780: loss = 0.0023262178279697177\n",
      "Iteration 34781: loss = 0.0023262085993151146\n",
      "Iteration 34782: loss = 0.002326199370758872\n",
      "Iteration 34783: loss = 0.0023261901423009884\n",
      "Iteration 34784: loss = 0.0023261809139414635\n",
      "Iteration 34785: loss = 0.002326171685680295\n",
      "Iteration 34786: loss = 0.0023261624575174823\n",
      "Iteration 34787: loss = 0.0023261532294530244\n",
      "Iteration 34788: loss = 0.0023261440014869187\n",
      "Iteration 34789: loss = 0.002326134773619165\n",
      "Iteration 34790: loss = 0.002326125545849762\n",
      "Iteration 34791: loss = 0.0023261163181787095\n",
      "Iteration 34792: loss = 0.002326107090606004\n",
      "Iteration 34793: loss = 0.002326097863131646\n",
      "Iteration 34794: loss = 0.002326088635755634\n",
      "Iteration 34795: loss = 0.0023260794084779654\n",
      "Iteration 34796: loss = 0.0023260701812986406\n",
      "Iteration 34797: loss = 0.002326060954217658\n",
      "Iteration 34798: loss = 0.002326051727235016\n",
      "Iteration 34799: loss = 0.002326042500350714\n",
      "Iteration 34800: loss = 0.0023260332735647498\n",
      "Iteration 34801: loss = 0.002326024046877123\n",
      "Iteration 34802: loss = 0.0023260148202878315\n",
      "Iteration 34803: loss = 0.002326005593796875\n",
      "Iteration 34804: loss = 0.0023259963674042514\n",
      "Iteration 34805: loss = 0.0023259871411099604\n",
      "Iteration 34806: loss = 0.0023259779149140004\n",
      "Iteration 34807: loss = 0.0023259686888163693\n",
      "Iteration 34808: loss = 0.0023259594628170675\n",
      "Iteration 34809: loss = 0.0023259502369160925\n",
      "Iteration 34810: loss = 0.0023259410111134433\n",
      "Iteration 34811: loss = 0.0023259317854091187\n",
      "Iteration 34812: loss = 0.0023259225598031177\n",
      "Iteration 34813: loss = 0.002325913334295439\n",
      "Iteration 34814: loss = 0.0023259041088860817\n",
      "Iteration 34815: loss = 0.0023258948835750437\n",
      "Iteration 34816: loss = 0.0023258856583623236\n",
      "Iteration 34817: loss = 0.0023258764332479216\n",
      "Iteration 34818: loss = 0.0023258672082318355\n",
      "Iteration 34819: loss = 0.0023258579833140645\n",
      "Iteration 34820: loss = 0.0023258487584946063\n",
      "Iteration 34821: loss = 0.0023258395337734604\n",
      "Iteration 34822: loss = 0.0023258303091506266\n",
      "Iteration 34823: loss = 0.0023258210846261018\n",
      "Iteration 34824: loss = 0.002325811860199886\n",
      "Iteration 34825: loss = 0.0023258026358719775\n",
      "Iteration 34826: loss = 0.002325793411642375\n",
      "Iteration 34827: loss = 0.002325784187511078\n",
      "Iteration 34828: loss = 0.002325774963478084\n",
      "Iteration 34829: loss = 0.0023257657395433923\n",
      "Iteration 34830: loss = 0.0023257565157070027\n",
      "Iteration 34831: loss = 0.002325747291968912\n",
      "Iteration 34832: loss = 0.002325738068329121\n",
      "Iteration 34833: loss = 0.002325728844787627\n",
      "Iteration 34834: loss = 0.0023257196213444294\n",
      "Iteration 34835: loss = 0.0023257103979995265\n",
      "Iteration 34836: loss = 0.0023257011747529178\n",
      "Iteration 34837: loss = 0.002325691951604601\n",
      "Iteration 34838: loss = 0.002325682728554576\n",
      "Iteration 34839: loss = 0.002325673505602841\n",
      "Iteration 34840: loss = 0.0023256642827493954\n",
      "Iteration 34841: loss = 0.0023256550599942364\n",
      "Iteration 34842: loss = 0.0023256458373373647\n",
      "Iteration 34843: loss = 0.002325636614778777\n",
      "Iteration 34844: loss = 0.002325627392318474\n",
      "Iteration 34845: loss = 0.0023256181699564536\n",
      "Iteration 34846: loss = 0.002325608947692715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34847: loss = 0.0023255997255272557\n",
      "Iteration 34848: loss = 0.0023255905034600764\n",
      "Iteration 34849: loss = 0.002325581281491174\n",
      "Iteration 34850: loss = 0.0023255720596205482\n",
      "Iteration 34851: loss = 0.0023255628378481977\n",
      "Iteration 34852: loss = 0.002325553616174121\n",
      "Iteration 34853: loss = 0.0023255443945983173\n",
      "Iteration 34854: loss = 0.0023255351731207856\n",
      "Iteration 34855: loss = 0.0023255259517415234\n",
      "Iteration 34856: loss = 0.0023255167304605307\n",
      "Iteration 34857: loss = 0.002325507509277806\n",
      "Iteration 34858: loss = 0.0023254982881933473\n",
      "Iteration 34859: loss = 0.0023254890672071544\n",
      "Iteration 34860: loss = 0.0023254798463192257\n",
      "Iteration 34861: loss = 0.0023254706255295596\n",
      "Iteration 34862: loss = 0.0023254614048381548\n",
      "Iteration 34863: loss = 0.0023254521842450107\n",
      "Iteration 34864: loss = 0.0023254429637501266\n",
      "Iteration 34865: loss = 0.0023254337433534994\n",
      "Iteration 34866: loss = 0.002325424523055129\n",
      "Iteration 34867: loss = 0.002325415302855014\n",
      "Iteration 34868: loss = 0.002325406082753154\n",
      "Iteration 34869: loss = 0.0023253968627495463\n",
      "Iteration 34870: loss = 0.0023253876428441906\n",
      "Iteration 34871: loss = 0.0023253784230370853\n",
      "Iteration 34872: loss = 0.0023253692033282296\n",
      "Iteration 34873: loss = 0.0023253599837176216\n",
      "Iteration 34874: loss = 0.0023253507642052606\n",
      "Iteration 34875: loss = 0.002325341544791145\n",
      "Iteration 34876: loss = 0.0023253323254752746\n",
      "Iteration 34877: loss = 0.002325323106257646\n",
      "Iteration 34878: loss = 0.0023253138871382603\n",
      "Iteration 34879: loss = 0.002325304668117115\n",
      "Iteration 34880: loss = 0.002325295449194208\n",
      "Iteration 34881: loss = 0.0023252862303695405\n",
      "Iteration 34882: loss = 0.0023252770116431094\n",
      "Iteration 34883: loss = 0.002325267793014914\n",
      "Iteration 34884: loss = 0.0023252585744849533\n",
      "Iteration 34885: loss = 0.0023252493560532257\n",
      "Iteration 34886: loss = 0.00232524013771973\n",
      "Iteration 34887: loss = 0.0023252309194844653\n",
      "Iteration 34888: loss = 0.0023252217013474295\n",
      "Iteration 34889: loss = 0.002325212483308623\n",
      "Iteration 34890: loss = 0.0023252032653680423\n",
      "Iteration 34891: loss = 0.0023251940475256883\n",
      "Iteration 34892: loss = 0.0023251848297815587\n",
      "Iteration 34893: loss = 0.002325175612135652\n",
      "Iteration 34894: loss = 0.002325166394587968\n",
      "Iteration 34895: loss = 0.002325157177138505\n",
      "Iteration 34896: loss = 0.0023251479597872606\n",
      "Iteration 34897: loss = 0.0023251387425342355\n",
      "Iteration 34898: loss = 0.0023251295253794275\n",
      "Iteration 34899: loss = 0.002325120308322835\n",
      "Iteration 34900: loss = 0.002325111091364457\n",
      "Iteration 34901: loss = 0.002325101874504293\n",
      "Iteration 34902: loss = 0.0023250926577423415\n",
      "Iteration 34903: loss = 0.0023250834410786002\n",
      "Iteration 34904: loss = 0.0023250742245130695\n",
      "Iteration 34905: loss = 0.0023250650080457467\n",
      "Iteration 34906: loss = 0.0023250557916766313\n",
      "Iteration 34907: loss = 0.002325046575405722\n",
      "Iteration 34908: loss = 0.0023250373592330173\n",
      "Iteration 34909: loss = 0.0023250281431585165\n",
      "Iteration 34910: loss = 0.002325018927182217\n",
      "Iteration 34911: loss = 0.0023250097113041205\n",
      "Iteration 34912: loss = 0.0023250004955242226\n",
      "Iteration 34913: loss = 0.0023249912798425235\n",
      "Iteration 34914: loss = 0.002324982064259022\n",
      "Iteration 34915: loss = 0.0023249728487737156\n",
      "Iteration 34916: loss = 0.002324963633386606\n",
      "Iteration 34917: loss = 0.0023249544180976886\n",
      "Iteration 34918: loss = 0.0023249452029069644\n",
      "Iteration 34919: loss = 0.0023249359878144307\n",
      "Iteration 34920: loss = 0.0023249267728200876\n",
      "Iteration 34921: loss = 0.0023249175579239333\n",
      "Iteration 34922: loss = 0.002324908343125966\n",
      "Iteration 34923: loss = 0.0023248991284261855\n",
      "Iteration 34924: loss = 0.00232488991382459\n",
      "Iteration 34925: loss = 0.002324880699321178\n",
      "Iteration 34926: loss = 0.002324871484915949\n",
      "Iteration 34927: loss = 0.002324862270608901\n",
      "Iteration 34928: loss = 0.0023248530564000326\n",
      "Iteration 34929: loss = 0.002324843842289344\n",
      "Iteration 34930: loss = 0.002324834628276833\n",
      "Iteration 34931: loss = 0.0023248254143624974\n",
      "Iteration 34932: loss = 0.0023248162005463377\n",
      "Iteration 34933: loss = 0.002324806986828352\n",
      "Iteration 34934: loss = 0.002324797773208539\n",
      "Iteration 34935: loss = 0.002324788559686897\n",
      "Iteration 34936: loss = 0.0023247793462634254\n",
      "Iteration 34937: loss = 0.0023247701329381226\n",
      "Iteration 34938: loss = 0.0023247609197109883\n",
      "Iteration 34939: loss = 0.0023247517065820202\n",
      "Iteration 34940: loss = 0.002324742493551217\n",
      "Iteration 34941: loss = 0.0023247332806185786\n",
      "Iteration 34942: loss = 0.002324724067784102\n",
      "Iteration 34943: loss = 0.0023247148550477876\n",
      "Iteration 34944: loss = 0.002324705642409633\n",
      "Iteration 34945: loss = 0.0023246964298696384\n",
      "Iteration 34946: loss = 0.002324687217427801\n",
      "Iteration 34947: loss = 0.00232467800508412\n",
      "Iteration 34948: loss = 0.0023246687928385953\n",
      "Iteration 34949: loss = 0.0023246595806912245\n",
      "Iteration 34950: loss = 0.002324650368642006\n",
      "Iteration 34951: loss = 0.00232464115669094\n",
      "Iteration 34952: loss = 0.0023246319448380235\n",
      "Iteration 34953: loss = 0.002324622733083257\n",
      "Iteration 34954: loss = 0.002324613521426638\n",
      "Iteration 34955: loss = 0.002324604309868166\n",
      "Iteration 34956: loss = 0.0023245950984078396\n",
      "Iteration 34957: loss = 0.0023245858870456575\n",
      "Iteration 34958: loss = 0.0023245766757816187\n",
      "Iteration 34959: loss = 0.002324567464615721\n",
      "Iteration 34960: loss = 0.0023245582535479647\n",
      "Iteration 34961: loss = 0.0023245490425783466\n",
      "Iteration 34962: loss = 0.002324539831706868\n",
      "Iteration 34963: loss = 0.0023245306209335255\n",
      "Iteration 34964: loss = 0.002324521410258319\n",
      "Iteration 34965: loss = 0.0023245121996812463\n",
      "Iteration 34966: loss = 0.0023245029892023077\n",
      "Iteration 34967: loss = 0.0023244937788215003\n",
      "Iteration 34968: loss = 0.0023244845685388236\n",
      "Iteration 34969: loss = 0.002324475358354277\n",
      "Iteration 34970: loss = 0.002324466148267858\n",
      "Iteration 34971: loss = 0.0023244569382795664\n",
      "Iteration 34972: loss = 0.0023244477283894003\n",
      "Iteration 34973: loss = 0.0023244385185973585\n",
      "Iteration 34974: loss = 0.0023244293089034404\n",
      "Iteration 34975: loss = 0.0023244200993076444\n",
      "Iteration 34976: loss = 0.002324410889809969\n",
      "Iteration 34977: loss = 0.002324401680410413\n",
      "Iteration 34978: loss = 0.0023243924711089763\n",
      "Iteration 34979: loss = 0.0023243832619056558\n",
      "Iteration 34980: loss = 0.0023243740528004517\n",
      "Iteration 34981: loss = 0.0023243648437933618\n",
      "Iteration 34982: loss = 0.0023243556348843857\n",
      "Iteration 34983: loss = 0.002324346426073522\n",
      "Iteration 34984: loss = 0.0023243372173607684\n",
      "Iteration 34985: loss = 0.0023243280087461254\n",
      "Iteration 34986: loss = 0.0023243188002295906\n",
      "Iteration 34987: loss = 0.0023243095918111627\n",
      "Iteration 34988: loss = 0.0023243003834908416\n",
      "Iteration 34989: loss = 0.0023242911752686243\n",
      "Iteration 34990: loss = 0.0023242819671445113\n",
      "Iteration 34991: loss = 0.0023242727591185003\n",
      "Iteration 34992: loss = 0.00232426355119059\n",
      "Iteration 34993: loss = 0.0023242543433607803\n",
      "Iteration 34994: loss = 0.002324245135629069\n",
      "Iteration 34995: loss = 0.002324235927995455\n",
      "Iteration 34996: loss = 0.002324226720459937\n",
      "Iteration 34997: loss = 0.0023242175130225137\n",
      "Iteration 34998: loss = 0.0023242083056831844\n",
      "Iteration 34999: loss = 0.0023241990984419476\n",
      "Iteration 35000: loss = 0.002324189891298802\n",
      "Iteration 35001: loss = 0.002324180684253746\n",
      "Iteration 35002: loss = 0.002324171477306779\n",
      "Iteration 35003: loss = 0.0023241622704579\n",
      "Iteration 35004: loss = 0.002324153063707107\n",
      "Iteration 35005: loss = 0.0023241438570543987\n",
      "Iteration 35006: loss = 0.0023241346504997737\n",
      "Iteration 35007: loss = 0.0023241254440432325\n",
      "Iteration 35008: loss = 0.002324116237684772\n",
      "Iteration 35009: loss = 0.0023241070314243915\n",
      "Iteration 35010: loss = 0.00232409782526209\n",
      "Iteration 35011: loss = 0.0023240886191978664\n",
      "Iteration 35012: loss = 0.0023240794132317185\n",
      "Iteration 35013: loss = 0.0023240702073636463\n",
      "Iteration 35014: loss = 0.002324061001593648\n",
      "Iteration 35015: loss = 0.0023240517959217223\n",
      "Iteration 35016: loss = 0.002324042590347868\n",
      "Iteration 35017: loss = 0.0023240333848720844\n",
      "Iteration 35018: loss = 0.002324024179494369\n",
      "Iteration 35019: loss = 0.0023240149742147216\n",
      "Iteration 35020: loss = 0.002324005769033141\n",
      "Iteration 35021: loss = 0.0023239965639496253\n",
      "Iteration 35022: loss = 0.0023239873589641744\n",
      "Iteration 35023: loss = 0.0023239781540767856\n",
      "Iteration 35024: loss = 0.0023239689492874586\n",
      "Iteration 35025: loss = 0.002323959744596192\n",
      "Iteration 35026: loss = 0.002323950540002984\n",
      "Iteration 35027: loss = 0.0023239413355078346\n",
      "Iteration 35028: loss = 0.002323932131110742\n",
      "Iteration 35029: loss = 0.002323922926811704\n",
      "Iteration 35030: loss = 0.0023239137226107208\n",
      "Iteration 35031: loss = 0.0023239045185077904\n",
      "Iteration 35032: loss = 0.0023238953145029123\n",
      "Iteration 35033: loss = 0.0023238861105960833\n",
      "Iteration 35034: loss = 0.002323876906787305\n",
      "Iteration 35035: loss = 0.002323867703076574\n",
      "Iteration 35036: loss = 0.0023238584994638893\n",
      "Iteration 35037: loss = 0.002323849295949251\n",
      "Iteration 35038: loss = 0.002323840092532657\n",
      "Iteration 35039: loss = 0.002323830889214106\n",
      "Iteration 35040: loss = 0.002323821685993596\n",
      "Iteration 35041: loss = 0.0023238124828711274\n",
      "Iteration 35042: loss = 0.002323803279846698\n",
      "Iteration 35043: loss = 0.002323794076920307\n",
      "Iteration 35044: loss = 0.002323784874091953\n",
      "Iteration 35045: loss = 0.002323775671361634\n",
      "Iteration 35046: loss = 0.00232376646872935\n",
      "Iteration 35047: loss = 0.0023237572661950986\n",
      "Iteration 35048: loss = 0.00232374806375888\n",
      "Iteration 35049: loss = 0.002323738861420692\n",
      "Iteration 35050: loss = 0.002323729659180533\n",
      "Iteration 35051: loss = 0.002323720457038403\n",
      "Iteration 35052: loss = 0.0023237112549942996\n",
      "Iteration 35053: loss = 0.0023237020530482216\n",
      "Iteration 35054: loss = 0.002323692851200169\n",
      "Iteration 35055: loss = 0.002323683649450139\n",
      "Iteration 35056: loss = 0.002323674447798132\n",
      "Iteration 35057: loss = 0.002323665246244145\n",
      "Iteration 35058: loss = 0.002323656044788178\n",
      "Iteration 35059: loss = 0.002323646843430229\n",
      "Iteration 35060: loss = 0.0023236376421702978\n",
      "Iteration 35061: loss = 0.002323628441008382\n",
      "Iteration 35062: loss = 0.0023236192399444812\n",
      "Iteration 35063: loss = 0.002323610038978594\n",
      "Iteration 35064: loss = 0.002323600838110719\n",
      "Iteration 35065: loss = 0.002323591637340855\n",
      "Iteration 35066: loss = 0.0023235824366690003\n",
      "Iteration 35067: loss = 0.002323573236095155\n",
      "Iteration 35068: loss = 0.0023235640356193163\n",
      "Iteration 35069: loss = 0.0023235548352414843\n",
      "Iteration 35070: loss = 0.002323545634961657\n",
      "Iteration 35071: loss = 0.0023235364347798325\n",
      "Iteration 35072: loss = 0.002323527234696011\n",
      "Iteration 35073: loss = 0.0023235180347101906\n",
      "Iteration 35074: loss = 0.0023235088348223703\n",
      "Iteration 35075: loss = 0.0023234996350325484\n",
      "Iteration 35076: loss = 0.002323490435340724\n",
      "Iteration 35077: loss = 0.002323481235746896\n",
      "Iteration 35078: loss = 0.0023234720362510625\n",
      "Iteration 35079: loss = 0.0023234628368532235\n",
      "Iteration 35080: loss = 0.002323453637553377\n",
      "Iteration 35081: loss = 0.0023234444383515213\n",
      "Iteration 35082: loss = 0.002323435239247656\n",
      "Iteration 35083: loss = 0.0023234260402417786\n",
      "Iteration 35084: loss = 0.0023234168413338893\n",
      "Iteration 35085: loss = 0.0023234076425239867\n",
      "Iteration 35086: loss = 0.002323398443812069\n",
      "Iteration 35087: loss = 0.0023233892451981347\n",
      "Iteration 35088: loss = 0.0023233800466821836\n",
      "Iteration 35089: loss = 0.002323370848264214\n",
      "Iteration 35090: loss = 0.0023233616499442247\n",
      "Iteration 35091: loss = 0.0023233524517222138\n",
      "Iteration 35092: loss = 0.002323343253598181\n",
      "Iteration 35093: loss = 0.0023233340555721244\n",
      "Iteration 35094: loss = 0.002323324857644043\n",
      "Iteration 35095: loss = 0.0023233156598139355\n",
      "Iteration 35096: loss = 0.0023233064620818015\n",
      "Iteration 35097: loss = 0.0023232972644476385\n",
      "Iteration 35098: loss = 0.002323288066911446\n",
      "Iteration 35099: loss = 0.002323278869473223\n",
      "Iteration 35100: loss = 0.0023232696721329667\n",
      "Iteration 35101: loss = 0.0023232604748906777\n",
      "Iteration 35102: loss = 0.0023232512777463547\n",
      "Iteration 35103: loss = 0.0023232420806999945\n",
      "Iteration 35104: loss = 0.002323232883751598\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = LogitRegression(0.001, 100000)\n",
    "model.fit(X_train, Y_train)\n",
    "plt.plot(model.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc382ddd",
   "metadata": {},
   "source": [
    "## Step 11 - Make a prediction on test dataset by counting how many correct/incorrect predictions your model makes and print your accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, weight = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lst = []\n",
    "weight_lst = []\n",
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa24b86",
   "metadata": {},
   "source": [
    "## Fitting for different values of learning rate and iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ccd0d",
   "metadata": {},
   "source": [
    "### Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fa85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model1 = LogitRegression(0.0005, 100000)\n",
    "model1.fit(X_train, Y_train)\n",
    "plt.plot(model1.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, weight = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e45417",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "print(weight)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)\n",
    "print(weight_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee559b",
   "metadata": {},
   "source": [
    "### Case 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c89b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model2 = LogitRegression(0.1, 10000)\n",
    "model2.fit(X_train, Y_train)\n",
    "plt.plot(model2.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84007b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, weight = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91989090",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "print(weight)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = max(accuracy_lst)\n",
    "i = accuracy_lst.index(max_accuracy)\n",
    "weight_max = weight_lst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( weight_max, open( \"weight_pickle.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244efb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pickle = pickle.load( open( \"weight_pickle.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Weights for best accuracy {weight_pickle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0db0ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec31439e",
   "metadata": {},
   "source": [
    "Part II: Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccdb8e",
   "metadata": {},
   "source": [
    "In this part, we implement linear regression model and apply this model to solve a\n",
    "problem based on the real-world dataset.\n",
    "Datasets that can be used for this part (provided in the zip folder):\n",
    "‚Ä¢ Flight price prediction dataset\n",
    "‚Ä¢ Breeding Bird Atlas\n",
    "‚Ä¢ Diamond dataset (Note: x, y and z columns refer to length, width, and depth\n",
    "respectively)\n",
    "‚Ä¢ Emissions by Country dataset\n",
    "‚Ä¢ Epicurious ‚Äì Recipes with Rating and nutrition\n",
    "Implement linear regression using the ordinary least squares (OLS) method to perform\n",
    "direct minimization of the squared loss function.\n",
    "ùêΩ(ùíò) =\n",
    "1\n",
    "2\n",
    "‚àë(ùë¶ùëñ ‚àí ùë§\n",
    "ùëáùë•ùëñ\n",
    ")\n",
    "2\n",
    "ùëÅ\n",
    "ùëñ=1\n",
    "In matrix-vector notation, the loss function can be written as:\n",
    "ùêΩ(ùíò) =\n",
    "1\n",
    "2\n",
    "‚àë(ùíö ‚àí ùëøùíò)\n",
    "ùëá\n",
    "(ùíö ‚àí ùëøùíò)\n",
    "ùëÅ\n",
    "ùëñ=1\n",
    "where ùëø is the input data matrix, ùíö is the target vector, and ùíò is the weight vector for\n",
    "regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64750c37",
   "metadata": {},
   "source": [
    "\n",
    "##### Step_1_Select one dataset from the list provided above. The datasets are located in the folder ‚Äúdataset‚Äù, use only the dataset provided in the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"C:/Users/viraj/Code_a/ML_Prof_Alina/Assignment_1/datasets (1)/datasets/diamond.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e62662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
